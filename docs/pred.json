[
  "\"\"\"\nConvert weights from https://github.com/google-research/nested-transformer\nNOTE: You'll need https://github.com/google/CommonLoopUtils, not included in requirements.txt\n\"\"\"\n\nimport sys\n\nimport numpy as np\nimport torch\n\nfrom clu import checkpoint\n\n\narch_depths = {\n    'nest_base': [2, 2, 20],\n    'nest_small': [2, 2, 20],\n    'nest_tiny': [2, 2, 8],\n}\n\n\ndef convert_nest(checkpoint_path, arch):\n    \"\"\"\n    Expects path to checkpoint which is a dir containing 4 files like in each of these folders\n        - https://console.cloud.google.com/storage/browser/gresearch/nest-checkpoints\n    `arch` is needed to \n    Returns a state dict that can be used with `torch.nn.Module.load_state_dict`\n    Hint: Follow timm.models.nest.Nest.__init__ and \n    https://github.com/google-research/nested-transformer/blob/main/models/nest_net.py\n    \"\"\"\n    assert arch in arch_depths, f\"Unknown arch {arch}\"\n    depths = arch_depths[arch]\n\n    # Load checkpoint\n    checkpoint = checkpoint.load(checkpoint_path)\n    state_dict = checkpoint['state_dict']\n\n    # Remove the last layer\n    del state_dict['last_layer.weight']\n    del state_dict['last_layer.bias']\n\n    # Get the last layer's weights\n    last_layer_weights = state_dict['last_layer.weight']\n    last_layer_bias = state_dict['last_layer.bias']\n\n    # Get the first layer",
  "import argparse\nimport hashlib\nimport os\n\nimport mxnet as mx\nimport gluoncv\nimport torch\nfrom timm import create_model\n\nparser = argparse.ArgumentParser(description='Convert from MXNet')\nparser.add_argument('--model', default='all', type=str, metavar='MODEL',\n                    help='Name of model to train (default: \"all\"')\n\n\ndef convert(mxnet_name, torch_name):\n    # download and load the pre-trained model\n    net = gluoncv.model_zoo.get_model(mxnet_name, pretrained=True)\n\n    # create corresponding torch model\n    torch_net = create_model(torch_name)\n\n    mxp = [(k, v) for k, v in net.collect_params().items() if 'running' not in k]\n    torchp = list(torch_net.named_parameters())\n    torch_params = {}\n\n    # convert parameters\n    # NOTE: we are relying on the fact that the order of parameters\n    # are usually exactly the same between these models, thus no key name mapping\n    # is necessary. Asserts will trip if this is not the case.\n    for (tn, tp) in zip(torchp, mxp):\n        if tp[1].shape != tn[1].shape:\n            raise ValueError(f'Shape mismatch between {tn[0]} and {tn[0]}')\n        torch_params[tp[0]] = tp[1]\n\n    # copy weights\n    for (tn, tp) in zip(torchp, mxp):\n        torch_net.load_parameter(tn[0], tp[1])\n\n    # copy biases\n    for (tn, tp) in zip(torchp, mxp):\n        torch_net.load_bias(",
  "from .version import __version__\nfrom .layers import is_scriptable, is_exportable, set_scriptable, set_exportable\nfrom .models import create_model, list_models\nfrom .utils import get_logger\n\nlogger = get_logger(__name__)\n\n__all__ = [\n    \"__version__\",\n    \"is_scriptable\",\n    \"is_exportable\",\n    \"set_scriptable\",\n    \"set_exportable\",\n    \"create_model\",\n    \"list_models\",\n    \"logger\",\n]\n",
  "__version__ = '0.1.0'\n\nfrom . import (\n    utils,\n    models,\n    datasets,\n    transformers,\n    image_processing,\n    image_transforms,\n    image_augmentation,\n    image_processing_tf,\n    image_transforms_tf,\n    image_augmentation_tf,\n    image_processing_tf_v2,\n    image_transforms_tf_v2,\n    image_augmentation_tf_v2,\n    image_processing_v2,\n    image_transforms_v2,\n    image_augmentation_v2,\n    image_processing_v3,\n    image_",
  "\"\"\" EfficientFormer\n\n@article{li2022efficientformer,\n  title={EfficientFormer: Vision Transformers at MobileNet Speed},\n  author={Li, Yanyu and Yuan, Geng and Wen, Yang and Hu, Eric and Evangelidis, Georgios and Tulyakov,\n   Sergey and Wang, Yanzhi and Ren, Jian},\n  journal={arXiv preprint arXiv:2206.01191},\n  year={2022}\n}\n\nBased on Apache 2.0 licensed code at https://github.com/snap-research/EfficientFormer, Copyright (c) 2022 Snap Inc.\n\nModifications and timm support by / Copyright 2022, Ross Wightman\n\"\"\"\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import DropPath, trunc_normal_, to_2tuple, Mlp\nfrom ._builder import build_model_with_cfg\n\n\nclass EfficientFormer(nn.Module):\n    def __init__(self, cfg, num_channels=3):\n        super().__init__()\n        self.cfg = cfg\n        self.num_channels = num_channels\n\n        self.patch_embed = nn.Conv2d(\n            self.cfg.num_channels, self.cfg.patch_size ** 2, kernel_size=self.cfg.patch_size, stride=self.cfg.patch_size\n        )\n        self.pos_embed = nn.Parameter(torch.zeros(1, 1, self.cfg.",
  "\"\"\"VGG\n\nAdapted from https://github.com/pytorch/vision 'vgg.py' (BSD-3-Clause) with a few changes for\ntimm functionality.\n\nCopyright 2021 Ross Wightman\n\"\"\"\nfrom typing import Union, List, Dict, Any, cast\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import ClassifierHead\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_module\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['VGG']\n\n\ncfgs: Dict[str, List[Union[str, int]]] = {\n    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256",
  "\"\"\" DeiT - Data-efficient Image Transformers\n\nDeiT model defs and weights from https://github.com/facebookresearch/deit, original copyright below\n\npaper: `DeiT: Data-efficient Image Transformers` - https://arxiv.org/abs/2012.12877\n\npaper: `DeiT III: Revenge of the ViT` - https://arxiv.org/abs/2204.07118\n\nModifications copyright 2021, Ross Wightman\n\"\"\"\n# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\nfrom functools import partial\nfrom typing import Sequence, Union\n\nimport torch\nfrom torch import nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import resample_abs_pos_embed\nfrom timm.models.vision_transformer import VisionTransformer, trunc_normal_, checkpoint_filter_fn\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom .configuration_deit import DeiTConfig\n\n\ndef deit_preprocess_input(x):\n    \"\"\"\n    Preprocesses the input image for DeiT.\n    \"\"\"\n    # Convert to channel-first\n    x = x.permute(0, 3, 1, 2)\n    # Rescale the input\n    x = x * 255.0\n    # Convert to channel-last\n    x = x.permute(0, 2, 3, 1)\n    return x\n\n\ndef deit_deconv_block(\n    x,\n    dim,\n    num_heads",
  "\"\"\" Res2Net and Res2NeXt\nAdapted from Official Pytorch impl at: https://github.com/gasvn/Res2Net/\nPaper: `Res2Net: A New Multi-scale Backbone Architecture` - https://arxiv.org/abs/1904.01169\n\"\"\"\nimport math\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\nfrom .resnet import ResNet\n\n__all__ = []\n\n\nclass Bottle2neck(nn.Module):\n    \"\"\" Res2Net/Res2NeXT Bottleneck\n    Adapted from https://github.com/gasvn/Res2Net/blob/master/res2net.py\n    \"\"\"\n    expansion = 4\n\n    def __init__(\n            self,\n            inplanes,\n            planes,\n            stride=1,\n            downsample=None,\n            cardinality=1,\n            base_width=26,\n            scale=4,\n            dilation=1,\n            norm_layer=None,\n            **kwargs\n    ):\n        super().__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self.conv1 = nn.Conv2d(\n            inplanes,\n            planes,\n            kernel_size=3,\n            stride=stride,\n            padding=dilation,\n            bias=False,\n            **kwargs\n        )\n        self.bn1 = norm_layer(planes)\n        self.conv2 = nn.Conv2d(\n            planes,\n            planes,\n            kernel_size=3,\n            stride=1,\n            padding",
  "from ._factory import *\n\nimport warnings\nwarnings.warn(f\"Importing from {__name__} is deprecated and will be removed in v0.13.0. Use {__name__}.get_image_classifier instead.\", FutureWarning)\n\nfrom .image_classifier import ImageClassifier\nfrom .image_classifier_fastai import ImageClassifierFastai\nfrom .image_classifier_vision import ImageClassifierVision\nfrom .image_classifier_vision_fastai import ImageClassifierVisionFastai\n\n__all__ = [\"ImageClassifier\", \"ImageClassifierFastai\", \"ImageClassifierVision\", \"ImageClassifierVisionFastai\"]\n",
  "from .beit import *\nfrom .byoanet import *\nfrom .byobnet import *\nfrom .cait import *\nfrom .coat import *\nfrom .convit import *\nfrom .convmixer import *\nfrom .convnext import *\nfrom .crossvit import *\nfrom .cspnet import *\nfrom .davit import *\nfrom .deit import *\nfrom .densenet import *\nfrom .dla import *\nfrom .dpn import *\nfrom .edgenext import *\nfrom .efficientformer import *\nfrom .efficientformer_v2 import *\nfrom .efficientnet import *\nfrom .eva import *\nfrom .focalnet import *\nfrom .gcvit import *\nfrom .ghostnet import *\nfrom .hardcorenas import *\nfrom .hrnet import *\nfrom .inception_resnet_v2 import *\nfrom .inception_v3 import *\nfrom .inception_v4 import *\nfrom .levit import *\nfrom .maxxvit import *\nfrom .metaformer import *\nfrom .mlp_mixer import *\nfrom .mobilenetv3 import *\nfrom .mobilenetv3_large import *\nfrom .mobilenetv3_small import *\nfrom .mobilenetv3_small_patch4_window7_224 import *\nfrom .mobilenetv3_tiny import *\nfrom .nasnet import *\nfrom .nasnet_mobile import *\nfrom .nasnet_mobile_backbone import *\nfrom .nasnet_mobile_encoder import *\nfrom .nasnet_mobile_encoder_backbone import *\nfrom .nasnet_mobile_encoder_backbone_v2 import *\nfrom .nasnet",
  "\"\"\" DaViT: Dual Attention Vision Transformers\n\nAs described in https://arxiv.org/abs/2204.03645\n\nInput size invariant transformer architecture that combines channel and spacial\nattention in each block. The attention mechanisms used are linear in complexity.\n\nDaViT model defs and weights adapted from https://github.com/dingmyu/davit, original copyright below\n\n\"\"\"\n# Copyright (c) 2022 Mingyu Ding\n# All rights reserved.\n# This source code is licensed under the MIT license\nfrom functools import partial\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import DropPath, to_2tuple, trunc_normal_, Mlp, LayerNorm2d, get_norm_layer, use_fused_attn\nfrom timm.layers import NormMlpClassifierHead, ClassifierHead\nfrom ._builder import build_model_with_cfg\nfrom .configuration_davit import DavitConfig\n\n\ndef trunc_normal(mean, std, a, b):\n    r\"\"\"\n    Implementation of TruncNormal from https://pytorch.org/docs/stable/nn.html#truncnormal\n    \"\"\"\n    return a + (b - a) * torch.erf((mean - a) / (std * torch.sqrt(2)))\n\n\ndef gelu(x):\n    return x * 0.5 * (1.0 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.04",
  "\"\"\"PyTorch ResNet\n\nThis started as a copy of https://github.com/pytorch/vision 'resnet.py' (BSD-3-Clause) with\nadditional dropout and dynamic global avg/max pool.\n\nResNeXt, SE-ResNeXt, SENet, and MXNet Gluon stem/downsample variants, tiered stems added by Ross Wightman\n\nCopyright 2019, Ross Wightman\n\"\"\"\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import DropBlock2d, DropPath, AvgPool2dSame, BlurPool2d, GroupNorm, create_attn, get_attn, \\\n    get_act_layer, get_norm_layer, create_classifier\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs, register_model_deprecations\nfrom .configuration_resnet import ResNetConfig\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False",
  "\"\"\"\nPorted to pytorch thanks to [tstandley](https://github.com/tstandley/Xception-PyTorch)\n\n@author: tstandley\nAdapted by cadene\n\nCreates an Xception Model as defined in:\n\nFrancois Chollet\nXception: Deep Learning with Depthwise Separable Convolutions\nhttps://arxiv.org/pdf/1610.02357.pdf\n\nThis weights ported from the Keras implementation. Achieves the following performance on the validation set:\n\nLoss:0.9173 Prec@1:78.892 Prec@5:94.292\n\nREMEMBER to set your image size to 3x299x299 for both test and validation\n\nnormalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                  std=[0.5, 0.5, 0.5])\n\nThe resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n\"\"\"\nimport torch.jit\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import models\nfrom torchvision.models.resnet import resnet50\nfrom torchvision.transforms import resize, center_crop, to_tensor, normalize\nfrom torchvision.utils import make_grid\n\nfrom transformers import XceptionConfig, XceptionForImageClassification, XceptionImageProcessor\nfrom transformers.utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\nXCEPTION_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"",
  "\"\"\"RegNet X, Y, Z, and more\n\nPaper: `Designing Network Design Spaces` - https://arxiv.org/abs/2003.13678\nOriginal Impl: https://github.com/facebookresearch/pycls/blob/master/pycls/models/regnet.py\n\nPaper: `Fast and Accurate Model Scaling` - https://arxiv.org/abs/2103.06877\nOriginal Impl: None\n\nBased on original PyTorch impl linked above, but re-wrote to use my own blocks (adapted from ResNet here)\nand cleaned up with more descriptive variable names.\n\nWeights from original pycls impl have been modified:\n* first layer from BGR -> RGB as most PyTorch models are\n* removed training specific dict entries from checkpoints and keep model state_dict only\n* remap names to match the ones here\n\nSupports weight loading from torchvision and classy-vision (incl VISSL SEER)\n\nA number of custom timm model definitions additions including:\n* stochastic depth, gradient checkpointing, layer-decay, configurable dilation\n* a pre-activation layer for the final linear layer\n* a separate linear layer for the final linear layer\n* a separate linear layer for the final linear layer\n* a separate linear layer for the final linear layer\n* a separate linear layer for the final linear layer\n* a separate linear layer for the final linear layer\n* a separate linear layer for the final linear layer\n* a separate linear layer for the final linear layer\n* a separate linear layer for the final linear layer\n* a separate linear layer for the final linear layer\n* a separate linear layer for the final linear layer\n* a separate linear layer for the final linear layer\n",
  "\"\"\" Model creation / weight loading / state_dict helpers\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport logging\nimport os\nfrom collections import OrderedDict\nfrom typing import Any, Callable, Dict, Optional, Union\n\nimport torch\ntry:\n    import safetensors.torch\n    _has_safetensors = True\nexcept ImportError:\n    _has_safetensors = False\n\n_logger = logging.getLogger(__name__)\n\n__all__ = ['clean_state_dict', 'load_state_dict', 'load_checkpoint', 'remap_state_dict', 'resume_checkpoint']\n\n\ndef clean_state_dict(state_dict: Dict[str, Any]) -> Dict[str, Any]:\n    # 'clean' checkpoint by removing .module prefix from state dict if it exists from parallel training\n    cleaned_state_dict = {}\n    for k, v in state_dict.items():\n        name = k[7:] if k.startswith('module.') else k\n        cleaned_state_dict[name] = v\n    return cleaned_state_dict\n\n\ndef load_state_dict(\n        checkpoint_path: str,\n        model: torch.nn.Module,\n        map_location: Optional[str] = None,\n        strict: bool = False,\n        device: Optional[torch.device] = None,\n) -> torch.nn.Module:\n    \"\"\"\n    Load a checkpoint from a given path.\n\n    Args:\n        checkpoint_path (str): Path to the checkpoint to load.\n        model (torch.nn.Module): The model to load the checkpoint into.\n        map_location (Optional[str], optional): The device to load the checkpoint on. Defaults to None.\n        strict (bool, optional): Whether to raise an exception",
  "\"\"\" Swin Transformer\nA PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`\n    - https://arxiv.org/pdf/2103.14030\n\nCode/weights from https://github.com/microsoft/Swin-Transformer, original copyright/license info below\n\nS3 (AutoFormerV2, https://arxiv.org/abs/2111.14725) Swin weights from\n    - https://github.com/microsoft/Cream/tree/main/AutoFormerV2\n\nModifications and additions for timm hacked together by / Copyright 2021, Ross Wightman\n\"\"\"\n# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\nimport logging\nimport math\nfrom typing import Callable, List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.models.layers import BatchNorm2d, GroupNorm, LayerNorm, make_divisible\nfrom timm.models.swin.modeling_swin import SwinConv2d, SwinLayer, SwinPreActBlock, SwinStem\nfrom timm.models.utils import PreTrainedModel\nfrom timm.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging as timm_logging\n\n\nlogger = logging.getLogger(__name__)\n\n_CONFIG_FOR_DOC = \"SwinConfig\"\n_CHECKPOINT",
  "\"\"\" PyTorch FX Based Feature Extraction Helpers\nUsing https://pytorch.org/vision/stable/feature_extraction.html\n\"\"\"\nfrom typing import Callable, List, Dict, Union, Type\n\nimport torch\nfrom torch import nn\n\nfrom ._features import _get_feature_info, _get_return_layers\n\ntry:\n    from torchvision.models.feature_extraction import create_feature_extractor as _create_feature_extractor\n    has_fx_feature_extraction = True\nexcept ImportError:\n    has_fx_feature_extraction = False\n\n# Layers we went to treat as leaf modules\nfrom timm.layers import Conv2dSame, ScaledStdConv2dSame, CondConv2d, StdConv2dSame\nfrom timm.layers.non_local_attn import BilinearAttnTransform\nfrom timm.layers.pool2d_same import MaxPool2dSame, AvgPool2dSame\n\n__all__ = ['register_notrace_module', 'is_notrace_module', 'get_notrace_modules',\n           'register_notrace_function', 'is_notrace_function', 'get_notrace_functions',\n           'register_notrace_layer', 'is_notrace_layer', 'get_notrace_layers',\n           'register_notrace_module_list', 'is_notrace_module_list', 'get_notrace_module_lists',\n           'register_notrace_function_list', 'is_notrace_function_list', 'get_notrace_function_lists',\n           'register_notrace_layer_list', 'is_notrace_layer_list', 'get_notrace_layer_lists',\n           'register_notrace_",
  "\"\"\" MaxVit and CoAtNet Vision Transformer - CNN Hybrids in PyTorch\n\nThis is a from-scratch implementation of both CoAtNet and MaxVit in PyTorch.\n\n99% of the implementation was done from papers, however last minute some adjustments were made\nbased on the (as yet unfinished?) public code release https://github.com/google-research/maxvit\n\nThere are multiple sets of models defined for both architectures. Typically, names with a\n `_rw` suffix are my own original configs prior to referencing https://github.com/google-research/maxvit.\nThese configs work well and appear to be a bit faster / lower resource than the paper.\n\nThe models without extra prefix / suffix' (coatnet_0_224, maxvit_tiny_224, etc), are intended to\nmatch paper, BUT, without any official pretrained weights it's difficult to confirm a 100% match.\n\nPapers:\n\nMaxViT: Multi-Axis Vision Transformer - https://arxiv.org/abs/2204.01697\n@article{tu2022maxvit,\n  title={MaxViT: Multi-Axis Vision Transformer},\n  author={<NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <NAME> and <",
  "\"\"\" Visformer\n\nPaper: Visformer: The Vision-friendly Transformer - https://arxiv.org/abs/2104.12533\n\nFrom original at https://github.com/danczs/Visformer\n\nModifications and additions for timm hacked together by / Copyright 2021, Ross Wightman\n\"\"\"\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import to_2tuple, trunc_normal_, DropPath, PatchEmbed, LayerNorm2d, create_classifier, use_fused_attn\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['Visformer']\n\n\nclass SpatialMlp(nn.Module):\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            drop=0.,\n            group=1,\n            **kwargs\n    ):\n        super().__init__()\n        hidden_features = hidden_features or in_features // group\n        out_features = out_features or in_features // group\n\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.act = act_layer()\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.",
  "\"\"\" EdgeNeXt\n\nPaper: `EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications`\n - https://arxiv.org/abs/2206.10589\n\nOriginal code and weights from https://github.com/mmaaz60/EdgeNeXt\n\nModifications and additions for timm by / Copyright 2022, Ross Wightman\n\"\"\"\nimport math\nfrom collections import OrderedDict\nfrom functools import partial\nfrom typing import Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import trunc_normal_tf_, DropPath, LayerNorm2d, Mlp, SelectAdaptivePool2d, create_conv2d, \\\n    use_fused_attn\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_module\nfrom ._manipulate import named_apply, checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs\n\n\n# TODO: add support for other models\ndef edge_next_layer(\n    input_tensor: torch.Tensor,\n    num_channels: int,\n    num_heads: int,\n    dim_feedforward: int,\n    dropout: float = 0.0,\n    attn_dropout: float = 0.0,\n    activation: str = \"gelu\",\n    use_layer_norm: bool = True,\n    layer_norm_eps: float = 1e-12,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    EdgeNeXt layer function.",
  "\"\"\" Selective Kernel Networks (ResNet base)\n\nPaper: Selective Kernel Networks (https://arxiv.org/abs/1903.06586)\n\nThis was inspired by reading 'Compounding the Performance Improvements...' (https://arxiv.org/abs/2001.06268)\nand a streamlined impl at https://github.com/clovaai/assembled-cnn but I ended up building something closer\nto the original paper with some modifications of my own to better balance param count vs accuracy.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\n\nfrom torch import nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import SelectiveKernel, ConvNormAct, create_attn\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\nfrom .resnet import ResNet\n\n\nclass SelectiveKernelBasic(nn.Module):\n    expansion = 1\n\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            dilation=1,\n            groups=1,\n            bias=True,\n            norm_layer=nn.BatchNorm2d,\n            **kwargs\n    ):\n        super().__init__()\n        self.conv1 = ConvNormAct(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias,\n            norm_layer=norm_layer,",
  "\"\"\" ReXNet\n\nA PyTorch impl of `ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network` -\nhttps://arxiv.org/abs/2007.00992\n\nAdapted from original impl at https://github.com/clovaai/rexnet\nCopyright (c) 2020-present NAVER Corp. MIT license\n\nChanges for timm, feature extraction, and rounded channel variant hacked together by Ross Wightman\nCopyright 2020 Ross Wightman\n\"\"\"\n\nfrom functools import partial\nfrom math import ceil\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import ClassifierHead, create_act_layer, ConvNormAct, DropPath, make_divisible, SEModule\nfrom ._builder import build_model_with_cfg\nfrom ._efficientnet_builder import efficientnet_init_weights\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import generate_default_cfgs, register_model\n\n__all__ = [\n    \"ReXNet\",\n    \"re_xnet_101_32x4d\",\n    \"re_xnet_101_32x4d_2x\",\n    \"re_xnet_101_32x4d_4x\",\n    \"re_xnet_101_32x4d_8x\",\n    \"re_xnet_101_32x4d_16x\",\n    \"re_xnet_101_32x4d_32x",
  "\"\"\" Vision Transformer (ViT) in PyTorch\n\nA PyTorch implement of Vision Transformers as described in:\n\n'Exploring Plain Vision Transformer Backbones for Object Detection'\n    - https://arxiv.org/abs/2203.16527\n\n'Segment Anything Model (SAM)'\n    - https://github.com/facebookresearch/segment-anything/\n\n\"\"\"\nimport logging\nfrom functools import partial\nfrom typing import Callable, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import PatchEmbed, Mlp, DropPath, PatchDropout, LayerNorm2d, ClassifierHead, NormMlpClassifierHead,\\\n    Format, resample_abs_pos_embed_nhwc\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import generate_default_cfg\nfrom .image_processing_vit import get_size_dict, get_size_dict_from_inputs\nfrom .vit import ViTConfig, ViTFeatureExtractor, ViTImageProcessor, ViTImageResizer, ViTPreTrainedModel\n\n\nlogger = logging.getLogger(__name__)\n\nVIT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"facebook/vit-base-patch16-224\",\n    \"facebook/vit-large-patch16-224\",\n    \"facebook/vit-base-patch32-",
  "\"\"\"Pytorch impl of Aligned Xception 41, 65, 71\n\nThis is a correct, from scratch impl of Aligned Xception (Deeplab) models compatible with TF weights at\nhttps://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import ClassifierHead, ConvNormAct, create_conv2d, get_norm_act_layer\nfrom timm.layers.helpers import to_3tuple\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['XceptionAligned']\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            dilation=1,\n            groups=1,\n            bias=True,\n            norm_layer=None,\n            **kwargs\n    ):\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_chs,\n            out_chs,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias,\n            norm_layer=norm_layer,\n            **kwargs\n        )\n        self.",
  "\"\"\" MobileViT\n\nPaper:\nV1: `MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer` - https://arxiv.org/abs/2110.02178\nV2: `Separable Self-attention for Mobile Vision Transformers` - https://arxiv.org/abs/2206.02680\n\nMobileVitBlock and checkpoints adapted from https://github.com/apple/ml-cvnets (original copyright below)\nLicense: https://github.com/apple/ml-cvnets/blob/main/LICENSE (Apple open source)\n\nRest of code, ByobNet, and Transformer block hacked together by / Copyright 2022, Ross Wightman\n\"\"\"\n#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2020 Apple Inc. All Rights Reserved.\n#\nimport math\nfrom typing import Callable, Tuple, Optional\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom timm.layers import to_2tuple, make_divisible, GroupNorm1, Conv2d, BatchNorm2d, LayerNorm, \\\n    LayerNorm2d, Mish, get_initializer\nfrom timm.models.layers import DropPath, to_channel_dimension_format, trunc_normal_\nfrom timm.models.vision_transformer.modeling_vit import (\n    ACT2FN,\n    ACT2FN_RELU,\n    ACT2FN_RELU_PRELU,\n    ACT2FN_RELU_PRELU_SELU,\n    ACT2FN_RELU_SELU,\n    ACT2FN_SELU,\n    ACT2FN_SELU",
  "\"\"\" Pooling-based Vision Transformer (PiT) in PyTorch\n\nA PyTorch implement of Pooling-based Vision Transformers as described in\n'Rethinking Spatial Dimensions of Vision Transformers' - https://arxiv.org/abs/2103.16302\n\nThis code was adapted from the original version at https://github.com/naver-ai/pit, original copyright below.\n\nModifications for timm by / Copyright 2020 Ross Wightman\n\"\"\"\n# PiT\n# Copyright 2021-present NAVER Corp.\n# Apache License v2.0\n\nimport math\nimport re\nfrom functools import partial\nfrom typing import Sequence, Tuple\n\nimport torch\nfrom torch import nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import trunc_normal_, to_2tuple, LayerNorm\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\nfrom .vision_transformer import Block\n\n\n__all__ = ['PoolingVisionTransformer']  # noqa\n\n_CONFIG_FOR_DOC = \"PoolingVisionTransformerConfig\"\n\nPOOLING_VISION_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"naver-ai/pit-base-patch16-224\",\n    \"naver-ai/pit-large-patch16-224\",\n    \"naver-ai/pit-xlarge-patch16-224\",\n    \"naver-ai/pit-224\",\n]\n\n# pylint: disable=line-too-long\n_CHECKPOINT_",
  "\"\"\"PyTorch CspNet\n\nA PyTorch implementation of Cross Stage Partial Networks including:\n* CSPResNet50\n* CSPResNeXt50\n* CSPDarkNet53\n* and DarkNet53 for good measure\n\nBased on paper `CSPNet: A New Backbone that can Enhance Learning Capability of CNN` - https://arxiv.org/abs/1911.11929\n\nReference impl via darknet cfg files at https://github.com/WongKinYiu/CrossStagePartialNetworks\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom dataclasses import dataclass, asdict, replace\nfrom functools import partial\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import ClassifierHead, ConvNormAct, ConvNormActAa, DropPath, get_attn, create_act_layer, make_divisible\nfrom ._builder import build_model_with_cfg\n\n\n@dataclass\nclass CspNetConfig:\n    \"\"\"\n    CSPNet configuration class.\n\n    Args:\n        num_classes (`int`, *optional*, defaults to 1000):\n            Number of classes for the model.\n        num_channels (`int`, *optional*, defaults to 3):\n            Number of channels in the input image.\n        width_coefficient (`float`, *optional*, defaults to 1.0):\n            Width coefficient for CSPResNet50 and CSPResNeXt50.\n        depth_coefficient (`float`, *optional*, defaults to 1.0):",
  "\"\"\" BEiT: BERT Pre-Training of Image Transformers (https://arxiv.org/abs/2106.08254)\n\nModel from official source: https://github.com/microsoft/unilm/tree/master/beit\n\n@inproceedings{beit,\ntitle={{BEiT}: {BERT} Pre-Training of Image Transformers},\nauthor={Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=p-BhZSz59o4}\n}\n\nBEiT-v2 from https://github.com/microsoft/unilm/tree/master/beit2\n\n@article{beitv2,\ntitle={{BEiT v2}: Masked Image Modeling with Vector-Quantized Visual Tokenizers},\nauthor={Zhiliang Peng and Li Dong and Hangbo Bao and Qixiang Ye and Furu Wei},\nyear={2022},\neprint={2208.06366},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n\nBEiT-v3 from https://github.com/microsoft/unilm/tree/master/beit3\n\n@article{beitv3,\ntitle={{BEiT v3}: A Better Image Modeling Framework with Vector Quantized Visual Tokenizers},\nauthor={Zhiliang Peng and Li Dong and Hangbo Bao and Qixiang Ye and Furu Wei},\nyear={2022},\neprint={2208.06366},\narchive",
  "\"\"\" Swin Transformer V2\nA PyTorch impl of : `Swin Transformer V2: Scaling Up Capacity and Resolution`\n    - https://arxiv.org/abs/2111.09883\n\nCode/weights from https://github.com/microsoft/Swin-Transformer, original copyright/license info below\n\nModifications and additions for timm hacked together by / Copyright 2022, Ross Wightman\n\"\"\"\n# --------------------------------------------------------\n# Swin Transformer V2\n# Copyright (c) 2022 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\nimport math\nfrom typing import Callable, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import PatchEmbed, Mlp, DropPath, to_2tuple, trunc_normal_, _assert, ClassifierHead\nfrom ._builder import build_model_with_cfg\n\n\n# Copied from https://github.com/microsoft/Swin-Transformer/blob/main/swin_transformer_v2.py#L27\ndef get_position_encoding_table(position_encoding_size: int, device: torch.device) -> torch.Tensor:\n    \"\"\"\n    Get the position encoding table.\n    \"\"\"\n    pe = torch.zeros(position_encoding_size, 2 * position_encoding_size, device=device)\n    position = torch.arange(0, position_encoding_size, device=device)\n    div_term = torch.",
  "\"\"\" Bring-Your-Own-Blocks Network\n\nA flexible network w/ dataclass based config for stacking those NN blocks.\n\nThis model is currently used to implement the following networks:\n\nGPU Efficient (ResNets) - gernet_l/m/s (original versions called genet, but this was already used (by SENet author)).\nPaper: `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090\nCode and weights: https://github.com/idstcv/GPU-Efficient-Networks, licensed Apache 2.0\n\nRepVGG - repvgg_*\nPaper: `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\nCode and weights: https://github.com/DingXiaoH/RepVGG, licensed MIT\n\nIn all cases the models have been modified to fit within the design of ByobNet. I've remapped\nthe original weights and verified accuracies.\n\nFor GPU Efficient nets, I used the original names, but I've also added the `gernet_l/m/s` suffix.\n\nFor RepVGG, I used the original names, but I've also added the `repvgg_*` suffix.\n\n## Usage\n\n### Importing the model\n\n```python\nfrom byobnet.models.byobnet import ByobNet\n```\n\n### Creating a network\n\n```python\nbyobnet = ByobNet(\n    num_layers=1,\n    num_blocks=[1, 2, 2, 2, 2, 2, 2",
  "\"\"\" Swin Transformer V2\n\nA PyTorch impl of : `Swin Transformer V2: Scaling Up Capacity and Resolution`\n    - https://arxiv.org/pdf/2111.09883\n\nCode adapted from https://github.com/ChristophReich1996/Swin-Transformer-V2, original copyright/license info below\n\nThis implementation is experimental and subject to change in manners that will break weight compat:\n* Size of the pos embed MLP are not spelled out in paper in terms of dim, fixed for all models? vary with num_heads?\n  * currently dim is fixed, I feel it may make sense to scale with num_heads (dim per head)\n* The specifics of the memory saving 'sequential attention' are not detailed, Christoph Reich has an impl at\n  GitHub link above. It needs further investigation as throughput vs mem tradeoff doesn't appear beneficial.\n* num_heads per stage is not detailed for Huge and Giant model variants\n* 'Giant' is 3B params in paper but ~2.6B here despite matching paper dim + block counts\n* experiments are ongoing, I'm not sure if the paper is still accurate\n\n### Overview\n\nThe Swin Transformer V2 architecture is a modified version of Swin Transformer V1.\n\nThe Swin Transformer V2 architecture is a modified version of Swin Transformer V1.\n\nThe Swin Transformer V2 architecture is a modified version of Swin Transformer V1.\n\nThe Swin Transformer V2 architecture is a modified version of Swin Transformer V1.\n\nThe Swin Transformer V2 architecture is a modified version of Swin Transformer V1.\n\nThe Swin Transformer V2 architecture is a modified version",
  "\"\"\"\nAn implementation of GhostNet Model as defined in:\nGhostNet: More Features from Cheap Operations. https://arxiv.org/abs/1911.11907\nThe train script of the model is similar to that of MobileNetV3\nOriginal model: https://github.com/huawei-noah/CV-backbones/tree/master/ghostnet_pytorch\n\"\"\"\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import SelectAdaptivePool2d, Linear, make_divisible\nfrom ._builder import build_model_with_cfg\nfrom ._efficientnet_blocks import SqueezeExcite, ConvBnAct\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['GhostNet']\n\n\n_SE_LAYER = partial(SqueezeExcite, gate_layer='hard_sigmoid', rd_round_fn=partial(make_divisible, 2))\n_CONV_BN_ACT_LAYER = partial(ConvBnAct, act_layer='hard_sigmoid', rd_round_fn=partial(make_divisible, 2))\n\n_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"huawei-noah/ghostnet_pytorch\",\n    # See all GhostNet models at https://huggingface.co/models?filter=ghostnet\n]\n\nghostnet_cfg = generate_default_cfgs()\n\nghostnet_backbone = {\n    'ghostnet_backbone",
  "\"\"\" Vision OutLOoker (VOLO) implementation\n\nPaper: `VOLO: Vision Outlooker for Visual Recognition` - https://arxiv.org/abs/2106.13112\n\nCode adapted from official impl at https://github.com/sail-sg/volo, original copyright in comment below\n\nModifications and additions for timm by / Copyright 2022, Ross Wightman\n\"\"\"\n# Copyright 2021 Sea Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport math\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.models.vision_outlooker.vision_outlooker_utils import (\n    get_image_size,\n    get_size_dict,\n    get_top_k_values,\n    get_top_k_values_with_probs,\n    get_top_k_values_with_probs_and_boxes,\n    get_top_k_values_with_probs_and_boxes_and_scores,\n    get_top_k_values_with_probs_and_boxes_and_",
  "\"\"\" EfficientNet, MobileNetV3, etc Blocks\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nfrom timm.layers import create_conv2d, DropPath, make_divisible, create_act_layer, get_norm_act_layer\n\n__all__ = [\n    'SqueezeExcite', 'ConvBnAct', 'DepthwiseSeparableConv', 'InvertedResidual', 'CondConvResidual', 'EdgeResidual']\n\n\ndef num_groups(group_size, channels):\n    if not group_size:  # 0 or None\n        return 1  # normal conv with 1 group\n    else:\n        # NOTE group_size == 1 -> depthwise conv\n        assert channels % group_size == 0\n        return channels // group_size\n\n\nclass SqueezeExcite(nn.Module):\n    \"\"\" Squeeze-and-Excitation w/ specific features for EfficientNet/MobileNet family\n\n    Args:\n        in_chs (int): input channels to layer\n        rd_ratio (float): reduction ratio of the layer\n        out_chs (int): output channels to layer\n        squeeze_excite_ratio (float): squeeze-excite ratio of the layer\n        squeeze_excite_initializer (str): initializer for squeeze-excite layer\n        squeeze_excite_activation (str): activation function for squeeze-excite layer\n        use_se (bool): whether to use squeeze-excite layer\n        use_se_after_conv (bool): whether to use squeeze-excite layer after conv layer\n        use_se_after_conv_after_act (bool",
  "\"\"\" NasNet-A (Large)\n nasnetalarge implementation grabbed from Cadene's pretrained models\n https://github.com/Cadene/pretrained-models.pytorch\n\"\"\"\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.layers import ConvNormAct, create_conv2d, create_pool2d, create_classifier\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['NASNetALarge']\n\n\n\nclass ActConvBn(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=''):\n        super(ActConvBn, self).__init__()\n        self.act = nn.ReLU()\n        self.conv = create_conv2d(\n            in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1)\n\n    def forward(self, x):\n        x = self.act(self.conv(x))\n        x = self.bn(x)\n        return x\n\n\nclass NASNetALarge(nn.Module):\n    r\"\"\"\n    Constructs a NASNet-A (Large) model according to the specified configuration. See\n    [NASNet-A](https://arxiv.org/abs/1707.07012) for more details.\n\n    Args:\n        cfg (Config): A configuration object that defines the model.\n        num_classes (int): The number of output classes.\n       ",
  "\"\"\" ConvNeXt\n\nPapers:\n* `A ConvNet for the 2020s` - https://arxiv.org/pdf/2201.03545.pdf\n@Article{liu2022convnet,\n  author  = {Zhuang Liu and Hanzi Mao and Chao-Yuan Wu and Christoph Feichtenhofer and Trevor Darrell and Saining Xie},\n  title   = {A ConvNet for the 2020s},\n  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year    = {2022},\n}\n\n* `ConvNeXt-V2 - Co-designing and Scaling ConvNets with Masked Autoencoders` - https://arxiv.org/abs/2301.00808\n@article{Woo2023ConvNeXtV2,\n  title={ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders},\n  author={Sanghyun Woo, Shoubhik and Hanzi Mao, Christoph and Chao-Yuan Wu, Christoph and Trevor Darrell, Trevor and Saining Xie, Sainath},\n  journal={arXiv preprint arXiv:2301.00808},\n  year={2023}\n}\n\n* `ConvNeXt-V3 - A New Architecture for Convolutional Neural Networks` - https://arxiv.org/abs/2302.03960\n@article{Woo2023ConvNeXtV3,\n ",
  "\"\"\"Pre-Activation ResNet v2 with GroupNorm and Weight Standardization.\n\nA PyTorch implementation of ResNetV2 adapted from the Google Big-Transfoer (BiT) source code\nat https://github.com/google-research/big_transfer to match timm interfaces. The BiT weights have\nbeen included here as pretrained models from their original .NPZ checkpoints.\n\nAdditionally, supports non pre-activation bottleneck for use as a backbone for Vision Transfomers (ViT) and\nextra padding support to allow porting of official Hybrid ResNet pretrained weights from\nhttps://github.com/google-research/vision_transformer\n\nThanks to the Google team for the above two repositories and associated papers:\n* Big Transfer (BiT): General Visual Representation Learning - https://arxiv.org/abs/1912.11370\n* An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - https://arxiv.org/abs/2010.11929\n* Knowledge distillation: A good teacher is patient and consistent - https://arxiv.org/abs/2002.10852\n\nThe original ResNet v2 paper is available at https://arxiv.org/abs/1512.03385.\n\nThe original ViT paper is available at https://arxiv.org/abs/2010.11929.\n\nThe original Hybrid ResNet paper is available at https://arxiv.org/abs/2002.10852.\n\nThe original Knowledge Distillation paper is available at https://arxiv.org/abs/2002.108",
  "\"\"\" Pyramid Vision Transformer v2\n\n@misc{wang2021pvtv2,\n      title={PVTv2: Improved Baselines with Pyramid Vision Transformer},\n      author={Wenhai Wang and Enze Xie and Xiang Li and Deng-Ping Fan and Kaitao Song and Ding Liang and\n        Tong Lu and Ping Luo and Ling Shao},\n      year={2021},\n      eprint={2106.13797},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\nBased on Apache 2.0 licensed code at https://github.com/whai362/PVT\n\nModifications and timm support by / Copyright 2022, Ross Wightman\n\"\"\"\n\nimport math\nfrom typing import Tuple, List, Callable, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import DropPath, to_channel_dimension_format, trunc_normal_\nfrom timm.models.layers import GroupNorm, make_divisible\nfrom timm.models.vision_transformer import VisionTransformer, VisionTransformerConfig\nfrom timm.utils import apply_chunking, find_pruneable_heads_and_indices, prune_linear_layer, stable_softmax\n\n\nclass PVTv2(VisionTransformer):\n    r\"\"\"\n    Constructs a PVTv2 model according to the specified configuration.\n\n    Args:\n        config (`VisionTransformerConfig`):\n            The configuration of the PVTv2 model. It contains the",
  "\"\"\" The EfficientNet Family in PyTorch\n\nAn implementation of EfficienNet that covers variety of related models with efficient architectures:\n\n* EfficientNet-V2\n  - `EfficientNetV2: Smaller Models and Faster Training` - https://arxiv.org/abs/2104.00298\n\n* EfficientNet (B0-B8, L2 + Tensorflow pretrained AutoAug/RandAug/AdvProp/NoisyStudent weight ports)\n  - EfficientNet: Rethinking Model Scaling for CNNs - https://arxiv.org/abs/1905.11946\n  - CondConv: Conditionally Parameterized Convolutions for Efficient Inference - https://arxiv.org/abs/1904.04971\n  - Adversarial Examples Improve Image Recognition - https://arxiv.org/abs/1911.09665\n  - Self-training with Noisy Student improves ImageNet classification - https://arxiv.org/abs/1911.04252\n\n* MixNet (Small, Medium, and Large)\n  - MixConv: Mixed Depthwise Convolutions for Efficient Inference - https://arxiv.org/abs/1904.04971\n  - MixNet: Mixing Depthwise Convolutions and Pointwise Convolutions for Efficient Inference - https://arxiv.org/abs/1904.04971\n\n* MobileNetV2\n  - MobileNetV2: Inverted Residuals and Linear Bottlenecks - https://arxiv.org/abs/1801.04381\n\n* ResNet\n  - ResNet: Deep Residual Learning for Image Recognition - https://arxiv",
  "\"\"\" MobileNet V3\n\nA PyTorch impl of MobileNet-V3, compatible with TF weights from official impl.\n\nPaper: Searching for MobileNetV3 - https://arxiv.org/abs/1905.02244\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\nfrom functools import partial\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.checkpoint import checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import SelectAdaptivePool2d, Linear, create_conv2d, get_norm_act_layer\nfrom ._builder import build_model_with_cfg, pretrained_cfg_for_features\nfrom ._efficientnet_blocks import SqueezeExcite\nfrom ._efficientnet_builder import EfficientNetBuilder, decode_arch_def, efficientnet_init_weights, \\\n    round_channels, round_filters\n\n\ndef conv_dw_same(in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, dilation=1,\n                 norm_layer=nn.BatchNorm2d):\n    \"\"\"\n    Creates a 2D convolution layer with depthwise separable convolution and same padding.\n    \"\"\"\n    return nn.Conv2d(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n        groups=groups,\n        dilation=",
  "\"\"\" MLP-Mixer, ResMLP, and gMLP in PyTorch\n\nThis impl originally based on MLP-Mixer paper.\n\nOfficial JAX impl: https://github.com/google-research/vision_transformer/blob/linen/vit_jax/models_mixer.py\n\nPaper: 'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n\n@article{tolstikhin2021,\n  title={MLP-Mixer: An all-MLP Architecture for Vision},\n  author={Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner,\n        Thomas and Yung, Jessica and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},\n  journal={arXiv preprint arXiv:2105.01601},\n  year={2021}\n}\n\nAlso see the following repos:\n\n- [MLP-Mixer](https://github.com/google-research/vision_transformer/tree/linen/vit_jax/models_mixer)\n- [ResMLP](https://github.com/google-research/vision_transformer/tree/linen/vit_jax/models_resmlp)\n- [gMLP](https://github.com/google-research/vision_transformer/tree/linen/vit_jax/models_gmlp)\n\n\"\"\"\n\nimport math\nfrom typing import Optional, Tuple, Union",
  "\"\"\" Class-Attention in Image Transformers (CaiT)\n\nPaper: 'Going deeper with Image Transformers' - https://arxiv.org/abs/2103.17239\n\nOriginal code and weights from https://github.com/facebookresearch/deit, copyright below\n\nModifications and additions for timm hacked together by / Copyright 2021, Ross Wightman\n\"\"\"\n# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import PatchEmbed, Mlp, DropPath, trunc_normal_, use_fused_attn\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['Cait', 'ClassAttn', 'LayerScaleBlockClassAttn', 'LayerScaleBlock', 'TalkingHeadAttn', 'TimmCait']\n\n\nclass LayerScaleBlock(nn.Module):\n    def __init__(self, dim, num_heads, mlp_dim, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.mlp_dim = mlp_dim\n        self.qkv_bias = qkv_bias\n        self.qk_scale = qk_scale or dim ** -0.5\n\n       ",
  "\"\"\" PyTorch implementation of DualPathNetworks\nBased on original MXNet implementation https://github.com/cypw/DPNs with\nmany ideas from another PyTorch implementation https://github.com/oyam/pytorch-DPNs.\n\nThis implementation is compatible with the pretrained weights from cypw's MXNet implementation.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom collections import OrderedDict\nfrom functools import partial\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DPN_MEAN, IMAGENET_DPN_STD, IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import BatchNormAct2d, ConvNormAct, create_conv2d, create_classifier, get_norm_act_layer\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['DPN']\n\n\nclass CatBnAct(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, groups=1,\n                 base_width=64, dilation_rate=1, norm_layer=BatchNormAct2d, **kwargs):\n        super().__init__()\n        self.conv = ConvNormAct(\n            in_planes,\n            out_planes,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            base_width=base_width,\n            dilation_rate=",
  "\"\"\" VoVNet (V1 & V2)\n\nPapers:\n* `An Energy and GPU-Computation Efficient Backbone Network` - https://arxiv.org/abs/1904.09730\n* `CenterMask : Real-Time Anchor-Free Instance Segmentation` - https://arxiv.org/abs/1911.06667\n\nLooked at  https://github.com/youngwanLEE/vovnet-detectron2 &\nhttps://github.com/stigma0617/VoVNet.pytorch/blob/master/models_vovnet/vovnet.py\nfor some reference, rewrote most of the code.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import ConvNormAct, SeparableConvNormAct, BatchNormAct2d, ClassifierHead, DropPath, \\\n    create_attn, get_model_output\nfrom timm.models.layers import GroupNorm, InstanceNorm, make_divisible\nfrom timm.models.utils import apply_chunking, find_pruneable_heads_and_indices, prune_linear_layer\n\n\nclass VoVNet(nn.Module):\n    def __init__(self,\n                 num_channels=3,\n                 num_classes=1000,\n                 num_blocks=6,\n                 num_heads=4,\n                 num_blocks_per_stage=2,\n                 num_stages=4,\n                 num_stages_per_",
  "\"\"\" ConViT Model\n\n@article{d2021convit,\n  title={ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases},\n  author={d'Ascoli, St{\\'e}phane and Touvron, Hugo and Leavitt, Matthew and Morcos, Ari and Biroli, Giulio and Sagun, Levent},\n  journal={arXiv preprint arXiv:2103.10697},\n  year={2021}\n}\n\nPaper link: https://arxiv.org/abs/2103.10697\nOriginal code: https://github.com/facebookresearch/convit, original copyright below\n\nModifications and additions for timm hacked together by / Copyright 2021, Ross Wightman\n\"\"\"\n# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the CC-by-NC license found in the\n# LICENSE file in the root directory of this source tree.\n#\n'''These modules are copied from the original Convit repository:\nhttps://github.com/facebookresearch/convit\n\nCopyright 2021, Facebook, Inc.\nAll rights reserved.\n\nThis source code is licensed under the CC-by-NC license found in the\nLICENSE file in the root directory of this source tree.\n'''\n\nimport math\nimport warnings\nfrom typing import Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss",
  "import hashlib\nimport json\nimport logging\nimport os\nimport sys\nfrom functools import partial\nfrom pathlib import Path\nfrom tempfile import TemporaryDirectory\nfrom typing import Iterable, Optional, Union\n\nimport torch\nfrom torch.hub import HASH_REGEX, download_url_to_file, urlparse\n\ntry:\n    from torch.hub import get_dir\nexcept ImportError:\n    from torch.hub import _get_torch_home as get_dir\n\ntry:\n    import safetensors.torch\n    _has_safetensors = True\nexcept ImportError:\n    _has_safetensors = False\n\nif sys.version_info >= (3, 8):\n    from typing import Literal\nelse:\n    from typing_extensions import Literal\n\nfrom timm import __version__\nfrom timm.models._pretrained import filter_pretrained_cfg\n\ntry:\n    from huggingface_hub import (\n        create_repo, get_hf_file_metadata,\n        hf_hub_download, hf_hub_url,\n        repo_type_and_id_from_hf_id, upload_folder)\n    from huggingface_hub.utils import EntryNotFoundError\n    hf_hub_exists = True\nexcept ImportError:\n    hf_hub_exists = False\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"facebook/wmt19-en-de\",\n    \"facebook/wmt19-de-en\",\n    \"facebook/wmt19-en-de-base\",\n    \"facebook/wmt19-de-en-base\",\n    \"facebook/wmt19-en-de-large\",\n    \"facebook/wmt19-de-en-large\",",
  "\"\"\" Cross-Covariance Image Transformer (XCiT) in PyTorch\n\nPaper:\n    - https://arxiv.org/abs/2106.09681\n\nSame as the official implementation, with some minor adaptations, original copyright below\n    - https://github.com/facebookresearch/xcit/blob/master/xcit.py\n\nModifications and additions for timm hacked together by / Copyright 2021, Ross Wightman\n\"\"\"\n# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\n\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import DropPath, trunc_normal_, to_2tuple\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_module\nfrom ._registry import register_model, generate_default_cfgs, register_model_deprecations\nfrom .cait import Classifier\nfrom .image_transformer import ImageTransformer\n\n\n# Copied from https://github.com/facebookresearch/xcit/blob/master/xcit.py\ndef get_classifier(cfg, num_labels=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None):\n    \"\"\"Construct a classifier from a configuration.\"\"\"\n    if replace_stride_with_dilation is None:\n        replace_stride_with_dilation = [1, 6, 12, 24,",
  "\"\"\"PyTorch SelecSLS Net example for ImageNet Classification\nLicense: CC BY 4.0 (https://creativecommons.org/licenses/by/4.0/legalcode)\nAuthor: Dushyant Mehta (@mehtadushy)\n\nSelecSLS (core) Network Architecture as proposed in \"XNect: Real-time Multi-person 3D\nHuman Pose Estimation with a Single RGB Camera, Mehta et al.\"\nhttps://arxiv.org/abs/1907.00837\n\nBased on ResNet implementation in https://github.com/rwightman/pytorch-image-models\nand SelecSLS Net implementation in https://github.com/mehtadushy/SelecSLS-Pytorch\n\"\"\"\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import create_classifier\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, register_module\n\n\nclass SelecSLSNet(nn.Module):\n    def __init__(self, cfg, num_channels=3):\n        super().__init__()\n        self.cfg = cfg\n        self.num_channels = num_channels\n\n        self.backbone = build_model_with_cfg(cfg.backbone, num_channels=num_channels)\n        self.classifier = create_classifier(cfg.classifier, num_channels=num_channels)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.classifier(x)\n        return x\n\n",
  "import os\nimport pkgutil\nfrom copy import deepcopy\n\nfrom torch import nn as nn\n\nfrom timm.layers import Conv2dSame, BatchNormAct2d, Linear\n\n__all__ = ['extract_layer', 'set_layer', 'adapt_model_from_string', 'adapt_model_from_file']\n\n\ndef extract_layer(model, layer):\n    layer = layer.split('.')\n    module = model\n    if hasattr(model, 'module') and layer[0] != 'module':\n        module = model.module\n    if not hasattr(model, 'module') and layer[0] == 'module':\n        layer = layer[1:]\n    for l in layer:\n        if hasattr(module, l):\n            if not l.isdigit():\n                module = getattr(module, l)\n            else:\n                module = module[int(l)]\n        else:\n            return module\n    return module\n\n\ndef set_layer(model, layer, val):\n    layer = layer.split('.')\n    module = model\n    if hasattr(model, 'module') and layer[0] != 'module':\n        module = model.module\n    lst_index = 0\n    module2 = module\n    for l in layer:\n        if hasattr(module2, l):\n            if not l.isdigit():\n                module2 = getattr(module2, l)\n            else:\n                module2 = module2[int(l)]\n        else:\n            lst_index += 1\n            module2 = deepcopy(module)\n            module2.module = deepcopy(module.module)\n            module2.module.name = module.module.name\n            module2.module.conv1 = deepcopy(module.module.conv1)\n            module2.module.bn1 = deepcopy(module.module.bn1)\n            module",
  "\"\"\" Pytorch Inception-Resnet-V2 implementation\nSourced from https://github.com/Cadene/tensorflow-model-zoo.torch (MIT License) which is\nbased upon Google's Tensorflow implementation and pretrained weights (Apache 2.0 License)\n\"\"\"\nfrom functools import partial\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import create_classifier, ConvNormAct\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import flatten_modules\nfrom ._registry import register_model, generate_default_cfgs, register_model_deprecations\n\n__all__ = ['InceptionResnetV2']\n\n\nclass Mixed_5b(nn.Module):\n    def __init__(self, conv_block=None):\n        super(Mixed_5b, self).__init__()\n        conv_block = conv_block or ConvNormAct\n\n        self.branch0 = conv_block(192, 96, kernel_size=3, stride=1, padding=1)\n        self.branch1 = conv_block(96, 96, kernel_size=3, stride=1, padding=1)\n        self.branch2 = conv_block(96, 96, kernel_size=3, stride=1, padding=1)\n        self.branch3 = conv_block(96, 96, kernel_size=3, stride=1, padding=1)\n        self.branch4 = conv_block(96, 96, kernel_size=3, stride",
  "\"\"\" Inception-V3\n\nOriginally from torchvision Inception3 model\nLicensed BSD-Clause 3 https://github.com/pytorch/vision/blob/master/LICENSE\n\"\"\"\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_STD, IMAGENET_DEFAULT_MEAN, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import trunc_normal_, create_classifier, Linear, ConvNormAct\nfrom ._builder import build_model_with_cfg\nfrom ._builder import resolve_pretrained_cfg\nfrom ._manipulate import flatten_modules\nfrom ._registry import register_model, generate_default_cfgs, register_model_deprecations\n\n__all__ = ['InceptionV3']  # model_registry will add each entrypoint fn to this\n\n\nclass InceptionA(nn.Module):\n\n    def __init__(self, in_channels, pool_features, conv_block=None):\n        super(InceptionA, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, pool_features, kernel_size=1, bias=False)\n        self.conv2 = nn.Conv2d(pool_features, pool_features, kernel_size=3, stride=1, padding=1, bias=False)\n        self.conv3 = nn.Conv2d(pool_features, pool_features * 2, kernel_size=3, stride=1, padding=1, bias=False)\n        self.conv4 = nn.Conv2d(pool_features * 2,",
  "\"\"\" Bring-Your-Own-Attention Network\n\nA flexible network w/ dataclass based config for stacking NN blocks including\nself-attention (or similar) layers.\n\nCurrently used to implement experimental variants of:\n  * Bottleneck Transformers\n  * Lambda ResNets\n  * HaloNets\n\nConsider all of the models definitions here as experimental WIP and likely to change.\n\nHacked together by / copyright Ross Wightman, 2021.\n\"\"\"\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\nfrom .byobnet import ByoBlockCfg, ByoModelCfg, ByobNet, interleave_blocks\n\n__all__ = []\n\n\nmodel_cfgs = dict(\n\n    botnet26t=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0",
  "\"\"\" LeViT\n\nPaper: `LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference`\n    - https://arxiv.org/abs/2104.01136\n\n@article{graham2021levit,\n  title={LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference},\n  author={Benjamin Graham and Alaaeldin El-Nouby and Hugo Touvron and Pierre Stock and Armand Joulin and Herv\\'e J\\'egou and Matthijs Douze},\n  journal={arXiv preprint arXiv:22104.01136},\n  year={2021}\n}\n\nAdapted from official impl at https://github.com/facebookresearch/LeViT, original copyright bellow.\n\nThis version combines both conv/linear models and fixes torchscript compatibility.\n\nModifications and additions for timm hacked together by / Copyright 2021, Ross Wightman\n\"\"\"\n\n# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree. An additional grant\n# of patent rights can be found in the PATENTS file in the same directory.\n\nimport math\nfrom typing import Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom ...activations import ACT2FN\nfrom ...modeling_outputs import BaseModelOutput, Base",
  "from ._builder import *\nfrom ._helpers import *\nfrom ._manipulate import *\nfrom ._prune import *\n\nimport warnings\n\nwarnings.warn(\n    \"The `tf.keras.applications.MobileNetV2` class is deprecated and will be removed in a future version. Please use \"\n    \"`tf.keras.applications.MobileNetV2V2` instead.\",\n    FutureWarning,\n)\n\nMOBILENET_V2_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"mobilenet_v2_1.0_224\",\n    \"mobilenet_v2_1.0_256\",\n    \"mobilenet_v2_1.0_3",
  "from ._features import *\n\nimport warnings\nwarnings.warn(f\"Importing from {__name__} is deprecated and will be removed in v0.10.0. Use {__name__}.features instead.\",\n              FutureWarning)\n\n__all__ = [\n    \"Feature\",\n    \"FeatureList\",\n    \"FeatureListWithMask\",\n    \"FeatureListWithMaskAndMaskedImage\",\n    \"FeatureListWithMaskAndMaskedImageAndMaskedText\",\n    \"FeatureListWithMaskAndMaskedImageAndMaskedTextAndMaskedVideo\",\n    \"FeatureListWithMaskAndMaskedImageAndMaskedTextAndMaskedVideoAndMaskedAudio\",\n    \"FeatureListWith",
  "import dataclasses\nimport logging\nimport os\nfrom copy import deepcopy\nfrom typing import Optional, Dict, Callable, Any, Tuple\n\nfrom torch import nn as nn\nfrom torch.hub import load_state_dict_from_url\n\nfrom timm.models._features import FeatureListNet, FeatureHookNet\nfrom timm.models._features_fx import FeatureGraphNet\nfrom timm.models._helpers import load_state_dict\nfrom timm.models._hub import has_hf_hub, download_cached_file, check_cached_file, load_state_dict_from_hf\nfrom timm.models._manipulate import adapt_input_conv\nfrom timm.models._pretrained import PretrainedCfg\nfrom timm.models._prune import adapt_model_from_file\nfrom timm.models._registry import get_pretrained_cfg\n\n_logger = logging.getLogger(__name__)\n\n# Global variables for rarely used pretrained checkpoint download progress and hash check.\n# Use set_pretrained_download_progress / set_pretrained_check_hash functions to toggle.\n_DOWNLOAD_PROGRESS = False\n_CHECK_HASH = False\n_USE_PRETRAINED_MODEL = True\n_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"facebook/wmt19-en-de\",\n    \"facebook/wmt19-de-en\",\n    \"facebook/wmt19-en-fr\",\n    \"facebook/wmt19-fr-en\",\n    \"facebook/wmt19-en-it\",\n    \"facebook/wmt19-it-en\",\n    \"facebook/wmt19-en-es\",\n    \"facebook/wmt19-es-en\",\n    \"",
  "\"\"\" EVA\n\nEVA from https://github.com/baaivision/EVA , paper: https://arxiv.org/abs/2211.07636\n\n@article{EVA,\n  title={EVA: Exploring the Limits of Masked Visual Representation Learning at Scale},\n  author={Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang,\n  Tiejun and Wang, Xinlong and Cao, Yue},\n  journal={arXiv preprint arXiv:2211.07636},\n  year={2022}\n}\n\nEVA-02: A Visual Representation for Neon Genesis - https://arxiv.org/abs/2303.11331\n@article{EVA02,\n  title={EVA-02: A Visual Representation for Neon Genesis},\n  author={Fang, Yuxin and Sun, Quan and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},\n  journal={arXiv preprint arXiv:2303.11331},\n  year={2023}\n}\n\nEVA-03: A Visual Representation for Neon Genesis - https://arxiv.org/abs/2303.11331\n@article{EVA03,\n  title={EVA-03: A Visual Representation for Neon Genesis},\n  author={Fang, Yuxin and Sun, Quan and Wang, Xinggang and Huang, Tiejun and Wang,",
  "\"\"\" Multi-Scale Vision Transformer v2\n\n@inproceedings{li2021improved,\n  title={MViTv2: Improved multiscale vision transformers for classification and detection},\n  author={Li, Yanghao and Wu, Chao-Yuan and Fan, Haoqi and Mangalam, Karttikeya and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph},\n  booktitle={CVPR},\n  year={2022}\n}\n\nCode adapted from original Apache 2.0 licensed impl at https://github.com/facebookresearch/mvit\nOriginal copyright below.\n\nModifications and timm support by / Copyright 2022, Ross Wightman\n\"\"\"\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved. All Rights Reserved.\nimport operator\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom functools import partial, reduce\nfrom typing import Union, List, Tuple, Optional\n\nimport torch\nimport torch.utils.checkpoint as checkpoint\nfrom torch import nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.models.layers import BatchNorm2d, GroupNorm, LayerNorm, make_divisible\nfrom timm.models.utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\nfrom timm.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"ViTConfig\"\n_",
  "\"\"\" CrossViT Model\n\n@inproceedings{\n    chen2021crossvit,\n    title={{CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification}},\n    author={Chun-Fu (Richard) Chen and Quanfu Fan and Rameswar Panda},\n    booktitle={International Conference on Computer Vision (ICCV)},\n    year={2021}\n}\n\nPaper link: https://arxiv.org/abs/2103.14899\nOriginal code: https://github.com/IBM/CrossViT/blob/main/models/crossvit.py\n\nNOTE: model names have been renamed from originals to represent actual input res all *_224 -> *_240 and *_384 -> *_408\n\nModifications and additions for timm hacked together by / Copyright 2021, Ross Wightman\n\"\"\"\n\n# Copyright IBM All Rights Reserved.\n# SPDX-License-Identifier: Apache-2.0\n\n\n\"\"\"\nModifed from Timm. https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/crossvit.py\n\"\"\"\n\nimport math\nfrom typing import Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\n\nfrom ...activations import ACT2FN\nfrom ...modeling_outputs import (\n    BaseModelOutput,\n    BaseModelOutputWithPastAndCrossAttentions,\n    MaskedLMOutput,\n    MultipleChoiceModelOutput,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutput,\n    TokenClassifierOutput,\n)\nfrom ...modeling_",
  "\"\"\" ConvMixer\n\n\"\"\"\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import SelectAdaptivePool2d\nfrom ._registry import register_model, generate_default_cfgs\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\n\n__all__ = ['ConvMixer']\n\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x):\n        return self.fn(x) + x\n\n\nclass ConvMixer(nn.Module):\n    def __init__(\n            self,\n            dim,\n            depth,\n            kernel_size=9,\n            patch_size=7,\n            in_chans=3,\n            num_classes=1000,\n            global_pool='avg',\n            drop_rate=0.,\n            act_layer=nn.GELU,\n            **kwargs,\n    ):\n        super().__init__()\n        self.num_classes = num_classes\n        self.num_features = dim\n        self.depth = depth\n        self.kernel_size = kernel_size\n        self.patch_size = patch_size\n        self.in_chans = in_chans\n        self.global_pool = global_pool\n        self.drop_rate = drop_rate\n        self.act_layer = act_layer\n\n        self.conv1 = nn.Conv2d(self.in_chans, self.num_features, kernel_size=self.kernel_size, stride=1, padding=self.patch_size//2)\n        self.bn1 = nn.BatchNorm",
  "\"\"\"\n pnasnet5large implementation grabbed from Cadene's pretrained models\n Additional credit to https://github.com/creafz\n\n https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/pnasnet.py\n\n\"\"\"\nfrom collections import OrderedDict\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.layers import ConvNormAct, create_conv2d, create_pool2d, create_classifier\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['PNASNet5Large']\n\n\nclass SeparableConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=''):\n        super(SeparableConv2d, self).__init__()\n        self.depthwise_conv2d = create_conv2d(\n            in_channels, in_channels, kernel_size=kernel_size,\n            stride=stride, padding=padding, groups=in_channels, bias=False)\n        self.pointwise_conv2d = create_conv2d(\n            in_channels, out_channels, kernel_size=1, stride=1, padding=padding, groups=out_channels, bias=False)\n\n    def forward(self, x):\n        x = self.depthwise_conv2d(x)\n        x = self.pointwise_conv2d(x)\n        return x\n\n\nclass PNASNet5Large(nn.Module):\n    r\"\"\"\n    Constructs a PNASNet5Large model according to the specified configuration.\n\n   ",
  "\"\"\" Deep Layer Aggregation and DLA w/ Res2Net\nDLA original adapted from Official Pytorch impl at: https://github.com/ucbdrive/dla\nDLA Paper: `Deep Layer Aggregation` - https://arxiv.org/abs/1707.06484\n\nRes2Net additions from: https://github.com/gasvn/Res2Net/\nRes2Net Paper: `Res2Net: A New Multi-scale Backbone Architecture` - https://arxiv.org/abs/1904.01169\n\"\"\"\nimport math\nfrom typing import List, Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import create_classifier\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['DLA']\n\n\nclass DlaBasic(nn.Module):\n    \"\"\"DLA Basic\"\"\"\n\n    def __init__(self, num_channels, num_classes=1000, **kwargs):\n        super().__init__()\n        self.num_channels = num_channels\n        self.num_classes = num_classes\n        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=",
  "\"\"\"\nPoolformer from MetaFormer is Actually What You Need for Vision https://arxiv.org/abs/2111.11418\n\nIdentityFormer, RandFormer, PoolFormerV2, ConvFormer, and CAFormer\nfrom MetaFormer Baselines for Vision https://arxiv.org/abs/2210.13452\n\nAll implemented models support feature extraction and variable input resolution.\n\nOriginal implementation by Weihao Yu et al.,\nadapted for timm by Fredo Guan and Ross Wightman.\n\nAdapted from https://github.com/sail-sg/metaformer, original copyright below\n\"\"\"\n\n# Copyright 2022 Garena Online Private Limited\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport warnings\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom ...activations import ACT2FN\nfrom ...modeling_outputs import (\n    BaseModel",
  "from ._registry import *\n\nimport warnings\nwarnings.warn(f\"Importing from {__name__} is deprecated and will be removed in v0.12.0. Use {Registry.__name__} instead.\", FutureWarning)\n\nfrom .registry import Registry\nfrom .utils import get_all_modules, is_torch_available, is_vision_available\n\nif is_torch_available():\n    from . import torch\n\nif is_vision_available():\n    from . import vision\n\n__all__ = get_all_modules()\n",
  "from ._features_fx import *\n\nimport warnings\nwarnings.warn(f\"Importing from {__name__} is deprecated and will be removed in v0.13.0. Use {__name__}.feature_extraction instead.\",\n              FutureWarning)\n\nfrom .feature_extraction import *\n",
  "\"\"\" Pytorch Inception-V4 implementation\nSourced from https://github.com/Cadene/tensorflow-model-zoo.torch (MIT License) which is\nbased upon Google's Tensorflow implementation and pretrained weights (Apache 2.0 License)\n\"\"\"\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import create_classifier, ConvNormAct\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['InceptionV4']\n\n\nclass Mixed3a(nn.Module):\n    def __init__(self, conv_block=ConvNormAct):\n        super(Mixed3a, self).__init__()\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n        self.conv = conv_block(64, 96, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        x0 = self.maxpool(x)\n        x1 = self.conv(x0)\n        return torch.cat([x0, x1], dim=1)\n\n\nclass Mixed4a(nn.Module):\n    def __init__(self, conv_block=ConvNormAct):\n        super(Mixed4a, self).__init__()\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n        self.conv = conv_block(128, 128, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        x0 = self.max",
  "from ._hub import *\n\nimport warnings\nwarnings.warn(f\"Importing from {__name__} is deprecated and will be removed in v0.15.0. Use {__name__}.hub instead.\",\n              FutureWarning)\n\nfrom .hub import Hub, HubConfig, HubModel, HubModelConfig, HubPreTrainedModel\nfrom .utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n)\n\n__all__ = [\n    \"Hub\",\n    \"HubConfig\",\n    \"HubModel\",\n    \"HubModelConfig\",\n    \"HubPreTr",
  "\"\"\" PyTorch Feature Extraction Helpers\n\nA collection of classes, functions, modules to help extract features from models\nand provide a common interface for describing them.\n\nThe return_layers, module re-writing idea inspired by torchvision IntermediateLayerGetter\nhttps://github.com/pytorch/vision/blob/d88d8961ae51507d0cb680329d985b1488b1b76b/torchvision/models/_utils.py\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom collections import OrderedDict, defaultdict\nfrom copy import deepcopy\nfrom functools import partial\nfrom typing import Dict, List, Sequence, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nfrom timm.layers import Format\n\n\n__all__ = ['FeatureInfo', 'FeatureHooks', 'FeatureDictNet', 'FeatureListNet', 'FeatureHookNet']\n\n\nclass FeatureInfo:\n\n    def __init__(self, feature_info: List[Dict], out_indices: Tuple[int]):\n        prev_reduction = 1\n        for feature in feature_info:\n            if feature[\"reduction\"] != prev_reduction:\n                raise ValueError(f\"Reduction of {feature['reduction']} is not compatible with {prev_reduction}\")\n            prev_reduction = feature[\"reduction\"]\n        self.feature_info = feature_info\n        self.out_indices = out_indices\n\n    def __getitem__(self, key):\n        return self.feature_info[key]\n\n    def __len__(self):\n        return len(self.feature_info)\n\n    def __repr__(self):\n        return f\"FeatureInfo({self.feature_info}, {self.",
  "\"\"\" Global Context ViT\n\nFrom scratch implementation of GCViT in the style of timm swin_transformer_v2_cr.py\n\nGlobal Context Vision Transformers -https://arxiv.org/abs/2206.09959\n\n@article{hatamizadeh2022global,\n  title={Global Context Vision Transformers},\n  author={Hatamizadeh, Ali and Yin, Hongxu and Kautz, Jan and Molchanov, Pavlo},\n  journal={arXiv preprint arXiv:2206.09959},\n  year={2022}\n}\n\nFree of any code related to NVIDIA GCVit impl at https://github.com/NVlabs/GCVit.\nThe license for this code release is Apache 2.0 with no commercial restrictions.\n\nHowever, weight files adapted from NVIDIA GCVit impl ARE under a non-commercial share-alike license\n(https://creativecommons.org/licenses/by-nc-sa/4.0/) until I have a chance to train new ones...\n\nHacked together by / Copyright 2022 NVIDIA Corporation. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n",
  "\"\"\"Pytorch Densenet implementation w/ tweaks\nThis file is a copy of https://github.com/pytorch/vision 'densenet.py' (BSD-3-Clause) with\nfixed kwargs passthrough and addition of dynamic global avg/max pool.\n\"\"\"\nimport re\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as cp\nfrom torch.jit.annotations import List\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import BatchNormAct2d, get_norm_act_layer, BlurPool2d, create_classifier\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import MATCH_PREV_GROUP\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['DenseNet']\n\n\nclass DenseLayer(nn.Module):\n    def __init__(\n            self,\n            num_input_features,\n            growth_rate,\n            bn_size,\n            norm_layer=BatchNormAct2d,\n            drop_rate=0.0,\n            use_res_connect=False,\n            res_connect_num=0,\n            res_connect_input=None,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(bn_size)\n        self.conv1 = nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(bn_size * growth_rate)\n        self.relu = nn.ReLU(inplace=True)\n        self.norm2",
  "import os\nfrom typing import Any, Dict, Optional, Union\nfrom urllib.parse import urlsplit\n\nfrom timm.layers import set_layer_config\nfrom ._helpers import load_checkpoint\nfrom ._hub import load_model_config_from_hf\nfrom ._pretrained import PretrainedCfg\nfrom ._registry import is_model, model_entrypoint, split_model_name_tag\n\n\n__all__ = ['parse_model_name', 'safe_model_name', 'create_model']\n\n\ndef parse_model_name(model_name: str):\n    if model_name.startswith('hf_hub'):\n        # NOTE for backwards compat, deprecate hf_hub use\n        model_name = model_name.replace('hf_hub', 'hf-hub')\n    parsed = urlsplit(model_name)\n    assert parsed.scheme in ('', 'timm', 'hf-hub')\n    if parsed.scheme == 'hf-hub':\n        # FIXME may use fragment as revision, currently `@` in URI path\n        return parsed.scheme, parsed.path\n    else:\n        model_name = os.path.split(parsed.path)[-1]\n        return 'timm', model_name\n\n\ndef safe_model_name(model_name: str, config: Optional[PretrainedCfg] = None) -> str:\n    \"\"\"\n    Returns a safe model name.\n\n    This function is intended to be used in a context manager.\n\n    Args:\n        model_name: The name of the model.\n        config: The configuration of the model.\n\n    Examples:\n        >>> with safe_model_name('timm-small-patch4-window7') as model_name:\n        ...     print(model_name)\n        timm-small-patch4-window7\n   ",
  "import copy\nfrom collections import deque, defaultdict\nfrom dataclasses import dataclass, field, replace, asdict\nfrom typing import Any, Deque, Dict, Tuple, Optional, Union\n\n\n__all__ = ['PretrainedCfg', 'filter_pretrained_cfg', 'DefaultCfg']\n\n\n@dataclass\nclass PretrainedCfg:\n    \"\"\"\n    \"\"\"\n    # weight source locations\n    url: Optional[Union[str, Tuple[str, str]]] = None  # remote URL\n    file: Optional[str] = None  # local / shared filesystem path\n    state_dict: Optional[Dict[str, Any]] = None  # in-memory state dict\n    hf_hub_id: Optional[str] = None  # Hugging Face Hub model id ('organization/model')\n    hf_hub_filename: Optional[str] = None  # Hugging Face Hub filename (overrides default)\n\n    source: Optional[str] = None  # source of cfg / weight location used (url, file, hf-hub)\n    architecture: Optional[str] = None  # architecture variant can be set when not implicit\n    tag: Optional[str] = None  # pretrained tag of source\n    custom_name: Optional[str] = None  # custom name for the pretrained model\n    custom_name_prefix: Optional[str] = None  # prefix for the custom name\n    custom_name_suffix: Optional[str] = None  # suffix for the custom name\n    custom_name_suffix_prefix: Optional[str] = None  # prefix for the custom name suffix\n    custom_name_suffix_suffix: Optional[str] = None  # suffix for the custom name suffix\n    custom_name_suffix_suffix_prefix: Optional[str] = None  # prefix for the custom name suffix suffix",
  "\"\"\" Model Registry\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport fnmatch\nimport re\nimport sys\nimport warnings\nfrom collections import defaultdict, deque\nfrom copy import deepcopy\nfrom dataclasses import replace\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Set, Sequence, Union, Tuple\n\nfrom ._pretrained import PretrainedCfg, DefaultCfg\n\n__all__ = [\n    'split_model_name_tag', 'get_arch_name', 'register_model', 'generate_default_cfgs',\n    'list_models', 'list_pretrained', 'is_model', 'model_entrypoint', 'list_modules', 'is_model_in_modules',\n    'get_pretrained_cfg_value', 'is_model_pretrained'\n]\n\n_module_to_models: Dict[str, Set[str]] = defaultdict(set)  # dict of sets to check membership of model in module\n_model_to_module: Dict[str, str] = {}  # mapping of model names to module names\n_model_entrypoints: Dict[str, Callable[..., Any]] = {}  # mapping of model names to model entrypoints\n_module_to_modules: Dict[str, Set[str]] = defaultdict(set)  # dict of sets to check membership of model in module\n_module_to_modules_list: Dict[str, List[str]] = defaultdict(list)  # dict of lists of module names\n_module_to_modules_list_list: Dict[str, List[List[str]]] = defaultdict(list)  # dict of lists of lists of module names\n_module_to_modules_list_list_list: Dict[",
  "\"\"\" Transformer in Transformer (TNT) in PyTorch\n\nA PyTorch implement of TNT as described in\n'Transformer in Transformer' - https://arxiv.org/abs/2103.00112\n\nThe official mindspore code is released and available at\nhttps://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/TNT\n\"\"\"\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import Mlp, DropPath, trunc_normal_, _assert, to_2tuple\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model\nfrom .vision_transformer import resize_pos_embed\n\n__all__ = ['TNT']  # model_registry will add each entrypoint fn to this\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7), 'conv_dim': 64, 'depths': [2, 2, 6, 2],\n        'heads': [1, 1, 16, 1], 'mlp_ratio': 4., 'dropout': 0.1, 'attention_dropout': 0.1, 'activation': 'relu',\n        'use_checkpoint': False, **kwargs\n    }\n\n\ndef _make_divisible(v, divisor, min_value=None):\n   ",
  "\"\"\" ResNeSt Models\n\nPaper: `ResNeSt: Split-Attention Networks` - https://arxiv.org/abs/2004.08955\n\nAdapted from original PyTorch impl w/ weights at https://github.com/zhanghang1989/ResNeSt by Hang Zhang\n\nModified for torchscript compat, and consistency with timm by Ross Wightman\n\"\"\"\nfrom torch import nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import SplitAttn\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\nfrom .resnet import ResNet\n\n\nclass ResNestBottleneck(nn.Module):\n    \"\"\"ResNet Bottleneck\n    \"\"\"\n    # pylint: disable=unused-argument\n    expansion = 4\n\n    def __init__(\n            self,\n            inplanes,\n            planes,\n            stride=1,\n            downsample=None,\n            radix=1,\n            cardinality=1,\n            base_width=64,\n            avd=False,\n            avd_first=False,\n            avd_last=False,\n            avd_stride=1,\n            avd_out_stride=1,\n            avd_out_channels=128,\n            avd_out_width=128,\n            avd_out_height=128,\n            avd_out_size=128,\n            avd_out_size_stride=1,\n            avd_out_size_channels=128,\n            avd_out_size_width=128,\n            avd_out_size_height=1",
  "\"\"\" EfficientNet, MobileNetV3, etc Builder\n\nAssembles EfficieNet and related network feature blocks from string definitions.\nHandles stride, dilation calculations, and selects feature extraction points.\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\n\nimport logging\nimport math\nimport re\nfrom copy import deepcopy\nfrom functools import partial\n\nimport torch.nn as nn\n\nfrom ._efficientnet_blocks import *\nfrom timm.layers import CondConv2d, get_condconv_initializer, get_act_layer, get_attn, make_divisible\n\n__all__ = [\"EfficientNetBuilder\", \"decode_arch_def\", \"efficientnet_init_weights\",\n           'resolve_bn_args', 'resolve_act_layer', 'round_channels', 'BN_MOMENTUM_TF_DEFAULT', 'BN_EPS_TF_DEFAULT']\n\n_logger = logging.getLogger(__name__)\n\n\n_DEBUG_BUILDER = False\n\n# Defaults used for Google/Tensorflow training of mobile networks /w RMSprop as per\n# papers and TF reference implementations. PT momentum equivelant of these defaults is\n# 0.9, 0.999, 0.999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999",
  "\"\"\" \nCoaT architecture.\n\nPaper: Co-Scale Conv-Attentional Image Transformers - https://arxiv.org/abs/2104.06399\n\nOfficial CoaT code at: https://github.com/mlpc-ucsd/CoaT\n\nModified from timm/models/vision_transformer.py\n\"\"\"\nfrom functools import partial\nfrom typing import Tuple, List, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import PatchEmbed, Mlp, DropPath, to_2tuple, trunc_normal_, _assert, LayerNorm\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['CoaT']\n\n\nclass ConvRelPosEnc(nn.Module):\n    \"\"\" Convolutional relative position encoding. \"\"\"\n    def __init__(self, head_chs, num_heads, window):\n        \"\"\"\n        Initialization.\n            Ch: Channels of the input feature map.\n            H: Number of heads.\n            W: Window size.\n        \"\"\"\n        super().__init__()\n        self.window = window\n        self.embed_dim = head_chs\n        self.num_heads = num_heads\n\n        # Positional encoding\n        pe = torch.zeros(self.num_heads, self.embed_dim, self.window, self.window)\n        position = torch.arange(self.window, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, self.embed_dim, 2",
  "\"\"\" Twins\nA PyTorch impl of : `Twins: Revisiting the Design of Spatial Attention in Vision Transformers`\n    - https://arxiv.org/pdf/2104.13840.pdf\n\nCode/weights from https://github.com/Meituan-AutoML/Twins, original copyright/license info below\n\n\"\"\"\n# --------------------------------------------------------\n# Twins\n# Copyright (c) 2021 Meituan\n# Licensed under The Apache 2.0 License [see LICENSE for details]\n# Written by Xinjie Li, Xiangxiang Chu\n# --------------------------------------------------------\nimport math\nfrom functools import partial\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import Mlp, DropPath, to_2tuple, trunc_normal_, use_fused_attn\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_module\nfrom ._registry import register_model\n\n\n# TODO: add support for masking\n# TODO: add support for batch norm\n# TODO: add support for activation\n# TODO: add support for positional encoding\n# TODO: add support for attention heads\n# TODO: add support for different attention heads\n# TODO: add support for different attention heads\n# TODO: add support for different attention heads\n# TODO: add support for different attention heads\n# TODO: add support for different attention heads\n# TODO: add support for different attention heads\n# TODO: add support for different attention heads\n# TODO: add support for different attention heads\n# TODO",
  "import collections.abc\nimport math\nimport re\nfrom collections import defaultdict\nfrom itertools import chain\nfrom typing import Any, Callable, Dict, Iterator, Tuple, Type, Union\n\nimport torch\nfrom torch import nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\n__all__ = ['model_parameters', 'named_apply', 'named_modules', 'named_modules_with_params', 'adapt_input_conv',\n           'group_with_matcher', 'group_modules', 'group_parameters', 'flatten_modules', 'checkpoint_seq']\n\n\ndef model_parameters(model: nn.Module, exclude_head: bool = False):\n    if exclude_head:\n        # FIXME this a bit of a quick and dirty hack to skip classifier head params based on ordering\n        return [p for p in model.parameters()][:-2]\n    else:\n        return model.parameters()\n\n\ndef named_apply(\n        fn: Callable,\n        module: nn.Module, name='',\n        depth_first: bool = True,\n        include_root: bool = False,\n) -> nn.Module:\n    if not depth_first and include_root:\n        fn(module=module, name=name)\n    for name, child in module.named_children():\n        named_apply(fn, child, name=name, depth_first=depth_first, include_root=include_root)\n    return module\n\n\ndef named_modules(module: nn.Module, exclude_head: bool = False) -> Iterator[Tuple[str, nn.Module]]:\n    if exclude_head:\n        # FIXME this a bit of a quick and dirty hack to skip classifier head params based on ordering\n        for name, child in module.named_children()[2:]:\n            yield name, child\n   ",
  "\"\"\"\nSEResNet implementation from Cadene's pretrained models\nhttps://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py\nAdditional credit to https://github.com/creafz\n\nOriginal model: https://github.com/hujie-frank/SENet\n\nResNet code gently borrowed from\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n\nFIXME I'm deprecating this model and moving them to ResNet as I don't want to maintain duplicate\nsupport for extras like dilation, switchable BN/activations, feature extraction, etc that don't exist here.\n\"\"\"\nimport math\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import create_classifier\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = [\n    \"SEResNet\",\n    \"seresnet101_32x4d\",\n    \"seresnet101_32x4d_seresnet\",\n    \"seresnet101_32x4d_seresnet_seresnet\",\n    \"seresnet101_32x4d_seresnet_seresnet_seresnet\",\n    \"seresnet101_32x4d_seresnet_seresnet_seresnet_seresnet\",\n    \"seresnet",
  "from functools import partial\n\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom ._builder import build_model_with_cfg\nfrom ._builder import pretrained_cfg_for_features\nfrom ._efficientnet_blocks import SqueezeExcite\nfrom ._efficientnet_builder import decode_arch_def, resolve_act_layer, resolve_bn_args, round_channels\nfrom ._registry import register_model, generate_default_cfgs\nfrom .mobilenetv3 import MobileNetV3, MobileNetV3Features\n\n__all__ = []  # model_registry will add each entrypoint fn to this\n\n\ndef _gen_hardcorenas(pretrained, variant, arch_def, **kwargs):\n    \"\"\"Creates a hardcorenas model\n\n    Ref impl: https://github.com/Alibaba-MIIL/HardCoReNAS\n    Paper: https://arxiv.org/abs/2102.11646\n\n    \"\"\"\n    num_features = 1280\n    se_layer = partial(SqueezeExcite, gate_layer='conv1', reduction='mean')\n    act_layer = partial(resolve_act_layer, inplace=True)\n    bn_args = partial(resolve_bn_args, eps=1e-05, momentum=0.1, affine=True)\n    round_channels = partial(round_channels, channels_in=1280, channels_out=1280)\n\n    if variant == 'hardcorenas':\n        model = MobileNetV3(\n            pretrained=pretrained,\n            num_features=num_features,\n            se_layer=se_layer,\n            act_layer",
  "\"\"\" Vision Transformer (ViT) in PyTorch\n\nA PyTorch implement of Vision Transformers as described in:\n\n'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale'\n    - https://arxiv.org/abs/2010.11929\n\n`How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers`\n    - https://arxiv.org/abs/2106.10270\n\n`FlexiViT: One Model for All Patch Sizes`\n    - https://arxiv.org/abs/2212.08013\n\nThe official jax code is released and available at\n  * https://github.com/google-research/vision_transformer\n  * https://github.com/google-research/big_vision\n\nAcknowledgments:\n  * The paper authors for releasing code and weights, thanks!\n  * I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch\n  * Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/char-rnn\n\n## Overview\n\nThe Vision Transformer (ViT) architecture is a general vision transformer architecture that can be used to\nextract features from any image. The architecture is based on the following paper:\n\n'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale'\n    - https://arxiv.org/abs/2010.11929\n\nThe architecture consists of a set of *patch-wise* convolutional layers, a set of *position-wise*\nattention layers, and a set of *self",
  "\"\"\" Hybrid Vision Transformer (ViT) in PyTorch\n\nA PyTorch implement of the Hybrid Vision Transformers as described in:\n\n'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale'\n    - https://arxiv.org/abs/2010.11929\n\n`How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers`\n    - https://arxiv.org/abs/2106.10270\n\nNOTE These hybrid model definitions depend on code in vision_transformer.py.\nThey were moved here to keep file sizes sane.\n\nHacked together by / Copyright 2020, Ross Wightman\n\"\"\"\nfrom functools import partial\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import StdConv2dSame, StdConv2d, to_2tuple\nfrom ._registry import generate_default_cfgs, register_model, register_transformer\n\n\n# Copied from https://github.com/facebookresearch/detr/blob/master/models/detr/detr_resnet.py\ndef conv_same(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n        nn.BatchNorm2d(out_channels),\n        nn.",
  "\"\"\" EfficientFormer-V2\n\n@article{\n    li2022rethinking,\n    title={Rethinking Vision Transformers for MobileNet Size and Speed},\n    author={Li, Yanyu and Hu, Ju and Wen, Yang and Evangelidis, Georgios and Salahi, Kamyar and Wang, Yanzhi and Tulyakov, Sergey and Ren, Jian},\n    journal={arXiv preprint arXiv:2212.08059},\n    year={2022}\n}\n\nSignificantly refactored and cleaned up for timm from original at: https://github.com/snap-research/EfficientFormer\n\nOriginal code licensed Apache 2.0, Copyright (c) 2022 Snap Inc.\n\nModifications and timm support by / Copyright 2023, Ross Wightman\n\"\"\"\nimport math\nfrom functools import partial\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.models.efficientformer import EfficientFormerConfig, EfficientFormerForImageClassification\nfrom timm.models.efficientformer.modeling_efficientformer import EfficientFormerForImageClassificationV2\nfrom timm.models.efficientformer.utils import (\n    EfficientFormerImageProcessor,\n    get_image_size,\n    make_divisible,\n    replace_return_dict,\n)\nfrom timm.utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\nEfficientFormerV2_PRETRAINED_MODEL_ARCHIVE_LIST = [",
  "\"\"\" Normalization Free Nets. NFNet, NF-RegNet, NF-ResNet (pre-activation) Models\n\nPaper: `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n    - https://arxiv.org/abs/2101.08692\n\nPaper: `High-Performance Large-Scale Image Recognition Without Normalization`\n    - https://arxiv.org/abs/2102.06171\n\nOfficial Deepmind JAX code: https://github.com/deepmind/deepmind-research/tree/master/nfnets\n\nStatus:\n* These models are a work in progress, experiments ongoing.\n* Pretrained weights for two models so far, more to come.\n* Model details updated to closer match official JAX code now that it's released\n* NF-ResNet, NF-RegNet-B, and NFNet-F models supported\n\nHacked together by / copyright Ross Wightman, 2021.\n\"\"\"\nfrom collections import OrderedDict\nfrom dataclasses import dataclass, replace\nfrom functools import partial\nfrom typing import Callable, Tuple, Optional\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom flax import linen as nn\nfrom flax.linen import flatten_dict, unflatten_dict\nfrom flax.traverse_util import flatten_dict_structure\n\nfrom transformers import (\n    ACT2FN,\n    PreTrainedModel,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPastAndCrossAttentions",
  "\"\"\" Relative Position Vision Transformer (ViT) in PyTorch\n\nNOTE: these models are experimental / WIP, expect changes\n\nHacked together by / Copyright 2022, Ross Wightman\n\"\"\"\nimport logging\nimport math\nfrom functools import partial\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom torch.jit import Final\nfrom torch.utils.checkpoint import checkpoint\n\nfrom timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import PatchEmbed, Mlp, DropPath, RelPosMlp, RelPosBias, use_fused_attn\nfrom ._builder import build_model_with_cfg\nfrom ._registry import generate_default_cfgs, register_model\n\n__all__ = ['VisionTransformerRelPos']  # model_registry will add each entrypoint fn to this\n\n_logger = logging.getLogger(__name__)\n\n\nclass RelPosAttention(nn.Module):\n    fused_attn: Final[bool]\n\n    def __init__(\n            self,\n            dim,\n            num_heads=8,\n            qkv_bias=False,\n            qk_scale=None,\n            attn_drop=0.0,\n            proj_drop=0.0,\n            fused_attn=False,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.qkv_bias = qkv_bias\n        self.qk_scale = qk_scale\n        self.attn_drop = attn_drop\n        self.proj_drop = proj_drop\n        self.fused_attn = fused_attn\n\n        self.q_proj = nn.",
  "\"\"\" Nested Transformer (NesT) in PyTorch\n\nA PyTorch implement of Aggregating Nested Transformers as described in:\n\n'Aggregating Nested Transformers'\n    - https://arxiv.org/abs/2105.12723\n\nThe official Jax code is released and available at https://github.com/google-research/nested-transformer. The weights\nhave been converted with convert/convert_nest_flax.py\n\nAcknowledgments:\n* The paper authors for sharing their research, code, and model weights\n* Ross Wightman's existing code off which I based this\n\nCopyright 2021 Alexander Soare\n\"\"\"\n\nimport collections.abc\nimport logging\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import PatchEmbed, Mlp, DropPath, create_classifier, trunc_normal_, _assert\nfrom timm.layers import create_conv2d, create_pool2d, to_channel_dimension_format\nfrom timm.models.layers import GroupNorm, get_initializer\nfrom timm.models.utils import apply_chunking, apply_dropout, find_pruneable_heads_and_indices, prune_linear_layer\nfrom timm.utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\nfrom timm.utils.misc import replace_return_docstrings\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC =",
  "\"\"\" FocalNet\n\nAs described in `Focal Modulation Networks` - https://arxiv.org/abs/2203.11926\n\nSignificant modifications and refactoring from the original impl at https://github.com/microsoft/FocalNet\n\nThis impl is/has:\n* fully convolutional, NCHW tensor layout throughout, seemed to have minimal performance impact but more flexible\n* re-ordered downsample / layer so that striding always at beginning of layer (stage)\n* no input size constraints or input resolution/H/W tracking through the model\n* torchscript fixed and a number of quirks cleaned up\n* feature extraction support via `features_only=True`\n\"\"\"\n# --------------------------------------------------------\n# FocalNets -- Focal Modulation Networks\n# Copyright (c) 2022 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Jianwei Yang (jianwyan@microsoft.com)\n# --------------------------------------------------------\nfrom functools import partial\nfrom typing import Callable, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.models.layers import BatchNorm2d, GroupNorm, GroupNorm2d, Mish, SyncBatchNorm\nfrom timm.models.utils import apply_activation, find_pruneable_heads_and_indices, prune_linear_layer\nfrom timm.models.vision_transformer import VisionTransformer\nfrom timm.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging\n\n\nlogger = logging.get_logger(__name__)",
  "\"\"\" RepViT\n\nPaper: `RepViT: Revisiting Mobile CNN From ViT Perspective`\n    - https://arxiv.org/abs/2307.09283\n\n@misc{wang2023repvit,\n      title={RepViT: Revisiting Mobile CNN From ViT Perspective}, \n      author={Ao Wang and Hui Chen and Zijia Lin and Hengjun Pu and Guiguang Ding},\n      year={2023},\n      eprint={2307.09283},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\nAdapted from official impl at https://github.com/jameslahm/RepViT\n\"\"\"\n\n__all__ = ['RepViT']\n\nimport torch.nn as nn\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom ._registry import register_model, generate_default_cfgs\nfrom ._builder import build_model_with_cfg\nfrom timm.layers import SqueezeExcite, trunc_normal_, to_ntuple, unfold\nfrom timm.models.layers import GroupNorm, InstanceNorm, PixelShuffle\n\n\nclass RepViT(nn.Module):\n    r\"\"\"\n    Constructs a RepViT model according to the specified configuration. See the\n    documentation for the [`RepViTConfig`] class for more information.\n\n    Args:\n        backbone (`RepViTBackbone`):\n            The backbone network.\n        head (`RepViTHead`):\n            The head network.\n        num_labels (`int`, *optional*, defaults to 1000):\n            The number of output labels.\n\n    Examples:\n\n    ```python\n   ",
  "\"\"\" HRNet\n\nCopied from https://github.com/HRNet/HRNet-Image-Classification\n\nOriginal header:\n  Copyright (c) Microsoft\n  Licensed under the MIT License.\n  Written by Bin Xiao (Bin.Xiao@microsoft.com)\n  Modified by Ke Sun (sunk@mail.ustc.edu.cn)\n\"\"\"\nimport logging\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import create_classifier\nfrom ._builder import build_model_with_cfg, pretrained_cfg_for_features\nfrom ._features import FeatureInfo\nfrom ._registry import register_model, generate_default_cfgs\nfrom .resnet import BasicBlock, Bottleneck  # leveraging ResNet block_types w/ additional features like SE\n\n__all__ = ['HighResolutionNet', 'HighResolutionNetFeatures']  # model_registry will add each entrypoint fn to this\n\n_BN_MOMENTUM = 0.1\n_logger = logging.getLogger(__name__)\n\n# TODO: add support for other block types\nblock_types = {\n    \"BasicBlock\": BasicBlock,\n    \"Bottleneck\": Bottleneck,\n}\n\n# TODO: add support for other block types\nblock_types_dict = {v: k for k, v in block_types.items()}\n\n# TODO: add support for other block types\nblock_types_dict_rev = {v: k for k, v in block_types_dict.items()}\n\n\nclass HighResolutionNet(nn.Module):\n    def __init__(self, cfg,",
  "\"\"\" Sequencer\n\nPaper: `Sequencer: Deep LSTM for Image Classification` - https://arxiv.org/pdf/2205.01972.pdf\n\n\"\"\"\n#  Copyright (c) 2022. Yuki Tatsunami\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n\nimport math\nfrom functools import partial\nfrom itertools import accumulate\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, DEFAULT_CROP_PCT\nfrom timm.layers import lecun_normal_, DropPath, Mlp, PatchEmbed, ClassifierHead\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import named_apply\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['Sequencer2d']  # model_registry will add each entrypoint fn to this\n\n\ndef _init_weights(module: nn.Module, name: str, head_bias: float = 0.0) -> None:\n    if name == 'patch_embed.proj':\n        nn.init.constant_(module.weight, 0.0)\n        nn.init.constant_(module.bias, head_bias)\n    elif name == 'patch_embed.norm':\n        nn.init.constant_(module.weight, 1.0)\n        nn.init.constant_(module.bias, 0.0)\n    elif name == 'pos_embed':\n        nn.init.constant_(module.weight, 0.0)\n        nn.init.constant_(module.",
  "\"\"\"\nTResNet: High Performance GPU-Dedicated Architecture\nhttps://arxiv.org/pdf/2003.13630.pdf\n\nOriginal model: https://github.com/mrT23/TResNet\n\n\"\"\"\nfrom collections import OrderedDict\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.layers import SpaceToDepth, BlurPool2d, ClassifierHead, SEModule,\\\n    ConvNormActAa, ConvNormAct, DropPath\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs, register_model_deprecations\n\n__all__ = ['TResNet']  # model_registry will add each entrypoint fn to this\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(\n            self,\n            inplanes,\n            planes,\n            stride=1,\n            downsample=None,\n            use_se=True,\n            aa_layer=None,\n            drop_path_rate=0.\n    ):\n        super(BasicBlock, self).__init__()\n        self.conv1 = ConvNormAct(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = ConvNormAct(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.shortcut = ConvNormAct(inplanes, planes, kernel_size=1, stride=stride, bias=False) if downsample is None else None",
  "# NOTE timm.models.layers is DEPRECATED, please use timm.layers, this is here to reduce breakages in transition\nfrom timm.layers.activations import *\nfrom timm.layers.adaptive_avgmax_pool import \\\n    adaptive_avgmax_pool2d, select_adaptive_pool2d, AdaptiveAvgMaxPool2d, SelectAdaptivePool2d\nfrom timm.layers.attention_pool2d import AttentionPool2d, RotAttentionPool2d, RotaryEmbedding\nfrom timm.layers.blur_pool import BlurPool2d\nfrom timm.layers.classifier import ClassifierHead, create_classifier\nfrom timm.layers.cond_conv2d import CondConv2d, get_condconv_initializer\nfrom timm.layers.config import is_exportable, is_scriptable, is_no_jit, set_exportable, set_scriptable, set_no_jit,\\\n    set_layer_config\nfrom timm.layers.conv2d_same import Conv2dSame, conv2d_same\nfrom timm.layers.conv_bn_act import ConvNormAct, conv_bn_act\nfrom timm.layers.deconv2d import Deconv2d, deconv2d\nfrom timm.layers.dropout import DropPath, get_dropout_schedule\nfrom timm.layers.group_norm import GroupNorm\nfrom timm.layers.layer_norm import LayerNorm\nfrom timm.layers.mask_pool import MaskPool2d\nfrom timm.layers.multi_head_attn import MultiHeadAttention, RotaryMultiHeadAttention\nfrom timm.layers.norm import get_norm\nfrom timm.layers.patch_",
  "\"\"\" Squeeze-and-Excitation Channel Attention\n\nAn SE implementation originally based on PyTorch SE-Net impl.\nHas since evolved with additional functionality / configuration.\n\nPaper: `Squeeze-and-Excitation Networks` - https://arxiv.org/abs/1709.01507\n\nAlso included is Effective Squeeze-Excitation (ESE).\nPaper: `CenterMask : Real-Time Anchor-Free Instance Segmentation` - https://arxiv.org/abs/1911.06667\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nfrom torch import nn as nn\n\nfrom .create_act import create_act_layer\nfrom .helpers import make_divisible\n\n\nclass SEModule(nn.Module):\n    \"\"\" SE Module as defined in original SE-Nets with a few additions\n    Additions include:\n        * divisor can be specified to keep channels % div == 0 (default: 8)\n        * reduction channels can be specified directly by arg (if rd_channels is set)\n        * reduction channels can be specified by float rd_ratio (default: 16)\n        * reduction channels can be specified by float rd_ratio (default: 16)\n        * reduction channels can be specified by float rd_ratio (default: 16)\n        * reduction channels can be specified by float rd_ratio (default: 16)\n        * reduction channels can be specified by float rd_ratio (default: 16)\n        * reduction channels can be specified by float rd_ratio (default: 16)\n        * reduction channels can be specified by float rd_ratio (default: 16)\n        * reduction channels can be specified by float rd_ratio",
  "from .activations import *\nfrom .adaptive_avgmax_pool import \\\n    adaptive_avgmax_pool2d, select_adaptive_pool2d, AdaptiveAvgMaxPool2d, SelectAdaptivePool2d\nfrom .attention_pool2d import AttentionPool2d, RotAttentionPool2d, RotaryEmbedding\nfrom .blur_pool import BlurPool2d\nfrom .classifier import ClassifierHead, create_classifier, NormMlpClassifierHead\nfrom .cond_conv2d import CondConv2d, get_condconv_initializer\nfrom .config import is_exportable, is_scriptable, is_no_jit, use_fused_attn, \\\n    set_exportable, set_scriptable, set_no_jit, set_layer_config, set_fused_attn\nfrom .conv2d_same import Conv2dSame, conv2d_same\nfrom .conv_bn_act import ConvNormAct, ConvNormActAa, ConvBnAct\nfrom .create_act import create_act_layer, get_act_layer, get_act_fn\nfrom .create_attn import get_attn, get_rot_attn, get_rotary_attn\nfrom .create_layer import create_layer, get_layer\nfrom .create_norm import create_norm_layer, get_norm_layer\nfrom .create_pool import create_pool_layer, get_pool_layer\nfrom .create_proj import create_proj_layer, get_proj_layer\nfrom .create_res_layer import create_res_layer, get_res_layer\nfrom .create_shortcut import create_shortcut_layer, get_shortcut_layer\nfrom .create_upsample import create_",
  "\"\"\"\nBlurPool layer inspired by\n - Kornia's Max_BlurPool2d\n - Making Convolutional Networks Shift-Invariant Again :cite:`zhang2019shiftinvar`\n\nHacked together by Chris Ha and Ross Wightman\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom .padding import get_padding\n\n\nclass BlurPool2d(nn.Module):\n    r\"\"\"Creates a module that computes blurs and downsample a given feature map.\n    See :cite:`zhang2019shiftinvar` for more details.\n    Corresponds to the Downsample class, which does blurring and subsampling\n\n    Args:\n        channels = Number of input channels\n        filt_size (int): binomial filter size for blurring. currently supports 3 (default) and 5.\n        stride (int): downsampling filter stride\n\n    Returns:\n        torch.Tensor: the transformed tensor.\n    \"\"\"\n    def __init__(self, channels, filt_size=3, stride=2) -> None:\n        super(BlurPool2d, self).__init__()\n        assert filt_size > 1, \"Filter size must be greater than 1\"\n        self.channels = channels\n        self.filt_size = filt_size\n        self.stride = stride\n        self.padding = get_padding(self.filt_size, self.stride)\n        self.conv = nn.Conv2d(self.channels, self.channels, kernel_size=self.filt_size, stride=self.stride, padding=self.padding)\n\n    def forward(self, x):\n        x = F.pad(x, self.padding, mode=\"constant\", value=0)\n        x = self.conv",
  "\"\"\" Model / Layer Config singleton state\n\"\"\"\nimport os\nimport warnings\nfrom typing import Any, Optional\n\nimport torch\n\n__all__ = [\n    'is_exportable', 'is_scriptable', 'is_no_jit', 'use_fused_attn',\n    'set_exportable', 'set_scriptable', 'set_no_jit', 'set_layer_config', 'set_fused_attn'\n]\n\n# Set to True if prefer to have layers with no jit optimization (includes activations)\n_NO_JIT = False\n\n# Set to True if prefer to have activation layers with no jit optimization\n# NOTE not currently used as no difference between no_jit and no_activation jit as only layers obeying\n# the jit flags so far are activations. This will change as more layers are updated and/or added.\n_NO_ACTIVATION_JIT = False\n\n# Set to True if exporting a model with Same padding via ONNX\n_EXPORTABLE = False\n\n# Set to True if wanting to use torch.jit.script on a model\n_SCRIPTABLE = False\n\n\n# use torch.scaled_dot_product_attention\nuse_fused_attn = True\n\n\ndef is_exportable(exportable: Optional[bool] = None) -> bool:\n    \"\"\"\n    Get the exportable state of the model.\n\n    Args:\n        exportable (bool, optional): If provided, will return the exportable state of the model.\n\n    Returns:\n        bool: True if exportable, False otherwise.\n    \"\"\"\n    if exportable is None:\n        return _EXPORTABLE\n    else:\n        _EXPORTABLE = exportable\n        return exportable\n\n\ndef is_scriptable(scriptable: Optional[bool] = None",
  "\"\"\" Linear layer (alternate definition)\n\"\"\"\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn as nn\n\n\nclass Linear(nn.Linear):\n    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n\n    Wraps torch.nn.Linear to support AMP + torchscript usage by manually casting\n    weight & bias to input.dtype to work around an issue w/ torch.addmm in this\n    case.\n\n    Args:\n        in_features (int): Size of each input sample.\n        out_features (int): Size of each output sample.\n        bias (bool, optional): If ``True``, adds a bias to the output. Default: ``True``\n        a_scale (float, optional): Scale factor for the first linear layer. Default: ``1.0``\n        b_scale (float, optional): Scale factor for the second linear layer. Default: ``1.0``\n        a_zero_point (int, optional): Zero point for the first linear layer. Default: ``0``\n       ",
  "\"\"\" Attention Factory\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport torch\nfrom functools import partial\n\nfrom .bottleneck_attn import BottleneckAttn\nfrom .cbam import CbamModule, LightCbamModule\nfrom .eca import EcaModule, CecaModule\nfrom .gather_excite import GatherExcite\nfrom .global_context import GlobalContext\nfrom .halo_attn import HaloAttn\nfrom .lambda_layer import LambdaLayer\nfrom .non_local_attn import NonLocalAttn, BatNonLocalAttn\nfrom .selective_kernel import SelectiveKernel\nfrom .split_attn import SplitAttn\nfrom .squeeze_excite import SEModule, EffectiveSEModule\n\n\ndef get_attn(attn_type):\n    if isinstance(attn_type, torch.nn.Module):\n        return attn_type\n    module_cls = None\n    if attn_type:\n        if isinstance(attn_type, str):\n            attn_type = attn_type.lower()\n            # Lightweight attention modules (channel and/or coarse spatial).\n            # Typically added to the model after the last convolutional layer.\n            if attn_type in [\"lightweight\", \"lightweight_channel\", \"lightweight_spatial\"]:\n                module_cls = LightCbamModule\n            # Channel attention modules (channel and/or coarse spatial).\n            # Typically added to the model after the last convolutional layer.\n            elif attn_type in [\"channel\", \"channel_channel\", \"channel_spatial\"]:\n                module_cls = CbamModule\n            # Channel attention modules (channel and/or coarse spatial).\n            # Typically added to the model after the last convolutional layer.\n            elif at",
  "\"\"\" Sin-cos, fourier, rotary position embedding modules and functions\n\nHacked together by / Copyright 2022 Ross Wightman\n\"\"\"\nimport math\nfrom typing import List, Tuple, Optional, Union\n\nimport torch\nfrom torch import nn as nn\n\nfrom .trace_utils import _assert\n\n\ndef pixel_freq_bands(\n        num_bands: int,\n        max_freq: float = 224.,\n        linear_bands: bool = True,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[torch.device] = None,\n):\n    if linear_bands:\n        bands = torch.linspace(1.0, max_freq / 2, num_bands, dtype=dtype, device=device)\n    else:\n        bands = 2 ** torch.linspace(0, math.log(max_freq, 2) - 1, num_bands, dtype=dtype, device=device)\n    return bands * torch.pi\n\n\ndef freq_bands(\n        num_bands: int,\n        temperature: float = 10000.,\n        step: int = 2,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[torch.device] = None,\n):\n    if temperature == 10000.:\n        bands = torch.linspace(1.0, 224.0, num_bands, dtype=dtype, device=device)\n    else:\n        bands = torch.linspace(1.0, 224.0 / temperature, num_bands, dtype=dtype, device=device)\n        bands = bands * torch.pi / (2 * temperature)\n    return bands\n\n\ndef sin_cos_pos_emb(",
  "\"\"\" Activation Factory\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom typing import Union, Callable, Type\n\nfrom .activations import *\nfrom .activations_jit import *\nfrom .activations_me import *\nfrom .config import is_exportable, is_scriptable, is_no_jit\n\n# PyTorch has an optimized, native 'silu' (aka 'swish') operator as of PyTorch 1.7.\n# Also hardsigmoid, hardswish, and soon mish. This code will use native version if present.\n# Eventually, the custom SiLU, Mish, Hard*, layers will be removed and only native variants will be used.\n_has_silu = 'silu' in dir(torch.nn.functional)\n_has_hardswish = 'hardswish' in dir(torch.nn.functional)\n_has_hardsigmoid = 'hardsigmoid' in dir(torch.nn.functional)\n_has_mish = 'mish' in dir(torch.nn.functional)\n\n\n_ACT_FN_DEFAULT = dict(\n    silu=F.silu,\n    hardswish=F.hardswish,\n    hardsigmoid=F.hardsigmoid,\n    mish=F.mish,\n)\n\n_ACT_FN_MAP = dict(\n    silu=_ACT_FN_DEFAULT['silu'],\n    hardswish=_ACT_FN_DEFAULT['hardswish'],\n    hardsigmoid=_ACT_FN_DEFAULT['hardsigmoid'],\n    mish=_ACT_FN_DEFAULT['mish'],\n)\n\n_ACT_FN_JIT_MAP = dict(\n    silu=_ACT_FN_JIT_DEFAULT['silu'],",
  "\"\"\" Split Attention Conv2d (for ResNeSt Models)\n\nPaper: `ResNeSt: Split-Attention Networks` - /https://arxiv.org/abs/2004.08955\n\nAdapted from original PyTorch impl at https://github.com/zhanghang1989/ResNeSt\n\nModified for torchscript compat, performance, and consistency with timm by Ross Wightman\n\"\"\"\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom .helpers import make_divisible\n\n\nclass RadixSoftmax(nn.Module):\n    def __init__(self, radix, cardinality):\n        super(RadixSoftmax, self).__init__()\n        self.radix = radix\n        self.cardinality = cardinality\n\n    def forward(self, x):\n        batch = x.size(0)\n        if self.radix > 1:\n            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n            x = F.softmax(x, dim=1)\n            x = x.reshape(batch, -1)\n        else:\n            x = torch.softmax(x, dim=1)\n        return x\n\n\nclass AttentionConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1,\n                 bias=True, radix=1, cardinality=1):\n        super(AttentionConv2d, self).__init__()\n        self.radix = radix\n        self.cardinality = cardinality\n        self.radix_softmax = RadixSoftmax(radix, cardinality)\n        self.conv = nn.Conv2d(in_channels,",
  "import torch\nimport math\nimport warnings\n\nfrom torch.nn.init import _calculate_fan_in_and_fan_out\n\n\ndef _trunc_normal_(tensor, mean, std, a, b):\n    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n    def norm_cdf(x):\n        # Computes standard normal cumulative distribution function\n        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\n    if (mean < a - 2 * std) or (mean > b + 2 * std):\n        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n                      \"The distribution of values may be incorrect.\",\n                      stacklevel=2)\n\n    # Values are generated by using a truncated uniform distribution and\n    # then using the inverse CDF for the normal distribution.\n    # Get upper and lower cdf values\n    l = norm_cdf((a - mean) / std)\n    u = norm_cdf((b - mean) / std)\n\n    # Get the shape of the tensor\n    shape = tensor.size()\n\n    # Generate the values\n    tensor.data.normal_(mean, std)\n\n    # Set the values to the truncated normal distribution\n    tensor.data.mul_(std).add_(a)\n\n    # Set the shape of the tensor\n    tensor.size = shape\n\n\ndef trunc_normal_(tensor, mean=0, std=1, a=0, b=1):\n    r\"\"\"\n    Truncates the normal distribution to the given limits.\n\n    The distribution of values is truncated to the range",
  "\"\"\" Depthwise Separable Conv Modules\n\nBasic DWS convs. Other variations of DWS exist with batch norm or activations between the\nDW and PW convs such as the Depthwise modules in MobileNetV2 / EfficientNet and Xception.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom torch import nn as nn\n\nfrom .create_conv2d import create_conv2d\nfrom .create_norm_act import get_norm_act_layer\n\n\nclass SeparableConvNormAct(nn.Module):\n    \"\"\" Separable Conv w/ trailing Norm and Activation\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1, padding='', bias=False,\n                 channel_multiplier=1.0, pw_kernel_size=1, norm_layer=nn.BatchNorm2d, act_layer=nn.ReLU,\n                 apply_act=True, drop_layer=None):\n        super(SeparableConvNormAct, self).__init__()\n\n        self.conv_dw = create_conv2d(\n            in_channels, int(out_channels * channel_multiplier), kernel_size=kernel_size, stride=stride,\n            dilation=dilation, padding=padding, bias=bias, channel_multiplier=channel_multiplier,\n            pw_kernel_size=pw_kernel_size, norm_layer=norm_layer, act_layer=act_layer, apply_act=apply_act,\n            drop_layer=drop_layer)\n\n        self.conv_pw = create_conv2d(\n            int(out_channels * channel_multiplier), out_channels, kernel_size=pw_kernel_size, stride=",
  "\"\"\" EvoNorm in PyTorch\n\nBased on `Evolving Normalization-Activation Layers` - https://arxiv.org/abs/2004.02967\n@inproceedings{NEURIPS2020,\n author = {Liu, Hanxiao and Brock, Andy and Simonyan, Karen and Le, Quoc},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},\n pages = {13539--13550},\n publisher = {Curran Associates, Inc.},\n title = {Evolving Normalization-Activation Layers},\n url = {https://proceedings.neurips.cc/paper/2020/file/9d4c03631b8b0c85ae08bf05eda37d0f-Paper.pdf},\n volume = {33},\n year = {2020}\n}\n\nAn attempt at getting decent performing EvoNorms in PyTorch.\n\nThe EvoNorm paper is a good reference for the EvoNorm architecture.\n\nThe EvoNorm paper is a good reference for the EvoNorm architecture.\n\nThe EvoNorm paper is a good reference for the EvoNorm architecture.\n\nThe EvoNorm paper is a good reference for the EvoNorm architecture.\n\nThe EvoNorm paper is a good reference for the EvoNorm architecture.\n\nThe EvoNorm paper is a good reference for the EvoNorm architecture.\n\nThe EvoNorm paper is a good reference for the EvoNorm architecture",
  "\"\"\" Global Context Attention Block\n\nPaper: `GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond`\n    - https://arxiv.org/abs/1904.11492\n\nOfficial code consulted as reference: https://github.com/xvjiarui/GCNet\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nfrom torch import nn as nn\nimport torch.nn.functional as F\n\nfrom .create_act import create_act_layer, get_act_layer\nfrom .helpers import make_divisible\nfrom .mlp import ConvMlp\nfrom .norm import LayerNorm2d\n\n\nclass GlobalContext(nn.Module):\n\n    def __init__(self, channels, use_attn=True, fuse_add=False, fuse_scale=True, init_last_zero=False,\n                 rd_ratio=1./8, rd_channels=None, rd_divisor=1, act_layer=nn.ReLU, gate_layer='sigmoid'):\n        super(GlobalContext, self).__init__()\n        act_layer = get_act_layer(act_layer)\n        gate_layer = get_act_layer(gate_layer)\n\n        self.use_attn = use_attn\n        self.fuse_add = fuse_add\n        self.fuse_scale = fuse_scale\n        self.init_last_zero = init_last_zero\n        self.rd_ratio = rd_ratio\n        self.rd_channels = rd_channels\n        self.rd_divisor = rd_divisor\n        self.act_layer = act_layer\n        self.gate_layer = gate_layer\n\n        if self.use_attn:\n            self.attn",
  "\"\"\" Test Time Pooling (Average-Max Pool)\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport logging\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom .adaptive_avgmax_pool import adaptive_avgmax_pool2d\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass TestTimePoolHead(nn.Module):\n    def __init__(self, base, original_pool=7):\n        super(TestTimePoolHead, self).__init__()\n        self.base = base\n        self.original_pool = original_pool\n        base_fc = self.base.get_classifier()\n        if isinstance(base_fc, nn.Conv2d):\n            self.fc = base_fc\n        else:\n            self.fc = nn.Conv2d(\n                self.base.num_features, self.base.num_classes, kernel_size=1, bias=True)\n            self.fc.weight.data.copy_(base_fc.weight.data.view(self.fc.weight.size()))\n            self.fc.bias.data.copy_(base_fc.bias.data)\n\n    def forward(self, x):\n        x = self.base(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        x = F.log_softmax(x, dim=1)\n        return x\n\n\nclass TestTimePooling(nn.Module):\n    def __init__(self, base, original_pool=7):\n        super(TestTimePooling, self).__init__()\n        self.base = base\n        self.original_pool = original_pool\n        self.avg_pool = adaptive_avgmax_pool",
  "\"\"\" Activations\n\nA collection of jit-scripted activations fn and modules with a common interface so that they can\neasily be swapped. All have an `inplace` arg even if not used.\n\nAll jit scripted activations are lacking in-place variations on purpose, scripted kernel fusion does not\ncurrently work across in-place op boundaries, thus performance is equal to or less than the non-scripted\nversions if they contain in-place ops.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\n\n@torch.jit.script\ndef swish_jit(x, inplace: bool = False):\n    \"\"\"Swish - Described in: https://arxiv.org/abs/1710.05941\n    \"\"\"\n    return x.mul(x.sigmoid())\n\n\n@torch.jit.script\ndef mish_jit(x, _inplace: bool = False):\n    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n    \"\"\"\n    return x * torch.tanh(F.softplus(x))\n\n\n@torch.jit.script\ndef gelu_jit(x, _inplace: bool = False):\n    \"\"\"GELU: Generalized Linear Unit - https://arxiv.org/abs/1606.08415\n    \"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, ",
  "\"\"\" MLP module w/ dropout and configurable activation layer\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom functools import partial\n\nfrom torch import nn as nn\n\nfrom .grn import GlobalResponseNorm\nfrom .helpers import to_2tuple\n\n\nclass Mlp(nn.Module):\n    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            norm_layer=None,\n            bias=True,\n            drop=0.,\n            use_conv=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = to_2tuple(bias)\n        drop_probs = to_2tuple(drop)\n        linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear\n\n        self.fc1 = linear_layer(in_features, hidden_features, bias=bias[0])\n        self.fc2 = linear_layer(hidden_features, hidden_features, bias=bias[1])\n        self.fc3 = linear_layer(hidden_features, out_features, bias=bias[2])\n\n        self.layer_norm1 = norm_layer(hidden_features)\n        self.layer_norm2 = norm_layer(hidden_features)\n        self.layer_norm3 = norm_layer(out_features)\n\n        self.dropout = nn.Dropout(drop_probs[0])\n        self.activation = act_layer()\n\n    def forward(self,",
  "\"\"\" PyTorch Mixed Convolution\n\nPaper: MixConv: Mixed Depthwise Convolutional Kernels (https://arxiv.org/abs/1907.09595)\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport torch\nfrom torch import nn as nn\n\nfrom .conv2d_same import create_conv2d_pad\n\n\ndef _split_channels(num_chan, num_groups):\n    split = [num_chan // num_groups for _ in range(num_groups)]\n    split[0] += num_chan - sum(split)\n    return split\n\n\nclass MixedConv2d(nn.ModuleDict):\n    \"\"\" Mixed Grouped Convolution\n\n    Based on MDConv and GroupedConv in MixNet impl:\n      https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mixnet/custom_layers.py\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=3,\n                 stride=1, padding='', dilation=1, depthwise=False, **kwargs):\n        super(MixedConv2d, self).__init__(\n            {\n                'depthwise': nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                      padding=padding, dilation=dilation, **kwargs),\n                'pointwise': nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding='SAME',\n                                       dilation=1, **kwargs),\n            }\n        )\n        self.depthwise = depthwise\n        self.pointwise = nn.Conv2d(out_channels, out_channels, kernel",
  "\"\"\" CBAM (sort-of) Attention\n\nExperimental impl of CBAM: Convolutional Block Attention Module: https://arxiv.org/abs/1807.06521\n\nWARNING: Results with these attention layers have been mixed. They can significantly reduce performance on\nsome tasks, especially fine-grained it seems. I may end up removing this impl.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\nfrom torch import nn as nn\nimport torch.nn.functional as F\n\nfrom .conv_bn_act import ConvNormAct\nfrom .create_act import create_act_layer, get_act_layer\nfrom .helpers import make_divisible\n\n\nclass ChannelAttn(nn.Module):\n    \"\"\" Original CBAM channel attention module, currently avg + max pool variant only.\n    \"\"\"\n    def __init__(\n            self, channels, rd_ratio=1./16, rd_channels=None, rd_divisor=1,\n            act_layer=nn.ReLU, gate_layer='sigmoid', mlp_bias=False):\n        super(ChannelAttn, self).__init__()\n        self.channels = channels\n        self.rd_ratio = rd_ratio\n        self.rd_channels = rd_channels\n        self.rd_divisor = rd_divisor\n        self.act_layer = act_layer\n        self.gate_layer = gate_layer\n        self.mlp_bias = mlp_bias\n\n        self.conv1 = ConvNormAct(\n            channels, channels, kernel_size=1, stride=1, padding=0, act_layer=act_layer, mlp_bias=mlp_bias)\n        self.conv2 = ConvNormAct(\n           ",
  "\"\"\" Image to Patch Embedding using Conv2d\n\nA convolution based approach to patchifying a 2D image w/ embedding projection.\n\nBased on code in:\n  * https://github.com/google-research/vision_transformer\n  * https://github.com/google-research/big_vision/tree/main/big_vision\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport logging\nfrom typing import Callable, List, Optional, Tuple, Union\n\nimport torch\nfrom torch import nn as nn\nimport torch.nn.functional as F\n\nfrom .format import Format, nchw_to\nfrom .helpers import to_2tuple\nfrom .trace_utils import _assert\n\n_logger = logging.getLogger(__name__)\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" 2D Image to Patch Embedding\n    \"\"\"\n    output_fmt: Format\n\n    def __init__(\n            self,\n            img_size: Optional[int] = 224,\n            patch_size: int = 16,\n            in_chans: int = 3,\n            embed_dim: int = 768,\n            depths: List[int] = [2, 2, 6, 2],\n            num_heads: List[int] = [1, 1, 12, 1],\n            window_size: int = 7,\n            embed_scale: float = 1.0,\n            attn_scale: float = 1.0,\n            proj_scale: float = 1.0,\n            attn_proj_scale: float = 1.0,\n            use_proj: bool = True,\n            use_attn: bool = True,\n            use_",
  "\"\"\" Create Conv2d Factory Method\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nfrom .mixed_conv2d import MixedConv2d\nfrom .cond_conv2d import CondConv2d\nfrom .conv2d_same import create_conv2d_pad\n\n\ndef create_conv2d(in_channels, out_channels, kernel_size, **kwargs):\n    \"\"\" Select a 2d convolution implementation based on arguments\n    Creates and returns one of torch.nn.Conv2d, Conv2dSame, MixedConv2d, or CondConv2d.\n\n    Used extensively by EfficientNet, MobileNetv3 and related networks.\n    \"\"\"\n    if isinstance(kernel_size, list):\n        assert 'num_experts' not in kwargs  # MixNet + CondConv combo not supported currently\n        if 'groups' in kwargs:\n            groups = kwargs.pop('groups')\n            if groups == in_channels:\n                kwargs['depthwise'] = True\n            else:\n                assert groups == 1\n        # We're going to use only lists for defining the MixedConv2d kernel groups,\n        # and the CondConv2d kernel groups.\n        elif 'num_experts' in kwargs:\n            num_experts = kwargs.pop('num_experts')\n            if num_experts == in_channels:\n                kwargs['depthwise'] = True\n            else:\n                assert num_experts == 1\n        else:\n            raise ValueError('Invalid kernel_size argument')\n    elif isinstance(kernel_size, int):\n        if 'groups' in kwargs:\n            groups = kwargs.pop('groups')\n            if groups == in_channels:\n                kwargs['depthwise'] = True\n            else:\n                assert",
  "\"\"\" PyTorch Conditionally Parameterized Convolution (CondConv)\n\nPaper: CondConv: Conditionally Parameterized Convolutions for Efficient Inference\n(https://arxiv.org/abs/1904.04971)\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport math\nfrom functools import partial\nimport numpy as np\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\nfrom .helpers import to_2tuple\nfrom .conv2d_same import conv2d_same\nfrom .padding import get_padding_value\n\n\ndef get_condconv_initializer(initializer, num_experts, expert_shape):\n    def condconv_initializer(weight):\n        \"\"\"CondConv initializer function.\"\"\"\n        num_params = np.prod(expert_shape)\n        if (len(weight.shape) != 2 or weight.shape[0] != num_experts or\n                weight.shape[1] != num_params):\n            raise (ValueError(\n                'CondConv variables must have shape [num_experts, num_params]'))\n        for i in range(num_experts):\n            weight[i] = initializer(weight[i].shape)\n    return condconv_initializer\n\n\nclass CondConv2d(nn.Module):\n    \"\"\" PyTorch Conditionally Parameterized Convolution (CondConv)\n\n    Paper: CondConv: Conditionally Parameterized Convolutions for Efficient Inference\n    (https://arxiv.org/abs/1904.04971)\n\n    Hacked together by / Copyright 2020 Ross Wightman\n    \"\"\"\n\n    def __init__(self,\n                 num_experts,\n                 expert_shape,\n                 num_channels,",
  "import torch\nimport torch.nn as nn\n\n\nclass SpaceToDepth(nn.Module):\n    bs: torch.jit.Final[int]\n\n    def __init__(self, block_size=4):\n        super().__init__()\n        assert block_size == 4\n        self.bs = block_size\n\n    def forward(self, x):\n        N, C, H, W = x.size()\n        x = x.view(N, C, H // self.bs, self.bs, W // self.bs, self.bs)  # (N, C, H//bs, bs, W//bs, bs)\n        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)\n        x = x.view(N, C * self.bs * self.bs, H // self.bs, W // self.bs)  # (N, C*bs^2, H//bs, W//bs)\n        return x\n\n\n@torch.jit.script\nclass SpaceToDepthJit:\n    def __call__(self, x):\n        N, C, H, W = x.size()\n        x = x.view(N, C, H // self.bs, self.bs, W // self.bs, self.bs)  # (N, C, H//bs, bs, W//bs, bs)\n        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)\n        x = x.view(N, C * self.bs *",
  "\"\"\" Normalization + Activation Layers\n\nProvides Norm+Act fns for standard PyTorch norm layers such as\n* BatchNorm\n* GroupNorm\n* LayerNorm\n\nThis allows swapping with alternative layers that are natively both norm + act such as\n* EvoNorm (evo_norm.py)\n* FilterResponseNorm (filter_response_norm.py)\n* InplaceABN (inplace_abn.py)\n\nHacked together by / Copyright 2022 Ross Wightman\n\"\"\"\nfrom typing import Union, List, Optional, Any\n\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\nfrom torchvision.ops.misc import FrozenBatchNorm2d\n\nfrom .create_act import get_act_layer\nfrom .fast_norm import is_fast_norm, fast_group_norm, fast_layer_norm\nfrom .trace_utils import _assert\n\n\ndef _create_act(act_layer, act_kwargs=None, inplace=False, apply_act=True):\n    act_layer = get_act_layer(act_layer)  # string -> nn.Module\n    act_kwargs = act_kwargs or {}\n    act = act_layer(**act_kwargs)\n    if inplace:\n        act = act.to(inplace=True)\n    if apply_act:\n        act = act.apply(lambda x: x.clamp(min=0, max=1))\n    return act\n\n\nclass NormAct(nn.Module):\n    r\"\"\"\n    Applies a normalization + activation function to a tensor.\n\n    Args:\n        norm_layer (`nn.Module`):\n            The normalization layer to use.\n        act_layer (`nn.Module`):\n            The activation layer to use.\n        inplace (`bool",
  "\"\"\" Relative position embedding modules and functions\n\nHacked together by / Copyright 2022 Ross Wightman\n\"\"\"\nimport math\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .mlp import Mlp\nfrom .weight_init import trunc_normal_\n\n\ndef gen_relative_position_index(\n        q_size: Tuple[int, int],\n        k_size: Optional[Tuple[int, int]] = None,\n        class_token: bool = False,\n) -> torch.Tensor:\n    # Adapted with significant modifications from Swin / BeiT codebases\n    # get pair-wise relative position index for each token inside the window\n    if k_size is None:\n        coords = torch.stack(\n            torch.meshgrid([\n                torch.arange(q_size[0]),\n                torch.arange(q_size[1])\n            ])\n        ).flatten(1)  # 2, Wh, Ww\n        relative_coords = coords[:, :, None] - coords[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.view(-1, 2)  # 2, Wh*Ww*2\n        relative_coords[:, 0] = relative_coords[:, 0].unsqueeze(1)  # 2, Wh*Ww*2, 1\n        relative_coords[:, 1] = relative_coords[:, 1].unsqueeze(1)  # 2, Wh*Ww*2, 1\n    else:\n        coords = torch.stack(\n            torch.meshgrid([\n                torch.arange(q_size[0]),\n                torch.arange(q_size[1]),\n                torch.arange(",
  "\"\"\" DropBlock, DropPath\n\nPyTorch implementations of DropBlock and DropPath (Stochastic Depth) regularization layers.\n\nPapers:\nDropBlock: A regularization method for convolutional networks (https://arxiv.org/abs/1810.12890)\n\nDeep Networks with Stochastic Depth (https://arxiv.org/abs/1603.09382)\n\nCode:\nDropBlock impl inspired by two Tensorflow impl that I liked:\n - https://github.com/tensorflow/tpu/blob/master/models/official/resnet/resnet_model.py#L74\n - https://github.com/clovaai/assembled-cnn/blob/master/nets/blocks.py\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef drop_block_2d(\n        x, drop_prob: float = 0.1, block_size: int = 7, gamma_scale: float = 1.0,\n        with_cp: bool = False, with_cp_mask: bool = False, with_cp_proj: bool = False,\n        with_cp_proj_mask: bool = False, with_cp_proj_conv: bool = False, with_cp_proj_conv_mask: bool = False,\n        with_cp_proj_conv_proj: bool = False, with_cp_proj_conv_proj_mask: bool = False,\n        with_cp_proj_conv_proj_conv: bool = False, with_cp_proj_conv_proj_conv_mask: bool",
  "\"\"\" Layer/Module Helpers\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom itertools import repeat\nimport collections.abc\n\n\n# From PyTorch internals\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n            return tuple(x)\n        return tuple(repeat(x, n))\n    return parse\n\n\nto_1tuple = _ntuple(1)\nto_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = _ntuple\n\n\ndef make_divisible(v, divisor=8, min_value=None, round_limit=.9):\n    min_value = min_value or divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < round_limit * v:\n        new_v += divisor\n    return new_v\n\n\n\ndef get_list_of_tuples(iterable):\n    \"\"\"\n    Returns a list of tuples from an iterable.\n    \"\"\"\n    return list(zip(*iterable))\n\ndef get_list_of_lists(iterable):\n    \"\"\"\n    Returns a list of lists from an iterable.\n    \"\"\"\n    return list(itertools.chain(*iterable))\n\ndef get_list_of_dicts(iterable):\n    \"\"\"\n    Returns a list of dicts from an iterable.\n    \"\"\"\n    return list(itertools.chain(*[iter(d) for d in iterable]))\n\ndef get_list_of_",
  "\"\"\" PyTorch selectable adaptive pooling\nAdaptive pooling with the ability to select the type of pooling from:\n    * 'avg' - Average pooling\n    * 'max' - Max pooling\n    * 'avgmax' - Sum of average and max pooling re-scaled by 0.5\n    * 'avgmaxc' - Concatenation of average and max pooling along feature dim, doubles feature dim\n\nBoth a functional and a nn.Module version of the pooling is provided.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom typing import Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .format import get_spatial_dim, get_channel_dim\n\n_int_tuple_2_t = Union[int, Tuple[int, int]]\n\n\ndef adaptive_pool_feat_mult(pool_type='avg'):\n    if pool_type.endswith('catavgmax'):\n        return 2\n    else:\n        return 1\n\n\ndef adaptive_avgmax_pool2d(x, output_size: _int_tuple_2_t = 1):\n    \"\"\"\n    Adaptive average max pooling 2D.\n    Adapted from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L1029\n    \"\"\"\n    if output_size[1] == 1:\n        output_size = output_size[0], output_size[0]\n    if output_size[0] == 1:\n        output_size = output_size[1], output_size[1]\n    if output_size[0] == 2 and output_size[1] == 2:\n        return F",
  "\"\"\" Filter Response Norm in PyTorch\n\nBased on `Filter Response Normalization Layer` - https://arxiv.org/abs/1911.09737\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport torch\nimport torch.nn as nn\n\nfrom .create_act import create_act_layer\nfrom .trace_utils import _assert\n\n\ndef inv_instance_rms(x, eps: float = 1e-5):\n    rms = x.square().float().mean(dim=(2, 3), keepdim=True).add(eps).rsqrt().to(x.dtype)\n    return rms.expand(x.shape)\n\n\nclass FilterResponseNormTlu2d(nn.Module):\n    def __init__(self, num_features, apply_act=True, eps=1e-5, rms=True, **_):\n        super(FilterResponseNormTlu2d, self).__init__()\n        self.apply_act = apply_act  # apply activation (non-linearity)\n        self.rms = rms\n        self.eps = eps\n        self.weight = nn.Parameter(torch.randn(num_features, num_features))\n        self.bias = nn.Parameter(torch.zeros(num_features))\n        self.act = create_act_layer(inplace=True)\n\n    def forward(self, x):\n        _assert(x.shape[1] == self.weight.shape[1], \"x.shape[1] != self.weight.shape[1]\")\n        if self.rms:\n            x = inv_instance_rms(x, eps=self.eps)\n        if self.apply_act:\n            x = self.act",
  "\"\"\" Lambda Layer\n\nPaper: `LambdaNetworks: Modeling Long-Range Interactions Without Attention`\n    - https://arxiv.org/abs/2102.08602\n\n@misc{2102.08602,\nAuthor = {Irwan Bello},\nTitle = {LambdaNetworks: Modeling Long-Range Interactions Without Attention},\nYear = {2021},\n}\n\nStatus:\nThis impl is a WIP. Code snippets in the paper were used as reference but\ngood chance some details are missing/wrong.\n\nI've only implemented local lambda conv based pos embeddings.\n\nFor a PyTorch impl that includes other embedding options checkout\nhttps://github.com/lucidrains/lambda-networks\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom .helpers import to_2tuple, make_divisible\nfrom .weight_init import trunc_normal_\n\n\ndef rel_pos_indices(size):\n    size = to_2tuple(size)\n    pos = torch.arange(size[0], device=size[1]).unsqueeze(0)\n    neg = torch.arange(size[1], device=size[1]).unsqueeze(1)\n    return torch.stack([pos, neg], dim=1)\n\n\ndef rel_pos_embeddings(size):\n    size = to_2tuple(size)\n    emb = torch.zeros(size[0], size[1], device=size[1])\n    emb[:, 0, :] = 1.0\n    emb[:, 1:, :] = 0.0\n    return emb\n\n\nclass LambdaLayer(nn.",
  "\"\"\" Selective Kernel Convolution/Attention\n\nPaper: Selective Kernel Networks (https://arxiv.org/abs/1903.06586)\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\nfrom torch import nn as nn\n\nfrom .conv_bn_act import ConvNormActAa\nfrom .helpers import make_divisible\nfrom .trace_utils import _assert\n\n\ndef _kernel_valid(k):\n    if isinstance(k, (list, tuple)):\n        for ki in k:\n            return _kernel_valid(ki)\n    assert k >= 3 and k % 2\n\n\nclass SelectiveKernelAttn(nn.Module):\n    def __init__(self, channels, num_paths=2, attn_channels=32, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):\n        \"\"\" Selective Kernel Attention Module\n\n        Selective Kernel attention mechanism factored out into its own module.\n\n        \"\"\"\n        super(SelectiveKernelAttn, self).__init__()\n        self.num_paths = num_paths\n        self.fc_reduce = nn.Conv2d(channels, attn_channels, kernel_size=1)\n        self.fc_expand = nn.Conv2d(attn_channels, channels, kernel_size=1)\n        self.attn = nn.Conv2d(channels, channels, kernel_size=1)\n        self.act = act_layer()\n        self.norm = norm_layer()\n\n    def forward(self, x, k, v, mask=None):\n        \"\"\" Forward pass\n\n        Args:\n            x (Tensor): Input feature map\n            k (Tensor): Kernel map\n            v (Tensor): Value map\n            mask (Tensor",
  "from typing import Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\n\n\nclass PatchDropout(nn.Module):\n    \"\"\"\n    https://arxiv.org/abs/2212.00794\n    \"\"\"\n    return_indices: torch.jit.Final[bool]\n\n    def __init__(\n            self,\n            prob: float = 0.5,\n            num_prefix_tokens: int = 1,\n            ordered: bool = False,\n            return_indices: bool = False,\n    ):\n        super().__init__()\n        assert 0 <= prob < 1.\n        self.prob = prob\n        self.num_prefix_tokens = num_prefix_tokens  # exclude CLS token (or other prefix tokens)\n        self.ordered = ordered\n        self.return_indices = return_indices\n\n    def forward(self, x) -> Union[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]:\n        if not self.training or self.prob == 0.:\n            if self.return_indices:\n                return x, None\n            return x\n\n        if self.num_prefix_tokens:\n            prefix_tokens, x = x[:, :self.num_prefix_tokens], x[:, self.num_prefix_tokens:]\n        else:\n            prefix_tokens = None\n\n        if self.ordered:\n            x = torch.cat([x[:, 1:], x[:, :-1]], dim=1)\n        else:\n            x = torch.cat([x[:, 1:], x[:, :-1]], dim=1)\n\n        if self.return_indices:\n            return x, prefix_tokens\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    def __init__(self, patch_size: int, embed_dim: int, patch_norm: bool",
  "\"\"\" Bottleneck Self Attention (Bottleneck Transformers)\n\nPaper: `Bottleneck Transformers for Visual Recognition` - https://arxiv.org/abs/2101.11605\n\n@misc{2101.11605,\nAuthor = {Aravind Srinivas and Tsung-Yi Lin and Niki Parmar and Jonathon Shlens and Pieter Abbeel and Ashish Vaswani},\nTitle = {Bottleneck Transformers for Visual Recognition},\nYear = {2021},\n}\n\nBased on ref gist at: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2\n\nThis impl is a WIP but given that it is based on the ref gist likely not too far off.\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .helpers import to_2tuple, make_causal_mask, make_causal_mask_list, get_mask_list_from_lengths\n\n\nclass BottleneckSelfAttention(nn.Module):\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n        num_patches: int,\n        patch_size: int,\n        patch_stride: int,\n        patch_padding: int,\n        dropout: float,\n        use_attention_mask: bool = True,\n        use_key_value_mask: bool = True,\n        use_layer_norm: bool = True,\n        layer",
  "\"\"\" Split BatchNorm\n\nA PyTorch BatchNorm layer that splits input batch into N equal parts and passes each through\na separate BN layer. The first split is passed through the parent BN layers with weight/bias\nkeys the same as the original BN. All other splits pass through BN sub-layers under the '.aux_bn'\nnamespace.\n\nThis allows easily removing the auxiliary BN layers after training to efficiently\nachieve the 'Auxiliary BatchNorm' as described in the AdvProp Paper, section 4.2,\n'Disentangled Learning via An Auxiliary BN'\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\nimport torch.nn as nn\n\n\nclass SplitBatchNorm2d(torch.nn.BatchNorm2d):\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n                 track_running_stats=True, num_splits=2):\n        super().__init__(num_features, eps, momentum, affine, track_running_stats)\n        assert num_splits > 1, 'Should have at least one aux BN layer (num_splits at least 2)'\n        self.num_splits = num_splits\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        self.num_splits = num_splits\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        self.num_splits = num_splits\n        self.num_features = num_features\n       ",
  "\"\"\" Conv2d w/ Same Padding\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Tuple, Optional\n\nfrom .config import is_exportable, is_scriptable\nfrom .padding import pad_same, pad_same_arg, get_padding_value\n\n\n_USE_EXPORT_CONV = False\n\n\ndef conv2d_same(\n        x,\n        weight: torch.Tensor,\n        bias: Optional[torch.Tensor] = None,\n        stride: Tuple[int, int] = (1, 1),\n        padding: Tuple[int, int] = (0, 0),\n        dilation: Tuple[int, int] = (1, 1),\n        groups: int = 1,\n):\n    x = pad_same(x, weight.shape[-2:], stride, dilation)\n    return F.conv2d(x, weight, bias, stride, (0, 0), dilation, groups)\n\n\nclass Conv2dSame(nn.Conv2d):\n    \"\"\" Tensorflow like 'Conv2d' with same padding\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.stride = (1, 1)\n        self.padding = (0, 0)\n        self.dilation = (1, 1)\n        self.groups = 1\n\n    def forward(self, x):\n        x = pad_same_arg(x, self.weight.shape[-2:], self.stride, self.dilation)\n        return F.conv2d(x, self.weight, self.bias, self.stride,",
  "\"\"\" Global Response Normalization Module\n\nBased on the GRN layer presented in\n`ConvNeXt-V2 - Co-designing and Scaling ConvNets with Masked Autoencoders` - https://arxiv.org/abs/2301.00808\n\nThis implementation\n* works for both NCHW and NHWC tensor layouts\n* uses affine param names matching existing torch norm layers\n* slightly improves eager mode performance via fused addcmul\n\nHacked together by / Copyright 2023 Ross Wightman\n\"\"\"\n\nimport torch\nfrom torch import nn as nn\n\n\nclass GlobalResponseNorm(nn.Module):\n    \"\"\" Global Response Normalization layer\n    \"\"\"\n    def __init__(self, dim, eps=1e-6, channels_last=True):\n        super().__init__()\n        self.eps = eps\n        if channels_last:\n            self.spatial_dim = (1, 2)\n            self.channel_dim = -1\n            self.wb_shape = (1, 1, 1, -1)\n        else:\n            self.spatial_dim = (2, 3)\n            self.channel_dim = 1\n            self.wb_shape = (1, 1, -1, 1)\n        self.conv = nn.Conv2d(dim, dim, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn = nn.BatchNorm2d(dim)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv_bias = nn.Conv2d(dim, dim, kernel_size=1, stride=1, padding=0, bias=True)\n\n    def forward(self, x):\n        x = self.",
  "import torch\nfrom torch import nn as nn\n\ntry:\n    from inplace_abn.functions import inplace_abn, inplace_abn_sync\n    has_iabn = True\nexcept ImportError:\n    has_iabn = False\n\n    def inplace_abn(x, weight, bias, running_mean, running_var,\n                    training=True, momentum=0.1, eps=1e-05, activation=\"leaky_relu\", activation_param=0.01):\n        raise ImportError(\n            \"Please install InplaceABN:'pip install git+https://github.com/mapillary/inplace_abn.git@v1.0.12'\")\n\n    def inplace_abn_sync(**kwargs):\n        inplace_abn(**kwargs)\n\n\nclass InplaceAbn(nn.Module):\n    \"\"\"Activated Batch Normalization\n\n    This gathers a BatchNorm and an activation function in a single module\n\n    Parameters\n    ----------\n    num_features : int\n        Number of feature channels in the input and output.\n    eps : float\n        Small constant to prevent numerical issues.\n    momentum : float\n        Momentum factor applied to compute running statistics.\n    affine : bool\n        If True, adds a learnable affine transformation to the output.\n    activation : str\n        The activation function to use.\n    activation_param : float\n        The parameter for the activation function.\n    \"\"\"\n\n    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True, activation=\"leaky_relu\",\n                 activation_param=0.01):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.activation = activation\n        self.activation",
  "\"\"\" NormAct (Normalizaiton + Activation Layer) Factory\n\nCreate norm + act combo modules that attempt to be backwards compatible with separate norm + act\nisntances in models. Where these are used it will be possible to swap separate BN + act layers with\ncombined modules like IABN or EvoNorms.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport types\nimport functools\n\nfrom .evo_norm import *\nfrom .filter_response_norm import FilterResponseNormAct2d, FilterResponseNormTlu2d\nfrom .norm_act import BatchNormAct2d, GroupNormAct, LayerNormAct, LayerNormAct2d\nfrom .inplace_abn import InplaceAbn\n\n_NORM_ACT_MAP = dict(\n    batchnorm=BatchNormAct2d,\n    batchnorm2d=BatchNormAct2d,\n    groupnorm=GroupNormAct,\n    groupnorm1=functools.partial(GroupNormAct, num_groups=1),\n    layernorm=LayerNormAct,\n    layernorm2d=LayerNormAct2d,\n    evonormb0=EvoNorm,\n    evonormb1=EvoNorm,\n    evonormb2=EvoNorm,\n    evonormb3=EvoNorm,\n    evonormb4=EvoNorm,\n    evonormb5=EvoNorm,\n    evonormb6=EvoNorm,\n    evonormb7=EvoNorm,\n    evonormb8=EvoNorm,\n    evonormb9=EvoNorm,\n    evonormb10=EvoNorm,\n    evonormb11=EvoNorm,\n    evon",
  "from enum import Enum\nfrom typing import Union\n\nimport torch\n\n\nclass Format(str, Enum):\n    NCHW = 'NCHW'\n    NHWC = 'NHWC'\n    NCL = 'NCL'\n    NLC = 'NLC'\n\n\nFormatT = Union[str, Format]\n\n\ndef get_spatial_dim(fmt: FormatT):\n    fmt = Format(fmt)\n    if fmt is Format.NLC:\n        dim = (1,)\n    elif fmt is Format.NCL:\n        dim = (2,)\n    elif fmt is Format.NHWC:\n        dim = (1, 2)\n    else:\n        dim = (2, 3)\n    return dim\n\n\ndef get_channel_dim(fmt: FormatT):\n    fmt = Format(fmt)\n    if fmt is Format.NHWC:\n        dim = 3\n    elif fmt is Format.NLC:\n        dim = 2\n    else:\n        dim = 1\n    return dim\n\n\ndef nchw_to(x: torch.Tensor, fmt: Format):\n    if fmt == Format.NHWC:\n        x = x.permute(0, 2, 3, 1)\n    elif fmt == Format.NCL:\n        x = x.permute(0, 2, 3, 1)\n    elif fmt == Format.NLC:\n        x = x.permute(0, 2, 3, 1)\n    else:\n        raise ValueError(f\"Unsupported format {fmt}\")\n    return x\n\n\ndef nhwc_to(x: torch.Tensor, fmt: Format):\n    if fmt == Format.NHWC:\n        x = x.permute(0, 3, 1, 2)",
  "\"\"\" Halo Self Attention\n\nPaper: `Scaling Local Self-Attention for Parameter Efficient Visual Backbones`\n    - https://arxiv.org/abs/2103.12731\n\n@misc{2103.12731,\nAuthor = {Ashish Vaswani and Prajit Ramachandran and Aravind Srinivas and Niki Parmar and Blake Hechtman and\n    Jonathon Shlens},\nTitle = {Scaling Local Self-Attention for Parameter Efficient Visual Backbones},\nYear = {2021},\n}\n\nStatus:\nThis impl is a WIP, there is no official ref impl and some details in paper weren't clear to me.\nThe attention mechanism works but it's slow as implemented.\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nfrom typing import List\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom .helpers import make_divisible\nfrom .weight_init import trunc_normal_\nfrom .trace_utils import _assert\n\n\ndef rel_logits_1d(q, k, v, eps=1e-6):\n    \"\"\"\n    Computes the dot product of the query and key vectors, then computes the softmax over the last dimension.\n    \"\"\"\n    # dot product of query and key vectors\n    dot_product = torch.matmul(q, k.transpose(-1, -2))\n    # compute softmax over the last dimension\n    softmax_product = F.softmax(dot_product, dim=-1)\n    # compute the dot product of the query and the softmax over the last dimension\n    dot_product = torch.matmul(softmax_product, v)\n    #",
  "\"\"\" Bilinear-Attention-Transform and Non-Local Attention\n\nPaper: `Non-Local Neural Networks With Grouped Bilinear Attentional Transforms`\n    - https://openaccess.thecvf.com/content_CVPR_2020/html/Chi_Non-Local_Neural_Networks_With_Grouped_Bilinear_Attentional_Transforms_CVPR_2020_paper.html\nAdapted from original code: https://github.com/BA-Transform/BAT-Image-Classification\n\"\"\"\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .conv_bn_act import ConvNormAct\nfrom .helpers import make_divisible\nfrom .trace_utils import _assert\n\n\nclass NonLocalAttn(nn.Module):\n    \"\"\"Spatial NL block for image classification.\n\n    This was adapted from https://github.com/BA-Transform/BAT-Image-Classification\n    Their NonLocal impl inspired by https://github.com/facebookresearch/video-nonlocal-net.\n    \"\"\"\n\n    def __init__(self, in_channels, use_scale=True,  rd_ratio=1.0, num_heads=8, num_blocks=4,\n                 kernel_size=3, stride=1, padding=1, dilation=1, dropout=0.0):\n        super().__init__()\n        self.in_channels = in_channels\n        self.use_scale = use_scale\n        self.rd_ratio = rd_ratio\n        self.num_heads = num_heads\n        self.num_blocks = num_blocks\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self",
  "\"\"\" Attention Pool 2D\n\nImplementations of 2D spatial feature pooling using multi-head attention instead of average pool.\n\nBased on idea in CLIP by OpenAI, licensed Apache 2.0\nhttps://github.com/openai/CLIP/blob/3b473b0e682c091a9e53623eebc1ca1657385717/clip/model.py\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nfrom typing import Union, Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom .helpers import to_2tuple\nfrom .pos_embed_sincos import apply_rot_embed, RotaryEmbedding\nfrom .weight_init import trunc_normal_\n\n\nclass RotAttentionPool2d(nn.Module):\n    \"\"\" Attention based 2D feature pooling w/ rotary (relative) pos embedding.\n    This is a multi-head attention based replacement for (spatial) average pooling in NN architectures.\n\n    Adapted from the AttentionPool2d in CLIP w/ rotary embedding instead of learned embeddings.\n    https://github.com/openai/CLIP/blob/3b473b0e682c091a9e53623eebc1ca1657385717/clip/model.py\n\n    Args:\n        dim (int): Dimensionality of the input feature map.\n        num_heads (int): Number of attention heads.\n        d_k (int): Dimensionality of the key vector.\n        d_v (int): Dimensionality of the value vector.\n        d_model (int): Dimensionality of the model.",
  "\"\"\" Position Embedding Utilities\n\nHacked together by / Copyright 2022 Ross Wightman\n\"\"\"\nimport logging\nimport math\nfrom typing import List, Tuple, Optional, Union\n\nimport torch\nimport torch.nn.functional as F\n\nfrom .helpers import to_2tuple\n\n_logger = logging.getLogger(__name__)\n\n\ndef resample_abs_pos_embed(\n        posemb,\n        new_size: List[int],\n        old_size: Optional[List[int]] = None,\n        num_prefix_tokens: int = 1,\n        interpolation: str = 'bicubic',\n        antialias: bool = True,\n        verbose: bool = False,\n):\n    # sort out sizes, assume square if old size not provided\n    num_pos_tokens = posemb.shape[1]\n    num_new_tokens = new_size[0] * new_size[1] + num_prefix_tokens\n    if num_new_tokens == num_pos_tokens and new_size[0] == new_size[1]:\n        return posemb\n\n    if not old_size:\n        hw = int(math.sqrt(num_pos_tokens))\n        old_size = [hw, hw]\n    else:\n        hw = old_size[0]\n        old_size = [hw, hw]\n\n    # compute the new position embedding\n    if interpolation == 'bicubic':\n        # bicubic interpolation\n        # https://github.com/pytorch/vision/blob/main/torchvision/transforms/transforms_utils.py#L109\n        # https://github.com/pytorch/vision/blob/main/torchvision/transforms/transforms_utils.py#L125\n        # https://github.com/pytorch/",
  "\"\"\"\nECA module from ECAnet\n\npaper: ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks\nhttps://arxiv.org/abs/1910.03151\n\nOriginal ECA model borrowed from https://github.com/BangguWu/ECANet\n\nModified circular ECA implementation and adaption for use in timm package\nby Chris Ha https://github.com/VRandme\n\nOriginal License:\n\nMIT License\n\nCopyright (c) 2019 BangguWu, Qilong Wang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\"\"\"\n\nimport math\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.models.layers import Conv2dSamePadding, GroupNorm, InstanceNorm",
  "\"\"\" Activations (memory-efficient w/ custom autograd)\n\nA collection of activations fn and modules with a common interface so that they can\neasily be swapped. All have an `inplace` arg even if not used.\n\nThese activations are not compatible with jit scripting or ONNX export of the model, please use either\nthe JIT or basic versions of the activations.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\n\n@torch.jit.script\ndef swish_jit_fwd(x):\n    return x.mul(torch.sigmoid(x))\n\n\n@torch.jit.script\ndef swish_jit_bwd(x, grad_output):\n    x_sigmoid = torch.sigmoid(x)\n    return grad_output * (x_sigmoid * (1 + x * (1 - x_sigmoid)))\n\n\nclass SwishJitAutoFn(torch.autograd.Function):\n    \"\"\" torch.jit.script optimised Swish w/ memory-efficient checkpoint\n    Inspired by conversation btw Jeremy and Mike.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input):\n        ctx.save_for_backward(input)\n        return swish_jit_fwd(input)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        return swish_jit_bwd(input, grad_output)\n\n\nclass SwishJitModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, input):\n        return SwishJitAutoFn.apply(input)\n\n\nclass Swish",
  "\"\"\" Normalization layers and wrappers\n\nNorm layer definitions that support fast norm and consistent channel arg order (always first arg).\n\nHacked together by / Copyright 2022 Ross Wightman\n\"\"\"\nimport numbers\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .fast_norm import is_fast_norm, fast_group_norm, fast_layer_norm, fast_rms_norm\n\n\nclass GroupNorm(nn.GroupNorm):\n    def __init__(self, num_channels, num_groups=32, eps=1e-5, affine=True):\n        # NOTE num_channels is swapped to first arg for consistency in swapping norm layers with BN\n        super().__init__(num_groups, num_channels, eps=eps, affine=affine)\n        self.fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)\n\n    def forward(self, x):\n        if self.fast_norm:\n            return fast_group_norm(x, self.num_groups, self.weight, self.bias, self.running_mean, self.running_var)\n        else:\n            return super().forward(x)\n\n\nclass LayerNorm(nn.LayerNorm):\n    def __init__(self, num_channels, eps=1e-5):\n        super().__init__(num_channels, eps=eps)\n        self.fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)\n\n    def forward(self, x):\n        if self.fast_norm:\n            return fast_layer_norm(x, self.weight, self.bias, self.running",
  "try:\n    from torch import _assert\nexcept ImportError:\n    def _assert(condition: bool, message: str):\n        assert condition, message\n\n\ndef _float_to_int(x: float) -> int:\n    \"\"\"\n    Converts a float to an integer.\n\n    Args:\n        x (float): The float to convert.\n\n    Returns:\n        int: The integer representation of x.\n    \"\"\"\n    return int(x)\n\n\ndef _int_to_float(x: int) -> float:\n    \"\"\"\n    Converts an integer to a float.\n\n    Args:\n        x (int): The integer to convert.\n\n    Returns:\n        float: The float representation of x.\n    \"\"\"\n    return float(x) / 255.0\n\n\ndef _int_to_byte(x: int) ->",
  "\"\"\" 'Fast' Normalization Functions\n\nFor GroupNorm and LayerNorm these functions bypass typical AMP upcast to float32.\n\nAdditionally, for LayerNorm, the APEX fused LN is used if available (which also does not upcast)\n\nHacked together by / Copyright 2022 Ross Wightman\n\"\"\"\nfrom typing import List, Optional\n\nimport torch\nfrom torch.nn import functional as F\n\ntry:\n    from apex.normalization.fused_layer_norm import fused_layer_norm_affine\n    has_apex = True\nexcept ImportError:\n    has_apex = False\n\ntry:\n    from apex.normalization.fused_layer_norm import fused_rms_norm_affine, fused_rms_norm\n    has_apex_rmsnorm = True\nexcept ImportError:\n    has_apex_rmsnorm = False\n\n\n# fast (ie lower precision LN) can be disabled with this flag if issues crop up\n_USE_FAST_NORM = False  # defaulting to False for now\n\n\ndef is_fast_norm():\n    return _USE_FAST_NORM\n\n\ndef set_fast_norm(value: bool):\n    global _USE_FAST_NORM\n    _USE_FAST_NORM = value\n\n\ndef group_norm(x: torch.Tensor, num_groups: int, eps: float = 1e-5, momentum: float = 0.99, **kwargs) -> torch.Tensor:\n    \"\"\"\n    Group normalization implementation.\n\n    Args:\n        x: input tensor\n        num_groups: number of groups\n        eps: epsilon value\n        momentum: momentum value\n\n    Returns:\n        output tensor\n    \"\"\"\n    # TODO: add support for group norm\n    raise NotImplementedError()\n\n",
  "\"\"\" Classifier head and layer factory\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom collections import OrderedDict\nfrom functools import partial\nfrom typing import Optional, Union, Callable\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nfrom .adaptive_avgmax_pool import SelectAdaptivePool2d\nfrom .create_act import get_act_layer\nfrom .create_norm import get_norm_layer\n\n\ndef _create_pool(\n        num_features: int,\n        num_classes: int,\n        pool_type: str = 'avg',\n        use_conv: bool = False,\n        input_fmt: Optional[str] = None,\n):\n    flatten_in_pool = not use_conv  # flatten when we use a Linear layer after pooling\n    if not pool_type:\n        assert num_classes == 0 or use_conv,\\\n            'Pooling can only be disabled if classifier is also removed or conv classifier is used'\n        flatten_in_pool = False  # disable flattening if pooling is pass-through (no pooling)\n    global_pool = SelectAdaptivePool2d(\n        num_features=num_features,\n        num_classes=num_classes,\n        pool_type=pool_type,\n        flatten_in_pool=flatten_in_pool,\n        input_fmt=input_fmt,\n    )\n    return global_pool\n\n\ndef _create_classifier(\n        num_features: int,\n        num_classes: int,\n        num_hidden_layers: int,\n        intermediate_size: int,\n        hidden_act: Optional[str] = None,\n        hidden_dropout_prob: float = 0.1,\n        attention_probs_dropout_prob:",
  "\"\"\" AvgPool2d w/ Same Padding\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import List, Tuple, Optional\n\nfrom .helpers import to_2tuple\nfrom .padding import pad_same, get_padding_value\n\n\ndef avg_pool2d_same(x, kernel_size: List[int], stride: List[int], padding: List[int] = (0, 0),\n                    ceil_mode: bool = False, count_include_pad: bool = True):\n    # FIXME how to deal with count_include_pad vs not for external padding?\n    x = pad_same(x, kernel_size, stride)\n    return F.avg_pool2d(x, kernel_size, stride, (0, 0), ceil_mode, count_include_pad)\n\n\nclass AvgPool2dSame(nn.AvgPool2d):\n    \"\"\" Tensorflow like 'SAME' wrapper for 2D average pooling\n    \"\"\"\n    def __init__(self, kernel_size: int, stride: int, padding: int = 0, ceil_mode: bool = False,\n                 count_include_pad: bool = True):\n        super().__init__(kernel_size, stride, padding, ceil_mode, count_include_pad)\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        return avg_pool2d_same(input, self.kernel_size, self.stride, self.padding, self.ceil_mode,\n                             self.count_include_pad)\n\n\ndef avg_pool2d_same_output_size(x, kernel",
  "\"\"\" Convolution with Weight Standardization (StdConv and ScaledStdConv)\n\nStdConv:\n@article{weightstandardization,\n  author    = {Siyuan Qiao and Huiyu Wang and Chenxi Liu and Wei Shen and Alan Yuille},\n  title     = {Weight Standardization},\n  journal   = {arXiv preprint arXiv:1903.10520},\n  year      = {2019},\n}\nCode: https://github.com/joe-siyuan-qiao/WeightStandardization\n\nScaledStdConv:\nPaper: `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n    - https://arxiv.org/abs/2101.08692\nOfficial Deepmind JAX code: https://github.com/deepmind/deepmind-research/tree/master/nfnets\n\nHacked together by / copyright Ross Wightman, 2021.\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .padding import get_padding, get_padding_value, make_padding_mask\nfrom .utils import add_module_forward, find_pruneable_heads_and_indices, prune_linear_layer\n\n\nclass StdConv(nn.Module):\n    r\"\"\"\n    Constructs a standard convolutional layer with weight standardization.\n\n    Args:\n        in_channels (`int`, *optional*, defaults to 3):\n            The number of channels in the input image.\n        out_channels (`int`, *optional*, defaults to 32):\n            The number of channels in the output image.\n        kernel_size (`int`, *optional*, defaults to 3):\n            The",
  "\"\"\" Activations\n\nA collection of activations fn and modules with a common interface so that they can\neasily be swapped. All have an `inplace` arg even if not used.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\n\ndef swish(x, inplace: bool = False):\n    \"\"\"Swish - Described in: https://arxiv.org/abs/1710.05941\n    \"\"\"\n    return x.mul_(x.sigmoid()) if inplace else x.mul(x.sigmoid())\n\n\nclass Swish(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(Swish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return swish(x, self.inplace)\n\n\ndef mish(x, inplace: bool = False):\n    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n    NOTE: I don't think this is the same as the original Mish paper.\n    \"\"\"\n    return x.mul_(x.tanh() * F.softplus(x)) if inplace else x.mul(x.tanh() * F.softplus(x))\n\n\nclass Mish(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(Mish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return mish(x, self.inplace)\n\n\ndef gelu(x, approximate=False, inplace: bool =",
  "\"\"\" Conv2d + BN + Act\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport functools\nfrom torch import nn as nn\n\nfrom .create_conv2d import create_conv2d\nfrom .create_norm_act import get_norm_act_layer\n\n\nclass ConvNormAct(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size=1,\n            stride=1,\n            padding='',\n            dilation=1,\n            groups=1,\n            bias=False,\n            apply_act=True,\n            norm_layer=nn.BatchNorm2d,\n            norm_kwargs=None,\n            act_layer=nn.ReLU,\n            act_kwargs=None,\n            drop_layer=None,\n    ):\n        super(ConvNormAct, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n        act_kwargs = act_kwargs or {}\n\n        self.conv = create_conv2d(\n            in_channels, out_channels, kernel_size, stride=stride,\n            padding=padding, dilation=dilation, groups=groups, bias=bias)\n\n        self.norm = norm_layer(out_channels, **norm_kwargs)\n        self.act = act_layer(**act_kwargs)\n\n        if drop_layer is not None:\n            self.drop = drop_layer()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.norm(x)\n        x = self.act(x)\n        if self.drop is not None:\n            x = self.drop(x)\n        return x\n\n\ndef conv_norm_act_layer(\n        in_channels,\n        out_channels,\n        kernel",
  "\"\"\" Padding Helpers\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn.functional as F\n\n\n# Calculate symmetric padding for a convolution\ndef get_padding(kernel_size: int, stride: int = 1, dilation: int = 1, **_) -> int:\n    padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n    return padding\n\n\n# Calculate asymmetric TensorFlow-like 'SAME' padding for a convolution\ndef get_same_padding(x: int, kernel_size: int, stride: int, dilation: int):\n    if isinstance(x, torch.Tensor):\n        return torch.clamp(((x / stride).ceil() - 1) * stride + (kernel_size - 1) * dilation + 1 - x, min=0)\n    else:\n        return max((math.ceil(x / stride) - 1) * stride + (kernel_size - 1) * dilation + 1 - x, 0)\n\n\n# Can SAME padding be used for a convolution?\ndef can_use_same_padding(kernel_size: int, stride: int = 1, dilation: int = 1, **_) -> bool:\n    return get_same_padding(kernel_size, kernel_size, stride, dilation) == 0\n\n\n# Can VALID padding be used for a convolution?\ndef can_use_valid_padding(kernel_size: int, stride: int = 1, dilation: int = 1, **_) -> bool:\n    return get_same_padding(kernel_size, kernel_",
  "\"\"\" Median Pool\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .helpers import to_2tuple, to_4tuple\n\n\nclass MedianPool2d(nn.Module):\n    \"\"\" Median pool (usable as median filter when stride=1) module.\n\n    Args:\n         kernel_size: size of pooling kernel, int or 2-tuple\n         stride: pool stride, int or 2-tuple\n         padding: pool padding, int or 4-tuple (l, r, t, b) as in pytorch F.pad\n         same: override padding and enforce same padding, boolean\n    \"\"\"\n    def __init__(self, kernel_size=3, stride=1, padding=0, same=False):\n        super(MedianPool2d, self).__init__()\n        self.k = to_2tuple(kernel_size)\n        self.stride = to_2tuple(stride)\n        self.padding = to_4tuple(padding)  # convert to l, r, t, b\n        self.same = same\n\n    def _padding(self, x):\n        \"\"\"\n        Pads input x with zeros on the left and right to the same size as the kernel.\n        \"\"\"\n        if self.same:\n            return F.pad(x, self.padding, mode='constant', value=0)\n        else:\n            return F.pad(x, self.padding, mode='reflect')\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: input tensor of shape (N, C, H, W)\n\n        Returns:\n            output tensor of shape (N, C, H, W)\n        \"\"\"\n        x = self._padding(x)\n        x =",
  "\"\"\" Gather-Excite Attention Block\n\nPaper: `Gather-Excite: Exploiting Feature Context in CNNs` - https://arxiv.org/abs/1810.12348\n\nOfficial code here, but it's only partial impl in Caffe: https://github.com/hujie-frank/GENet\n\nI've tried to support all of the extent both w/ and w/o params. I don't believe I've seen another\nimpl that covers all of the cases.\n\nNOTE: extent=0 + extra_params=False is equivalent to Squeeze-and-Excitation\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport math\n\nfrom torch import nn as nn\nimport torch.nn.functional as F\n\nfrom .create_act import create_act_layer, get_act_layer\nfrom .create_conv2d import create_conv2d\nfrom .helpers import make_divisible\nfrom .mlp import ConvMlp\n\n\nclass GatherExcite(nn.Module):\n    \"\"\" Gather-Excite Attention Module\n    \"\"\"\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 act_layer=None,\n                 use_bias=True,\n                 bias_initializer=None,\n                 kernel_initializer=None,\n                 bias_regularizer=None,\n                 kernel_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out",
  "\"\"\" Norm Layer Factory\n\nCreate norm modules by string (to mirror create_act and creat_norm-act fns)\n\nCopyright 2022 Ross Wightman\n\"\"\"\nimport types\nimport functools\n\nimport torch.nn as nn\n\nfrom .norm import GroupNorm, GroupNorm1, LayerNorm, LayerNorm2d\n\n_NORM_MAP = dict(\n    batchnorm=nn.BatchNorm2d,\n    batchnorm2d=nn.BatchNorm2d,\n    batchnorm1d=nn.BatchNorm1d,\n    groupnorm=GroupNorm,\n    groupnorm1=GroupNorm1,\n    layernorm=LayerNorm,\n    layernorm2d=LayerNorm2d,\n)\n_NORM_TYPES = {m for n, m in _NORM_MAP.items()}\n\n\ndef create_norm_layer(layer_name, num_features, **kwargs):\n    layer = get_norm_layer(layer_name)\n    layer_instance = layer(num_features, **kwargs)\n    return layer_instance\n\n\ndef get_norm_layer(norm_layer):\n    assert isinstance(norm_layer, (type, str, )):\n    if isinstance(norm_layer, str):\n        if norm_layer in _NORM_MAP:\n            layer = _NORM_MAP[norm_layer]\n        else:\n            raise ValueError(f\"Norm layer {norm_layer} not found in the list of available norm layers.\")\n    else:\n        layer = norm_layer\n    return layer\n\n\ndef create_norm_layer_from_cfg(cfg, num_features, **kwargs):\n    layer = get_norm_layer_from_cfg(cfg)\n    layer_instance = layer(num_features, **kwargs)\n    return layer_instance",
  "from typing import Optional\n\nimport torch\nfrom torch import nn\nfrom torch import nn, Tensor\nfrom torch.nn.modules.transformer import _get_activation_fn\n\n\ndef add_ml_decoder_head(model):\n    if hasattr(model, 'global_pool') and hasattr(model, 'fc'):  # most CNN models, like Resnet50\n        model.global_pool = nn.Identity()\n        del model.fc\n        num_classes = model.num_classes\n        num_features = model.num_features\n        model.fc = MLDecoder(num_classes=num_classes, initial_num_features=num_features)\n    elif hasattr(model, 'global_pool') and hasattr(model, 'classifier'):  # EfficientNet\n        model.global_pool = nn.Identity()\n        del model.classifier\n        num_classes = model.num_classes\n        num_features = model.num_features\n        model.classifier = MLDecoder(num_classes=num_classes, initial_num_features=num_features)\n    elif 'RegNet' in model._get_name() or 'TResNet' in model._get_name():  # hasattr(model, 'classifier')\n        num_classes = model.num_classes\n        num_features = model.num_features\n        model.classifier = MLDecoder(num_classes=num_classes, initial_num_features=num_features)\n    else:\n        raise ValueError(f\"Model {model} does not have a global_pool and a classifier\")\n\n\nclass MLDecoder(nn.Module):\n    def __init__(self, num_classes=1000, initial_num_features=2048):\n        super().__init__()\n        self.num_classes = num_classes",
  "import torch\nimport torch.nn as nn\n\n\nclass AsymmetricLossMultiLabel(nn.Module):\n    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False):\n        super(AsymmetricLossMultiLabel, self).__init__()\n\n        self.gamma_neg = gamma_neg\n        self.gamma_pos = gamma_pos\n        self.clip = clip\n        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n        self.eps = eps\n\n    def forward(self, x, y):\n        \"\"\"\"\n        Parameters\n        ----------\n        x: input logits\n        y: targets (multi-label binarized vector)\n        \"\"\"\n\n        # Calculating Probabilities\n        x_sigmoid = torch.sigmoid(x)\n        xs_pos = x_sigmoid\n        xs_neg = 1 - x_sigmoid\n\n        # Asymmetric Clipping\n        if self.clip is not None and self.clip > 0:\n            xs_neg = (xs_neg + self.clip).clamp(max=1)\n\n        # Basic CE calculation\n        los = torch.mean(torch.abs(xs_pos - xs_neg))\n\n        # Focal Loss\n        if not self.disable_torch_grad_focal_loss:\n            if self.gamma_neg > 0:\n                gamma = self.gamma_neg\n            else:\n                gamma = 1\n            if self.gamma_pos > 0:\n                gamma_pos = self.gamma_pos\n            else:\n                gamma_pos = 1\n            if gamma_pos == 0:\n                gamma_pos = 1e-8\n            if gamma == 0:\n                gamma = 1e-",
  "from .asymmetric_loss import AsymmetricLossMultiLabel, AsymmetricLossSingleLabel\nfrom .binary_cross_entropy import BinaryCrossEntropyMultiLabel, BinaryCrossEntropySingleLabel\nfrom .binary_cross_entropy_with_logits import BinaryCrossEntropyWithLogitsMultiLabel, BinaryCrossEntropyWithLogitsSingleLabel\nfrom .categorical_cross_entropy import CategoricalCrossEntropyMultiLabel, CategoricalCrossEntropySingleLabel\nfrom .categorical_cross_entropy_with_logits import CategoricalCrossEntropyWithLogitsMultiLabel, CategoricalCrossEntropyWithLogitsSingleLabel\nfrom .ce_loss import CELossMultiLabel, CELossSingleLabel\nfrom .ce_loss_with_logits import CELossWithLogitsMultiLabel,",
  "\"\"\" Binary Cross Entropy w/ a few extras\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BinaryCrossEntropy(nn.Module):\n    \"\"\" BCE with optional one-hot from dense targets, label smoothing, thresholding\n    NOTE for experiments comparing CE to BCE /w label smoothing, may remove\n    \"\"\"\n    def __init__(\n            self, smoothing=0.1, target_threshold: Optional[float] = None, weight: Optional[torch.Tensor] = None,\n            reduction: str = 'mean', pos_weight: Optional[torch.Tensor] = None):\n        super(BinaryCrossEntropy, self).__init__()\n        assert 0. <= smoothing < 1.0\n        self.smoothing = smoothing\n        self.target_threshold = target_threshold\n        self.reduction = reduction\n        self.register_buffer('weight', weight)\n        self.register_buffer('pos_weight', pos_weight)\n\n    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        assert x.shape == target.shape\n        if self.target_threshold is not None:\n            target = F.one_hot(target, num_classes=2)\n            if self.weight is not None:\n                weight = self.weight\n            else:\n                weight = torch.ones_like(target)\n            if self.pos_weight is not None:\n                pos_weight = self.pos_weight\n            else:\n                pos_weight = torch.ones_like(target)\n            loss = F.binary_cross_entropy_with_logits(\n                input=x, target=target, weight=weight,",
  "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .cross_entropy import LabelSmoothingCrossEntropy\n\n\nclass JsdCrossEntropy(nn.Module):\n    \"\"\" Jensen-Shannon Divergence + Cross-Entropy Loss\n\n    Based on impl here: https://github.com/google-research/augmix/blob/master/imagenet.py\n    From paper: 'AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty -\n    https://arxiv.org/abs/1912.02781\n\n    Hacked together by / Copyright 2020 Ross Wightman\n    \"\"\"\n    def __init__(self, num_splits=3, alpha=12, smoothing=0.1):\n        super().__init__()\n        self.num_splits = num_splits\n        self.alpha = alpha\n        if smoothing is not None and smoothing > 0:\n            self.cross_entropy_loss = LabelSmoothingCrossEntropy(smoothing)\n        else:\n            self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n\n    def __call__(self, output, target):\n        split_size = output.shape[0] // self.num_splits\n        output = output.view(self.num_splits, split_size, -1)\n        target = target.view(self.num_splits, split_size)\n        loss = 0\n        for i in range(self.num_splits):\n            loss += self.cross_entropy_loss(output[i], target[i])\n        return loss / self.num_splits\n\n\nclass JsdLoss(nn.Module):\n    \"\"\" Jensen-Shannon Divergence + Cross-Entropy Loss\n\n    Based on impl here:",
  "\"\"\" Cross Entropy w/ smoothing or soft targets\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass LabelSmoothingCrossEntropy(nn.Module):\n    \"\"\" NLL loss with label smoothing.\n    \"\"\"\n    def __init__(self, smoothing=0.1):\n        super(LabelSmoothingCrossEntropy, self).__init__()\n        assert smoothing < 1.0\n        self.smoothing = smoothing\n        self.confidence = 1. - smoothing\n\n    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        logprobs = F.log_softmax(x, dim=-1)\n        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        nll_loss = nll_loss.squeeze(1)\n        smooth_loss = -logprobs.mean(dim=-1)\n        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        return loss.mean()\n\n\nclass SoftTargetCrossEntropy(nn.Module):\n\n    def __init__(self, smoothing=0.1):\n        super(SoftTargetCrossEntropy, self).__init__()\n        assert smoothing < 1.0\n        self.smoothing = smoothing\n        self.confidence = 1. - smoothing\n\n    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        logprobs = F.log_softmax(x, dim=-1)\n        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        nll_loss = nll_loss.squeeze(1)\n        smooth_",
  "\"\"\" Distributed training/validation utils\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport os\n\nimport torch\nfrom torch import distributed as dist\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    hvd = None\n\nfrom .model import unwrap_model\n\n\ndef reduce_tensor(tensor, n):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    rt /= n\n    return rt\n\n\ndef distribute_bn(model, world_size, reduce=False):\n    # ensure every node has the same running bn stats\n    for bn_name, bn_buf in unwrap_model(model).named_buffers(recurse=True):\n        if ('running_mean' in bn_name) or ('running_var' in bn_name):\n            if reduce:\n                # average bn stats across whole group\n                torch.distributed.all_reduce(bn_buf, op=dist.ReduceOp.SUM)\n                bn_buf /= float(world_size)\n            else:\n                # broadcast bn stats from rank 0 to whole group\n                torch.distributed.broadcast(bn_buf, src=0)\n                bn_buf /= float(world_size)\n\n    # ensure every node has the same running bn stats\n    for bn_name, bn_buf in unwrap_model(model).named_buffers(recurse=True):\n        if ('num_batches_tracked' in bn_name):\n            bn_buf = reduce_tensor(bn_buf, world_size)\n            torch.distributed.broadcast(bn_buf, src=0)\n\n\ndef get_rank():\n    \"\"\"\n    Returns the rank of the current process.\n    \"\"\"\n    return hvd.",
  "from .agc import adaptive_clip_grad\nfrom .checkpoint_saver import CheckpointSaver\nfrom .clip_grad import dispatch_clip_grad\nfrom .cuda import ApexScaler, NativeScaler\nfrom .decay_batch import decay_batch_step, check_batch_size_retry\nfrom .distributed import distribute_bn, reduce_tensor, init_distributed_device,\\\n    world_info_from_env, is_distributed_env, is_primary\nfrom .jit import set_use_jit, get_use_jit\nfrom .lr_scheduler import get_scheduler\nfrom .mask_fill import mask_fill\nfrom .model_utils import get_model_size, get_model_size_in_bytes\nfrom .optimizer import get_optimizer, get_optimizer_by_name\nfrom .parallel import get_parallel_mode, get_parallel_mode_by_name, get_parallel_num_devices\nfrom .progress_bar import ProgressBar\nfrom .utils import (\n    add_code_sample_docstrings,\n    add_start_docstrings",
  "\"\"\" Model / state_dict utils\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport fnmatch\n\nimport torch\nfrom torchvision.ops.misc import FrozenBatchNorm2d\n\nfrom timm.layers import BatchNormAct2d, SyncBatchNormAct, FrozenBatchNormAct2d,\\\n    freeze_batch_norm_2d, unfreeze_batch_norm_2d\nfrom .model_ema import ModelEma\n\n\ndef unwrap_model(model):\n    if isinstance(model, ModelEma):\n        return unwrap_model(model.ema)\n    else:\n        return model.module if hasattr(model, 'module') else model\n\n\ndef get_state_dict(model, unwrap_fn=unwrap_model):\n    return unwrap_fn(model).state_dict()\n\n\ndef avg_sq_ch_mean(model, input, output):\n    \"\"\" calculate average channel square mean of output activations\n    \"\"\"\n    return torch.mean(output.mean(axis=[0, 2, 3]) ** 2).item()\n\n\ndef avg_ch_var(model, input, output):\n    \"\"\" calculate average channel variance of output activations\n    \"\"\"\n    return torch.mean(output.var(axis=[0, 2, 3])).item()\n\n\ndef avg_ch_std(model, input, output):\n    \"\"\" calculate average channel standard deviation of output activations\n    \"\"\"\n    return torch.mean(output.std(axis=[0, 2, 3])).item()\n\n\ndef avg_ch_std_sq(model, input, output):\n    \"\"\" calculate average channel standard deviation squared of output activations\n    \"\"\"\n    return torch.mean(output.std(axis=[0, 2, ",
  "\"\"\" JIT scripting/tracing utils\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport os\n\nimport torch\n\n\ndef set_jit_legacy():\n    \"\"\" Set JIT executor to legacy w/ support for op fusion\n    This is hopefully a temporary need in 1.5/1.5.1/1.6 to restore performance due to changes\n    in the JIT exectutor. These API are not supported so could change.\n    \"\"\"\n    #\n    assert hasattr(torch._C, '_jit_set_profiling_executor'), \"Old JIT behavior doesn't exist!\"\n    torch._C._jit_set_profiling_executor(False)\n    torch._C._jit_set_profiling_mode(False)\n    torch._C._jit_override_can_fuse_on_gpu(True)\n    #torch._C._jit_set_texpr_fuser_enabled(True)\n\n\ndef set_jit_fuser(fuser):\n    if fuser == \"te\":\n        # default fuser should be == 'te'\n        torch._C._jit_set_profiling_executor(True)\n        torch._C._jit_set_profiling_mode(True)\n        torch._C._jit_override_can_fuse_on_gpu(True)\n    elif fuser == \"tf\":\n        # default fuser should be == 'tf'\n        torch._C._jit_set_profiling_executor(True)\n        torch._C._jit_set_profiling_mode(True)\n        torch._C._jit_override_can_fuse_on_gpu(True)\n    elif fuser == \"tf-lite\":\n        # default fuser should be == 'tf-lite'\n        torch._C._jit_set_profiling_executor(True",
  "\"\"\" Eval metrics and related\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\n\nclass AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res",
  "import torch\n\nfrom timm.utils.agc import adaptive_clip_grad\n\n\ndef dispatch_clip_grad(parameters, value: float, mode: str = 'norm', norm_type: float = 2.0):\n    \"\"\" Dispatch to gradient clipping method\n\n    Args:\n        parameters (Iterable): model parameters to clip\n        value (float): clipping value/factor/norm, mode dependant\n        mode (str): clipping mode, one of 'norm', 'value', 'agc'\n        norm_type (float): clipping norm type, one of 2.0, 1.0, 0.0\n    \"\"\"\n    if mode == 'norm':\n        if norm_type == 2.0:\n            adaptive_clip_grad(parameters, value)\n        elif norm_type == 1.0:\n            torch.nn.utils.clip_grad_norm_(parameters, value)\n        elif norm_type == 0.0:\n            torch.nn.utils.clip_grad_value_(parameters, value)\n    elif mode == 'value':\n        torch.nn.utils.clip_grad_value_(",
  "\"\"\" Summary utilities\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport csv\nimport os\nfrom collections import OrderedDict\ntry: \n    import wandb\nexcept ImportError:\n    pass\n\n\ndef get_outdir(path, *paths, inc=False):\n    outdir = os.path.join(path, *paths)\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n    elif inc:\n        count = 1\n        outdir_inc = outdir + '-' + str(count)\n        while os.path.exists(outdir_inc):\n            count = count + 1\n            outdir_inc = outdir + '-' + str(count)\n            assert count < 100\n        outdir = outdir_inc\n        os.makedirs(outdir)\n    return outdir\n\n\ndef update_summary(\n        epoch,\n        train_metrics,\n        eval_metrics,\n        filename,\n        lr=None,\n        write_header=False,\n        log_wandb=False,\n):\n    rowd = OrderedDict(epoch=epoch)\n    rowd.update([('train_' + k, v) for k, v in train_metrics.items()])\n    rowd.update([('eval_' + k, v) for k, v in eval_metrics.items()])\n    if lr is not None:\n        rowd['lr'] = lr\n    if write_header:\n        with open(filename, 'w', newline='') as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=['name', 'value'])\n            writer.writeheader()\n    with open(filename, 'a', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=['name', 'value'])\n        writer",
  "\"\"\" Exponential Moving Average (EMA) of model updates\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport logging\nfrom collections import OrderedDict\nfrom copy import deepcopy\n\nimport torch\nimport torch.nn as nn\n\n_logger = logging.getLogger(__name__)\n\n\nclass ModelEma:\n    \"\"\" Model Exponential Moving Average (DEPRECATED)\n\n    Keep a moving average of everything in the model state_dict (parameters and buffers).\n    This version is deprecated, it does not work with scripted models. Will be removed eventually.\n\n    This is intended to allow functionality like\n    https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n\n    A smoothed version of the weights is necessary for some training schemes to perform well.\n    E.g. Google's hyper-params for training MNASNet, MobileNet-V3, EfficientNet, etc that use\n    RMSprop with a short 2.4-3 epoch decay period and slow LR decay rate of .96-.99 requires EMA\n    smoothing of weights to match results. Pay attention to the decay constant you are using\n    relative to the original LR decay rate.\n\n    Args:\n        model_state_dict (dict):\n            The model state_dict.\n        decay_rate (float):\n            The decay rate.\n        epsilon (float):\n            The epsilon value.\n        use_moving_average (bool):\n            Whether to use the moving average or not.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_state_dict: dict,\n        decay_rate: float = 0.99,\n        epsilon: float = 1e-8,\n        use_moving_average: bool = True,\n    ) -> None:\n        self.",
  "\"\"\" Batch size decay and retry helpers.\n\nCopyright 2022 Ross Wightman\n\"\"\"\nimport math\n\n\ndef decay_batch_step(batch_size, num_intra_steps=2, no_odd=False):\n    \"\"\" power of two batch-size decay with intra steps\n\n    Decay by stepping between powers of 2:\n    * determine power-of-2 floor of current batch size (base batch size)\n    * divide above value by num_intra_steps to determine step size\n    * floor batch_size to nearest multiple of step_size (from base batch size)\n    Examples:\n     num_steps == 4 --> 64, 56, 48, 40, 32, 28, 24, 20, 16, 14, 12, 10, 8, 7, 6, 5, 4, 3, 2, 1\n     num_steps (no_odd=True) == 4 --> 64, 56, 48, 40, 32, 28, 24, 20, 16, 14, 12, 10, 8, 7, 6, 5, 4, 3, 2, 1\n    \"\"\"\n    if no_odd:\n        batch_size = math.ceil(batch_size / 2 ** num_intra_steps)\n    else:\n        batch_size = math.ceil(batch_size / 2 ** num_intra_steps) * 2 ** num_intra_steps\n    return batch_size\n\n\ndef retry_batch_step(batch_size, num_intra_steps",
  "\"\"\" Adaptive Gradient Clipping\n\nAn impl of AGC, as per (https://arxiv.org/abs/2102.06171):\n\n@article{brock2021high,\n  author={Andrew Brock and Soham De and Samuel L. Smith and Karen Simonyan},\n  title={High-Performance Large-Scale Image Recognition Without Normalization},\n  journal={arXiv preprint arXiv:},\n  year={2021}\n}\n\nCode references:\n  * Official JAX impl (paper authors): https://github.com/deepmind/deepmind-research/tree/master/nfnets\n  * Phil Wang's PyTorch gist: https://gist.github.com/lucidrains/0d6560077edac419ab5d3aa29e674d5c\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport torch\n\n\ndef unitwise_norm(x, norm_type=2.0):\n    if x.ndim <= 1:\n        return x.norm(norm_type)\n    else:\n        return torch.norm(x, norm_type, 2, 1, True)\n\n\ndef agc(x, eps=1e-6, norm_type=2.0):\n    \"\"\"\n    Adaptive Gradient Clipping\n    Args:\n        x: input tensor\n        eps: epsilon for numerical stability\n        norm_type: 2.0 for L2 norm, 1.0 for L1 norm\n    \"\"\"\n    if x.ndim <= 1:\n        return x.clamp(-eps, eps)\n    else:\n        return torch.clamp(x, -eps, eps)\n\n\ndef a",
  "from typing import Optional, Tuple, List\n\nimport torch\n\n\ndef onnx_forward(onnx_file, example_input):\n    import onnxruntime\n\n    sess_options = onnxruntime.SessionOptions()\n    session = onnxruntime.InferenceSession(onnx_file, sess_options)\n    input_name = session.get_inputs()[0].name\n    output = session.run([], {input_name: example_input.numpy()})\n    output = output[0]\n    return output\n\n\ndef onnx_export(\n        model: torch.nn.Module,\n        output_file: str,\n        example_input: Optional[torch.Tensor] = None,\n        training: bool = False,\n        verbose: bool = False,\n        check: bool = True,\n        check_forward: bool = False,\n        batch_size: int = 64,\n        input_size: Tuple[int, int, int] = None,\n        opset: Optional[int] = None,\n        dynamic_size: bool = False,\n        aten_fallback: bool = False,\n        keep_initializers: Optional[bool] = None,\n        input_names: List[str] = None,\n        output_names: List[str] = None,\n        dynamic_axes: Optional[Dict[str, List[int]]] = None,\n        **kwargs,\n) -> None:\n    import onnxruntime\n\n    if not input_names:\n        input_names = [model.input_names[0]]\n    if not output_names:\n        output_names = [model.output_names[0]]\n\n    if not input_size:\n        input_size = model.input_size\n    if not opset:\n        opset = model.opset\n    if not dynamic_size:\n        dynamic_size",
  "\"\"\" Checkpoint Saver\n\nTrack top-n training checkpoints and maintain recovery checkpoints on specified intervals.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport glob\nimport operator\nimport os\nimport logging\n\nimport torch\n\nfrom .model import unwrap_model, get_state_dict\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass CheckpointSaver:\n    def __init__(\n            self,\n            model,\n            optimizer,\n            args=None,\n            model_ema=None,\n            amp_scaler=None,\n            checkpoint_prefix='checkpoint',\n            recovery_prefix='recovery',\n            checkpoint_dir='',\n            recovery_dir='',\n            decreasing=False,\n            max_history=10,\n            unwrap_fn=unwrap_model):\n\n        # objects to save state_dicts of\n        self.model = model\n        self.optimizer = optimizer\n        self.args = args\n        self.model_ema = model_ema\n        self.amp_scaler = amp_scaler\n\n        # state\n        self.checkpoint_files = []  # (filename, metric) tuples in order of decreasing betterness\n        self.best_epoch = None\n        self.best_metric = None\n        self.decreasing = decreasing\n        self.max_history = max_history\n        self.unwrap_fn = unwrap_fn\n\n        # directories\n        self.checkpoint_dir = checkpoint_dir\n        self.recovery_dir = recovery_dir\n\n        # load checkpoint\n        if checkpoint_dir:\n            self.load_checkpoint()\n\n    def save_checkpoint(self, epoch, metric):\n        \"\"\"\n        Save a checkpoint.\n\n        Args:\n            epoch (int): current epoch\n            metric (float): current metric\n        \"\"\"\n        # save state_dict\n        state_dict = get_state_",
  "import random\nimport numpy as np\nimport torch\n\n\ndef random_seed(seed=42, rank=0):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if rank == 0:\n        torch.distributed.init_process_group(backend=\"nccl\", init_method=\"tcp://127.0.0.1:23456\")\n\n\ndef get_device():\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    else:\n        return torch.device(\"cpu\")\n",
  "\"\"\" CUDA / AMP utils\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\n\ntry:\n    from apex import amp\n    has_apex = True\nexcept ImportError:\n    amp = None\n    has_apex = False\n\nfrom .clip_grad import dispatch_clip_grad\n\n\nclass ApexScaler:\n    state_dict_key = \"amp\"\n\n    def __call__(\n            self,\n            loss,\n            optimizer,\n            clip_grad=None,\n            clip_mode='norm',\n            parameters=None,\n            create_graph=False,\n            need_update=True,\n    ):\n        with amp.scale_loss(loss, optimizer) as scaled_loss:\n            scaled_loss.backward(create_graph=create_graph)\n        if need_update:\n            if clip_grad is not None:\n                dispatch_clip_grad(amp.master_params(optimizer), clip_grad, mode=clip_mode)\n            optimizer.step()\n\n    def state_dict(self):\n        if 'state_dict' in amp.__dict__:\n            return amp.state_dict()\n\n    def load_state_dict(self, state_dict):\n        if 'load_state_dict' in amp.__dict__:\n            amp.load_state_dict(state_dict)\n\n\ndef get_scaler(loss, optimizer):\n    if amp is None:\n        return None\n    return ApexScaler()\n\n\ndef get_amp_scaler(loss, optimizer):\n    if amp is None:\n        return None\n    return amp.GradScaler()\n\n\ndef get_amp_optimizer(loss, optimizer):\n    if amp is None:\n        return None\n    return amp.initialize(optimizer, opt_level=1)\n\n\ndef get_amp_scaler",
  "\"\"\" Misc utils\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport argparse\nimport ast\nimport re\n\n\ndef natural_key(string_):\n    \"\"\"See http://www.codinghorror.com/blog/archives/001018.html\"\"\"\n    return [int(s) if s.isdigit() else s for s in re.split(r'(\\d+)', string_.lower())]\n\n\ndef add_bool_arg(parser, name, default=False, help=''):\n    dest_name = name.replace('-', '_')\n    group = parser.add_mutually_exclusive_group(required=False)\n    group.add_argument('--' + name, dest=dest_name, action='store_true', help=help)\n    group.add_argument('--no-' + name, dest=dest_name, action='store_false', help=help)\n    parser.set_defaults(**{dest_name: default})\n\n\nclass ParseKwargs(argparse.Action):\n    def __call__(self, parser, namespace, values, option_string=None):\n        kw = {}\n        for arg in values.split(','):\n            arg = arg.strip()\n            if '=' not in arg:\n                raise ValueError('Invalid argument: {}'.format(arg))\n            key, value = arg.split('=')\n            kw[key] = value\n        setattr(namespace, self.dest, kw)\n\n\ndef parse_kwargs(args):\n    kwargs = {}\n    for arg in args.split(','):\n        arg = arg.strip()\n        if '=' not in arg:\n            raise ValueError('Invalid argument: {}'.format(arg))\n        key, value = arg.split('=')\n        kwargs[key] = value\n    return",
  "\"\"\" Logging helpers\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport logging\nimport logging.handlers\n\n\nclass FormatterNoInfo(logging.Formatter):\n    def __init__(self, fmt='%(levelname)s: %(message)s'):\n        logging.Formatter.__init__(self, fmt)\n\n    def format(self, record):\n        if record.levelno == logging.INFO:\n            return str(record.getMessage())\n        return logging.Formatter.format(self, record)\n\n\ndef setup_default_logging(default_level=logging.INFO, log_path=''):\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(default_level)\n    console_handler.setFormatter(FormatterNoInfo())\n    logger = logging.getLogger()\n    logger.setLevel(default_level)\n    logger.addHandler(console_handler)\n\n    if log_path:\n        file_handler = logging.handlers.RotatingFileHandler(log_path, maxBytes=10000000, backupCount=10)\n        file_handler.setLevel(default_level)\n        file_handler.setFormatter(FormatterNoInfo())\n        logger.addHandler(file_handler)\n\n",
  "\"\"\" Polynomial Scheduler\n\nPolynomial LR schedule with warmup, noise.\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport math\nimport logging\n\nimport torch\n\nfrom .scheduler import Scheduler\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass PolyLRScheduler(Scheduler):\n    \"\"\" Polynomial LR Scheduler w/ warmup, noise, and k-decay\n\n    k-decay option based on `k-decay: A New Method For Learning Rate Schedule` - https://arxiv.org/abs/2004.05909\n    \"\"\"\n\n    def __init__(\n            self,\n            optimizer: torch.optim.Optimizer,\n            t_initial: int,\n            power: float = 0.5,\n            lr_min: float = 0.,\n            cycle_mul: float = 1.,\n            cycle_decay: float = 1.,\n            cycle_limit: int = 1,\n            warmup_t=0,\n            warmup_lr_init=0,\n            warmup_prefix=False,\n            t_in_epochs=True,\n            noise_range_t=None,\n            noise_pct=0.0,\n            noise_std=0.0,\n            k_decay=0.0,\n            k_decay_t=0,\n            k_decay_prefix=False,\n            k_decay_limit=1,\n            k_decay_cycle_mul=1.0,\n            k_decay_cycle_decay=1.0,\n            k_decay_cycle_limit=1,\n            k_decay_warmup_t=0,\n            k_decay_warmup_lr_init=0,\n            k_decay_warmup_prefix=False,\n            k_decay_warm",
  "from .cosine_lr import CosineLRScheduler\nfrom .multistep_lr import MultiStepLRScheduler\nfrom .plateau_lr import PlateauLRScheduler\nfrom .poly_lr import PolyLRScheduler\nfrom .step_lr import StepLRScheduler\nfrom .warmup_cosine_lr import WarmupCosineLRScheduler\nfrom .warmup_poly_lr import WarmupPolyLRScheduler\nfrom .warmup_step_lr import WarmupStepLRScheduler\nfrom .warmup_warmup_cosine_lr import WarmupWarmupCosineLRScheduler\nfrom .warmup_warmup_poly_lr import WarmupWarmupPolyLRScheduler\nfrom .warmup_warmup_step_lr import",
  "\"\"\" Plateau Scheduler\n\nAdapts PyTorch plateau scheduler and allows application of noise, warmup.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\n\nfrom .scheduler import Scheduler\n\n\nclass PlateauLRScheduler(Scheduler):\n    \"\"\"Decay the LR by a factor every time the validation loss plateaus.\"\"\"\n\n    def __init__(\n            self,\n            optimizer,\n            decay_rate=0.1,\n            patience_t=10,\n            verbose=True,\n            threshold=1e-4,\n            cooldown_t=0,\n            warmup_t=0,\n            warmup_lr_init=0,\n            lr_min=0,\n            mode='max',\n            noise_range_t=None,\n            noise_type='normal',\n            noise_pct=0.67,\n            noise_std=1.0,\n            noise_seed=None,\n            initialize=True,\n    ):\n        super().__init__(\n            optimizer,\n            'lr',\n            noise_range_t=noise_range_t,\n            noise_type=noise_type,\n            noise_pct=noise_pct,\n            noise_std=noise_std,\n            noise_seed=noise_seed,\n            initialize=initialize,\n        )\n\n        self.decay_rate = decay_rate\n        self.patience_t = patience_t\n        self.verbose = verbose\n        self.threshold = threshold\n        self.cooldown_t = cooldown_t\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.lr_min = lr_min\n        self.mode = mode\n\n    def get_lr(self):\n        \"\"\"Get the current learning",
  "\"\"\" Scheduler Factory\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nfrom typing import List, Union\n\nfrom torch.optim import Optimizer\n\nfrom .cosine_lr import CosineLRScheduler\nfrom .multistep_lr import MultiStepLRScheduler\nfrom .plateau_lr import PlateauLRScheduler\nfrom .poly_lr import PolyLRScheduler\nfrom .step_lr import StepLRScheduler\nfrom .tanh_lr import TanhLRScheduler\n\n\ndef scheduler_kwargs(cfg):\n    \"\"\" cfg/argparse to kwargs helper\n    Convert scheduler args in argparse args or cfg (.dot) like object to keyword args.\n    \"\"\"\n    eval_metric = getattr(cfg, 'eval_metric', 'top1')\n    plateau_mode = 'min' if 'loss' in eval_metric else 'max'\n    kwargs = dict(\n        sched=cfg.sched,\n        num_epochs=getattr(cfg, 'epochs', 100),\n        decay_epochs=getattr(cfg, 'decay_epochs', 30),\n        decay_milestones=getattr(cfg, 'decay_milestones', [30, 60, 90]),\n        warmup_epochs=getattr(cfg, 'warmup_epochs', 0),\n        warmup_milestones=getattr(cfg, 'warmup_milestones', [0, 10, 20]),\n        step_size=getattr(cfg, 'step_size', 0.1),\n        gamma=getattr(cfg, 'gamma', 0.1),\n        mode=getattr(cfg, 'mode', 'min'),\n        last_epoch=0,\n        verbose=False,\n        reduce_on_plateau=False,\n       ",
  "\"\"\" Cosine Scheduler\n\nCosine LR schedule with warmup, cycle/restarts, noise, k-decay.\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport logging\nimport math\nimport numpy as np\nimport torch\n\nfrom .scheduler import Scheduler\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass CosineLRScheduler(Scheduler):\n    \"\"\"\n    Cosine decay with restarts.\n    This is described in the paper https://arxiv.org/abs/1608.03983.\n\n    Inspiration from\n    https://github.com/allenai/allennlp/blob/master/allennlp/training/learning_rate_schedulers/cosine.py\n\n    k-decay option based on `k-decay: A New Method For Learning Rate Schedule` - https://arxiv.org/abs/2004.05909\n    \"\"\"\n\n    def __init__(\n            self,\n            optimizer: torch.optim.Optimizer,\n            t_initial: int,\n            lr_min: float = 0.,\n            cycle_mul: float = 1.,\n            cycle_decay: float = 1.,\n            cycle_restart: float = 0.5,\n            k_decay: float = 0.0,\n            k_decay_restart: float = 0.0,\n            k_decay_cycle: float = 0.0,\n            k_decay_cycle_restart: float = 0.0,\n            k_decay_cycle_restart_min: float = 0.0,\n            k_decay_cycle_restart_max: float = 0.0,\n            use_k_decay: bool = True,\n            use_restarts: bool = True,\n            use",
  "\"\"\" MultiStep LR Scheduler\n\nBasic multi step LR schedule with warmup, noise.\n\"\"\"\nimport torch\nimport bisect\nfrom timm.scheduler.scheduler import Scheduler\nfrom typing import List\n\nclass MultiStepLRScheduler(Scheduler):\n    \"\"\"\n    \"\"\"\n\n    def __init__(\n            self,\n            optimizer: torch.optim.Optimizer,\n            decay_t: List[int],\n            decay_rate: float = 1.,\n            warmup_t=0,\n            warmup_lr_init=0,\n            warmup_prefix=True,\n            t_in_epochs=True,\n            noise_range_t=None,\n            noise_pct=0.67,\n            noise_std=1.0,\n            noise_seed=42,\n            initialize=True,\n    ) -> None:\n        super().__init__(\n            optimizer,\n            param_group_field=\"lr\",\n            t_in_epochs=t_in_epochs,\n            noise_range_t=noise_range_t,\n            noise_pct=noise_pct,\n            noise_std=noise_std,\n            noise_seed=noise_seed,\n            initialize=initialize,\n        )\n\n        self.decay_t = decay_t\n        self.decay_rate = decay_rate\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.warmup_prefix = warmup_prefix\n        self.t_in_epochs = t_in_epochs\n\n    def get_lr(self):\n        \"\"\"\n        \"\"\"\n        if self.t_in_epochs:\n            return [self.get_lr_t(epoch) for epoch in range(self.num_epochs)]\n        else:\n            return [self.get_lr_t",
  "import abc\nfrom abc import ABC\nfrom typing import Any, Dict, Optional\n\nimport torch\n\n\nclass Scheduler(ABC):\n    \"\"\" Parameter Scheduler Base Class\n    A scheduler base class that can be used to schedule any optimizer parameter groups.\n\n    Unlike the builtin PyTorch schedulers, this is intended to be consistently called\n    * At the END of each epoch, before incrementing the epoch count, to calculate next epoch's value\n    * At the END of each optimizer update, after incrementing the update count, to calculate next update's value\n\n    The schedulers built on this should try to remain as stateless as possible (for simplicity).\n\n    This family of schedulers is attempting to avoid the confusion of the meaning of 'last_epoch'\n    and -1 values for special behaviour. All epoch and update counts must be tracked in the training\n    code and explicitly passed in to the schedulers on the corresponding step or step_update call.\n\n    Based on ideas from:\n     * https://github.com/pytorch/fairseq/tree/master/fairseq/optim/lr_scheduler\n     * https://github.com/allenai/allennlp/tree/master/allennlp/training/learning_rate_schedulers\n    \"\"\"\n\n    def __init__(self, optimizer, last_epoch=-1):\n        self.optimizer = optimizer\n        self.last_epoch = last_epoch\n\n    def get_last_epoch(self):\n        return self.last_epoch\n\n    def get_lr(self):\n        return self.optimizer.param_groups[0][\"lr\"]\n\n    def get_last_update(self):\n        return self.optimizer.param_groups[0][\"last_update\"]\n\n    def get_last_value(self):\n        return self.optimizer.param_groups[0][\"last_value\"]\n\n    def get_next_value(self",
  "\"\"\" TanH Scheduler\n\nTanH schedule with warmup, cycle/restarts, noise.\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport logging\nimport math\nimport numpy as np\nimport torch\n\nfrom .scheduler import Scheduler\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass TanhLRScheduler(Scheduler):\n    \"\"\"\n    Hyberbolic-Tangent decay with restarts.\n    This is described in the paper https://arxiv.org/abs/1806.01593\n    \"\"\"\n\n    def __init__(\n            self,\n            optimizer: torch.optim.Optimizer,\n            t_initial: int,\n            lb: float = -7.,\n            ub: float = 3.,\n            lr_min: float = 0.,\n            cycle_mul: float = 1.,\n            cycle_decay: float = 1.,\n            cycle_limit: int = 1,\n            warmup_t=0,\n            warmup_lr_init=0,\n            warmup_prefix=False,\n            t_in_epochs=True,\n            noise_range_t=None,\n            noise_pct=0.0,\n            noise_std=0.0,\n            noise_type=\"gaussian\",\n            noise_mean=0.0,\n            noise_std_dev=0.0,\n            noise_mode=\"cycle\",\n            noise_cycle_mul=1.,\n            noise_cycle_decay=1.,\n            noise_cycle_limit=1,\n            noise_prefix=False,\n            noise_in_epochs=True,\n            **kwargs\n    ):\n        super().__init__(optimizer, **kwargs)\n        self.t_initial = t_initial\n        self.lb = lb\n        self.ub = ub\n        self.",
  "\"\"\" Step Scheduler\n\nBasic step LR schedule with warmup, noise.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nimport torch\n\nfrom .scheduler import Scheduler\n\n\nclass StepLRScheduler(Scheduler):\n    \"\"\"\n    \"\"\"\n\n    def __init__(\n            self,\n            optimizer: torch.optim.Optimizer,\n            decay_t: float,\n            decay_rate: float = 1.,\n            warmup_t=0,\n            warmup_lr_init=0,\n            warmup_prefix=True,\n            t_in_epochs=True,\n            noise_range_t=None,\n            noise_pct=0.67,\n            noise_std=1.0,\n            noise_seed=42,\n            initialize=True,\n    ) -> None:\n        super().__init__(\n            optimizer,\n            param_group_field=\"lr\",\n            t_in_epochs=t_in_epochs,\n            noise_range_t=noise_range_t,\n            noise_pct=noise_pct,\n            noise_std=noise_std,\n            noise_seed=noise_seed,\n            initialize=initialize,\n        )\n\n        self.decay_t = decay_t\n        self.decay_rate = decay_rate\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.warmup_prefix = warmup_prefix\n        self.t_in_epochs = t_in_epochs\n        self.noise_range_t = noise_range_t\n        self.noise_pct = noise_pct\n        self.noise_std = noise_std\n        self.noise_seed = noise_seed\n        self.initialize = initialize\n\n    def get_lr(",
  "from .auto_augment import RandAugment, AutoAugment, rand_augment_ops, auto_augment_policy,\\\n    rand_augment_transform, auto_augment_transform\nfrom .config import resolve_data_config, resolve_model_data_config\nfrom .constants import *\nfrom .dataset import ImageDataset, IterableImageDataset, AugMixDataset\nfrom .dataset_factory import create_dataset\nfrom .dataset_info import DatasetInfo, CustomDatasetInfo\nfrom .imagenet_utils import get_imagenet_mean, get_imagenet_std\nfrom .image_processing import (\n    get_image_size,\n    get_image_size_from_image_id,\n    get_image_size_from_image_name,\n    get_image_size_from_image_path,\n    get_image_size_from_image_tensor,\n    get_image_size_from_image_tensor_list,\n    get_image_size_from_image_urls,\n    get_image_size_from_image_urls_list,\n    get",
  "import logging\nfrom .constants import *\n\n\n_logger = logging.getLogger(__name__)\n\n\ndef resolve_data_config(\n        args=None,\n        pretrained_cfg=None,\n        model=None,\n        use_test_size=False,\n        verbose=False\n):\n    assert model or args or pretrained_cfg, \"At least one of model, args, or pretrained_cfg required for data config.\"\n    args = args or {}\n    pretrained_cfg = pretrained_cfg or {}\n    if not pretrained_cfg and model is not None and hasattr(model, 'pretrained_cfg'):\n        pretrained_cfg = model.pretrained_cfg\n    data_config = {}\n\n    # Resolve input/image size\n    in_chans = 3\n    if args.get('chans', None) is not None:\n        in_chans = args['chans']\n\n    input_size = (in_chans, 224, 224)\n    if args.get('input_size', None) is not None:\n        assert isinstance(args['input_size'], (tuple, list))\n        assert len(args['input_size']) == 3\n        input_size = tuple(args['input_size'])\n\n    if args.get('use_test_size', None) is not None:\n        use_test_size = args['use_test_size']\n\n    if args.get('verbose', None) is not None:\n        verbose = args['verbose']\n\n    if args.get('num_workers', None) is not None:\n        num_workers = args['num_workers']\n\n    if args.get('batch_size', None) is not None:\n        batch_size = args['batch_size']\n\n    if args.get('num_workers', None) is not None:",
  "\"\"\" Transforms Factory\nFactory methods for building image transforms for use with TIMM (PyTorch Image Models)\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\nimport math\n\nimport torch\nfrom torchvision import transforms\n\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, DEFAULT_CROP_PCT\nfrom timm.data.auto_augment import rand_augment_transform, augment_and_mix_transform, auto_augment_transform\nfrom timm.data.transforms import str_to_interp_mode, str_to_pil_interp, RandomResizedCropAndInterpolation,\\\n    ResizeKeepRatio, CenterCropOrPad, ToNumpy\nfrom timm.data.random_erasing import RandomErasing\n\n\ndef transforms_noaug_train(\n        img_size=224,\n        interpolation='bilinear',\n        use_prefetcher=False,\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD,\n):\n    if interpolation == 'random':\n        # random interpolation not supported with noaug\n        raise ValueError(\"random interpolation not supported with noaug\")\n    if interpolation == 'nearest':\n        interpolation = 'bilinear'\n    if interpolation == 'bilinear':\n        interpolation = 'bicubic'\n    if interpolation == 'bicubic':\n        interpolation = 'trilinear'\n    if interpolation == 'trilinear':\n        interpolation = 'lanczos'\n    if interpolation == 'lanczos':\n        interpolation = 'nearest'\n    if interpolation == 'area':\n        interpolation = 'area'\n    if interpolation == 'area_weighted':\n        interpolation = 'area_weighted'\n    if interpolation == 'nearest_neighbor':\n       ",
  "\"\"\" Loader Factory, Fast Collate, CUDA Prefetcher\n\nPrefetcher and Fast Collate inspired by NVIDIA APEX example at\nhttps://github.com/NVIDIA/apex/commit/d5e2bb4bdeedd27b1dfaf5bb2b24d6c000dee9be#diff-cf86c282ff7fba81fad27a559379d5bf\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\nimport logging\nimport random\nfrom contextlib import suppress\nfrom functools import partial\nfrom itertools import repeat\nfrom typing import Callable\n\nimport torch\nimport torch.utils.data\nimport numpy as np\n\nfrom .constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom .dataset import IterableImageDataset\nfrom .distributed_sampler import OrderedDistributedSampler, RepeatAugSampler\nfrom .random_erasing import RandomErasing\nfrom .mixup import FastCollateMixup\nfrom .transforms_factory import create_transform\n\n_logger = logging.getLogger(__name__)\n\n\nclass Prefetcher(torch.utils.data.DataLoader):\n    \"\"\"\n    A PyTorch DataLoader that supports prefetching.\n\n    Args:\n        dataset (`IterableImageDataset`):\n            The dataset to be loaded.\n        batch_size (`int`, *optional*, defaults to 32):\n            The batch size.\n        shuffle (`bool`, *optional*, defaults to `True`):\n            Whether to shuffle the data.\n        num_workers (`int`, *optional*, defaults to 0):\n            The number of data loading workers.\n        pin_memory (`bool`, *optional*, defaults to `False`):\n            Whether to pin memory.\n       ",
  "\"\"\" Random Erasing (Cutout)\n\nOriginally inspired by impl at https://github.com/zhunzhong07/Random-Erasing, Apache 2.0\nCopyright Zhun Zhong & Liang Zheng\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\nimport random\nimport math\n\nimport torch\n\n\ndef _get_pixels(per_pixel, rand_color, patch_size, dtype=torch.float32, device='cuda'):\n    # NOTE I've seen CUDA illegal memory access errors being caused by the normal_()\n    # paths, flip the order so normal is run on CPU if this becomes a problem\n    # Issue has been fixed in master https://github.com/pytorch/pytorch/issues/19508\n    if per_pixel:\n        return torch.empty(patch_size, dtype=dtype, device=device).normal_()\n    elif rand_color:\n        return torch.empty((patch_size[0], 1, 1), dtype=dtype, device=device).normal_()\n    else:\n        return torch.zeros((patch_size[0], 1, 1), dtype=dtype, device=device)\n\n\ndef random_erasing(img, per_pixel=True, rand_color=True, patch_size=(1, 1), **kwargs):\n    \"\"\"\n    Random Erasing (Cutout)\n\n    Args:\n        img (Tensor): Image to be erased.\n        per_pixel (bool): If True, erasing will be done per pixel.\n        rand_color (bool): If True, erasing will be done with random color.\n        patch_size (tuple): Patch size.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tensor: Er",
  "import math\nimport numbers\nimport random\nimport warnings\nfrom typing import List, Sequence\n\nimport torch\nimport torchvision.transforms.functional as F\ntry:\n    from torchvision.transforms.functional import InterpolationMode\n    has_interpolation_mode = True\nexcept ImportError:\n    has_interpolation_mode = False\nfrom PIL import Image\nimport numpy as np\n\n\nclass ToNumpy:\n\n    def __call__(self, pil_img):\n        np_img = np.array(pil_img, dtype=np.uint8)\n        if np_img.ndim < 3:\n            np_img = np.expand_dims(np_img, axis=-1)\n        np_img = np.rollaxis(np_img, 2)  # HWC to CHW\n        return np_img\n\n\nclass ToTensor:\n\n    def __init__(self, dtype=torch.float32):\n        self.dtype = dtype\n\n    def __call__(self, pil_img):\n        np_img = np.array(pil_img, dtype=np.uint8)\n        if np_img.ndim < 3:\n            np_img = np.expand_dims(np_img, axis=-1)\n        np_img = np.rollaxis(np_img, 2)  # HWC to CHW\n        return torch.from_numpy(np_img).type(self.dtype)\n\n\nclass ToPIL:\n\n    def __init__(self, pil_transform=None):\n        self.pil_transform = pil_transform\n\n    def __call__(self, tensor):\n        if self.pil_transform is None:\n            return tensor.cpu().numpy()\n        else:\n            return self.pil_transform(tensor)\n\n\nclass Compose:\n\n    def __",
  "DEFAULT_CROP_PCT = 0.875\nDEFAULT_CROP_MODE = 'center'\nIMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\nIMAGENET_INCEPTION_MEAN = (0.5, 0.5, 0.5)\nIMAGENET_INCEPTION_STD = (0.5, 0.5, 0.5)\nIMAGENET_DPN_MEAN = (124 / 255, 117 / 255, 104 / 255)\nIMAGENET_DPN_STD = tuple([1 / (.0167 * 255)] * 3)\nOPENAI_CLIP_MEAN = (0.48145466, 0.4578275, 0.40821073)\nOPENAI_CLIP_STD = (0.26862954, 0.26130258, 0.27577711)\n\n# ImageNet mean and std\nIMAGENET_MEAN = IMAGENET_DEFAULT_MEAN\nIMAGENET_STD = IMAGENET_DEFAULT_STD\n\n# Inception mean and std\nINCEPTION_MEAN = IMAGENET_INCEPTION_MEAN\nINCEPTION_STD = IMAGENET_",
  "\"\"\" Dataset Factory\n\nHacked together by / Copyright 2021, Ross Wightman\n\"\"\"\nimport os\n\nfrom torchvision.datasets import CIFAR100, CIFAR10, MNIST, KMNIST, FashionMNIST, ImageFolder\ntry:\n    from torchvision.datasets import Places365\n    has_places365 = True\nexcept ImportError:\n    has_places365 = False\ntry:\n    from torchvision.datasets import INaturalist\n    has_inaturalist = True\nexcept ImportError:\n    has_inaturalist = False\ntry:\n    from torchvision.datasets import QMNIST\n    has_qmnist = True\nexcept ImportError:\n    has_qmnist = False\ntry:\n    from torchvision.datasets import ImageNet\n    has_imagenet = True\nexcept ImportError:\n    has_imagenet = False\n\nfrom .dataset import IterableImageDataset, ImageDataset\n\n_TORCH_BASIC_DS = dict(\n    cifar10=CIFAR10,\n    cifar100=CIFAR100,\n    mnist=MNIST,\n    kmnist=KMNIST,\n    fashionmnist=FashionMNIST,\n    imagefolder=ImageFolder,\n    places365=Places365,\n    inaturalist=INaturalist,\n    qmnist=QMNIST,\n    imagenet=ImageNet,\n)\n\n_TORCH_BASIC_DS_NAMES = dict(\n    cifar10=(\"cifar10\", \"cifar100\"),\n    cifar100=(\"cifar100\", \"cifar100\"),\n    mnist=(\"mnist\", \"mnist\"),\n    kmnist=(\"kmnist\", \"k",
  "import math\nimport torch\nfrom torch.utils.data import Sampler\nimport torch.distributed as dist\n\n\nclass OrderedDistributedSampler(Sampler):\n    \"\"\"Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    \"\"\"\n\n    def __init__(self, dataset, num_replicas=None, rank=None):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.total_size = len(self.dataset)\n        self.indices = list(range(self.total_size))\n        if self.rank == 0:\n            self.reindex()\n\n    def __iter__(self):\n        return self\n\n    def __len__(self):\n        return self.total_size\n\n    def reindex(self):\n        \"\"\"Rearranges indices of the dataset to balance the load among processes.\"\"\"\n        if self.rank == 0:\n            # deterministically shuffle indices\n            indices =",
  "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Optional, Union\n\n\nclass DatasetInfo(ABC):\n\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def num_classes(self):\n        pass\n\n    @abstractmethod\n    def label_names(self):\n        pass\n\n    @abstractmethod\n    def label_descriptions(self, detailed: bool = False, as_dict: bool = False) -> Union[List[str], Dict[str, str]]:\n        pass\n\n    @abstractmethod\n    def index_to_label_name(self, index) -> str:\n        pass\n\n    @abstractmethod\n    def index_to_description(self, index: int, detailed: bool = False) -> str:\n        pass\n\n    @abstractmethod\n    def label_name_to_description(self, label: str, detailed: bool = False) -> str:\n        pass\n\n\nclass CustomDatasetInfo(DatasetInfo):\n    \"\"\" DatasetInfo that wraps passed values for custom datasets.\"\"\"\n\n    def __init__(\n            self,\n            label_names: Union[List[str], Dict[int, str]],\n            label_descriptions: Optional[Dict[str, str]] = None\n    ):\n        super().__init__()\n        assert len(label_names) == len(label_descriptions)\n        self.label_names = label_names\n        self.label_descriptions = label_descriptions\n\n    def num_classes(self):\n        return len(self.label_names)\n\n    def label_names(self):\n        return self.label_names\n\n    def label_descriptions(self, detailed: bool = False, as_dict: bool = False) -> Union[List[str], Dict[str, str]]:\n        if as_dict:\n            return self.label_descriptions\n        else:\n            if detailed:\n                return [self.label_descriptions[label]",
  "\"\"\" Quick n Simple Image Folder, Tarfile based DataSet\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\nimport io\nimport logging\nfrom typing import Optional\n\nimport torch\nimport torch.utils.data as data\nfrom PIL import Image\n\nfrom .readers import create_reader\n\n_logger = logging.getLogger(__name__)\n\n\n_ERROR_RETRY = 50\n\n\nclass ImageDataset(data.Dataset):\n\n    def __init__(\n            self,\n            root,\n            reader=None,\n            split='train',\n            class_map=None,\n            load_bytes=False,\n            img_mode='RGB',\n            transform=None,\n            target_transform=None,\n    ):\n        if reader is None or isinstance(reader, str):\n            reader = create_reader(\n                reader or '',\n                root=root,\n                split=split,\n                class_map=class_map\n            )\n        self.reader = reader\n        self.load_bytes = load_bytes\n        self.img_mode = img_mode\n        self.transform = transform\n        self.target_transform = target_transform\n        self._consecutive_errors = 0\n\n    def __getitem__(self, index):\n        try:\n            img, target = self.reader[index]\n        except Exception as e:\n            if self._consecutive_errors < _ERROR_RETRY:\n                self._consecutive_errors += 1\n                raise e\n            raise e\n\n        if self.load_bytes:\n            img = img.to(torch.uint8)\n        else:\n            img = img.convert(self.img_mode)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform",
  "\"\"\" Real labels evaluator for ImageNet\nPaper: `Are we done with ImageNet?` - https://arxiv.org/abs/2006.07159\nBased on Numpy example at https://github.com/google-research/reassessed-imagenet\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport os\nimport json\nimport numpy as np\nimport pkgutil\n\n\nclass RealLabelsImagenet:\n\n    def __init__(self, filenames, real_json=None, topk=(1, 5)):\n        if real_json is not None:\n            with open(real_json) as real_labels:\n                real_labels = json.load(real_labels)\n        else:\n            real_labels = json.loads(\n                pkgutil.get_data(__name__, os.path.join('_info', 'imagenet_real_labels.json')).decode('utf-8'))\n        real_labels = {f'ILSVRC2012_val_{i + 1:08d}.JPEG': labels for i, labels in enumerate(real_labels)}\n        self.real_labels = real_labels\n        self.filenames = filenames\n        self.topk = topk\n\n    def __call__(self, image_ids):\n        \"\"\"\n        Args:\n            image_ids (list[str]): list of image ids\n        Returns:\n            dict[str, list[str]]: list of real labels for each image\n        \"\"\"\n        image_ids = [self.filenames[i] for i in image_ids]\n        real_labels = []\n        for image_id in image_ids:\n            if image_id in self.real_labels:\n                real_labels.append(self.real_labels[image_id",
  "\"\"\" Tensorflow Preprocessing Adapter\n\nAllows use of Tensorflow preprocessing pipeline in PyTorch Transform\n\nCopyright of original Tensorflow code below.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\n# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"ImageNet preprocessing for MnasNet.\"\"\"\nimport tensorflow.compat.v1 as tf\nimport numpy as np\n\nIMAGE_SIZE = 224\nCROP_PADDING = 32\n\ntf.compat.v1.disable_eager_execution()\n\n# pylint: disable=g-import-not-at-top\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import resize\nfrom tensorflow.keras.utils import to_categorical\n\nfrom transformers import PreprocessingConfig\n\n\nclass ImageNetPreprocessingAdapter(object):\n    \"\"\"ImageNet preprocessing for MnasNet.\"\"\"\n\n    def __init__(self, config: Pre",
  "import csv\nimport os\nimport pkgutil\nimport re\nfrom typing import Dict, List, Optional, Union\n\nfrom .dataset_info import DatasetInfo\n\n\n# NOTE no ambiguity wrt to mapping from # classes to ImageNet subset so far, but likely to change\n_NUM_CLASSES_TO_SUBSET = {\n    1000: 'imagenet-1k',\n    11221: 'imagenet-21k-miil',  # miil subset of fall11\n    11821: 'imagenet-12k',  # timm specific 12k subset of fall11\n    21841: 'imagenet-22k',  # as in fall11.tar\n    21842: 'imagenet-22k-ms',  # a Microsoft (for FocalNet) remapping of 22k w/ moves ImageNet-1k classes to first 1000\n    21843: 'imagenet-21k-goog',  # Google's ImageNet full has two classes not in fall11\n}\n\n_SUBSETS = {\n    'imagenet-1k': ['train', 'val', 'test'],\n    'imagenet-21k-miil': ['train', 'val', 'test'],\n    'imagenet-12k': ['train', 'val', 'test'],\n    'imagenet-22k': ['train', 'val', 'test'],\n    'imagenet-22k-ms': ['train', 'val', 'test'],\n    'imagenet-21k-goog': ['train', 'val', 'test'],\n}\n\n_IMAGE_SIZE = {\n    'imagenet-1",
  "\"\"\" AutoAugment, RandAugment, AugMix, and 3-Augment for PyTorch\n\nThis code implements the searched ImageNet policies with various tweaks and improvements and\ndoes not include any of the search code.\n\nAA and RA Implementation adapted from:\n    https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py\n\nAugMix adapted from:\n    https://github.com/google-research/augmix\n\n3-Augment based on: https://github.com/facebookresearch/deit/blob/main/README_revenge.md\n\nPapers:\n    AutoAugment: Learning Augmentation Policies from Data - https://arxiv.org/abs/1805.09501\n    Learning Data Augmentation Strategies for Object Detection - https://arxiv.org/abs/1906.11172\n    RandAugment: Practical automated data augmentation... - https://arxiv.org/abs/1909.13719\n    AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty - https://arxiv.org/abs/1906.11172\n    Revenge: A New Data Augmentation Method for Robustness and Uncertainty - https://arxiv.org/abs/1906.11172\n\n\"\"\"\n\nimport math\nimport warnings\nfrom typing import Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers import (\n    AutoAugment,\n    RandAugment,\n    AugMix,\n    Image",
  "\"\"\" Mixup and Cutmix\n\nPapers:\nmixup: Beyond Empirical Risk Minimization (https://arxiv.org/abs/1710.09412)\n\nCutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features (https://arxiv.org/abs/1905.04899)\n\nCode Reference:\nCutMix: https://github.com/clovaai/CutMix-PyTorch\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\nimport numpy as np\nimport torch\n\n\ndef one_hot(x, num_classes, on_value=1., off_value=0.):\n    x = x.long().view(-1, 1)\n    return torch.full((x.size()[0], num_classes), off_value, device=x.device).scatter_(1, x, on_value)\n\n\ndef mixup_target(target, num_classes, lam=1., smoothing=0.0):\n    off_value = smoothing / num_classes\n    on_value = 1. - off_value\n    target = target.view(-1, 1)\n    target = one_hot(target, num_classes, on_value=on_value, off_value=off_value)\n    return target * lam + (1 - target) * off_value\n\n\ndef cutmix_target(target, num_classes, lam=1., smoothing=0.0):\n    off_value = smoothing / num_classes\n    on_value = 1. - off_value\n    target = target.view(-1, 1)\n    target = one_hot(target, num_",
  "\"\"\" A dataset reader that extracts images from folders\n\nFolders are scanned recursively to find image files. Labels are based\non the folder hierarchy, just leaf folders by default.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport os\nfrom typing import Dict, List, Optional, Set, Tuple, Union\n\nfrom timm.utils.misc import natural_key\n\nfrom .class_map import load_class_map\nfrom .img_extensions import get_img_extensions\nfrom .reader import Reader\n\n\ndef find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n):\n    \"\"\" Walk folder recursively to discover images and map them to classes by folder names.\n\n    Args:\n        folder: root of folder to recrusively search\n        types: types (file extensions) to search for in path\n        class_to_idx: specify mapping for class (folder name) to class index if set\n        leaf_name_only: use only leaf folders (no subfolders)\n        sort: sort images by class index\n\n    Returns:\n        images: list of images\n        targets: list of targets\n    \"\"\"\n    images = []\n    targets = []\n    if not types:\n        types = get_img_extensions()\n    if not class_to_idx:\n        class_to_idx = load_class_map()\n    if not os.path.isdir(folder):\n        raise ValueError(f\"Folder {folder} does not exist\")\n    for root, dirs, files in os.walk(folder):\n        if root == folder:\n            continue\n        for name in",
  "from .reader_factory import create_reader\nfrom .writer_factory import create_writer\n\n__all__ = [\"create_reader\", \"create_writer\"]\n",
  "import os\nimport pickle\n\n\ndef load_class_map(map_or_filename, root=''):\n    if isinstance(map_or_filename, dict):\n        assert dict, 'class_map dict must be non-empty'\n        return map_or_filename\n    class_map_path = map_or_filename\n    if not os.path.exists(class_map_path):\n        class_map_path = os.path.join(root, class_map_path)\n        assert os.path.exists(class_map_path), 'Cannot locate specified class map file (%s)' % map_or_filename\n    class_map = {}\n    with open(class_map_path, 'rb') as f:\n        class_map = pickle.load(f)\n    return class_map\n",
  "from multiprocessing import Value\n\n\nclass SharedCount:\n    def __init__(self, epoch: int = 0):\n        self.shared_epoch = Value('i', epoch)\n\n    @property\n    def epoch(self):\n        return self.shared_epoch.value\n\n    def increment(self):\n        self.shared_epoch.value += 1\n\n    def decrement(self):\n        self.shared_epoch.value -= 1\n",
  "import os\n\nfrom .reader_image_folder import ReaderImageFolder\nfrom .reader_image_in_tar import ReaderImageInTar\n\n\ndef create_reader(name, root, split='train', **kwargs):\n    name = name.lower()\n    name = name.split('/', 1)\n    prefix = ''\n    if len(name) > 1:\n        prefix = name[0]\n    name = name[-1]\n\n    # FIXME improve the selection right now just tfds prefix or fallback path, will need options to\n    # explicitly select other options shortly\n    if prefix == 'hfds':\n        from .reader_hfds import ReaderHfds  # defer tensorflow import\n        reader = ReaderHfds(root, name, split=split, **kwargs)\n    elif prefix == 'tfds':\n        from .reader_tfds import ReaderTfds  # defer tensorflow import\n        reader = ReaderTfds(root, name, split=split, **kwargs)\n    elif prefix == 'wds':\n        from .reader_wds import ReaderWds\n        kwargs.pop('download', False)\n        reader = ReaderWds(root, name, split=split, **kwargs)\n    else:\n        assert os.path.exists(root)\n        reader = ReaderImageFolder(root, name, split=split, **kwargs)\n    return reader\n",
  "\"\"\" Dataset reader that wraps Hugging Face datasets\n\nHacked together by / Copyright 2022 Ross Wightman\n\"\"\"\nimport io\nimport math\nimport torch\nimport torch.distributed as dist\nfrom PIL import Image\n\ntry:\n    import datasets\nexcept ImportError as e:\n    print(\"Please install Hugging Face datasets package `pip install datasets`.\")\n    exit(1)\nfrom .class_map import load_class_map\nfrom .reader import Reader\n\n\ndef get_class_labels(info, label_key='label'):\n    if 'label' not in info.features:\n        return {}\n    class_label = info.features[label_key]\n    class_to_idx = {n: class_label.str2int(n) for n in class_label.names}\n    return class_to_idx\n\n\nclass ReaderHfds(Reader):\n\n    def __init__(\n            self,\n            root,\n            name,\n            split='train',\n            class_map=None,\n            label_key='label',\n            download=False,\n    ):\n        \"\"\"\n        \"\"\"\n        super().__init__()\n        self.root = root\n        self.split = split\n        self.name = name\n        self.label_key = label_key\n        self.class_map = class_map\n        self.download = download\n        self.info = None\n        self.class_to_idx = None\n\n    def _get_info(self):\n        if self.info is None:\n            self.info = datasets.load_dataset(self.name, split=self.split)\n        return self.info\n\n    def _get_class_to_idx(self):\n        if self.class_to_idx is None:\n            self.class_to_idx = get_class_labels",
  "\"\"\" Dataset reader that wraps TFDS datasets\n\nWraps many (most?) TFDS image-classification datasets\nfrom https://github.com/tensorflow/datasets\nhttps://www.tensorflow.org/datasets/catalog/overview#image_classification\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nimport os\nfrom typing import Optional\n\nimport torch\nimport torch.distributed as dist\nfrom PIL import Image\n\ntry:\n    import tensorflow as tf\n    tf.config.set_visible_devices([], 'GPU')  # Hands off my GPU! (or pip install tensorflow-cpu)\n    import tensorflow_datasets as tfds\n    try:\n        tfds.even_splits('', 1, drop_remainder=False)  # non-buggy even_splits has drop_remainder arg\n        has_buggy_even_splits = False\n    except TypeError:\n        print(\"Warning: This version of tfds doesn't have the latest even_splits impl. \"\n              \"Please update or use tfds-nightly for better fine-grained split behaviour.\")\n        has_buggy_even_splits = True\n    # NOTE uncomment below if having file limit issues on Windows\n    # os.environ['TF_DETERMINISTIC_OPS'] = '1'\nexcept ImportError:\n    print(\"Warning: Could not import tensorflow or tensorflow-datasets. \"\n          \"Please install them or use tfds-nightly for better fine-grained split behaviour.\")\n    has_buggy_even_splits = True\n\nfrom transformers import ImageProcessor, is_tf_available, is_vision_available\n\n\nif is_tf_available():\n    import tensorflow as tf\n\n    class TFDSImageProcessor(ImageProcessor):\n        def __init__(self, *args, **kwargs):\n            super",
  "from abc import abstractmethod\n\n\nclass Reader:\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def _filename(self, index, basename=False, absolute=False):\n        pass\n\n    def filename(self, index, basename=False, absolute=False):\n        return self._filename(index, basename, absolute)\n\n    @abstractmethod\n    def read(self, index, basename=False, absolute=False):\n        pass\n\n    def read_all(self, basename=False, absolute=False):\n        return [self.read(i, basename, absolute) for i in range(self.num_examples)]\n\n    @abstractmethod\n    def num_examples(self):\n        pass\n\n    @abstractmethod\n    def num_features(self):\n        pass\n\n    @abstractmethod\n    def num_targets(self):\n        pass\n\n    @abstractmethod\n    def num_classes(self):\n        pass\n\n\nclass",
  "\"\"\" A dataset reader that reads single tarfile based datasets\n\nThis reader can read datasets consisting if a single tarfile containing images.\nI am planning to deprecated it in favour of ParerImageInTar.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport os\nimport tarfile\n\nfrom timm.utils.misc import natural_key\n\nfrom .class_map import load_class_map\nfrom .img_extensions import get_img_extensions\nfrom .reader import Reader\n\n\ndef extract_tarinfo(tarfile, class_to_idx=None, sort=True):\n    extensions = get_img_extensions(as_set=True)\n    files = []\n    labels = []\n    for ti in tarfile.getmembers():\n        if not ti.isfile():\n            continue\n        dirname, basename = os.path.split(ti.path)\n        label = os.path.basename(dirname)\n        ext = os.path.splitext(basename)[1]\n        if ext.lower() in extensions:\n            files.append(ti)\n            labels.append(label)\n    if class_to_idx is None:\n        unique_labels = set(labels)\n        class_to_idx = {label: i for i, label in enumerate(unique_labels)}\n    if sort:\n        files.sort(key=natural_key)\n        labels.sort()\n    return files, labels, class_to_idx\n\n\nclass TarReader(Reader):\n    def __init__(self, root, class_to_idx=None, sort=True, **kwargs):\n        super().__init__(root, **kwargs)\n        self.class_to_idx = class_to_idx\n        self.sort = sort\n        self.files, self.labels,",
  "\"\"\" A dataset reader that reads tarfile based datasets\n\nThis reader can extract image samples from:\n* a single tar of image files\n* a folder of multiple tarfiles containing imagefiles\n* a tar of tars containing image files\n\nLabels are based on the combined folder and/or tar name structure.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport logging\nimport os\nimport pickle\nimport tarfile\nfrom glob import glob\nfrom typing import List, Tuple, Dict, Set, Optional, Union\n\nimport numpy as np\n\nfrom timm.utils.misc import natural_key\n\nfrom .class_map import load_class_map\nfrom .img_extensions import get_img_extensions\nfrom .reader import Reader\n\n_logger = logging.getLogger(__name__)\nCACHE_FILENAME_SUFFIX = '_tarinfos.pickle'\n\n\nclass TarState:\n\n    def __init__(self, tf: tarfile.TarFile = None, ti: tarfile.TarInfo = None):\n        self.tf: tarfile.TarFile = tf\n        self.ti: tarfile.TarInfo = ti\n        self.children: Dict[str, TarState] = {}\n        self.files: List[str] = []\n        self.images: List[np.ndarray] = []\n        self.labels: List[np.ndarray] = []\n        self.class_map: Optional[Dict[str, int]] = None\n        self.class_names: Optional[List[str]] = None\n        self.has_labels: bool = False\n        self.has_class_map: bool = False\n        self.has_class_names: bool = False\n\n    def __repr__(self):\n        return f\"TarState(tf={self.",
  "\"\"\" Dataset reader for webdataset\n\nHacked together by / Copyright 2022 Ross Wightman\n\"\"\"\nimport io\nimport json\nimport logging\nimport math\nimport os\nimport random\nimport sys\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom itertools import islice\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\n\nimport torch\nimport torch.distributed as dist\nimport yaml\nfrom PIL import Image\nfrom torch.utils.data import Dataset, IterableDataset, get_worker_info\n\ntry:\n    import webdataset as wds\n    from webdataset.filters import _shuffle\n    from webdataset.shardlists import expand_urls\n    from webdataset.tariterators import base_plus_ext, url_opener, tar_file_expander, valid_sample\nexcept ImportError:\n    wds = None\n    expand_urls = None\n\nfrom .class_map import load_class_map\nfrom .reader import Reader\nfrom .shared_count import SharedCount\n\n_logger = logging.getLogger(__name__)\n\nSHUFFLE_SIZE = int(os.environ.get('WDS_SHUFFLE_SIZE', 1000000))\n\n# TODO: remove this when webdataset is updated\ntry:\n    from webdataset.filters import _shuffle\nexcept ImportError:\n    pass\nelse:\n    _shuffle = partial(_shuffle, SHUFFLE_SIZE)\n\ntry:\n    from webdataset.shardlists import expand_urls\nexcept ImportError:\n    pass\nelse:\n    expand_urls = partial(expand_urls, SHUFFLE_SIZE)\n\ntry:\n    from webdataset.tariterators import base_plus_ext, url_opener, tar_file_expand",
  "from copy import deepcopy\n\n__all__ = ['get_img_extensions', 'is_img_extension', 'set_img_extensions', 'add_img_extensions', 'del_img_extensions']\n\n\nIMG_EXTENSIONS = ('.png', '.jpg', '.jpeg')  # singleton, kept public for bwd compat use\n_IMG_EXTENSIONS_SET = set(IMG_EXTENSIONS)  # set version, private, kept in sync\n\n\ndef _set_extensions(extensions):\n    global IMG_EXTENSIONS\n    global _IMG_EXTENSIONS_SET\n    dedupe = set()  # NOTE de-duping tuple while keeping original order\n    IMG_EXTENSIONS = tuple(x for x in extensions if x not in dedupe and not dedupe.add(x))\n    _IMG_EXTENSIONS_SET = set(extensions)\n\n\ndef _valid_extension(x: str):\n    return x and isinstance(x, str) and len(x) >= 2 and x.startswith('.')\n\n\ndef is_img_extension(ext):\n    return ext in _IMG_EXTENSIONS_SET\n\n\ndef get_img_extensions(as_set=False):\n    \"\"\"\n    Return a list of all image extensions.\n\n    Args:\n        as_set (bool): If True, return a set of all image extensions. Otherwise, return a list.\n\n    Returns:\n        list: List of all image extensions.\n    \"\"\"\n    if as_set:\n        return list(_IMG_EXTENSIONS_SET)\n    else:\n        return list(IMG_EXTENSIONS)\n\n\ndef set_img_extensions(extensions):\n    \"\"\"\n    Set the list of image extensions.\n\n    Args:\n        extensions (list): List of image extensions.\n    \"\"\"\n    _set_extensions(extensions",
  "\"\"\"RAdam Optimizer.\nImplementation lifted from: https://github.com/LiyuanLucasLiu/RAdam\nPaper: `On the Variance of the Adaptive Learning Rate and Beyond` - https://arxiv.org/abs/1908.03265\n\"\"\"\nimport math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(\n            lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n            buffer=[[None, None, None] for _ in range(10)])\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n                if d_p.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients, please consider SparseAdam instead')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['buffer'] = [[None, None, None] for _ in range(10)]\n\n                step = state['step",
  "from .adabelief import AdaBelief\nfrom .adafactor import Adafactor\nfrom .adahessian import Adahessian\nfrom .adamp import AdamP\nfrom .adamw import AdamW\nfrom .adan import Adan\nfrom .lamb import Lamb\nfrom .lars import Lars\nfrom .lookahead import Lookahead\nfrom .madgrad import Madgrad\nfrom .nag import NAG\nfrom .nadam import Nadam\nfrom .noam import Noam\nfrom .optuna import Optuna\nfrom .radam import RAdam\nfrom .rmsprop import RMSprop\nfrom .sgd import SGD\nfrom .sparseadam import SparseAdam\nfrom .sparsemlp import SparseMLP\nfrom .sparsemlp_v2 import SparseMLPV2\nfrom .sparsemlp_v3 import SparseMLPV3\nfrom .sparsemlp_v4 import SparseMLPV4\nfrom .sparsemlp_v5",
  "\"\"\" Adafactor Optimizer\n\nLifted from https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py\n\nOriginal header/copyright below.\n\n\"\"\"\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport torch\nimport math\n\n\nclass Adafactor(torch.optim.Optimizer):\n    \"\"\"Implements Adafactor algorithm.\n    This implementation is based on: `Adafactor: Adaptive Learning Rates with Sublinear Memory Cost`\n    (see https://arxiv.org/abs/1804.04235)\n\n    Note that this optimizer internally adjusts the learning rate depending on the\n    *scale_parameter*, *relative_step* and *warmup_init* options.\n\n    To use a manual (external) learning rate schedule you should set `scale_parameter=False` and\n    `relative_step=False`.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining parameter groups\n        lr (float, optional): external learning rate (default: 1e-3)\n        scale_parameter (bool, optional): whether to scale the learning rate by the *scale_parameter*\n            option (default: True)\n        relative_step (bool, optional): whether to use a relative step (default: True)\n        warmup_init (bool, optional): whether to initialize the warmup rate (default: True)\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, scale_parameter=True, relative_step=True, warmup_init=True):\n        defaults = {\"lr\": lr, \"scale_parameter",
  "\"\"\" PyTorch LARS / LARC Optimizer\n\nAn implementation of LARS (SGD) + LARC in PyTorch\n\nBased on:\n  * PyTorch SGD: https://github.com/pytorch/pytorch/blob/1.7/torch/optim/sgd.py#L100\n  * NVIDIA APEX LARC: https://github.com/NVIDIA/apex/blob/master/apex/parallel/LARC.py\n\nAdditional cleanup and modifications to properly support PyTorch XLA.\n\nCopyright 2021 Ross Wightman\n\"\"\"\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass Lars(Optimizer):\n    \"\"\" LARS for PyTorch\n    \n    Paper: `Large batch training of Convolutional Networks` - https://arxiv.org/pdf/1708.03888.pdf\n\n    Args:\n        params (iterable): iterable of parameters to optimize or dicts defining parameter groups.\n        lr (float, optional): learning rate (default: 1.0).\n        momentum (float, optional): momentum factor (default: 0)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        nesterov (bool, optional): whether to use Nesterov momentum (default: False)\n        centered (bool, optional): whether to use centered LARS (default: False)\n        eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-8)\n        max_grad_norm (float, optional): max gradient norm (default: 1.0)\n    \"\"\"\n\n    def __init__(self, params, lr=1.0, momentum=0, weight_decay=0, nesterov=False, centered=False,",
  "\"\"\" Adan Optimizer\n\nAdan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models[J]. arXiv preprint arXiv:2208.06677, 2022.\n    https://arxiv.org/abs/2208.06677\n\nImplementation adapted from https://github.com/sail-sg/Adan\n\"\"\"\n\nimport math\n\nimport torch\n\nfrom torch.optim import Optimizer\n\n\nclass Adan(Optimizer):\n    \"\"\"\n    Implements a pytorch variant of Adan\n    Adan was proposed in\n    Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models[J]. arXiv preprint arXiv:2208.06677, 2022.\n    https://arxiv.org/abs/2208.06677\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        betas (Tuple[float, float, flot], optional): coefficients used for computing\n            running averages of gradient and its square. (default: (0.9, 0.999, 0.9999))\n        eps (float, optional): term added to the denominator to improve numerical stability. (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsgrad (bool, optional): whether to use AMSGrad or not. (default: False)\n        max_grad_norm (float, optional): max gradient norm. (default: 1.0)\n    \"\"\"",
  "\"\"\" NAdamW Optimizer\n\nBased on simplified algorithm in https://github.com/mlcommons/algorithmic-efficiency/tree/main/baselines/nadamw\n\nAdded multi-tensor (foreach) path.\n\"\"\"\nimport math\nfrom typing import List, Optional\n\nimport torch\nfrom torch import Tensor\n\n\n# Modified from github.com/pytorch/pytorch/blob/v1.12.1/torch/optim/adamw.py.\nclass NAdamW(torch.optim.Optimizer):\n    r\"\"\"Implements NAdamW algorithm.\n\n      See Table 1 in https://arxiv.org/abs/1910.05446 for the implementation of\n      the NAdam algorithm (there is also a comment in the code which highlights\n      the only difference of NAdamW and AdamW).\n      For further details regarding the algorithm we refer to\n      `Decoupled Weight Decay Regularization`_.\n\n      Args:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsgrad (bool, optional): whether to use AMSGrad or not (default: False)\n        max_grad_norm (float, optional): max gradient norm (default: 1.0)\n        betas_correction (bool, optional): whether to",
  "\"\"\" RMSProp modified to behave like Tensorflow impl\n\nOriginally cut & paste from PyTorch RMSProp\nhttps://github.com/pytorch/pytorch/blob/063946d2b3f3f1e953a2a3b54e0b34f1393de295/torch/optim/rmsprop.py\nLicensed under BSD-Clause 3 (ish), https://github.com/pytorch/pytorch/blob/master/LICENSE\n\nModifications Copyright 2021 Ross Wightman\n\"\"\"\n\nimport torch\nfrom torch.optim import Optimizer\n\n\nclass RMSpropTF(Optimizer):\n    \"\"\"Implements RMSprop algorithm (TensorFlow style epsilon)\n\n    NOTE: This is a direct cut-and-paste of PyTorch RMSprop with eps applied before sqrt\n    and a few other modifications to closer match Tensorflow for matching hyper-params.\n\n    Noteworthy changes include:\n    1. Epsilon applied inside square-root\n    2. square_avg initialized to ones\n    3. LR scaling of update accumulated in momentum buffer\n\n    Proposed by G. Hinton in his\n    `course <http://cs229.stanford.edu/syllabus.html#sec10.5.2>`__\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, momentum=0.9, centered=True, eps=1e-8):\n        defaults = dict(lr=lr, momentum=momentum, centered=centered, eps=eps)\n        super().__init__(params, defaults)\n        self.lr = lr\n        self.momentum = momentum\n        self.centered = centered\n        self.eps = eps\n        self.square_avg = torch.ones_like(self",
  "\"\"\" AdaHessian Optimizer\n\nLifted from https://github.com/davda54/ada-hessian/blob/master/ada_hessian.py\nOriginally licensed MIT, Copyright 2020, David Samuel\n\"\"\"\nimport torch\n\n\nclass Adahessian(torch.optim.Optimizer):\n    \"\"\"\n    Implements the AdaHessian algorithm from \"ADAHESSIAN: An Adaptive Second OrderOptimizer for Machine Learning\"\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining parameter groups\n        lr (float, optional): learning rate (default: 0.1)\n        betas ((float, float), optional): coefficients used for computing running averages of gradient and the\n            squared hessian trace (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0.0)\n        hessian_power (float, optional): exponent of the hessian trace (default: 1.0)\n        update_interval (int, optional): interval between two updates (default: 1)\n        max_iter (int, optional): maximum number of iterations (default: 1000)\n    \"\"\"\n\n    def __init__(self, params, lr=0.1, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.0, hessian_power=1.0,\n                 update_interval=1, max_iter=1000):\n        defaults = dict(lr=lr, betas=betas, eps=eps",
  "\"\"\" PyTorch MADGRAD optimizer\n\nMADGRAD: https://arxiv.org/abs/2101.11075\n\nCode from: https://github.com/facebookresearch/madgrad\n\"\"\"\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom typing import TYPE_CHECKING, Any, Callable, Optional\n\nimport torch\nimport torch.optim\n\nif TYPE_CHECKING:\n    from torch.optim.optimizer import _params_t\nelse:\n    _params_t = Any\n\n\nclass MADGRAD(torch.optim.Optimizer):\n    \"\"\"\n    MADGRAD_: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic\n    Optimization.\n\n    .. _MADGRAD: https://arxiv.org/abs/2101.11075\n\n    MADGRAD is a general purpose optimizer that can be used in place of SGD or\n    Adam may converge faster and generalize better. Currently GPU-only.\n    Typically, the same learning rate schedule is used for both SGD and Adam.\n\n    Args:\n        params ([_params_t]):\n            The parameters to optimize.\n        lr ([float]):\n            The initial learning rate.\n        momentum ([float]):\n            The momentum of the optimizer.\n        weight_decay ([float]):\n            The weight decay of the optimizer.\n        nesterov ([bool]):\n            Whether to use Nesterov momentum.\n        eps ([float]):\n            The epsilon value for numerical stability.\n        max_grad_norm ([float]):\n            The maximum gradient norm.\n        alpha ([float]):\n            The alpha value for the momentum term.\n        beta ([float]):\n           ",
  "\"\"\"\nSGDP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py\n\nPaper: `Slowing Down the Weight Norm Increase in Momentum-based Optimizers` - https://arxiv.org/abs/2006.08217\nCode: https://github.com/clovaai/AdamP\n\nCopyright (c) 2020-present NAVER Corp.\nMIT license\n\"\"\"\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.optim.optimizer import Optimizer, required\nimport math\n\nfrom .adamp import projection\n\n\nclass SGDP(Optimizer):\n    def __init__(self, params, lr=required, momentum=0, dampening=0,\n                 weight_decay=0, nesterov=False, eps=1e-8, delta=0.1, wd_ratio=0.1):\n        defaults = dict(\n            lr=lr, momentum=momentum, dampening=dampening, weight_decay=weight_decay,\n            nesterov=nesterov, eps=eps, delta=delta, wd_ratio=wd_ratio)\n        super().__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        for group in self.param_groups:\n            group['lr'] = state['state_dict']['param_groups'][0]['lr']\n            group['momentum'] = state['state_dict']['param_groups'][0]['momentum']\n            group['dampening'] = state['state_dict']['param_groups'][0]['dampening']\n            group['weight_decay'] = state['state_dict']['param_groups'][0]['weight",
  "\"\"\" Optimizer Factory w/ Custom Weight Decay\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport logging\nfrom itertools import islice\nfrom typing import Optional, Callable, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom timm.models import group_parameters\n\nfrom .adabelief import AdaBelief\nfrom .adafactor import Adafactor\nfrom .adahessian import Adahessian\nfrom .adamp import AdamP\nfrom .adan import Adan\nfrom .lamb import Lamb\nfrom .lars import Lars\nfrom .lion import Lion\nfrom .lookahead import Lookahead\nfrom .madgrad import MADGRAD\nfrom .nadam import Nadam\nfrom .nadamw import NAdamW\nfrom .nvnovograd import NvNovoGrad\nfrom .radam import RAdam\nfrom .rmsprop_tf import RMSpropTF\nfrom .sgdp import SGDP\n\n\n_logger = logging.getLogger(__name__)\n\n\n# optimizers to default to multi-tensor\n_DEFAULT_FOREACH = {\n    'lion': Lion,\n    'lion_v2': Lion,\n    'lion_v3': Lion,\n    'lion_v4': Lion,\n    'lion_v5': Lion,\n    'lion_v6': Lion,\n    'lion_v7': Lion,\n    'lion_v8': Lion,\n    'lion_v9': Lion,\n    'lion_v10': Lion,\n    'lion_v11': Lion,\n    'lion_v12': Lion",
  "\"\"\" Lion Optimizer\nPaper: `Symbolic Discovery of Optimization Algorithms` - https://arxiv.org/abs/2302.06675\nOriginal Impl: https://github.com/google/automl/tree/master/lion\n\"\"\"\n# Copyright 2023 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom typing import List\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass Lion(Optimizer):\n    r\"\"\"Implements Lion algorithm.\"\"\"\n\n    def __init__(\n            self,\n            params,\n            lr=1e-3,\n            momentum=0.9,\n            weight_decay=0,\n            l2_reg=0,\n            max_grad_norm=1.0,\n            num_workers=1,\n            device=None,\n            **kwargs\n    ):\n        if device is None:\n            device = params[0].device\n        defaults = {\n            \"lr\": lr,\n            \"momentum\": momentum,\n            \"weight_decay\": weight_decay,\n            \"l2_reg\": l2_reg,\n            \"max_grad_norm\": max_grad_norm,\n            \"num_workers",
  "\"\"\" Nvidia NovoGrad Optimizer.\nOriginal impl by Nvidia from Jasper example:\n    - https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechRecognition/Jasper\nPaper: `Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks`\n    - https://arxiv.org/abs/1905.11286\n\"\"\"\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\nimport math\n\n\nclass NvNovoGrad(Optimizer):\n    \"\"\"\n    Implements Novograd algorithm.\n\n    Args:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.95, 0.98))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        grad_clip (float, optional): gradient clipping threshold (default: 10)\n        max_grad_norm (float, optional): max gradient norm (default: 10)\n        momentum (float, optional): momentum (default: 0)\n        nesterov (bool, optional): whether to use Nesterov momentum (default: False)\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.95, 0.98), eps=1e-8,\n                 weight_decay=0, grad_clip=10, max_",
  "\"\"\" Lookahead Optimizer Wrapper.\nImplementation modified from: https://github.com/alphadl/lookahead.pytorch\nPaper: `Lookahead Optimizer: k steps forward, 1 step back` - https://arxiv.org/abs/1907.08610\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom collections import OrderedDict\nfrom typing import Callable, Dict\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\nfrom collections import defaultdict\n\n\nclass Lookahead(Optimizer):\n    def __init__(self, base_optimizer, alpha=0.5, k=6):\n        # NOTE super().__init__() not called on purpose\n        self._optimizer_step_pre_hooks: Dict[int, Callable] = OrderedDict()\n        self._optimizer_step_post_hooks: Dict[int, Callable] = OrderedDict()\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f'Invalid slow update rate: {alpha}')\n        if not 1 <= k:\n            raise ValueError(f'Invalid lookahead steps: {k}')\n        defaults = dict(lookahead_alpha=alpha, lookahead_k=k)\n        self.lookahead_alpha = alpha\n        self.lookahead_k = k\n        self.lookahead_step = 0\n        self.lookahead_step_pre = 0\n        self.lookahead_step_post = 0\n        self.lookahead_step_total = 0\n        self.lookahead_step_pre_total = 0\n        self.lookahead_step_post_total = 0\n        self.lookahead_step_total_pre = 0\n        self.lookahead_step_total_post = 0\n       ",
  "import math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass AdaBelief(Optimizer):\n    r\"\"\"Implements AdaBelief algorithm. Modified from Adam in PyTorch\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-16)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False)\n        decoupled_decay (boolean, optional): (default: True) If set as True, then\n            the optimizer uses decoupled weight decay as in AdamW\n        fixed_decay (boolean, optional): (default: False) If set as True, then the\n            optimizer uses fixed weight decay as in AdamW\n        decoupled_weight_decay (boolean, optional): (default: True) If set as True, then\n            the optimizer uses decoupled weight decay as in AdamW\n        fixed_weight_decay (boolean, optional): (default: False) If set as True, then\n            the optimizer uses fixed weight decay as in AdamW\n        decoupled_amsgrad (boolean, optional): (default: True) If set as True, then\n            the optimizer uses decoupled AMSGrad variant as in AdamW\n        fixed",
  "\"\"\" PyTorch Lamb optimizer w/ behaviour similar to NVIDIA FusedLamb\n\nThis optimizer code was adapted from the following (starting with latest)\n* https://github.com/HabanaAI/Model-References/blob/2b435114fe8e31f159b1d3063b8280ae37af7423/PyTorch/nlp/bert/pretraining/lamb.py\n* https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/Transformer-XL/pytorch/lamb.py\n* https://github.com/cybertronai/pytorch-lamb\n\nUse FusedLamb if you can (GPU). The reason for including this variant of Lamb is to have a version that is\nsimilar in behaviour to APEX FusedLamb if you aren't using NVIDIA GPUs or cannot install/use APEX.\n\nIn addition to some cleanup, this Lamb impl has been modified to support PyTorch XLA and has been tested on TPU.\n\nOriginal copyrights for above sources are below.\n\nModifications Copyright 2020, NVIDIA CORPORATION. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License",
  "\"\"\"\nAdamP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/adamp.py\n\nPaper: `Slowing Down the Weight Norm Increase in Momentum-based Optimizers` - https://arxiv.org/abs/2006.08217\nCode: https://github.com/clovaai/AdamP\n\nCopyright (c) 2020-present NAVER Corp.\nMIT license\n\"\"\"\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.optim.optimizer import Optimizer\nimport math\n\n\ndef _channel_view(x) -> torch.Tensor:\n    return x.reshape(x.size(0), -1)\n\n\ndef _layer_view(x) -> torch.Tensor:\n    return x.reshape(1, -1)\n\n\ndef projection(p, grad, perturb, delta: float, wd_ratio: float, eps: float):\n    wd = 1.\n    expand_size = (-1,) + (1,) * (len(p.shape) - 1)\n    for view_func in [_channel_view, _layer_view]:\n        p = view_func(p)\n    p = p.view(p.size(0), -1)\n    grad = grad.view(grad.size(0), -1)\n    perturb = perturb.view(perturb.size(0), -1)\n    delta = delta.view(delta.size(0), -1)\n    p_norm = torch.norm(p, dim=1, keepdim=True)\n    grad_norm = torch.norm(grad, dim=1, keepdim=True)\n    perturb",
  "\"\"\" AdamW Optimizer\nImpl copied from PyTorch master\n\nNOTE: Builtin optim.AdamW is used by the factory, this impl only serves as a Python based reference, will be removed\nsomeday\n\"\"\"\nimport math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass AdamW(Optimizer):\n    r\"\"\"Implements AdamW algorithm.\n\n    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.\n    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper \"On the Convergence of Adam and Beyond\"\n            (default: False)\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-2, amsgrad=False):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n       ",
  "import math\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass Nadam(Optimizer):\n    \"\"\"Implements Nadam algorithm (a variant of Adam based on Nesterov momentum).\n\n    It has been proposed in `Incorporating Nesterov Momentum into Adam`__.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 2e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        schedule_decay (float, optional): momentum schedule decay (default: 4e-3)\n\n    __ http://cs229.stanford.edu/proj2015/054_report.pdf\n    __ http://www.cs.toronto.edu/~fritz/absps/momentum.pdf\n\n        Originally taken from: https://github.com/pytorch/pytorch/blob/master/torch/optim/nadam.py\n    \"\"\"\n\n    def __init__(self, params, lr=2e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, schedule_decay=4e-3):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0]",
  "import numpy as np\nimport pandas as pd\n\n\nresults = {\n    'results-imagenet.csv': [\n        'results-imagenet-real.csv',\n        'results-imagenetv2-matched-frequency.csv',\n        'results-sketch.csv'\n    ],\n    'results-imagenet-a-clean.csv': [\n        'results-imagenet-a.csv',\n    ],\n    'results-imagenet-r-clean.csv': [\n        'results-imagenet-r.csv',\n    ],\n}\n\n\ndef diff(base_df, test_csv):\n    base_models = base_df['model'].values\n    test_df = pd.read_csv(test_csv)\n    test_models  = test_df['model'].values\n\n    rank_diff = np.zeros_like(test_models, dtype='object')\n    top1_diff = np.zeros_like(test_models, dtype='object')\n    top5_diff = np.zeros_like(test_models, dtype='object')\n    \n    for rank, model in enumerate(test_models):\n        if model in base_models:            \n            base_rank = int(np.where(base_models == model)[0][0])\n            test_rank = int(np.where(test_models == model)[0][0])\n            rank_diff[rank] = test_rank - base_rank\n            top1_diff[rank] = test_df['top1'][rank] - base_df['top1'][rank]\n            top5_diff[rank] = test_df['top5'][rank] - base_df['top5'][rank]\n\n    return rank_diff, top1_diff, top5_diff\n\n\ndef main():\n    for test"
]