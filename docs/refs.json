[
  "\"\"\"\nConvert weights from https://github.com/google-research/nested-transformer\nNOTE: You'll need https://github.com/google/CommonLoopUtils, not included in requirements.txt\n\"\"\"\n\nimport sys\n\nimport numpy as np\nimport torch\n\nfrom clu import checkpoint\n\n\narch_depths = {\n    'nest_base': [2, 2, 20],\n    'nest_small': [2, 2, 20],\n    'nest_tiny': [2, 2, 8],\n}\n\n\ndef convert_nest(checkpoint_path, arch):\n    \"\"\"\n    Expects path to checkpoint which is a dir containing 4 files like in each of these folders\n        - https://console.cloud.google.com/storage/browser/gresearch/nest-checkpoints\n    `arch` is needed to \n    Returns a state dict that can be used with `torch.nn.Module.load_state_dict`\n    Hint: Follow timm.models.nest.Nest.__init__ and \n    https://github.com/google-research/nested-transformer/blob/main/models/nest_net.py\n    \"\"\"\n    assert arch in ['nest_base', 'nest_small', 'nest_tiny'], \"Your `arch` is not supported\"\n\n    flax_dict = checkpoint.load_state_dict(checkpoint_path)['optimizer']['target']\n    state_dict = {}\n\n    # Patch embedding\n    state_dict['patch_embed.proj.weight'] = torch.tensor(\n        flax_dict['PatchEmbedding_0']['Conv_0']['kernel']).permute(3, 2, 0, 1)\n    state_dict['patch_embed.proj.bias'] = torch.tensor(flax_dict['PatchEmbedding_0']['Conv_0']['bias'])\n    \n    # Positional embeddings\n    posemb_keys = [k for k in flax_dict.keys() if k.startswith('PositionEmbedding')]\n    for i, k in enumerate(posemb_keys):\n        state_dict[f'levels.{i}.pos_embed'] = torch.tensor(flax_dict[k]['pos_embedding'])\n    \n    # Transformer encoders\n    depths = arch_depths[arch]\n    for level in range(len(depths)):\n        for layer in range(depths[level]):\n            global_layer_ix = sum(depths[:level]) + layer\n            # Norms\n            for i in range(2):\n                state_dict[f'levels.{level}.transformer_encoder.{layer}.norm{i+1}.weight'] = torch.tensor(\n                    flax_dict[f'EncoderNDBlock_{global_layer_ix}'][f'LayerNorm_{i}']['scale'])\n                state_dict[f'levels.{level}.transformer_encoder.{layer}.norm{i+1}.bias'] = torch.tensor(\n                    flax_dict[f'EncoderNDBlock_{global_layer_ix}'][f'LayerNorm_{i}']['bias'])\n            # Attention qkv\n            w_q = flax_dict[f'EncoderNDBlock_{global_layer_ix}']['MultiHeadAttention_0']['DenseGeneral_0']['kernel']\n            w_kv = flax_dict[f'EncoderNDBlock_{global_layer_ix}']['MultiHeadAttention_0']['DenseGeneral_1']['kernel']\n            # Pay attention to dims here (maybe get pen and paper)\n            w_kv = np.concatenate(np.split(w_kv, 2, -1), 1)\n            w_qkv = np.concatenate([w_q, w_kv], 1)\n            state_dict[f'levels.{level}.transformer_encoder.{layer}.attn.qkv.weight'] = torch.tensor(w_qkv).flatten(1).permute(1,0)\n            b_q = flax_dict[f'EncoderNDBlock_{global_layer_ix}']['MultiHeadAttention_0']['DenseGeneral_0']['bias']\n            b_kv = flax_dict[f'EncoderNDBlock_{global_layer_ix}']['MultiHeadAttention_0']['DenseGeneral_1']['bias']\n            # Pay attention to dims here (maybe get pen and paper)\n            b_kv = np.concatenate(np.split(b_kv, 2, -1), 0)\n            b_qkv = np.concatenate([b_q, b_kv], 0)\n            state_dict[f'levels.{level}.transformer_encoder.{layer}.attn.qkv.bias'] = torch.tensor(b_qkv).reshape(-1)\n            # Attention proj\n            w_proj = flax_dict[f'EncoderNDBlock_{global_layer_ix}']['MultiHeadAttention_0']['proj_kernel']\n            w_proj = torch.tensor(w_proj).permute(2, 1, 0).flatten(1)\n            state_dict[f'levels.{level}.transformer_encoder.{layer}.attn.proj.weight'] = w_proj\n            state_dict[f'levels.{level}.transformer_encoder.{layer}.attn.proj.bias'] = torch.tensor(\n                flax_dict[f'EncoderNDBlock_{global_layer_ix}']['MultiHeadAttention_0']['bias'])\n            # MLP\n            for i in range(2):\n                state_dict[f'levels.{level}.transformer_encoder.{layer}.mlp.fc{i+1}.weight'] = torch.tensor(\n                    flax_dict[f'EncoderNDBlock_{global_layer_ix}']['MlpBlock_0'][f'Dense_{i}']['kernel']).permute(1, 0)\n                state_dict[f'levels.{level}.transformer_encoder.{layer}.mlp.fc{i+1}.bias'] = torch.tensor(\n                    flax_dict[f'EncoderNDBlock_{global_layer_ix}']['MlpBlock_0'][f'Dense_{i}']['bias'])\n\n    # Block aggregations (ConvPool)\n    for level in range(1, len(depths)):\n        # Convs\n        state_dict[f'levels.{level}.pool.conv.weight'] = torch.tensor(\n            flax_dict[f'ConvPool_{level-1}']['Conv_0']['kernel']).permute(3, 2, 0, 1)\n        state_dict[f'levels.{level}.pool.conv.bias'] = torch.tensor(\n            flax_dict[f'ConvPool_{level-1}']['Conv_0']['bias'])\n        # Norms\n        state_dict[f'levels.{level}.pool.norm.weight'] = torch.tensor(\n                    flax_dict[f'ConvPool_{level-1}']['LayerNorm_0']['scale'])\n        state_dict[f'levels.{level}.pool.norm.bias'] = torch.tensor(\n                    flax_dict[f'ConvPool_{level-1}']['LayerNorm_0']['bias'])\n\n    # Final norm\n    state_dict[f'norm.weight'] = torch.tensor(flax_dict['LayerNorm_0']['scale'])\n    state_dict[f'norm.bias'] = torch.tensor(flax_dict['LayerNorm_0']['bias'])\n\n    # Classifier\n    state_dict['head.weight'] = torch.tensor(flax_dict['Dense_0']['kernel']).permute(1, 0)\n    state_dict['head.bias'] = torch.tensor(flax_dict['Dense_0']['bias'])\n\n    return state_dict\n\n\nif __name__ == '__main__':\n    variant = sys.argv[1] # base, small, or tiny\n    state_dict = convert_nest(f'./nest-{variant[0]}_imagenet', f'nest_{variant}')\n    torch.save(state_dict, f'./jx_nest_{variant}.pth')",
  "import argparse\nimport hashlib\nimport os\n\nimport mxnet as mx\nimport gluoncv\nimport torch\nfrom timm import create_model\n\nparser = argparse.ArgumentParser(description='Convert from MXNet')\nparser.add_argument('--model', default='all', type=str, metavar='MODEL',\n                    help='Name of model to train (default: \"all\"')\n\n\ndef convert(mxnet_name, torch_name):\n    # download and load the pre-trained model\n    net = gluoncv.model_zoo.get_model(mxnet_name, pretrained=True)\n\n    # create corresponding torch model\n    torch_net = create_model(torch_name)\n\n    mxp = [(k, v) for k, v in net.collect_params().items() if 'running' not in k]\n    torchp = list(torch_net.named_parameters())\n    torch_params = {}\n\n    # convert parameters\n    # NOTE: we are relying on the fact that the order of parameters\n    # are usually exactly the same between these models, thus no key name mapping\n    # is necessary. Asserts will trip if this is not the case.\n    for (tn, tv), (mn, mv) in zip(torchp, mxp):\n        m_split = mn.split('_')\n        t_split = tn.split('.')\n        print(t_split, m_split)\n        print(tv.shape, mv.shape)\n\n        # ensure ordering of BN params match since their sizes are not specific\n        if m_split[-1] == 'gamma':\n            assert t_split[-1] == 'weight'\n        if m_split[-1] == 'beta':\n            assert t_split[-1] == 'bias'\n\n        # ensure shapes match\n        assert all(t == m for t, m in zip(tv.shape, mv.shape))\n\n        torch_tensor = torch.from_numpy(mv.data().asnumpy())\n        torch_params[tn] = torch_tensor\n\n    # convert buffers (batch norm running stats)\n    mxb = [(k, v) for k, v in net.collect_params().items() if any(x in k for x in ['running_mean', 'running_var'])]\n    torchb = [(k, v) for k, v in torch_net.named_buffers() if 'num_batches' not in k]\n    for (tn, tv), (mn, mv) in zip(torchb, mxb):\n        print(tn, mn)\n        print(tv.shape, mv.shape)\n\n        # ensure ordering of BN params match since their sizes are not specific\n        if 'running_var' in tn:\n            assert 'running_var' in mn\n        if 'running_mean' in tn:\n            assert 'running_mean' in mn\n            \n        torch_tensor = torch.from_numpy(mv.data().asnumpy())\n        torch_params[tn] = torch_tensor\n\n    torch_net.load_state_dict(torch_params)\n    torch_filename = './%s.pth' % torch_name\n    torch.save(torch_net.state_dict(), torch_filename)\n    with open(torch_filename, 'rb') as f:\n        sha_hash = hashlib.sha256(f.read()).hexdigest()\n    final_filename = os.path.splitext(torch_filename)[0] + '-' + sha_hash[:8] + '.pth'\n    os.rename(torch_filename, final_filename)\n    print(\"=> Saved converted model to '{}, SHA256: {}'\".format(final_filename, sha_hash))\n\n\ndef map_mx_to_torch_model(mx_name):\n    torch_name = mx_name.lower()\n    if torch_name.startswith('se_'):\n        torch_name = torch_name.replace('se_', 'se')\n    elif torch_name.startswith('senet_'):\n        torch_name = torch_name.replace('senet_', 'senet')\n    elif torch_name.startswith('inceptionv3'):\n        torch_name = torch_name.replace('inceptionv3', 'inception_v3')\n    torch_name = 'gluon_' + torch_name\n    return torch_name\n\n\nALL = ['resnet18_v1b', 'resnet34_v1b', 'resnet50_v1b', 'resnet101_v1b', 'resnet152_v1b',\n       'resnet50_v1c', 'resnet101_v1c', 'resnet152_v1c', 'resnet50_v1d', 'resnet101_v1d', 'resnet152_v1d',\n       #'resnet50_v1e', 'resnet101_v1e', 'resnet152_v1e',\n       'resnet50_v1s', 'resnet101_v1s', 'resnet152_v1s', 'resnext50_32x4d', 'resnext101_32x4d', 'resnext101_64x4d',\n       'se_resnext50_32x4d', 'se_resnext101_32x4d', 'se_resnext101_64x4d', 'senet_154', 'inceptionv3']\n\n\ndef main():\n    args = parser.parse_args()\n\n    if not args.model or args.model == 'all':\n        for mx_model in ALL:\n            torch_model = map_mx_to_torch_model(mx_model)\n            convert(mx_model, torch_model)\n    else:\n        mx_model = args.model\n        torch_model = map_mx_to_torch_model(mx_model)\n        convert(mx_model, torch_model)\n\n\nif __name__ == '__main__':\n    main()\n",
  "from .version import __version__\nfrom .layers import is_scriptable, is_exportable, set_scriptable, set_exportable\nfrom .models import create_model, list_models, list_pretrained, is_model, list_modules, model_entrypoint, \\\n    is_model_pretrained, get_pretrained_cfg, get_pretrained_cfg_value\n",
  "__version__ = '0.9.3dev0'\n",
  "\"\"\" EfficientFormer\n\n@article{li2022efficientformer,\n  title={EfficientFormer: Vision Transformers at MobileNet Speed},\n  author={Li, Yanyu and Yuan, Geng and Wen, Yang and Hu, Eric and Evangelidis, Georgios and Tulyakov,\n   Sergey and Wang, Yanzhi and Ren, Jian},\n  journal={arXiv preprint arXiv:2206.01191},\n  year={2022}\n}\n\nBased on Apache 2.0 licensed code at https://github.com/snap-research/EfficientFormer, Copyright (c) 2022 Snap Inc.\n\nModifications and timm support by / Copyright 2022, Ross Wightman\n\"\"\"\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import DropPath, trunc_normal_, to_2tuple, Mlp\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import generate_default_cfgs, register_model\n\n__all__ = ['EfficientFormer']  # model_registry will add each entrypoint fn to this\n\n\nEfficientFormer_width = {\n    'l1': (48, 96, 224, 448),\n    'l3': (64, 128, 320, 512),\n    'l7': (96, 192, 384, 768),\n}\n\nEfficientFormer_depth = {\n    'l1': (3, 2, 6, 4),\n    'l3': (4, 4, 12, 6),\n    'l7': (6, 6, 18, 8),\n}\n\n\nclass Attention(torch.nn.Module):\n    attention_bias_cache: Dict[str, torch.Tensor]\n\n    def __init__(\n            self,\n            dim=384,\n            key_dim=32,\n            num_heads=8,\n            attn_ratio=4,\n            resolution=7\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        self.scale = key_dim ** -0.5\n        self.key_dim = key_dim\n        self.key_attn_dim = key_dim * num_heads\n        self.val_dim = int(attn_ratio * key_dim)\n        self.val_attn_dim = self.val_dim * num_heads\n        self.attn_ratio = attn_ratio\n\n        self.qkv = nn.Linear(dim, self.key_attn_dim * 2 + self.val_attn_dim)\n        self.proj = nn.Linear(self.val_attn_dim, dim)\n\n        resolution = to_2tuple(resolution)\n        pos = torch.stack(torch.meshgrid(torch.arange(resolution[0]), torch.arange(resolution[1]))).flatten(1)\n        rel_pos = (pos[..., :, None] - pos[..., None, :]).abs()\n        rel_pos = (rel_pos[0] * resolution[1]) + rel_pos[1]\n        self.attention_biases = torch.nn.Parameter(torch.zeros(num_heads, resolution[0] * resolution[1]))\n        self.register_buffer('attention_bias_idxs', torch.LongTensor(rel_pos))\n        self.attention_bias_cache = {}  # per-device attention_biases cache (data-parallel compat)\n\n    @torch.no_grad()\n    def train(self, mode=True):\n        super().train(mode)\n        if mode and self.attention_bias_cache:\n            self.attention_bias_cache = {}  # clear ab cache\n\n    def get_attention_biases(self, device: torch.device) -> torch.Tensor:\n        if torch.jit.is_tracing() or self.training:\n            return self.attention_biases[:, self.attention_bias_idxs]\n        else:\n            device_key = str(device)\n            if device_key not in self.attention_bias_cache:\n                self.attention_bias_cache[device_key] = self.attention_biases[:, self.attention_bias_idxs]\n            return self.attention_bias_cache[device_key]\n\n    def forward(self, x):  # x (B,N,C)\n        B, N, C = x.shape\n        qkv = self.qkv(x)\n        qkv = qkv.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n        q, k, v = qkv.split([self.key_dim, self.key_dim, self.val_dim], dim=3)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn + self.get_attention_biases(x.device)\n\n        attn = attn.softmax(dim=-1)\n        x = (attn @ v).transpose(1, 2).reshape(B, N, self.val_attn_dim)\n        x = self.proj(x)\n        return x\n\n\nclass Stem4(nn.Sequential):\n    def __init__(self, in_chs, out_chs, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):\n        super().__init__()\n        self.stride = 4\n\n        self.add_module('conv1', nn.Conv2d(in_chs, out_chs // 2, kernel_size=3, stride=2, padding=1))\n        self.add_module('norm1', norm_layer(out_chs // 2))\n        self.add_module('act1', act_layer())\n        self.add_module('conv2', nn.Conv2d(out_chs // 2, out_chs, kernel_size=3, stride=2, padding=1))\n        self.add_module('norm2', norm_layer(out_chs))\n        self.add_module('act2', act_layer())\n\n\nclass Downsample(nn.Module):\n    \"\"\"\n    Downsampling via strided conv w/ norm\n    Input: tensor in shape [B, C, H, W]\n    Output: tensor in shape [B, C, H/stride, W/stride]\n    \"\"\"\n\n    def __init__(self, in_chs, out_chs, kernel_size=3, stride=2, padding=None, norm_layer=nn.BatchNorm2d):\n        super().__init__()\n        if padding is None:\n            padding = kernel_size // 2\n        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride, padding=padding)\n        self.norm = norm_layer(out_chs)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.norm(x)\n        return x\n\n\nclass Flat(nn.Module):\n\n    def __init__(self, ):\n        super().__init__()\n\n    def forward(self, x):\n        x = x.flatten(2).transpose(1, 2)\n        return x\n\n\nclass Pooling(nn.Module):\n    \"\"\"\n    Implementation of pooling for PoolFormer\n    --pool_size: pooling size\n    \"\"\"\n\n    def __init__(self, pool_size=3):\n        super().__init__()\n        self.pool = nn.AvgPool2d(pool_size, stride=1, padding=pool_size // 2, count_include_pad=False)\n\n    def forward(self, x):\n        return self.pool(x) - x\n\n\nclass ConvMlpWithNorm(nn.Module):\n    \"\"\"\n    Implementation of MLP with 1*1 convolutions.\n    Input: tensor with shape [B, C, H, W]\n    \"\"\"\n\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            norm_layer=nn.BatchNorm2d,\n            drop=0.\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Conv2d(in_features, hidden_features, 1)\n        self.norm1 = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n        self.act = act_layer()\n        self.fc2 = nn.Conv2d(hidden_features, out_features, 1)\n        self.norm2 = norm_layer(out_features) if norm_layer is not None else nn.Identity()\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.norm1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.norm2(x)\n        x = self.drop(x)\n        return x\n\n\nclass LayerScale(nn.Module):\n    def __init__(self, dim, init_values=1e-5, inplace=False):\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n\n\nclass MetaBlock1d(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            mlp_ratio=4.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            proj_drop=0.,\n            drop_path=0.,\n            layer_scale_init_value=1e-5\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.token_mixer = Attention(dim)\n        self.norm2 = norm_layer(dim)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.ls1 = LayerScale(dim, layer_scale_init_value)\n        self.ls2 = LayerScale(dim, layer_scale_init_value)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.ls1(self.token_mixer(self.norm1(x))))\n        x = x + self.drop_path(self.ls2(self.mlp(self.norm2(x))))\n        return x\n\n\nclass LayerScale2d(nn.Module):\n    def __init__(self, dim, init_values=1e-5, inplace=False):\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        gamma = self.gamma.view(1, -1, 1, 1)\n        return x.mul_(gamma) if self.inplace else x * gamma\n\n\nclass MetaBlock2d(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            pool_size=3,\n            mlp_ratio=4.,\n            act_layer=nn.GELU,\n            norm_layer=nn.BatchNorm2d,\n            proj_drop=0.,\n            drop_path=0.,\n            layer_scale_init_value=1e-5\n    ):\n        super().__init__()\n        self.token_mixer = Pooling(pool_size=pool_size)\n        self.ls1 = LayerScale2d(dim, layer_scale_init_value)\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.mlp = ConvMlpWithNorm(\n            dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n            drop=proj_drop,\n        )\n        self.ls2 = LayerScale2d(dim, layer_scale_init_value)\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        x = x + self.drop_path1(self.ls1(self.token_mixer(x)))\n        x = x + self.drop_path2(self.ls2(self.mlp(x)))\n        return x\n\n\nclass EfficientFormerStage(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            dim_out,\n            depth,\n            downsample=True,\n            num_vit=1,\n            pool_size=3,\n            mlp_ratio=4.,\n            act_layer=nn.GELU,\n            norm_layer=nn.BatchNorm2d,\n            norm_layer_cl=nn.LayerNorm,\n            proj_drop=.0,\n            drop_path=0.,\n            layer_scale_init_value=1e-5,\n):\n        super().__init__()\n        self.grad_checkpointing = False\n\n        if downsample:\n            self.downsample = Downsample(in_chs=dim, out_chs=dim_out, norm_layer=norm_layer)\n            dim = dim_out\n        else:\n            assert dim == dim_out\n            self.downsample = nn.Identity()\n\n        blocks = []\n        if num_vit and num_vit >= depth:\n            blocks.append(Flat())\n\n        for block_idx in range(depth):\n            remain_idx = depth - block_idx - 1\n            if num_vit and num_vit > remain_idx:\n                blocks.append(\n                    MetaBlock1d(\n                        dim,\n                        mlp_ratio=mlp_ratio,\n                        act_layer=act_layer,\n                        norm_layer=norm_layer_cl,\n                        proj_drop=proj_drop,\n                        drop_path=drop_path[block_idx],\n                        layer_scale_init_value=layer_scale_init_value,\n                    ))\n            else:\n                blocks.append(\n                    MetaBlock2d(\n                        dim,\n                        pool_size=pool_size,\n                        mlp_ratio=mlp_ratio,\n                        act_layer=act_layer,\n                        norm_layer=norm_layer,\n                        proj_drop=proj_drop,\n                        drop_path=drop_path[block_idx],\n                        layer_scale_init_value=layer_scale_init_value,\n                    ))\n                if num_vit and num_vit == remain_idx:\n                    blocks.append(Flat())\n\n        self.blocks = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        x = self.downsample(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        return x\n\n\nclass EfficientFormer(nn.Module):\n\n    def __init__(\n            self,\n            depths,\n            embed_dims=None,\n            in_chans=3,\n            num_classes=1000,\n            global_pool='avg',\n            downsamples=None,\n            num_vit=0,\n            mlp_ratios=4,\n            pool_size=3,\n            layer_scale_init_value=1e-5,\n            act_layer=nn.GELU,\n            norm_layer=nn.BatchNorm2d,\n            norm_layer_cl=nn.LayerNorm,\n            drop_rate=0.,\n            proj_drop_rate=0.,\n            drop_path_rate=0.,\n            **kwargs\n    ):\n        super().__init__()\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n\n        self.stem = Stem4(in_chans, embed_dims[0], norm_layer=norm_layer)\n        prev_dim = embed_dims[0]\n\n        # stochastic depth decay rule\n        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n        downsamples = downsamples or (False,) + (True,) * (len(depths) - 1)\n        stages = []\n        for i in range(len(depths)):\n            stage = EfficientFormerStage(\n                prev_dim,\n                embed_dims[i],\n                depths[i],\n                downsample=downsamples[i],\n                num_vit=num_vit if i == 3 else 0,\n                pool_size=pool_size,\n                mlp_ratio=mlp_ratios,\n                act_layer=act_layer,\n                norm_layer_cl=norm_layer_cl,\n                norm_layer=norm_layer,\n                proj_drop=proj_drop_rate,\n                drop_path=dpr[i],\n                layer_scale_init_value=layer_scale_init_value,\n            )\n            prev_dim = embed_dims[i]\n            stages.append(stage)\n\n        self.stages = nn.Sequential(*stages)\n\n        # Classifier head\n        self.num_features = embed_dims[-1]\n        self.norm = norm_layer_cl(self.num_features)\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n        # assuming model is always distilled (valid for current checkpoints, will split def if that changes)\n        self.head_dist = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()\n        self.distilled_training = False  # must set this True to train w/ distillation token\n\n        self.apply(self._init_weights)\n\n    # init for classification\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {k for k, _ in self.named_parameters() if 'attention_biases' in k}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^stem',  # stem and embed\n            blocks=[(r'^stages\\.(\\d+)', None), (r'^norm', (99999,))]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        for s in self.stages:\n            s.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head, self.head_dist\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n        self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    @torch.jit.ignore\n    def set_distilled_training(self, enable=True):\n        self.distilled_training = enable\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        x = self.stages(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool == 'avg':\n            x = x.mean(dim=1)\n        x = self.head_drop(x)\n        if pre_logits:\n            return x\n        x, x_dist = self.head(x), self.head_dist(x)\n        if self.distilled_training and self.training and not torch.jit.is_scripting():\n            # only return separate classification predictions when training in distilled mode\n            return x, x_dist\n        else:\n            # during standard train/finetune, inference average the classifier predictions\n            return (x + x_dist) / 2\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _checkpoint_filter_fn(state_dict, model):\n    \"\"\" Remap original checkpoints -> timm \"\"\"\n    if 'stem.0.weight' in state_dict:\n        return state_dict  # non-original checkpoint, no remapping needed\n\n    out_dict = {}\n    import re\n    stage_idx = 0\n    for k, v in state_dict.items():\n        if k.startswith('patch_embed'):\n            k = k.replace('patch_embed.0', 'stem.conv1')\n            k = k.replace('patch_embed.1', 'stem.norm1')\n            k = k.replace('patch_embed.3', 'stem.conv2')\n            k = k.replace('patch_embed.4', 'stem.norm2')\n\n        if re.match(r'network\\.(\\d+)\\.proj\\.weight', k):\n            stage_idx += 1\n        k = re.sub(r'network.(\\d+).(\\d+)', f'stages.{stage_idx}.blocks.\\\\2', k)\n        k = re.sub(r'network.(\\d+).proj', f'stages.{stage_idx}.downsample.conv', k)\n        k = re.sub(r'network.(\\d+).norm', f'stages.{stage_idx}.downsample.norm', k)\n\n        k = re.sub(r'layer_scale_([0-9])', r'ls\\1.gamma', k)\n        k = k.replace('dist_head', 'head_dist')\n        out_dict[k] = v\n    return out_dict\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None, 'fixed_input_size': True,\n        'crop_pct': .95, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.conv1', 'classifier': ('head', 'head_dist'),\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'efficientformer_l1.snap_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n    'efficientformer_l3.snap_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n    'efficientformer_l7.snap_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n})\n\n\ndef _create_efficientformer(variant, pretrained=False, **kwargs):\n    model = build_model_with_cfg(\n        EfficientFormer, variant, pretrained,\n        pretrained_filter_fn=_checkpoint_filter_fn,\n        **kwargs)\n    return model\n\n\n@register_model\ndef efficientformer_l1(pretrained=False, **kwargs) -> EfficientFormer:\n    model_args = dict(\n        depths=EfficientFormer_depth['l1'],\n        embed_dims=EfficientFormer_width['l1'],\n        num_vit=1,\n    )\n    return _create_efficientformer('efficientformer_l1', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef efficientformer_l3(pretrained=False, **kwargs) -> EfficientFormer:\n    model_args = dict(\n        depths=EfficientFormer_depth['l3'],\n        embed_dims=EfficientFormer_width['l3'],\n        num_vit=4,\n    )\n    return _create_efficientformer('efficientformer_l3', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef efficientformer_l7(pretrained=False, **kwargs) -> EfficientFormer:\n    model_args = dict(\n        depths=EfficientFormer_depth['l7'],\n        embed_dims=EfficientFormer_width['l7'],\n        num_vit=8,\n    )\n    return _create_efficientformer('efficientformer_l7', pretrained=pretrained, **dict(model_args, **kwargs))\n\n",
  "\"\"\"VGG\n\nAdapted from https://github.com/pytorch/vision 'vgg.py' (BSD-3-Clause) with a few changes for\ntimm functionality.\n\nCopyright 2021 Ross Wightman\n\"\"\"\nfrom typing import Union, List, Dict, Any, cast\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import ClassifierHead\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_module\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['VGG']\n\n\ncfgs: Dict[str, List[Union[str, int]]] = {\n    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n}\n\n\n@register_notrace_module  # reason: FX can't symbolically trace control flow in forward method\nclass ConvMlp(nn.Module):\n\n    def __init__(\n            self,\n            in_features=512,\n            out_features=4096,\n            kernel_size=7,\n            mlp_ratio=1.0,\n            drop_rate: float = 0.2,\n            act_layer: nn.Module = None,\n            conv_layer: nn.Module = None,\n    ):\n        super(ConvMlp, self).__init__()\n        self.input_kernel_size = kernel_size\n        mid_features = int(out_features * mlp_ratio)\n        self.fc1 = conv_layer(in_features, mid_features, kernel_size, bias=True)\n        self.act1 = act_layer(True)\n        self.drop = nn.Dropout(drop_rate)\n        self.fc2 = conv_layer(mid_features, out_features, 1, bias=True)\n        self.act2 = act_layer(True)\n\n    def forward(self, x):\n        if x.shape[-2] < self.input_kernel_size or x.shape[-1] < self.input_kernel_size:\n            # keep the input size >= 7x7\n            output_size = (max(self.input_kernel_size, x.shape[-2]), max(self.input_kernel_size, x.shape[-1]))\n            x = F.adaptive_avg_pool2d(x, output_size)\n        x = self.fc1(x)\n        x = self.act1(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.act2(x)\n        return x\n\n\nclass VGG(nn.Module):\n\n    def __init__(\n            self,\n            cfg: List[Any],\n            num_classes: int = 1000,\n            in_chans: int = 3,\n            output_stride: int = 32,\n            mlp_ratio: float = 1.0,\n            act_layer: nn.Module = nn.ReLU,\n            conv_layer: nn.Module = nn.Conv2d,\n            norm_layer: nn.Module = None,\n            global_pool: str = 'avg',\n            drop_rate: float = 0.,\n    ) -> None:\n        super(VGG, self).__init__()\n        assert output_stride == 32\n        self.num_classes = num_classes\n        self.num_features = 4096\n        self.drop_rate = drop_rate\n        self.grad_checkpointing = False\n        self.use_norm = norm_layer is not None\n        self.feature_info = []\n        prev_chs = in_chans\n        net_stride = 1\n        pool_layer = nn.MaxPool2d\n        layers: List[nn.Module] = []\n        for v in cfg:\n            last_idx = len(layers) - 1\n            if v == 'M':\n                self.feature_info.append(dict(num_chs=prev_chs, reduction=net_stride, module=f'features.{last_idx}'))\n                layers += [pool_layer(kernel_size=2, stride=2)]\n                net_stride *= 2\n            else:\n                v = cast(int, v)\n                conv2d = conv_layer(prev_chs, v, kernel_size=3, padding=1)\n                if norm_layer is not None:\n                    layers += [conv2d, norm_layer(v), act_layer(inplace=True)]\n                else:\n                    layers += [conv2d, act_layer(inplace=True)]\n                prev_chs = v\n        self.features = nn.Sequential(*layers)\n        self.feature_info.append(dict(num_chs=prev_chs, reduction=net_stride, module=f'features.{len(layers) - 1}'))\n\n        self.pre_logits = ConvMlp(\n            prev_chs,\n            self.num_features,\n            7,\n            mlp_ratio=mlp_ratio,\n            drop_rate=drop_rate,\n            act_layer=act_layer,\n            conv_layer=conv_layer,\n        )\n        self.head = ClassifierHead(\n            self.num_features,\n            num_classes,\n            pool_type=global_pool,\n            drop_rate=drop_rate,\n        )\n\n        self._initialize_weights()\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        # this treats BN layers as separate groups for bn variants, a lot of effort to fix that\n        return dict(stem=r'^features\\.0', blocks=r'^features\\.(\\d+)')\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, 'gradient checkpointing not supported'\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.head = ClassifierHead(\n            self.num_features,\n            self.num_classes,\n            pool_type=global_pool,\n            drop_rate=self.drop_rate,\n        )\n\n    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: torch.Tensor, pre_logits: bool = False):\n        x = self.pre_logits(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n    def _initialize_weights(self) -> None:\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n\ndef _filter_fn(state_dict):\n    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n    out_dict = {}\n    for k, v in state_dict.items():\n        k_r = k\n        k_r = k_r.replace('classifier.0', 'pre_logits.fc1')\n        k_r = k_r.replace('classifier.3', 'pre_logits.fc2')\n        k_r = k_r.replace('classifier.6', 'head.fc')\n        if 'classifier.0.weight' in k:\n            v = v.reshape(-1, 512, 7, 7)\n        if 'classifier.3.weight' in k:\n            v = v.reshape(-1, 4096, 1, 1)\n        out_dict[k_r] = v\n    return out_dict\n\n\ndef _create_vgg(variant: str, pretrained: bool, **kwargs: Any) -> VGG:\n    cfg = variant.split('_')[0]\n    # NOTE: VGG is one of few models with stride==1 features w/ 6 out_indices [0..5]\n    out_indices = kwargs.pop('out_indices', (0, 1, 2, 3, 4, 5))\n    model = build_model_with_cfg(\n        VGG,\n        variant,\n        pretrained,\n        model_cfg=cfgs[cfg],\n        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),\n        pretrained_filter_fn=_filter_fn,\n        **kwargs,\n    )\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bilinear',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'features.0', 'classifier': 'head.fc',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'vgg11.tv_in1k': _cfg(hf_hub_id='timm/'),\n    'vgg13.tv_in1k': _cfg(hf_hub_id='timm/'),\n    'vgg16.tv_in1k': _cfg(hf_hub_id='timm/'),\n    'vgg19.tv_in1k': _cfg(hf_hub_id='timm/'),\n    'vgg11_bn.tv_in1k': _cfg(hf_hub_id='timm/'),\n    'vgg13_bn.tv_in1k': _cfg(hf_hub_id='timm/'),\n    'vgg16_bn.tv_in1k': _cfg(hf_hub_id='timm/'),\n    'vgg19_bn.tv_in1k': _cfg(hf_hub_id='timm/'),\n})\n\n\n@register_model\ndef vgg11(pretrained: bool = False, **kwargs: Any) -> VGG:\n    r\"\"\"VGG 11-layer model (configuration \"A\") from\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n    \"\"\"\n    model_args = dict(**kwargs)\n    return _create_vgg('vgg11', pretrained=pretrained, **model_args)\n\n\n@register_model\ndef vgg11_bn(pretrained: bool = False, **kwargs: Any) -> VGG:\n    r\"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n    \"\"\"\n    model_args = dict(norm_layer=nn.BatchNorm2d, **kwargs)\n    return _create_vgg('vgg11_bn', pretrained=pretrained, **model_args)\n\n\n@register_model\ndef vgg13(pretrained: bool = False, **kwargs: Any) -> VGG:\n    r\"\"\"VGG 13-layer model (configuration \"B\")\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n    \"\"\"\n    model_args = dict(**kwargs)\n    return _create_vgg('vgg13', pretrained=pretrained, **model_args)\n\n\n@register_model\ndef vgg13_bn(pretrained: bool = False, **kwargs: Any) -> VGG:\n    r\"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n    \"\"\"\n    model_args = dict(norm_layer=nn.BatchNorm2d, **kwargs)\n    return _create_vgg('vgg13_bn', pretrained=pretrained, **model_args)\n\n\n@register_model\ndef vgg16(pretrained: bool = False, **kwargs: Any) -> VGG:\n    r\"\"\"VGG 16-layer model (configuration \"D\")\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n    \"\"\"\n    model_args = dict(**kwargs)\n    return _create_vgg('vgg16', pretrained=pretrained, **model_args)\n\n\n@register_model\ndef vgg16_bn(pretrained: bool = False, **kwargs: Any) -> VGG:\n    r\"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n    \"\"\"\n    model_args = dict(norm_layer=nn.BatchNorm2d, **kwargs)\n    return _create_vgg('vgg16_bn', pretrained=pretrained, **model_args)\n\n\n@register_model\ndef vgg19(pretrained: bool = False, **kwargs: Any) -> VGG:\n    r\"\"\"VGG 19-layer model (configuration \"E\")\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n    \"\"\"\n    model_args = dict(**kwargs)\n    return _create_vgg('vgg19', pretrained=pretrained, **model_args)\n\n\n@register_model\ndef vgg19_bn(pretrained: bool = False, **kwargs: Any) -> VGG:\n    r\"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`._\n    \"\"\"\n    model_args = dict(norm_layer=nn.BatchNorm2d, **kwargs)\n    return _create_vgg('vgg19_bn', pretrained=pretrained, **model_args)",
  "\"\"\" DeiT - Data-efficient Image Transformers\n\nDeiT model defs and weights from https://github.com/facebookresearch/deit, original copyright below\n\npaper: `DeiT: Data-efficient Image Transformers` - https://arxiv.org/abs/2012.12877\n\npaper: `DeiT III: Revenge of the ViT` - https://arxiv.org/abs/2204.07118\n\nModifications copyright 2021, Ross Wightman\n\"\"\"\n# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\nfrom functools import partial\nfrom typing import Sequence, Union\n\nimport torch\nfrom torch import nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import resample_abs_pos_embed\nfrom timm.models.vision_transformer import VisionTransformer, trunc_normal_, checkpoint_filter_fn\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import generate_default_cfgs, register_model, register_model_deprecations\n\n__all__ = ['VisionTransformerDistilled']  # model_registry will add each entrypoint fn to this\n\n\nclass VisionTransformerDistilled(VisionTransformer):\n    \"\"\" Vision Transformer w/ Distillation Token and Head\n\n    Distillation token & head support for `DeiT: Data-efficient Image Transformers`\n        - https://arxiv.org/abs/2012.12877\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        weight_init = kwargs.pop('weight_init', '')\n        super().__init__(*args, **kwargs, weight_init='skip')\n        assert self.global_pool in ('token',)\n\n        self.num_prefix_tokens = 2\n        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n        self.pos_embed = nn.Parameter(\n            torch.zeros(1, self.patch_embed.num_patches + self.num_prefix_tokens, self.embed_dim))\n        self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if self.num_classes > 0 else nn.Identity()\n        self.distilled_training = False  # must set this True to train w/ distillation token\n\n        self.init_weights(weight_init)\n\n    def init_weights(self, mode=''):\n        trunc_normal_(self.dist_token, std=.02)\n        super().init_weights(mode=mode)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^cls_token|pos_embed|patch_embed|dist_token',\n            blocks=[\n                (r'^blocks\\.(\\d+)', None),\n                (r'^norm', (99999,))]  # final norm w/ last block\n        )\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head, self.head_dist\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n        self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n\n    @torch.jit.ignore\n    def set_distilled_training(self, enable=True):\n        self.distilled_training = enable\n\n    def _intermediate_layers(\n            self,\n            x: torch.Tensor,\n            n: Union[int, Sequence] = 1,\n    ):\n        outputs, num_blocks = [], len(self.blocks)\n        take_indices = set(range(num_blocks - n, num_blocks) if isinstance(n, int) else n)\n\n        # forward pass\n        x = self.patch_embed(x)\n        x = torch.cat((\n            self.cls_token.expand(x.shape[0], -1, -1),\n            self.dist_token.expand(x.shape[0], -1, -1),\n            x),\n            dim=1)\n        x = self.pos_drop(x + self.pos_embed)\n        x = self.patch_drop(x)\n        x = self.norm_pre(x)\n        for i, blk in enumerate(self.blocks):\n            x = blk(x)\n            if i in take_indices:\n                outputs.append(x)\n\n        return outputs\n\n    def forward_features(self, x) -> torch.Tensor:\n        x = self.patch_embed(x)\n        x = torch.cat((\n            self.cls_token.expand(x.shape[0], -1, -1),\n            self.dist_token.expand(x.shape[0], -1, -1),\n            x),\n            dim=1)\n        x = self.pos_drop(x + self.pos_embed)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False) -> torch.Tensor:\n        x, x_dist = x[:, 0], x[:, 1]\n        if pre_logits:\n            return (x + x_dist) / 2\n        x = self.head(x)\n        x_dist = self.head_dist(x_dist)\n        if self.distilled_training and self.training and not torch.jit.is_scripting():\n            # only return separate classification predictions when training in distilled mode\n            return x, x_dist\n        else:\n            # during standard train / finetune, inference average the classifier predictions\n            return (x + x_dist) / 2\n\n\ndef _create_deit(variant, pretrained=False, distilled=False, **kwargs):\n    if kwargs.get('features_only', None):\n        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n    model_cls = VisionTransformerDistilled if distilled else VisionTransformer\n    model = build_model_with_cfg(\n        model_cls,\n        variant,\n        pretrained,\n        pretrained_filter_fn=partial(checkpoint_filter_fn, adapt_layer_scale=True),\n        **kwargs,\n    )\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    # deit models (FB weights)\n    'deit_tiny_patch16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth'),\n    'deit_small_patch16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth'),\n    'deit_base_patch16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth'),\n    'deit_base_patch16_384.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_base_patch16_384-8de9b5d1.pth',\n        input_size=(3, 384, 384), crop_pct=1.0),\n\n    'deit_tiny_distilled_patch16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_tiny_distilled_patch16_224-b40b3cf7.pth',\n        classifier=('head', 'head_dist')),\n    'deit_small_distilled_patch16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_small_distilled_patch16_224-649709d9.pth',\n        classifier=('head', 'head_dist')),\n    'deit_base_distilled_patch16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_224-df68dfff.pth',\n        classifier=('head', 'head_dist')),\n    'deit_base_distilled_patch16_384.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth',\n        input_size=(3, 384, 384), crop_pct=1.0,\n        classifier=('head', 'head_dist')),\n\n    'deit3_small_patch16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_3_small_224_1k.pth'),\n    'deit3_small_patch16_384.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_3_small_384_1k.pth',\n        input_size=(3, 384, 384), crop_pct=1.0),\n    'deit3_medium_patch16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_3_medium_224_1k.pth'),\n    'deit3_base_patch16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_3_base_224_1k.pth'),\n    'deit3_base_patch16_384.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_3_base_384_1k.pth',\n        input_size=(3, 384, 384), crop_pct=1.0),\n    'deit3_large_patch16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_3_large_224_1k.pth'),\n    'deit3_large_patch16_384.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_3_large_384_1k.pth',\n        input_size=(3, 384, 384), crop_pct=1.0),\n    'deit3_huge_patch14_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_3_huge_224_1k.pth'),\n\n    'deit3_small_patch16_224.fb_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_3_small_224_21k.pth',\n        crop_pct=1.0),\n    'deit3_small_patch16_384.fb_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_3_small_384_21k.pth',\n        input_size=(3, 384, 384), crop_pct=1.0),\n    'deit3_medium_patch16_224.fb_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_3_medium_224_21k.pth',\n        crop_pct=1.0),\n    'deit3_base_patch16_224.fb_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_3_base_224_21k.pth',\n        crop_pct=1.0),\n    'deit3_base_patch16_384.fb_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_3_base_384_21k.pth',\n        input_size=(3, 384, 384), crop_pct=1.0),\n    'deit3_large_patch16_224.fb_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_3_large_224_21k.pth',\n        crop_pct=1.0),\n    'deit3_large_patch16_384.fb_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_3_large_384_21k.pth',\n        input_size=(3, 384, 384), crop_pct=1.0),\n    'deit3_huge_patch14_224.fb_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/deit_3_huge_224_21k_v1.pth',\n        crop_pct=1.0),\n})\n\n\n@register_model\ndef deit_tiny_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" DeiT-tiny model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3)\n    model = _create_deit('deit_tiny_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef deit_small_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" DeiT-small model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6)\n    model = _create_deit('deit_small_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef deit_base_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" DeiT base model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12)\n    model = _create_deit('deit_base_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef deit_base_patch16_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" DeiT base model @ 384x384 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12)\n    model = _create_deit('deit_base_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef deit_tiny_distilled_patch16_224(pretrained=False, **kwargs) -> VisionTransformerDistilled:\n    \"\"\" DeiT-tiny distilled model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3)\n    model = _create_deit(\n        'deit_tiny_distilled_patch16_224', pretrained=pretrained, distilled=True, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef deit_small_distilled_patch16_224(pretrained=False, **kwargs) -> VisionTransformerDistilled:\n    \"\"\" DeiT-small distilled model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6)\n    model = _create_deit(\n        'deit_small_distilled_patch16_224', pretrained=pretrained, distilled=True, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef deit_base_distilled_patch16_224(pretrained=False, **kwargs) -> VisionTransformerDistilled:\n    \"\"\" DeiT-base distilled model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12)\n    model = _create_deit(\n        'deit_base_distilled_patch16_224', pretrained=pretrained, distilled=True, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef deit_base_distilled_patch16_384(pretrained=False, **kwargs) -> VisionTransformerDistilled:\n    \"\"\" DeiT-base distilled model @ 384x384 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12)\n    model = _create_deit(\n        'deit_base_distilled_patch16_384', pretrained=pretrained, distilled=True, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef deit3_small_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" DeiT-3 small model @ 224x224 from paper (https://arxiv.org/abs/2204.07118).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6, no_embed_class=True, init_values=1e-6)\n    model = _create_deit('deit3_small_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef deit3_small_patch16_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" DeiT-3 small model @ 384x384 from paper (https://arxiv.org/abs/2204.07118).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6, no_embed_class=True, init_values=1e-6)\n    model = _create_deit('deit3_small_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef deit3_medium_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" DeiT-3 medium model @ 224x224 (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=512, depth=12, num_heads=8, no_embed_class=True, init_values=1e-6)\n    model = _create_deit('deit3_medium_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef deit3_base_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" DeiT-3 base model @ 224x224 from paper (https://arxiv.org/abs/2204.07118).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, no_embed_class=True, init_values=1e-6)\n    model = _create_deit('deit3_base_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef deit3_base_patch16_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" DeiT-3 base model @ 384x384 from paper (https://arxiv.org/abs/2204.07118).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, no_embed_class=True, init_values=1e-6)\n    model = _create_deit('deit3_base_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef deit3_large_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" DeiT-3 large model @ 224x224 from paper (https://arxiv.org/abs/2204.07118).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16, no_embed_class=True, init_values=1e-6)\n    model = _create_deit('deit3_large_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef deit3_large_patch16_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" DeiT-3 large model @ 384x384 from paper (https://arxiv.org/abs/2204.07118).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16, no_embed_class=True, init_values=1e-6)\n    model = _create_deit('deit3_large_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef deit3_huge_patch14_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" DeiT-3 base model @ 384x384 from paper (https://arxiv.org/abs/2204.07118).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"\n    model_args = dict(patch_size=14, embed_dim=1280, depth=32, num_heads=16, no_embed_class=True, init_values=1e-6)\n    model = _create_deit('deit3_huge_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\nregister_model_deprecations(__name__, {\n    'deit3_small_patch16_224_in21ft1k': 'deit3_small_patch16_224.fb_in22k_ft_in1k',\n    'deit3_small_patch16_384_in21ft1k': 'deit3_small_patch16_384.fb_in22k_ft_in1k',\n    'deit3_medium_patch16_224_in21ft1k': 'deit3_medium_patch16_224.fb_in22k_ft_in1k',\n    'deit3_base_patch16_224_in21ft1k': 'deit3_base_patch16_224.fb_in22k_ft_in1k',\n    'deit3_base_patch16_384_in21ft1k': 'deit3_base_patch16_384.fb_in22k_ft_in1k',\n    'deit3_large_patch16_224_in21ft1k': 'deit3_large_patch16_224.fb_in22k_ft_in1k',\n    'deit3_large_patch16_384_in21ft1k': 'deit3_large_patch16_384.fb_in22k_ft_in1k',\n    'deit3_huge_patch14_224_in21ft1k': 'deit3_huge_patch14_224.fb_in22k_ft_in1k'\n})\n",
  "\"\"\" Res2Net and Res2NeXt\nAdapted from Official Pytorch impl at: https://github.com/gasvn/Res2Net/\nPaper: `Res2Net: A New Multi-scale Backbone Architecture` - https://arxiv.org/abs/1904.01169\n\"\"\"\nimport math\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\nfrom .resnet import ResNet\n\n__all__ = []\n\n\nclass Bottle2neck(nn.Module):\n    \"\"\" Res2Net/Res2NeXT Bottleneck\n    Adapted from https://github.com/gasvn/Res2Net/blob/master/res2net.py\n    \"\"\"\n    expansion = 4\n\n    def __init__(\n            self,\n            inplanes,\n            planes,\n            stride=1,\n            downsample=None,\n            cardinality=1,\n            base_width=26,\n            scale=4,\n            dilation=1,\n            first_dilation=None,\n            act_layer=nn.ReLU,\n            norm_layer=None,\n            attn_layer=None,\n            **_,\n    ):\n        super(Bottle2neck, self).__init__()\n        self.scale = scale\n        self.is_first = stride > 1 or downsample is not None\n        self.num_scales = max(1, scale - 1)\n        width = int(math.floor(planes * (base_width / 64.0))) * cardinality\n        self.width = width\n        outplanes = planes * self.expansion\n        first_dilation = first_dilation or dilation\n\n        self.conv1 = nn.Conv2d(inplanes, width * scale, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(width * scale)\n\n        convs = []\n        bns = []\n        for i in range(self.num_scales):\n            convs.append(nn.Conv2d(\n                width, width, kernel_size=3, stride=stride, padding=first_dilation,\n                dilation=first_dilation, groups=cardinality, bias=False))\n            bns.append(norm_layer(width))\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList(bns)\n        if self.is_first:\n            # FIXME this should probably have count_include_pad=False, but hurts original weights\n            self.pool = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1)\n        else:\n            self.pool = None\n\n        self.conv3 = nn.Conv2d(width * scale, outplanes, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(outplanes)\n        self.se = attn_layer(outplanes) if attn_layer is not None else None\n\n        self.relu = act_layer(inplace=True)\n        self.downsample = downsample\n\n    def zero_init_last(self):\n        if getattr(self.bn3, 'weight', None) is not None:\n            nn.init.zeros_(self.bn3.weight)\n\n    def forward(self, x):\n        shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        spx = torch.split(out, self.width, 1)\n        spo = []\n        sp = spx[0]  # redundant, for torchscript\n        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):\n            if i == 0 or self.is_first:\n                sp = spx[i]\n            else:\n                sp = sp + spx[i]\n            sp = conv(sp)\n            sp = bn(sp)\n            sp = self.relu(sp)\n            spo.append(sp)\n        if self.scale > 1:\n            if self.pool is not None:  # self.is_first == True, None check for torchscript\n                spo.append(self.pool(spx[-1]))\n            else:\n                spo.append(spx[-1])\n        out = torch.cat(spo, 1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.se is not None:\n            out = self.se(out)\n\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out += shortcut\n        out = self.relu(out)\n\n        return out\n\n\ndef _create_res2net(variant, pretrained=False, **kwargs):\n    return build_model_with_cfg(ResNet, variant, pretrained, **kwargs)\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bilinear',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'conv1', 'classifier': 'fc',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'res2net50_26w_4s.in1k': _cfg(hf_hub_id='timm/'),\n    'res2net50_48w_2s.in1k': _cfg(hf_hub_id='timm/'),\n    'res2net50_14w_8s.in1k': _cfg(hf_hub_id='timm/'),\n    'res2net50_26w_6s.in1k': _cfg(hf_hub_id='timm/'),\n    'res2net50_26w_8s.in1k': _cfg(hf_hub_id='timm/'),\n    'res2net101_26w_4s.in1k': _cfg(hf_hub_id='timm/'),\n    'res2next50.in1k': _cfg(hf_hub_id='timm/'),\n    'res2net50d.in1k': _cfg(hf_hub_id='timm/', first_conv='conv1.0'),\n    'res2net101d.in1k': _cfg(hf_hub_id='timm/', first_conv='conv1.0'),\n})\n\n\n@register_model\ndef res2net50_26w_4s(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a Res2Net-50 26w4s model.\n    \"\"\"\n    model_args = dict(\n        block=Bottle2neck, layers=[3, 4, 6, 3], base_width=26, block_args=dict(scale=4))\n    return _create_res2net('res2net50_26w_4s', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef res2net101_26w_4s(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a Res2Net-101 26w4s model.\n    \"\"\"\n    model_args = dict(\n        block=Bottle2neck, layers=[3, 4, 23, 3], base_width=26, block_args=dict(scale=4))\n    return _create_res2net('res2net101_26w_4s', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef res2net50_26w_6s(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a Res2Net-50 26w6s model.\n    \"\"\"\n    model_args = dict(\n        block=Bottle2neck, layers=[3, 4, 6, 3], base_width=26, block_args=dict(scale=6))\n    return _create_res2net('res2net50_26w_6s', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef res2net50_26w_8s(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a Res2Net-50 26w8s model.\n    \"\"\"\n    model_args = dict(\n        block=Bottle2neck, layers=[3, 4, 6, 3], base_width=26, block_args=dict(scale=8))\n    return _create_res2net('res2net50_26w_8s', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef res2net50_48w_2s(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a Res2Net-50 48w2s model.\n    \"\"\"\n    model_args = dict(\n        block=Bottle2neck, layers=[3, 4, 6, 3], base_width=48, block_args=dict(scale=2))\n    return _create_res2net('res2net50_48w_2s', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef res2net50_14w_8s(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a Res2Net-50 14w8s model.\n    \"\"\"\n    model_args = dict(\n        block=Bottle2neck, layers=[3, 4, 6, 3], base_width=14, block_args=dict(scale=8))\n    return _create_res2net('res2net50_14w_8s', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef res2next50(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Construct Res2NeXt-50 4s\n    \"\"\"\n    model_args = dict(\n        block=Bottle2neck, layers=[3, 4, 6, 3], base_width=4, cardinality=8, block_args=dict(scale=4))\n    return _create_res2net('res2next50', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef res2net50d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Construct Res2Net-50\n    \"\"\"\n    model_args = dict(\n        block=Bottle2neck, layers=[3, 4, 6, 3], base_width=26, stem_type='deep',\n        avg_down=True, stem_width=32, block_args=dict(scale=4))\n    return _create_res2net('res2net50d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef res2net101d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Construct Res2Net-50\n    \"\"\"\n    model_args = dict(\n        block=Bottle2neck, layers=[3, 4, 23, 3], base_width=26, stem_type='deep',\n        avg_down=True, stem_width=32, block_args=dict(scale=4))\n    return _create_res2net('res2net101d', pretrained, **dict(model_args, **kwargs))\n",
  "from ._factory import *\n\nimport warnings\nwarnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", DeprecationWarning)\n",
  "from .beit import *\nfrom .byoanet import *\nfrom .byobnet import *\nfrom .cait import *\nfrom .coat import *\nfrom .convit import *\nfrom .convmixer import *\nfrom .convnext import *\nfrom .crossvit import *\nfrom .cspnet import *\nfrom .davit import *\nfrom .deit import *\nfrom .densenet import *\nfrom .dla import *\nfrom .dpn import *\nfrom .edgenext import *\nfrom .efficientformer import *\nfrom .efficientformer_v2 import *\nfrom .efficientnet import *\nfrom .eva import *\nfrom .focalnet import *\nfrom .gcvit import *\nfrom .ghostnet import *\nfrom .hardcorenas import *\nfrom .hrnet import *\nfrom .inception_resnet_v2 import *\nfrom .inception_v3 import *\nfrom .inception_v4 import *\nfrom .levit import *\nfrom .maxxvit import *\nfrom .metaformer import *\nfrom .mlp_mixer import *\nfrom .mobilenetv3 import *\nfrom .mobilevit import *\nfrom .mvitv2 import *\nfrom .nasnet import *\nfrom .nest import *\nfrom .nfnet import *\nfrom .pit import *\nfrom .pnasnet import *\nfrom .pvt_v2 import *\nfrom .regnet import *\nfrom .repvit import *\nfrom .res2net import *\nfrom .resnest import *\nfrom .resnet import *\nfrom .resnetv2 import *\nfrom .rexnet import *\nfrom .selecsls import *\nfrom .senet import *\nfrom .sequencer import *\nfrom .sknet import *\nfrom .swin_transformer import *\nfrom .swin_transformer_v2 import *\nfrom .swin_transformer_v2_cr import *\nfrom .tnt import *\nfrom .tresnet import *\nfrom .twins import *\nfrom .vgg import *\nfrom .visformer import *\nfrom .vision_transformer import *\nfrom .vision_transformer_hybrid import *\nfrom .vision_transformer_relpos import *\nfrom .vision_transformer_sam import *\nfrom .volo import *\nfrom .vovnet import *\nfrom .xception import *\nfrom .xception_aligned import *\nfrom .xcit import *\n\nfrom ._builder import build_model_with_cfg, load_pretrained, load_custom_pretrained, resolve_pretrained_cfg, \\\n    set_pretrained_download_progress, set_pretrained_check_hash\nfrom ._factory import create_model, parse_model_name, safe_model_name\nfrom ._features import FeatureInfo, FeatureHooks, FeatureHookNet, FeatureListNet, FeatureDictNet\nfrom ._features_fx import FeatureGraphNet, GraphExtractNet, create_feature_extractor, \\\n    register_notrace_module, is_notrace_module, get_notrace_modules, \\\n    register_notrace_function, is_notrace_function, get_notrace_functions\nfrom ._helpers import clean_state_dict, load_state_dict, load_checkpoint, remap_state_dict, resume_checkpoint\nfrom ._hub import load_model_config_from_hf, load_state_dict_from_hf, push_to_hf_hub\nfrom ._manipulate import model_parameters, named_apply, named_modules, named_modules_with_params, \\\n    group_modules, group_parameters, checkpoint_seq, adapt_input_conv\nfrom ._pretrained import PretrainedCfg, DefaultCfg, filter_pretrained_cfg\nfrom ._prune import adapt_model_from_string\nfrom ._registry import split_model_name_tag, get_arch_name, generate_default_cfgs, register_model, \\\n    register_model_deprecations, model_entrypoint, list_models, list_pretrained, get_deprecated_models, \\\n    is_model, list_modules, is_model_in_modules, is_model_pretrained, get_pretrained_cfg, get_pretrained_cfg_value\n",
  "\"\"\" DaViT: Dual Attention Vision Transformers\n\nAs described in https://arxiv.org/abs/2204.03645\n\nInput size invariant transformer architecture that combines channel and spacial\nattention in each block. The attention mechanisms used are linear in complexity.\n\nDaViT model defs and weights adapted from https://github.com/dingmyu/davit, original copyright below\n\n\"\"\"\n# Copyright (c) 2022 Mingyu Ding\n# All rights reserved.\n# This source code is licensed under the MIT license\nfrom functools import partial\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import DropPath, to_2tuple, trunc_normal_, Mlp, LayerNorm2d, get_norm_layer, use_fused_attn\nfrom timm.layers import NormMlpClassifierHead, ClassifierHead\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_function\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import generate_default_cfgs, register_model\n\n__all__ = ['DaVit']\n\n\nclass ConvPosEnc(nn.Module):\n    def __init__(self, dim: int, k: int = 3, act: bool = False):\n        super(ConvPosEnc, self).__init__()\n\n        self.proj = nn.Conv2d(dim, dim, k, 1, k // 2, groups=dim)\n        self.act = nn.GELU() if act else nn.Identity()\n\n    def forward(self, x: Tensor):\n        feat = self.proj(x)\n        x = x + self.act(feat)\n        return x\n\n\nclass Stem(nn.Module):\n    \"\"\" Size-agnostic implementation of 2D image to patch embedding,\n        allowing input size to be adjusted during model forward operation\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chs=3,\n            out_chs=96,\n            stride=4,\n            norm_layer=LayerNorm2d,\n    ):\n        super().__init__()\n        stride = to_2tuple(stride)\n        self.stride = stride\n        self.in_chs = in_chs\n        self.out_chs = out_chs\n        assert stride[0] == 4  # only setup for stride==4\n        self.conv = nn.Conv2d(\n            in_chs,\n            out_chs,\n            kernel_size=7,\n            stride=stride,\n            padding=3,\n        )\n        self.norm = norm_layer(out_chs)\n\n    def forward(self, x: Tensor):\n        B, C, H, W = x.shape\n        x = F.pad(x, (0, (self.stride[1] - W % self.stride[1]) % self.stride[1]))\n        x = F.pad(x, (0, 0, 0, (self.stride[0] - H % self.stride[0]) % self.stride[0]))\n        x = self.conv(x)\n        x = self.norm(x)\n        return x\n\n\nclass Downsample(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            norm_layer=LayerNorm2d,\n    ):\n        super().__init__()\n        self.in_chs = in_chs\n        self.out_chs = out_chs\n\n        self.norm = norm_layer(in_chs)\n        self.conv = nn.Conv2d(\n            in_chs,\n            out_chs,\n            kernel_size=2,\n            stride=2,\n            padding=0,\n        )\n\n    def forward(self, x: Tensor):\n        B, C, H, W = x.shape\n        x = self.norm(x)\n        x = F.pad(x, (0, (2 - W % 2) % 2))\n        x = F.pad(x, (0, 0, 0, (2 - H % 2) % 2))\n        x = self.conv(x)\n        return x\n\n\nclass ChannelAttention(nn.Module):\n\n    def __init__(self, dim, num_heads=8, qkv_bias=False):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n\n    def forward(self, x: Tensor):\n        B, N, C = x.shape\n\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)\n\n        k = k * self.scale\n        attention = k.transpose(-1, -2) @ v\n        attention = attention.softmax(dim=-1)\n        x = (attention @ q.transpose(-1, -2)).transpose(-1, -2)\n        x = x.transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        return x\n\n\nclass ChannelBlock(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            ffn=True,\n            cpe_act=False,\n    ):\n        super().__init__()\n\n        self.cpe1 = ConvPosEnc(dim=dim, k=3, act=cpe_act)\n        self.ffn = ffn\n        self.norm1 = norm_layer(dim)\n        self.attn = ChannelAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias)\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.cpe2 = ConvPosEnc(dim=dim, k=3, act=cpe_act)\n\n        if self.ffn:\n            self.norm2 = norm_layer(dim)\n            self.mlp = Mlp(\n                in_features=dim,\n                hidden_features=int(dim * mlp_ratio),\n                act_layer=act_layer,\n            )\n            self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        else:\n            self.norm2 = None\n            self.mlp = None\n            self.drop_path2 = None\n\n    def forward(self, x: Tensor):\n        B, C, H, W = x.shape\n\n        x = self.cpe1(x).flatten(2).transpose(1, 2)\n\n        cur = self.norm1(x)\n        cur = self.attn(cur)\n        x = x + self.drop_path1(cur)\n\n        x = self.cpe2(x.transpose(1, 2).view(B, C, H, W))\n\n        if self.mlp is not None:\n            x = x.flatten(2).transpose(1, 2)\n            x = x + self.drop_path2(self.mlp(self.norm2(x)))\n            x = x.transpose(1, 2).view(B, C, H, W)\n\n        return x\n\n\ndef window_partition(x: Tensor, window_size: Tuple[int, int]):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size[0], window_size[0], W // window_size[1], window_size[1], C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0], window_size[1], C)\n    return windows\n\n\n@register_notrace_function  # reason: int argument is a Proxy\ndef window_reverse(windows: Tensor, window_size: Tuple[int, int], H: int, W: int):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    C = windows.shape[-1]\n    x = windows.view(-1, H // window_size[0], W // window_size[1], window_size[0], window_size[1], C)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, C)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n    \"\"\"\n    fused_attn: torch.jit.Final[bool]\n\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n        self.fused_attn = use_fused_attn()\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x: Tensor):\n        B_, N, C = x.shape\n\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)\n\n        if self.fused_attn:\n            x = F.scaled_dot_product_attention(q, k, v)\n        else:\n            q = q * self.scale\n            attn = (q @ k.transpose(-2, -1))\n            attn = self.softmax(attn)\n            x = attn @ v\n\n        x = x.transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        return x\n\n\nclass SpatialBlock(nn.Module):\n    r\"\"\" Windows Block.\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            window_size=7,\n            mlp_ratio=4.,\n            qkv_bias=True,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            ffn=True,\n            cpe_act=False,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.ffn = ffn\n        self.num_heads = num_heads\n        self.window_size = to_2tuple(window_size)\n        self.mlp_ratio = mlp_ratio\n\n        self.cpe1 = ConvPosEnc(dim=dim, k=3, act=cpe_act)\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim,\n            self.window_size,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n        )\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.cpe2 = ConvPosEnc(dim=dim, k=3, act=cpe_act)\n        if self.ffn:\n            self.norm2 = norm_layer(dim)\n            mlp_hidden_dim = int(dim * mlp_ratio)\n            self.mlp = Mlp(\n                in_features=dim,\n                hidden_features=mlp_hidden_dim,\n                act_layer=act_layer,\n            )\n            self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        else:\n            self.norm2 = None\n            self.mlp = None\n            self.drop_path1 = None\n\n    def forward(self, x: Tensor):\n        B, C, H, W = x.shape\n\n        shortcut = self.cpe1(x).flatten(2).transpose(1, 2)\n\n        x = self.norm1(shortcut)\n        x = x.view(B, H, W, C)\n\n        pad_l = pad_t = 0\n        pad_r = (self.window_size[1] - W % self.window_size[1]) % self.window_size[1]\n        pad_b = (self.window_size[0] - H % self.window_size[0]) % self.window_size[0]\n        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n        _, Hp, Wp, _ = x.shape\n\n        x_windows = window_partition(x, self.window_size)\n        x_windows = x_windows.view(-1, self.window_size[0] * self.window_size[1], C)\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows)\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size[0], self.window_size[1], C)\n        x = window_reverse(attn_windows, self.window_size, Hp, Wp)\n\n        # if pad_r > 0 or pad_b > 0:\n        x = x[:, :H, :W, :].contiguous()\n\n        x = x.view(B, H * W, C)\n        x = shortcut + self.drop_path1(x)\n\n        x = self.cpe2(x.transpose(1, 2).view(B, C, H, W))\n\n        if self.mlp is not None:\n            x = x.flatten(2).transpose(1, 2)\n            x = x + self.drop_path2(self.mlp(self.norm2(x)))\n            x = x.transpose(1, 2).view(B, C, H, W)\n\n        return x\n\n\nclass DaVitStage(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            depth=1,\n            downsample=True,\n            attn_types=('spatial', 'channel'),\n            num_heads=3,\n            window_size=7,\n            mlp_ratio=4,\n            qkv_bias=True,\n            drop_path_rates=(0, 0),\n            norm_layer=LayerNorm2d,\n            norm_layer_cl=nn.LayerNorm,\n            ffn=True,\n            cpe_act=False\n    ):\n        super().__init__()\n\n        self.grad_checkpointing = False\n\n        # downsample embedding layer at the beginning of each stage\n        if downsample:\n            self.downsample = Downsample(in_chs, out_chs, norm_layer=norm_layer)\n        else:\n            self.downsample = nn.Identity()\n\n        '''\n         repeating alternating attention blocks in each stage\n         default: (spatial -> channel) x depth\n         \n         potential opportunity to integrate with a more general version of ByobNet/ByoaNet\n         since the logic is similar\n        '''\n        stage_blocks = []\n        for block_idx in range(depth):\n            dual_attention_block = []\n            for attn_idx, attn_type in enumerate(attn_types):\n                if attn_type == 'spatial':\n                    dual_attention_block.append(SpatialBlock(\n                        dim=out_chs,\n                        num_heads=num_heads,\n                        mlp_ratio=mlp_ratio,\n                        qkv_bias=qkv_bias,\n                        drop_path=drop_path_rates[block_idx],\n                        norm_layer=norm_layer_cl,\n                        ffn=ffn,\n                        cpe_act=cpe_act,\n                        window_size=window_size,\n                    ))\n                elif attn_type == 'channel':\n                    dual_attention_block.append(ChannelBlock(\n                        dim=out_chs,\n                        num_heads=num_heads,\n                        mlp_ratio=mlp_ratio,\n                        qkv_bias=qkv_bias,\n                        drop_path=drop_path_rates[block_idx],\n                        norm_layer=norm_layer_cl,\n                        ffn=ffn,\n                        cpe_act=cpe_act\n                    ))\n            stage_blocks.append(nn.Sequential(*dual_attention_block))\n        self.blocks = nn.Sequential(*stage_blocks)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    def forward(self, x: Tensor):\n        x = self.downsample(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        return x\n\n\nclass DaVit(nn.Module):\n    r\"\"\" DaViT\n        A PyTorch implementation of `DaViT: Dual Attention Vision Transformers`  - https://arxiv.org/abs/2204.03645\n        Supports arbitrary input sizes and pyramid feature extraction\n        \n    Args:\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        depths (tuple(int)): Number of blocks in each stage. Default: (1, 1, 3, 1)\n        embed_dims (tuple(int)): Patch embedding dimension. Default: (96, 192, 384, 768)\n        num_heads (tuple(int)): Number of attention heads in different layers. Default: (3, 6, 12, 24)\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chans=3,\n            depths=(1, 1, 3, 1),\n            embed_dims=(96, 192, 384, 768),\n            num_heads=(3, 6, 12, 24),\n            window_size=7,\n            mlp_ratio=4,\n            qkv_bias=True,\n            norm_layer='layernorm2d',\n            norm_layer_cl='layernorm',\n            norm_eps=1e-5,\n            attn_types=('spatial', 'channel'),\n            ffn=True,\n            cpe_act=False,\n            drop_rate=0.,\n            drop_path_rate=0.,\n            num_classes=1000,\n            global_pool='avg',\n            head_norm_first=False,\n    ):\n        super().__init__()\n        num_stages = len(embed_dims)\n        assert num_stages == len(num_heads) == len(depths)\n        norm_layer = partial(get_norm_layer(norm_layer), eps=norm_eps)\n        norm_layer_cl = partial(get_norm_layer(norm_layer_cl), eps=norm_eps)\n        self.num_classes = num_classes\n        self.num_features = embed_dims[-1]\n        self.drop_rate = drop_rate\n        self.grad_checkpointing = False\n        self.feature_info = []\n\n        self.stem = Stem(in_chans, embed_dims[0], norm_layer=norm_layer)\n        in_chs = embed_dims[0]\n\n        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n        stages = []\n        for stage_idx in range(num_stages):\n            out_chs = embed_dims[stage_idx]\n            stage = DaVitStage(\n                in_chs,\n                out_chs,\n                depth=depths[stage_idx],\n                downsample=stage_idx > 0,\n                attn_types=attn_types,\n                num_heads=num_heads[stage_idx],\n                window_size=window_size,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                drop_path_rates=dpr[stage_idx],\n                norm_layer=norm_layer,\n                norm_layer_cl=norm_layer_cl,\n                ffn=ffn,\n                cpe_act=cpe_act,\n            )\n            in_chs = out_chs\n            stages.append(stage)\n            self.feature_info += [dict(num_chs=out_chs, reduction=2, module=f'stages.{stage_idx}')]\n\n        self.stages = nn.Sequential(*stages)\n\n        # if head_norm_first == true, norm -> global pool -> fc ordering, like most other nets\n        # otherwise pool -> norm -> fc, the default DaViT order, similar to ConvNeXt\n        # FIXME generalize this structure to ClassifierHead\n        if head_norm_first:\n            self.norm_pre = norm_layer(self.num_features)\n            self.head = ClassifierHead(\n                self.num_features,\n                num_classes,\n                pool_type=global_pool,\n                drop_rate=self.drop_rate,\n            )\n        else:\n            self.norm_pre = nn.Identity()\n            self.head = NormMlpClassifierHead(\n                self.num_features,\n                num_classes,\n                pool_type=global_pool,\n                drop_rate=self.drop_rate,\n                norm_layer=norm_layer,\n            )\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n        for stage in self.stages:\n            stage.set_grad_checkpointing(enable=enable)\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.head.reset(num_classes, global_pool=global_pool)\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.stages, x)\n        else:\n            x = self.stages(x)\n        x = self.norm_pre(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        x = self.head.global_pool(x)\n        x = self.head.norm(x)\n        x = self.head.flatten(x)\n        x = self.head.drop(x)\n        return x if pre_logits else self.head.fc(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    \"\"\" Remap MSFT checkpoints -> timm \"\"\"\n    if 'head.fc.weight' in state_dict:\n        return state_dict  # non-MSFT checkpoint\n\n    if 'state_dict' in state_dict:\n        state_dict = state_dict['state_dict']\n\n    import re\n    out_dict = {}\n    for k, v in state_dict.items():\n        k = re.sub(r'patch_embeds.([0-9]+)', r'stages.\\1.downsample', k)\n        k = re.sub(r'main_blocks.([0-9]+)', r'stages.\\1.blocks', k)\n        k = k.replace('downsample.proj', 'downsample.conv')\n        k = k.replace('stages.0.downsample', 'stem')\n        k = k.replace('head.', 'head.fc.')\n        k = k.replace('norms.', 'head.norm.')\n        k = k.replace('cpe.0', 'cpe1')\n        k = k.replace('cpe.1', 'cpe2')\n        out_dict[k] = v\n    return out_dict\n\n\ndef _create_davit(variant, pretrained=False, **kwargs):\n    default_out_indices = tuple(i for i, _ in enumerate(kwargs.get('depths', (1, 1, 3, 1))))\n    out_indices = kwargs.pop('out_indices', default_out_indices)\n\n    model = build_model_with_cfg(\n        DaVit,\n        variant,\n        pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),\n        **kwargs)\n\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.95, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.conv', 'classifier': 'head.fc',\n        **kwargs\n    }\n\n\n# TODO contact authors to get larger pretrained models\ndefault_cfgs = generate_default_cfgs({\n    # official microsoft weights from https://github.com/dingmyu/davit\n    'davit_tiny.msft_in1k': _cfg(\n        hf_hub_id='timm/'),\n    'davit_small.msft_in1k': _cfg(\n        hf_hub_id='timm/'),\n    'davit_base.msft_in1k': _cfg(\n        hf_hub_id='timm/'),\n    'davit_large': _cfg(),\n    'davit_huge': _cfg(),\n    'davit_giant': _cfg(),\n})\n\n\n@register_model\ndef davit_tiny(pretrained=False, **kwargs) -> DaVit:\n    model_kwargs = dict(\n        depths=(1, 1, 3, 1), embed_dims=(96, 192, 384, 768), num_heads=(3, 6, 12, 24), **kwargs)\n    return _create_davit('davit_tiny', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef davit_small(pretrained=False, **kwargs) -> DaVit:\n    model_kwargs = dict(\n        depths=(1, 1, 9, 1), embed_dims=(96, 192, 384, 768), num_heads=(3, 6, 12, 24), **kwargs)\n    return _create_davit('davit_small', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef davit_base(pretrained=False, **kwargs) -> DaVit:\n    model_kwargs = dict(\n        depths=(1, 1, 9, 1), embed_dims=(128, 256, 512, 1024), num_heads=(4, 8, 16, 32), **kwargs)\n    return _create_davit('davit_base', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef davit_large(pretrained=False, **kwargs) -> DaVit:\n    model_kwargs = dict(\n        depths=(1, 1, 9, 1), embed_dims=(192, 384, 768, 1536), num_heads=(6, 12, 24, 48), **kwargs)\n    return _create_davit('davit_large', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef davit_huge(pretrained=False, **kwargs) -> DaVit:\n    model_kwargs = dict(\n        depths=(1, 1, 9, 1), embed_dims=(256, 512, 1024, 2048), num_heads=(8, 16, 32, 64), **kwargs)\n    return _create_davit('davit_huge', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef davit_giant(pretrained=False, **kwargs) -> DaVit:\n    model_kwargs = dict(\n        depths=(1, 1, 12, 3), embed_dims=(384, 768, 1536, 3072), num_heads=(12, 24, 48, 96), **kwargs)\n    return _create_davit('davit_giant', pretrained=pretrained, **model_kwargs)\n",
  "\"\"\"PyTorch ResNet\n\nThis started as a copy of https://github.com/pytorch/vision 'resnet.py' (BSD-3-Clause) with\nadditional dropout and dynamic global avg/max pool.\n\nResNeXt, SE-ResNeXt, SENet, and MXNet Gluon stem/downsample variants, tiered stems added by Ross Wightman\n\nCopyright 2019, Ross Wightman\n\"\"\"\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import DropBlock2d, DropPath, AvgPool2dSame, BlurPool2d, GroupNorm, create_attn, get_attn, \\\n    get_act_layer, get_norm_layer, create_classifier\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs, register_model_deprecations\n\n__all__ = ['ResNet', 'BasicBlock', 'Bottleneck']  # model_registry will add each entrypoint fn to this\n\n\ndef get_padding(kernel_size, stride, dilation=1):\n    padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n    return padding\n\n\ndef create_aa(aa_layer, channels, stride=2, enable=True):\n    if not aa_layer or not enable:\n        return nn.Identity()\n    if issubclass(aa_layer, nn.AvgPool2d):\n        return aa_layer(stride)\n    else:\n        return aa_layer(channels=channels, stride=stride)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(\n            self,\n            inplanes,\n            planes,\n            stride=1,\n            downsample=None,\n            cardinality=1,\n            base_width=64,\n            reduce_first=1,\n            dilation=1,\n            first_dilation=None,\n            act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d,\n            attn_layer=None,\n            aa_layer=None,\n            drop_block=None,\n            drop_path=None,\n    ):\n        super(BasicBlock, self).__init__()\n\n        assert cardinality == 1, 'BasicBlock only supports cardinality of 1'\n        assert base_width == 64, 'BasicBlock does not support changing base width'\n        first_planes = planes // reduce_first\n        outplanes = planes * self.expansion\n        first_dilation = first_dilation or dilation\n        use_aa = aa_layer is not None and (stride == 2 or first_dilation != dilation)\n\n        self.conv1 = nn.Conv2d(\n            inplanes, first_planes, kernel_size=3, stride=1 if use_aa else stride, padding=first_dilation,\n            dilation=first_dilation, bias=False)\n        self.bn1 = norm_layer(first_planes)\n        self.drop_block = drop_block() if drop_block is not None else nn.Identity()\n        self.act1 = act_layer(inplace=True)\n        self.aa = create_aa(aa_layer, channels=first_planes, stride=stride, enable=use_aa)\n\n        self.conv2 = nn.Conv2d(\n            first_planes, outplanes, kernel_size=3, padding=dilation, dilation=dilation, bias=False)\n        self.bn2 = norm_layer(outplanes)\n\n        self.se = create_attn(attn_layer, outplanes)\n\n        self.act2 = act_layer(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.drop_path = drop_path\n\n    def zero_init_last(self):\n        if getattr(self.bn2, 'weight', None) is not None:\n            nn.init.zeros_(self.bn2.weight)\n\n    def forward(self, x):\n        shortcut = x\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.drop_block(x)\n        x = self.act1(x)\n        x = self.aa(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n\n        if self.se is not None:\n            x = self.se(x)\n\n        if self.drop_path is not None:\n            x = self.drop_path(x)\n\n        if self.downsample is not None:\n            shortcut = self.downsample(shortcut)\n        x += shortcut\n        x = self.act2(x)\n\n        return x\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(\n            self,\n            inplanes,\n            planes,\n            stride=1,\n            downsample=None,\n            cardinality=1,\n            base_width=64,\n            reduce_first=1,\n            dilation=1,\n            first_dilation=None,\n            act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d,\n            attn_layer=None,\n            aa_layer=None,\n            drop_block=None,\n            drop_path=None,\n    ):\n        super(Bottleneck, self).__init__()\n\n        width = int(math.floor(planes * (base_width / 64)) * cardinality)\n        first_planes = width // reduce_first\n        outplanes = planes * self.expansion\n        first_dilation = first_dilation or dilation\n        use_aa = aa_layer is not None and (stride == 2 or first_dilation != dilation)\n\n        self.conv1 = nn.Conv2d(inplanes, first_planes, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(first_planes)\n        self.act1 = act_layer(inplace=True)\n\n        self.conv2 = nn.Conv2d(\n            first_planes, width, kernel_size=3, stride=1 if use_aa else stride,\n            padding=first_dilation, dilation=first_dilation, groups=cardinality, bias=False)\n        self.bn2 = norm_layer(width)\n        self.drop_block = drop_block() if drop_block is not None else nn.Identity()\n        self.act2 = act_layer(inplace=True)\n        self.aa = create_aa(aa_layer, channels=width, stride=stride, enable=use_aa)\n\n        self.conv3 = nn.Conv2d(width, outplanes, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(outplanes)\n\n        self.se = create_attn(attn_layer, outplanes)\n\n        self.act3 = act_layer(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.drop_path = drop_path\n\n    def zero_init_last(self):\n        if getattr(self.bn3, 'weight', None) is not None:\n            nn.init.zeros_(self.bn3.weight)\n\n    def forward(self, x):\n        shortcut = x\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.drop_block(x)\n        x = self.act2(x)\n        x = self.aa(x)\n\n        x = self.conv3(x)\n        x = self.bn3(x)\n\n        if self.se is not None:\n            x = self.se(x)\n\n        if self.drop_path is not None:\n            x = self.drop_path(x)\n\n        if self.downsample is not None:\n            shortcut = self.downsample(shortcut)\n        x += shortcut\n        x = self.act3(x)\n\n        return x\n\n\ndef downsample_conv(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        dilation=1,\n        first_dilation=None,\n        norm_layer=None,\n):\n    norm_layer = norm_layer or nn.BatchNorm2d\n    kernel_size = 1 if stride == 1 and dilation == 1 else kernel_size\n    first_dilation = (first_dilation or dilation) if kernel_size > 1 else 1\n    p = get_padding(kernel_size, stride, first_dilation)\n\n    return nn.Sequential(*[\n        nn.Conv2d(\n            in_channels, out_channels, kernel_size, stride=stride, padding=p, dilation=first_dilation, bias=False),\n        norm_layer(out_channels)\n    ])\n\n\ndef downsample_avg(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        dilation=1,\n        first_dilation=None,\n        norm_layer=None,\n):\n    norm_layer = norm_layer or nn.BatchNorm2d\n    avg_stride = stride if dilation == 1 else 1\n    if stride == 1 and dilation == 1:\n        pool = nn.Identity()\n    else:\n        avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d\n        pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)\n\n    return nn.Sequential(*[\n        pool,\n        nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False),\n        norm_layer(out_channels)\n    ])\n\n\ndef drop_blocks(drop_prob=0.):\n    return [\n        None, None,\n        partial(DropBlock2d, drop_prob=drop_prob, block_size=5, gamma_scale=0.25) if drop_prob else None,\n        partial(DropBlock2d, drop_prob=drop_prob, block_size=3, gamma_scale=1.00) if drop_prob else None]\n\n\ndef make_blocks(\n        block_fn,\n        channels,\n        block_repeats,\n        inplanes,\n        reduce_first=1,\n        output_stride=32,\n        down_kernel_size=1,\n        avg_down=False,\n        drop_block_rate=0.,\n        drop_path_rate=0.,\n        **kwargs,\n):\n    stages = []\n    feature_info = []\n    net_num_blocks = sum(block_repeats)\n    net_block_idx = 0\n    net_stride = 4\n    dilation = prev_dilation = 1\n    for stage_idx, (planes, num_blocks, db) in enumerate(zip(channels, block_repeats, drop_blocks(drop_block_rate))):\n        stage_name = f'layer{stage_idx + 1}'  # never liked this name, but weight compat requires it\n        stride = 1 if stage_idx == 0 else 2\n        if net_stride >= output_stride:\n            dilation *= stride\n            stride = 1\n        else:\n            net_stride *= stride\n\n        downsample = None\n        if stride != 1 or inplanes != planes * block_fn.expansion:\n            down_kwargs = dict(\n                in_channels=inplanes,\n                out_channels=planes * block_fn.expansion,\n                kernel_size=down_kernel_size,\n                stride=stride,\n                dilation=dilation,\n                first_dilation=prev_dilation,\n                norm_layer=kwargs.get('norm_layer'),\n            )\n            downsample = downsample_avg(**down_kwargs) if avg_down else downsample_conv(**down_kwargs)\n\n        block_kwargs = dict(reduce_first=reduce_first, dilation=dilation, drop_block=db, **kwargs)\n        blocks = []\n        for block_idx in range(num_blocks):\n            downsample = downsample if block_idx == 0 else None\n            stride = stride if block_idx == 0 else 1\n            block_dpr = drop_path_rate * net_block_idx / (net_num_blocks - 1)  # stochastic depth linear decay rule\n            blocks.append(block_fn(\n                inplanes,\n                planes,\n                stride,\n                downsample,\n                first_dilation=prev_dilation,\n                drop_path=DropPath(block_dpr) if block_dpr > 0. else None,\n                **block_kwargs,\n            ))\n            prev_dilation = dilation\n            inplanes = planes * block_fn.expansion\n            net_block_idx += 1\n\n        stages.append((stage_name, nn.Sequential(*blocks)))\n        feature_info.append(dict(num_chs=inplanes, reduction=net_stride, module=stage_name))\n\n    return stages, feature_info\n\n\nclass ResNet(nn.Module):\n    \"\"\"ResNet / ResNeXt / SE-ResNeXt / SE-Net\n\n    This class implements all variants of ResNet, ResNeXt, SE-ResNeXt, and SENet that\n      * have > 1 stride in the 3x3 conv layer of bottleneck\n      * have conv-bn-act ordering\n\n    This ResNet impl supports a number of stem and downsample options based on the v1c, v1d, v1e, and v1s\n    variants included in the MXNet Gluon ResNetV1b model. The C and D variants are also discussed in the\n    'Bag of Tricks' paper: https://arxiv.org/pdf/1812.01187. The B variant is equivalent to torchvision default.\n\n    ResNet variants (the same modifications can be used in SE/ResNeXt models as well):\n      * normal, b - 7x7 stem, stem_width = 64, same as torchvision ResNet, NVIDIA ResNet 'v1.5', Gluon v1b\n      * c - 3 layer deep 3x3 stem, stem_width = 32 (32, 32, 64)\n      * d - 3 layer deep 3x3 stem, stem_width = 32 (32, 32, 64), average pool in downsample\n      * e - 3 layer deep 3x3 stem, stem_width = 64 (64, 64, 128), average pool in downsample\n      * s - 3 layer deep 3x3 stem, stem_width = 64 (64, 64, 128)\n      * t - 3 layer deep 3x3 stem, stem width = 32 (24, 48, 64), average pool in downsample\n      * tn - 3 layer deep 3x3 stem, stem width = 32 (24, 32, 64), average pool in downsample\n\n    ResNeXt\n      * normal - 7x7 stem, stem_width = 64, standard cardinality and base widths\n      * same c,d, e, s variants as ResNet can be enabled\n\n    SE-ResNeXt\n      * normal - 7x7 stem, stem_width = 64\n      * same c, d, e, s variants as ResNet can be enabled\n\n    SENet-154 - 3 layer deep 3x3 stem (same as v1c-v1s), stem_width = 64, cardinality=64,\n        reduction by 2 on width of first bottleneck convolution, 3x3 downsample convs after first block\n    \"\"\"\n\n    def __init__(\n            self,\n            block,\n            layers,\n            num_classes=1000,\n            in_chans=3,\n            output_stride=32,\n            global_pool='avg',\n            cardinality=1,\n            base_width=64,\n            stem_width=64,\n            stem_type='',\n            replace_stem_pool=False,\n            block_reduce_first=1,\n            down_kernel_size=1,\n            avg_down=False,\n            act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d,\n            aa_layer=None,\n            drop_rate=0.0,\n            drop_path_rate=0.,\n            drop_block_rate=0.,\n            zero_init_last=True,\n            block_args=None,\n    ):\n        \"\"\"\n        Args:\n            block (nn.Module): class for the residual block. Options are BasicBlock, Bottleneck.\n            layers (List[int]) : number of layers in each block\n            num_classes (int): number of classification classes (default 1000)\n            in_chans (int): number of input (color) channels. (default 3)\n            output_stride (int): output stride of the network, 32, 16, or 8. (default 32)\n            global_pool (str): Global pooling type. One of 'avg', 'max', 'avgmax', 'catavgmax' (default 'avg')\n            cardinality (int): number of convolution groups for 3x3 conv in Bottleneck. (default 1)\n            base_width (int): bottleneck channels factor. `planes * base_width / 64 * cardinality` (default 64)\n            stem_width (int): number of channels in stem convolutions (default 64)\n            stem_type (str): The type of stem (default ''):\n                * '', default - a single 7x7 conv with a width of stem_width\n                * 'deep' - three 3x3 convolution layers of widths stem_width, stem_width, stem_width * 2\n                * 'deep_tiered' - three 3x3 conv layers of widths stem_width//4 * 3, stem_width, stem_width * 2\n            replace_stem_pool (bool): replace stem max-pooling layer with a 3x3 stride-2 convolution\n            block_reduce_first (int): Reduction factor for first convolution output width of residual blocks,\n                1 for all archs except senets, where 2 (default 1)\n            down_kernel_size (int): kernel size of residual block downsample path,\n                1x1 for most, 3x3 for senets (default: 1)\n            avg_down (bool): use avg pooling for projection skip connection between stages/downsample (default False)\n            act_layer (str, nn.Module): activation layer\n            norm_layer (str, nn.Module): normalization layer\n            aa_layer (nn.Module): anti-aliasing layer\n            drop_rate (float): Dropout probability before classifier, for training (default 0.)\n            drop_path_rate (float): Stochastic depth drop-path rate (default 0.)\n            drop_block_rate (float): Drop block rate (default 0.)\n            zero_init_last (bool): zero-init the last weight in residual path (usually last BN affine weight)\n            block_args (dict): Extra kwargs to pass through to block module\n        \"\"\"\n        super(ResNet, self).__init__()\n        block_args = block_args or dict()\n        assert output_stride in (8, 16, 32)\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        self.grad_checkpointing = False\n        \n        act_layer = get_act_layer(act_layer)\n        norm_layer = get_norm_layer(norm_layer)\n\n        # Stem\n        deep_stem = 'deep' in stem_type\n        inplanes = stem_width * 2 if deep_stem else 64\n        if deep_stem:\n            stem_chs = (stem_width, stem_width)\n            if 'tiered' in stem_type:\n                stem_chs = (3 * (stem_width // 4), stem_width)\n            self.conv1 = nn.Sequential(*[\n                nn.Conv2d(in_chans, stem_chs[0], 3, stride=2, padding=1, bias=False),\n                norm_layer(stem_chs[0]),\n                act_layer(inplace=True),\n                nn.Conv2d(stem_chs[0], stem_chs[1], 3, stride=1, padding=1, bias=False),\n                norm_layer(stem_chs[1]),\n                act_layer(inplace=True),\n                nn.Conv2d(stem_chs[1], inplanes, 3, stride=1, padding=1, bias=False)])\n        else:\n            self.conv1 = nn.Conv2d(in_chans, inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = norm_layer(inplanes)\n        self.act1 = act_layer(inplace=True)\n        self.feature_info = [dict(num_chs=inplanes, reduction=2, module='act1')]\n\n        # Stem pooling. The name 'maxpool' remains for weight compatibility.\n        if replace_stem_pool:\n            self.maxpool = nn.Sequential(*filter(None, [\n                nn.Conv2d(inplanes, inplanes, 3, stride=1 if aa_layer else 2, padding=1, bias=False),\n                create_aa(aa_layer, channels=inplanes, stride=2) if aa_layer is not None else None,\n                norm_layer(inplanes),\n                act_layer(inplace=True),\n            ]))\n        else:\n            if aa_layer is not None:\n                if issubclass(aa_layer, nn.AvgPool2d):\n                    self.maxpool = aa_layer(2)\n                else:\n                    self.maxpool = nn.Sequential(*[\n                        nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n                        aa_layer(channels=inplanes, stride=2)])\n            else:\n                self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        # Feature Blocks\n        channels = [64, 128, 256, 512]\n        stage_modules, stage_feature_info = make_blocks(\n            block,\n            channels,\n            layers,\n            inplanes,\n            cardinality=cardinality,\n            base_width=base_width,\n            output_stride=output_stride,\n            reduce_first=block_reduce_first,\n            avg_down=avg_down,\n            down_kernel_size=down_kernel_size,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n            aa_layer=aa_layer,\n            drop_block_rate=drop_block_rate,\n            drop_path_rate=drop_path_rate,\n            **block_args,\n        )\n        for stage in stage_modules:\n            self.add_module(*stage)  # layer1, layer2, etc\n        self.feature_info.extend(stage_feature_info)\n\n        # Head (Pooling and Classifier)\n        self.num_features = 512 * block.expansion\n        self.global_pool, self.fc = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)\n\n        self.init_weights(zero_init_last=zero_init_last)\n\n    @torch.jit.ignore\n    def init_weights(self, zero_init_last=True):\n        for n, m in self.named_modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n        if zero_init_last:\n            for m in self.modules():\n                if hasattr(m, 'zero_init_last'):\n                    m.zero_init_last()\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(stem=r'^conv1|bn1|maxpool', blocks=r'^layer(\\d+)' if coarse else r'^layer(\\d+)\\.(\\d+)')\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self, name_only=False):\n        return 'fc' if name_only else self.fc\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.global_pool, self.fc = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        x = self.maxpool(x)\n\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq([self.layer1, self.layer2, self.layer3, self.layer4], x, flatten=True)\n        else:\n            x = self.layer1(x)\n            x = self.layer2(x)\n            x = self.layer3(x)\n            x = self.layer4(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        x = self.global_pool(x)\n        if self.drop_rate:\n            x = F.dropout(x, p=float(self.drop_rate), training=self.training)\n        return x if pre_logits else self.fc(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_resnet(variant, pretrained=False, **kwargs):\n    return build_model_with_cfg(ResNet, variant, pretrained, **kwargs)\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bilinear',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'conv1', 'classifier': 'fc',\n        **kwargs\n    }\n\n\ndef _tcfg(url='', **kwargs):\n    return _cfg(url=url, **dict({'interpolation': 'bicubic'}, **kwargs))\n\n\ndef _ttcfg(url='', **kwargs):\n    return _cfg(url=url, **dict({\n        'interpolation': 'bicubic', 'test_input_size': (3, 288, 288), 'test_crop_pct': 0.95,\n        'origin_url': 'https://github.com/huggingface/pytorch-image-models',\n    }, **kwargs))\n\n\ndef _rcfg(url='', **kwargs):\n    return _cfg(url=url, **dict({\n        'interpolation': 'bicubic', 'crop_pct': 0.95, 'test_input_size': (3, 288, 288), 'test_crop_pct': 1.0,\n        'origin_url': 'https://github.com/huggingface/pytorch-image-models', 'paper_ids': 'arXiv:2110.00476'\n    }, **kwargs))\n\n\ndef _r3cfg(url='', **kwargs):\n    return _cfg(url=url, **dict({\n        'interpolation': 'bicubic', 'input_size': (3, 160, 160), 'pool_size': (5, 5),\n        'crop_pct': 0.95, 'test_input_size': (3, 224, 224), 'test_crop_pct': 0.95,\n        'origin_url': 'https://github.com/huggingface/pytorch-image-models', 'paper_ids': 'arXiv:2110.00476',\n    }, **kwargs))\n\n\ndef _gcfg(url='', **kwargs):\n    return _cfg(url=url, **dict({\n        'interpolation': 'bicubic',\n        'origin_url': 'https://cv.gluon.ai/model_zoo/classification.html',\n    }, **kwargs))\n\n\ndefault_cfgs = generate_default_cfgs({\n    # ResNet and Wide ResNet trained w/ timm (RSB paper and others)\n    'resnet10t.c3_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet10t_176_c3-f3215ab1.pth',\n        input_size=(3, 176, 176), pool_size=(6, 6), test_crop_pct=0.95, test_input_size=(3, 224, 224),\n        first_conv='conv1.0'),\n    'resnet14t.c3_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet14t_176_c3-c4ed2c37.pth',\n        input_size=(3, 176, 176), pool_size=(6, 6), test_crop_pct=0.95, test_input_size=(3, 224, 224),\n        first_conv='conv1.0'),\n    'resnet18.a1_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet18_a1_0-d63eafa0.pth'),\n    'resnet18.a2_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet18_a2_0-b61bd467.pth'),\n    'resnet18.a3_in1k': _r3cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet18_a3_0-40c531c8.pth'),\n    'resnet18d.ra2_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet18d_ra2-48a79e06.pth',\n        first_conv='conv1.0'),\n    'resnet34.a1_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet34_a1_0-46f8f793.pth'),\n    'resnet34.a2_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet34_a2_0-82d47d71.pth'),\n    'resnet34.a3_in1k': _r3cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet34_a3_0-a20cabb6.pth',\n        crop_pct=0.95),\n    'resnet34.bt_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34-43635321.pth'),\n    'resnet34d.ra2_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34d_ra2-f8dcfcaf.pth',\n        first_conv='conv1.0'),\n    'resnet26.bt_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet26-9aa10e23.pth'),\n    'resnet26d.bt_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet26d-69e92c46.pth',\n        first_conv='conv1.0'),\n    'resnet26t.ra2_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/resnet26t_256_ra2-6f6fa748.pth',\n        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8),\n        crop_pct=0.94, test_input_size=(3, 320, 320), test_crop_pct=1.0),\n    'resnet50.a1_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1_0-14fe96d1.pth'),\n    'resnet50.a1h_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1h2_176-001a1197.pth',\n        input_size=(3, 176, 176), pool_size=(6, 6), crop_pct=0.9, test_input_size=(3, 224, 224), test_crop_pct=1.0),\n    'resnet50.a2_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a2_0-a2746f79.pth'),\n    'resnet50.a3_in1k': _r3cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a3_0-59cae1ef.pth'),\n    'resnet50.b1k_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_b1k-532a802a.pth'),\n    'resnet50.b2k_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_b2k-1ba180c1.pth'),\n    'resnet50.c1_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_c1-5ba5e060.pth'),\n    'resnet50.c2_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_c2-d01e05b2.pth'),\n    'resnet50.d_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_d-f39db8af.pth'),\n    'resnet50.ram_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/resnet50_ram-a26f946b.pth'),\n    'resnet50.am_in1k': _tcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/resnet50_am-6c502b37.pth'),\n    'resnet50.ra_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/resnet50_ra-85ebb6e5.pth'),\n    'resnet50.bt_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/rw_resnet50-86acaeed.pth'),\n    'resnet50d.ra2_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet50d_ra2-464e36ba.pth',\n        first_conv='conv1.0'),\n    'resnet50d.a1_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50d_a1_0-e20cff14.pth',\n        first_conv='conv1.0'),\n    'resnet50d.a2_in1k': _rcfg(\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50d_a2_0-a3adc64d.pth',\n        first_conv='conv1.0'),\n    'resnet50d.a3_in1k': _r3cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50d_a3_0-403fdfad.pth',\n        first_conv='conv1.0'),\n    'resnet50t.untrained': _ttcfg(first_conv='conv1.0'),\n    'resnet101.a1h_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet101_a1h-36d3f2aa.pth'),\n    'resnet101.a1_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet101_a1_0-cdcb52a9.pth'),\n    'resnet101.a2_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet101_a2_0-6edb36c7.pth'),\n    'resnet101.a3_in1k': _r3cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet101_a3_0-1db14157.pth'),\n    'resnet101d.ra2_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet101d_ra2-2803ffab.pth',\n        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.95,\n        test_crop_pct=1.0, test_input_size=(3, 320, 320)),\n    'resnet152.a1h_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet152_a1h-dc400468.pth'),\n    'resnet152.a1_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet152_a1_0-2eee8a7a.pth'),\n    'resnet152.a2_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet152_a2_0-b4c6978f.pth'),\n    'resnet152.a3_in1k': _r3cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet152_a3_0-134d4688.pth'),\n    'resnet152d.ra2_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet152d_ra2-5cac0439.pth',\n        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.95,\n        test_crop_pct=1.0, test_input_size=(3, 320, 320)),\n    'resnet200.untrained': _ttcfg(),\n    'resnet200d.ra2_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet200d_ra2-bdba9bf9.pth',\n        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.95,\n        test_crop_pct=1.0, test_input_size=(3, 320, 320)),\n    'wide_resnet50_2.racm_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth'),\n\n    # torchvision resnet weights\n    'resnet18.tv_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/resnet18-5c106cde.pth',\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n    'resnet34.tv_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n    'resnet50.tv_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/resnet50-19c8e357.pth',\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n    'resnet50.tv2_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/resnet50-11ad3fa6.pth',\n        input_size=(3, 176, 176), pool_size=(6, 6), test_input_size=(3, 224, 224), test_crop_pct=0.965,\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n    'resnet101.tv_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n    'resnet101.tv2_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/resnet101-cd907fc2.pth',\n        input_size=(3, 176, 176), pool_size=(6, 6), test_input_size=(3, 224, 224), test_crop_pct=0.965,\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n    'resnet152.tv_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n    'resnet152.tv2_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/resnet152-f82ba261.pth',\n        input_size=(3, 176, 176), pool_size=(6, 6), test_input_size=(3, 224, 224), test_crop_pct=0.965,\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n    'wide_resnet50_2.tv_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n    'wide_resnet50_2.tv2_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/wide_resnet50_2-9ba9bcbe.pth',\n        input_size=(3, 176, 176), pool_size=(6, 6), test_input_size=(3, 224, 224), test_crop_pct=0.965,\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n    'wide_resnet101_2.tv_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n    'wide_resnet101_2.tv2_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/wide_resnet101_2-d733dc28.pth',\n        input_size=(3, 176, 176), pool_size=(6, 6), test_input_size=(3, 224, 224), test_crop_pct=0.965,\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n\n    # ResNets w/ alternative norm layers\n    'resnet50_gn.a1h_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_gn_a1h2-8fe6c4d0.pth',\n        crop_pct=0.94),\n\n    # ResNeXt trained in timm (RSB paper and others)\n    'resnext50_32x4d.a1h_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnext50_32x4d_a1h-0146ab0a.pth'),\n    'resnext50_32x4d.a1_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnext50_32x4d_a1_0-b5a91a1d.pth'),\n    'resnext50_32x4d.a2_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnext50_32x4d_a2_0-efc76add.pth'),\n    'resnext50_32x4d.a3_in1k': _r3cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnext50_32x4d_a3_0-3e450271.pth'),\n    'resnext50_32x4d.ra_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/resnext50_32x4d_ra-d733960d.pth'),\n    'resnext50d_32x4d.bt_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnext50d_32x4d-103e99f8.pth',\n        first_conv='conv1.0'),\n    'resnext101_32x4d.untrained': _ttcfg(),\n    'resnext101_64x4d.c1_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/resnext101_64x4d_c-0d0e0cc0.pth'),\n\n    # torchvision ResNeXt weights\n    'resnext50_32x4d.tv_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n    'resnext101_32x8d.tv_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n    'resnext101_64x4d.tv_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/resnext101_64x4d-173b62eb.pth',\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n    'resnext50_32x4d.tv2_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/resnext50_32x4d-1a0047aa.pth',\n        input_size=(3, 176, 176), pool_size=(6, 6), test_input_size=(3, 224, 224), test_crop_pct=0.965,\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n    'resnext101_32x8d.tv2_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/resnext101_32x8d-110c445d.pth',\n        input_size=(3, 176, 176), pool_size=(6, 6), test_input_size=(3, 224, 224), test_crop_pct=0.965,\n        license='bsd-3-clause', origin_url='https://github.com/pytorch/vision'),\n\n    #  ResNeXt models - Weakly Supervised Pretraining on Instagram Hashtags\n    #  from https://github.com/facebookresearch/WSL-Images\n    #  Please note the CC-BY-NC 4.0 license on these weights, non-commercial use only.\n    'resnext101_32x8d.fb_wsl_ig1b_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/ig_resnext101_32x8-c38310e5.pth',\n        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/WSL-Images'),\n    'resnext101_32x16d.fb_wsl_ig1b_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/ig_resnext101_32x16-c6f796b0.pth',\n        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/WSL-Images'),\n    'resnext101_32x32d.fb_wsl_ig1b_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/ig_resnext101_32x32-e4b90b00.pth',\n        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/WSL-Images'),\n    'resnext101_32x48d.fb_wsl_ig1b_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/ig_resnext101_32x48-3e41cc8a.pth',\n        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/WSL-Images'),\n\n    #  Semi-Supervised ResNe*t models from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models\n    #  Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.\n    'resnet18.fb_ssl_yfcc100m_ft_in1k':  _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnet18-d92f0530.pth',\n        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),\n    'resnet50.fb_ssl_yfcc100m_ft_in1k':  _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnet50-08389792.pth',\n        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),\n    'resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext50_32x4-ddb3e555.pth',\n        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),\n    'resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x4-dc43570a.pth',\n        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),\n    'resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x8-2cfe2f8b.pth',\n        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),\n    'resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x16-15fffa57.pth',\n        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),\n\n    #  Semi-Weakly Supervised ResNe*t models from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models\n    #  Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.\n    'resnet18.fb_swsl_ig1b_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnet18-118f1556.pth',\n        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),\n    'resnet50.fb_swsl_ig1b_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnet50-16a12f1b.pth',\n        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),\n    'resnext50_32x4d.fb_swsl_ig1b_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext50_32x4-72679e44.pth',\n        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),\n    'resnext101_32x4d.fb_swsl_ig1b_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x4-3f87e46b.pth',\n        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),\n    'resnext101_32x8d.fb_swsl_ig1b_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x8-b4712904.pth',\n        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),\n    'resnext101_32x16d.fb_swsl_ig1b_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x16-f3559a9c.pth',\n        license='cc-by-nc-4.0', origin_url='https://github.com/facebookresearch/semi-supervised-ImageNet1K-models'),\n\n    #  Efficient Channel Attention ResNets\n    'ecaresnet26t.ra2_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecaresnet26t_ra2-46609757.pth',\n        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8),\n        test_crop_pct=0.95, test_input_size=(3, 320, 320)),\n    'ecaresnetlight.miil_in1k': _tcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/ecaresnetlight-75a9c627.pth',\n        test_crop_pct=0.95, test_input_size=(3, 288, 288)),\n    'ecaresnet50d.miil_in1k': _tcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/ecaresnet50d-93c81e3b.pth',\n        first_conv='conv1.0', test_crop_pct=0.95, test_input_size=(3, 288, 288)),\n    'ecaresnet50d_pruned.miil_in1k': _tcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/ecaresnet50d_p-e4fa23c2.pth',\n        first_conv='conv1.0', test_crop_pct=0.95, test_input_size=(3, 288, 288)),\n    'ecaresnet50t.ra2_in1k': _tcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecaresnet50t_ra2-f7ac63c4.pth',\n        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8),\n        test_crop_pct=0.95, test_input_size=(3, 320, 320)),\n    'ecaresnet50t.a1_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/ecaresnet50t_a1_0-99bd76a8.pth',\n        first_conv='conv1.0'),\n    'ecaresnet50t.a2_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/ecaresnet50t_a2_0-b1c7b745.pth',\n        first_conv='conv1.0'),\n    'ecaresnet50t.a3_in1k': _r3cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/ecaresnet50t_a3_0-8cc311f1.pth',\n        first_conv='conv1.0'),\n    'ecaresnet101d.miil_in1k': _tcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/ecaresnet101d-153dad65.pth',\n        first_conv='conv1.0', test_crop_pct=0.95, test_input_size=(3, 288, 288)),\n    'ecaresnet101d_pruned.miil_in1k': _tcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/ecaresnet101d_p-9e74cb91.pth',\n        first_conv='conv1.0', test_crop_pct=0.95, test_input_size=(3, 288, 288)),\n    'ecaresnet200d.untrained': _ttcfg(\n        first_conv='conv1.0', input_size=(3, 256, 256), crop_pct=0.95, pool_size=(8, 8)),\n    'ecaresnet269d.ra2_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecaresnet269d_320_ra2-7baa55cb.pth',\n        first_conv='conv1.0', input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=0.95,\n        test_crop_pct=1.0, test_input_size=(3, 352, 352)),\n\n    #  Efficient Channel Attention ResNeXts\n    'ecaresnext26t_32x4d.untrained': _tcfg(first_conv='conv1.0'),\n    'ecaresnext50t_32x4d.untrained': _tcfg(first_conv='conv1.0'),\n\n    #  Squeeze-Excitation ResNets, to eventually replace the models in senet.py\n    'seresnet18.untrained': _ttcfg(),\n    'seresnet34.untrained': _ttcfg(),\n    'seresnet50.a1_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/seresnet50_a1_0-ffa00869.pth',\n        crop_pct=0.95),\n    'seresnet50.a2_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/seresnet50_a2_0-850de0d9.pth',\n        crop_pct=0.95),\n    'seresnet50.a3_in1k': _r3cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/seresnet50_a3_0-317ecd56.pth',\n        crop_pct=0.95),\n    'seresnet50.ra2_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet50_ra_224-8efdb4bb.pth'),\n    'seresnet50t.untrained': _ttcfg(\n        first_conv='conv1.0'),\n    'seresnet101.untrained': _ttcfg(),\n    'seresnet152.untrained': _ttcfg(),\n    'seresnet152d.ra2_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet152d_ra2-04464dd2.pth',\n        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.95,\n        test_crop_pct=1.0, test_input_size=(3, 320, 320)\n    ),\n    'seresnet200d.untrained': _ttcfg(\n        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8)),\n    'seresnet269d.untrained': _ttcfg(\n        first_conv='conv1.0', input_size=(3, 256, 256), pool_size=(8, 8)),\n\n    #  Squeeze-Excitation ResNeXts, to eventually replace the models in senet.py\n    'seresnext26d_32x4d.bt_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext26d_32x4d-80fa48a3.pth',\n        first_conv='conv1.0'),\n    'seresnext26t_32x4d.bt_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext26tn_32x4d-569cb627.pth',\n        first_conv='conv1.0'),\n    'seresnext50_32x4d.racm_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext50_32x4d_racm-a304a460.pth'),\n    'seresnext101_32x4d.untrained': _ttcfg(),\n    'seresnext101_32x8d.ah_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/seresnext101_32x8d_ah-e6bc4c0a.pth'),\n    'seresnext101d_32x8d.ah_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/seresnext101d_32x8d_ah-191d7b94.pth',\n        first_conv='conv1.0'),\n\n    # ResNets with anti-aliasing / blur pool\n    'resnetaa50d.sw_in12k_ft_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        first_conv='conv1.0', crop_pct=0.95, test_crop_pct=1.0),\n    'resnetaa101d.sw_in12k_ft_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        first_conv='conv1.0', crop_pct=0.95, test_crop_pct=1.0),\n    'seresnextaa101d_32x8d.sw_in12k_ft_in1k_288': _ttcfg(\n        hf_hub_id='timm/',\n        crop_pct=0.95, input_size=(3, 288, 288), pool_size=(9, 9), test_input_size=(3, 320, 320), test_crop_pct=1.0,\n        first_conv='conv1.0'),\n    'seresnextaa101d_32x8d.sw_in12k_ft_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        first_conv='conv1.0', test_crop_pct=1.0),\n    'seresnextaa201d_32x8d.sw_in12k_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        interpolation='bicubic', first_conv='conv1.0', pool_size=(12, 12), input_size=(3, 384, 384), crop_pct=1.0),\n    'seresnextaa201d_32x8d.sw_in12k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=11821, interpolation='bicubic', first_conv='conv1.0',\n        crop_pct=0.95, input_size=(3, 320, 320), pool_size=(10, 10), test_input_size=(3, 384, 384), test_crop_pct=1.0),\n\n    'resnetaa50d.sw_in12k': _ttcfg(\n        hf_hub_id='timm/',\n        num_classes=11821, first_conv='conv1.0', crop_pct=0.95, test_crop_pct=1.0),\n    'resnetaa50d.d_in12k': _ttcfg(\n        hf_hub_id='timm/',\n        num_classes=11821, first_conv='conv1.0', crop_pct=0.95, test_crop_pct=1.0),\n    'resnetaa101d.sw_in12k': _ttcfg(\n        hf_hub_id='timm/',\n        num_classes=11821, first_conv='conv1.0', crop_pct=0.95, test_crop_pct=1.0),\n    'seresnextaa101d_32x8d.sw_in12k': _ttcfg(\n        hf_hub_id='timm/',\n        num_classes=11821, first_conv='conv1.0', crop_pct=0.95, test_crop_pct=1.0),\n\n    'resnetblur18.untrained': _ttcfg(),\n    'resnetblur50.bt_in1k': _ttcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnetblur50-84f4748f.pth'),\n    'resnetblur50d.untrained': _ttcfg(first_conv='conv1.0'),\n    'resnetblur101d.untrained': _ttcfg(first_conv='conv1.0'),\n    'resnetaa50.a1h_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnetaa50_a1h-4cf422b3.pth'),\n\n    'seresnetaa50d.untrained': _ttcfg(first_conv='conv1.0'),\n    'seresnextaa101d_32x8d.ah_in1k': _rcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/seresnextaa101d_32x8d_ah-83c8ae12.pth',\n        first_conv='conv1.0'),\n\n    # ResNet-RS models\n    'resnetrs50.tf_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs50_ema-6b53758b.pth',\n        input_size=(3, 160, 160), pool_size=(5, 5), crop_pct=0.91, test_input_size=(3, 224, 224),\n        interpolation='bicubic', first_conv='conv1.0'),\n    'resnetrs101.tf_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs101_i192_ema-1509bbf6.pth',\n        input_size=(3, 192, 192), pool_size=(6, 6), crop_pct=0.94, test_input_size=(3, 288, 288),\n        interpolation='bicubic', first_conv='conv1.0'),\n    'resnetrs152.tf_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs152_i256_ema-a9aff7f9.pth',\n        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, test_input_size=(3, 320, 320),\n        interpolation='bicubic', first_conv='conv1.0'),\n    'resnetrs200.tf_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/resnetrs200_c-6b698b88.pth',\n        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, test_input_size=(3, 320, 320),\n        interpolation='bicubic', first_conv='conv1.0'),\n    'resnetrs270.tf_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs270_ema-b40e674c.pth',\n        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, test_input_size=(3, 352, 352),\n        interpolation='bicubic', first_conv='conv1.0'),\n    'resnetrs350.tf_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs350_i256_ema-5a1aa8f1.pth',\n        input_size=(3, 288, 288), pool_size=(9, 9), crop_pct=1.0, test_input_size=(3, 384, 384),\n        interpolation='bicubic', first_conv='conv1.0'),\n    'resnetrs420.tf_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rs-weights/resnetrs420_ema-972dee69.pth',\n        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=1.0, test_input_size=(3, 416, 416),\n        interpolation='bicubic', first_conv='conv1.0'),\n\n    # gluon resnet weights\n    'resnet18.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet18_v1b-0757602b.pth'),\n    'resnet34.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet34_v1b-c6d82d59.pth'),\n    'resnet50.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet50_v1b-0ebe02e2.pth'),\n    'resnet101.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet101_v1b-3b017079.pth'),\n    'resnet152.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet152_v1b-c1edb0dd.pth'),\n    'resnet50c.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet50_v1c-48092f55.pth',\n        first_conv='conv1.0'),\n    'resnet101c.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet101_v1c-1f26822a.pth',\n        first_conv='conv1.0'),\n    'resnet152c.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet152_v1c-a3bb0b98.pth',\n        first_conv='conv1.0'),\n    'resnet50d.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet50_v1d-818a1b1b.pth',\n        first_conv='conv1.0'),\n    'resnet101d.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet101_v1d-0f9c8644.pth',\n        first_conv='conv1.0'),\n    'resnet152d.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet152_v1d-bd354e12.pth',\n        first_conv='conv1.0'),\n    'resnet50s.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet50_v1s-1762acc0.pth',\n        first_conv='conv1.0'),\n    'resnet101s.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet101_v1s-60fe0cc1.pth',\n        first_conv='conv1.0'),\n    'resnet152s.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet152_v1s-dcc41b81.pth',\n        first_conv='conv1.0'),\n    'resnext50_32x4d.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnext50_32x4d-e6a097c1.pth'),\n    'resnext101_32x4d.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnext101_32x4d-b253c8c4.pth'),\n    'resnext101_64x4d.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnext101_64x4d-f9a8e184.pth'),\n    'seresnext50_32x4d.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_seresnext50_32x4d-90cf2d6e.pth'),\n    'seresnext101_32x4d.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_seresnext101_32x4d-cf52900d.pth'),\n    'seresnext101_64x4d.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_seresnext101_64x4d-f9926f93.pth'),\n    'senet154.gluon_in1k': _gcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_senet154-70a1a3c0.pth',\n        first_conv='conv1.0'),\n})\n\n\n@register_model\ndef resnet10t(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-10-T model.\n    \"\"\"\n    model_args = dict(block=BasicBlock, layers=[1, 1, 1, 1], stem_width=32, stem_type='deep_tiered', avg_down=True)\n    return _create_resnet('resnet10t', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet14t(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-14-T model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[1, 1, 1, 1], stem_width=32, stem_type='deep_tiered', avg_down=True)\n    return _create_resnet('resnet14t', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet18(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-18 model.\n    \"\"\"\n    model_args = dict(block=BasicBlock, layers=[2, 2, 2, 2])\n    return _create_resnet('resnet18', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet18d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-18-D model.\n    \"\"\"\n    model_args = dict(block=BasicBlock, layers=[2, 2, 2, 2], stem_width=32, stem_type='deep', avg_down=True)\n    return _create_resnet('resnet18d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet34(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-34 model.\n    \"\"\"\n    model_args = dict(block=BasicBlock, layers=[3, 4, 6, 3])\n    return _create_resnet('resnet34', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet34d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-34-D model.\n    \"\"\"\n    model_args = dict(block=BasicBlock, layers=[3, 4, 6, 3], stem_width=32, stem_type='deep', avg_down=True)\n    return _create_resnet('resnet34d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet26(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-26 model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[2, 2, 2, 2])\n    return _create_resnet('resnet26', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet26t(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-26-T model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[2, 2, 2, 2], stem_width=32, stem_type='deep_tiered', avg_down=True)\n    return _create_resnet('resnet26t', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet26d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-26-D model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[2, 2, 2, 2], stem_width=32, stem_type='deep', avg_down=True)\n    return _create_resnet('resnet26d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet50(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-50 model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3],  **kwargs)\n    return _create_resnet('resnet50', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet50c(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-50-C model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], stem_width=32, stem_type='deep')\n    return _create_resnet('resnet50c', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet50d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-50-D model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], stem_width=32, stem_type='deep', avg_down=True)\n    return _create_resnet('resnet50d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet50s(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-50-S model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], stem_width=64, stem_type='deep')\n    return _create_resnet('resnet50s', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet50t(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-50-T model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], stem_width=32, stem_type='deep_tiered', avg_down=True)\n    return _create_resnet('resnet50t', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet101(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-101 model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3])\n    return _create_resnet('resnet101', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet101c(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-101-C model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], stem_width=32, stem_type='deep')\n    return _create_resnet('resnet101c', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet101d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-101-D model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], stem_width=32, stem_type='deep', avg_down=True)\n    return _create_resnet('resnet101d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet101s(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-101-S model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], stem_width=64, stem_type='deep')\n    return _create_resnet('resnet101s', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet152(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-152 model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3])\n    return _create_resnet('resnet152', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet152c(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-152-C model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3], stem_width=32, stem_type='deep')\n    return _create_resnet('resnet152c', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet152d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-152-D model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3], stem_width=32, stem_type='deep', avg_down=True)\n    return _create_resnet('resnet152d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet152s(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-152-S model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3], stem_width=64, stem_type='deep')\n    return _create_resnet('resnet152s', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet200(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-200 model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 24, 36, 3])\n    return _create_resnet('resnet200', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet200d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-200-D model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 24, 36, 3], stem_width=32, stem_type='deep', avg_down=True)\n    return _create_resnet('resnet200d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef wide_resnet50_2(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a Wide ResNet-50-2 model.\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], base_width=128)\n    return _create_resnet('wide_resnet50_2', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef wide_resnet101_2(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a Wide ResNet-101-2 model.\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], base_width=128)\n    return _create_resnet('wide_resnet101_2', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnet50_gn(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-50 model w/ GroupNorm\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3],  **kwargs)\n    return _create_resnet('resnet50_gn', pretrained, norm_layer=GroupNorm, **model_args)\n\n\n@register_model\ndef resnext50_32x4d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNeXt50-32x4d model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], cardinality=32, base_width=4)\n    return _create_resnet('resnext50_32x4d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnext50d_32x4d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNeXt50d-32x4d model. ResNext50 w/ deep stem & avg pool downsample\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 6, 3],  cardinality=32, base_width=4,\n        stem_width=32, stem_type='deep', avg_down=True)\n    return _create_resnet('resnext50d_32x4d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnext101_32x4d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNeXt-101 32x4d model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=4)\n    return _create_resnet('resnext101_32x4d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnext101_32x8d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNeXt-101 32x8d model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=8)\n    return _create_resnet('resnext101_32x8d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnext101_32x16d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNeXt-101 32x16d model\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=16)\n    return _create_resnet('resnext101_32x16d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnext101_32x32d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNeXt-101 32x32d model\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=32)\n    return _create_resnet('resnext101_32x32d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnext101_64x4d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNeXt101-64x4d model.\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], cardinality=64, base_width=4)\n    return _create_resnet('resnext101_64x4d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef ecaresnet26t(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs an ECA-ResNeXt-26-T model.\n    This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels\n    in the deep stem and ECA attn.\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[2, 2, 2, 2], stem_width=32,\n        stem_type='deep_tiered', avg_down=True, block_args=dict(attn_layer='eca'))\n    return _create_resnet('ecaresnet26t', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef ecaresnet50d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-50-D model with eca.\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 6, 3], stem_width=32, stem_type='deep', avg_down=True,\n        block_args=dict(attn_layer='eca'))\n    return _create_resnet('ecaresnet50d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef ecaresnet50d_pruned(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-50-D model pruned with eca.\n        The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 6, 3], stem_width=32, stem_type='deep', avg_down=True,\n        block_args=dict(attn_layer='eca'))\n    return _create_resnet('ecaresnet50d_pruned', pretrained, pruned=True, **dict(model_args, **kwargs))\n\n\n@register_model\ndef ecaresnet50t(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs an ECA-ResNet-50-T model.\n    Like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels in the deep stem and ECA attn.\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 6, 3], stem_width=32,\n        stem_type='deep_tiered', avg_down=True, block_args=dict(attn_layer='eca'))\n    return _create_resnet('ecaresnet50t', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef ecaresnetlight(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-50-D light model with eca.\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[1, 1, 11, 3], stem_width=32, avg_down=True,\n        block_args=dict(attn_layer='eca'))\n    return _create_resnet('ecaresnetlight', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef ecaresnet101d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-101-D model with eca.\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 23, 3], stem_width=32, stem_type='deep', avg_down=True,\n        block_args=dict(attn_layer='eca'))\n    return _create_resnet('ecaresnet101d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef ecaresnet101d_pruned(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-101-D model pruned with eca.\n       The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 23, 3], stem_width=32, stem_type='deep', avg_down=True,\n        block_args=dict(attn_layer='eca'))\n    return _create_resnet('ecaresnet101d_pruned', pretrained, pruned=True, **dict(model_args, **kwargs))\n\n\n@register_model\ndef ecaresnet200d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-200-D model with ECA.\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 24, 36, 3], stem_width=32, stem_type='deep', avg_down=True,\n        block_args=dict(attn_layer='eca'))\n    return _create_resnet('ecaresnet200d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef ecaresnet269d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-269-D model with ECA.\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 30, 48, 8], stem_width=32, stem_type='deep', avg_down=True,\n        block_args=dict(attn_layer='eca'))\n    return _create_resnet('ecaresnet269d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef ecaresnext26t_32x4d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs an ECA-ResNeXt-26-T model.\n    This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels\n    in the deep stem. This model replaces SE module with the ECA module\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[2, 2, 2, 2], cardinality=32, base_width=4, stem_width=32,\n        stem_type='deep_tiered', avg_down=True, block_args=dict(attn_layer='eca'))\n    return _create_resnet('ecaresnext26t_32x4d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef ecaresnext50t_32x4d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs an ECA-ResNeXt-50-T model.\n    This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels\n    in the deep stem. This model replaces SE module with the ECA module\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[2, 2, 2, 2], cardinality=32, base_width=4, stem_width=32,\n        stem_type='deep_tiered', avg_down=True, block_args=dict(attn_layer='eca'))\n    return _create_resnet('ecaresnext50t_32x4d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnet18(pretrained=False, **kwargs) -> ResNet:\n    model_args = dict(block=BasicBlock, layers=[2, 2, 2, 2], block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnet18', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnet34(pretrained=False, **kwargs) -> ResNet:\n    model_args = dict(block=BasicBlock, layers=[3, 4, 6, 3], block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnet34', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnet50(pretrained=False, **kwargs) -> ResNet:\n    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnet50', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnet50t(pretrained=False, **kwargs) -> ResNet:\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 6, 3],  stem_width=32, stem_type='deep_tiered',\n        avg_down=True, block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnet50t', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnet101(pretrained=False, **kwargs) -> ResNet:\n    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnet101', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnet152(pretrained=False, **kwargs) -> ResNet:\n    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3], block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnet152', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnet152d(pretrained=False, **kwargs) -> ResNet:\n    model_args = dict(\n        block=Bottleneck, layers=[3, 8, 36, 3], stem_width=32, stem_type='deep',\n        avg_down=True, block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnet152d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnet200d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-200-D model with SE attn.\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 24, 36, 3], stem_width=32, stem_type='deep',\n        avg_down=True, block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnet200d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnet269d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-269-D model with SE attn.\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 30, 48, 8], stem_width=32, stem_type='deep',\n        avg_down=True, block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnet269d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnext26d_32x4d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a SE-ResNeXt-26-D model.`\n    This is technically a 28 layer ResNet, using the 'D' modifier from Gluon / bag-of-tricks for\n    combination of deep stem and avg_pool in downsample.\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[2, 2, 2, 2], cardinality=32, base_width=4, stem_width=32,\n        stem_type='deep', avg_down=True, block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnext26d_32x4d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnext26t_32x4d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a SE-ResNet-26-T model.\n    This is technically a 28 layer ResNet, like a 'D' bag-of-tricks model but with tiered 24, 32, 64 channels\n    in the deep stem.\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[2, 2, 2, 2], cardinality=32, base_width=4, stem_width=32,\n        stem_type='deep_tiered', avg_down=True, block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnext26t_32x4d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnext26tn_32x4d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a SE-ResNeXt-26-T model.\n    NOTE I deprecated previous 't' model defs and replaced 't' with 'tn', this was the only tn model of note\n    so keeping this def for backwards compat with any uses out there. Old 't' model is lost.\n    \"\"\"\n    return seresnext26t_32x4d(pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef seresnext50_32x4d(pretrained=False, **kwargs) -> ResNet:\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 6, 3], cardinality=32, base_width=4,\n        block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnext50_32x4d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnext101_32x4d(pretrained=False, **kwargs) -> ResNet:\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=4,\n        block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnext101_32x4d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnext101_32x8d(pretrained=False, **kwargs) -> ResNet:\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=8,\n        block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnext101_32x8d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnext101d_32x8d(pretrained=False, **kwargs) -> ResNet:\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=8,\n        stem_width=32, stem_type='deep', avg_down=True,\n        block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnext101d_32x8d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnext101_64x4d(pretrained=False, **kwargs) -> ResNet:\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 23, 3], cardinality=64, base_width=4,\n        block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnext101_64x4d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef senet154(pretrained=False, **kwargs) -> ResNet:\n    model_args = dict(\n        block=Bottleneck, layers=[3, 8, 36, 3], cardinality=64, base_width=4, stem_type='deep',\n        down_kernel_size=3, block_reduce_first=2, block_args=dict(attn_layer='se'))\n    return _create_resnet('senet154', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetblur18(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-18 model with blur anti-aliasing\n    \"\"\"\n    model_args = dict(block=BasicBlock, layers=[2, 2, 2, 2], aa_layer=BlurPool2d)\n    return _create_resnet('resnetblur18', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetblur50(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-50 model with blur anti-aliasing\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], aa_layer=BlurPool2d)\n    return _create_resnet('resnetblur50', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetblur50d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-50-D model with blur anti-aliasing\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 6, 3], aa_layer=BlurPool2d,\n        stem_width=32, stem_type='deep', avg_down=True)\n    return _create_resnet('resnetblur50d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetblur101d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-101-D model with blur anti-aliasing\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 23, 3], aa_layer=BlurPool2d,\n        stem_width=32, stem_type='deep', avg_down=True)\n    return _create_resnet('resnetblur101d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetaa34d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-34-D model w/ avgpool anti-aliasing\n    \"\"\"\n    model_args = dict(\n        block=BasicBlock, layers=[3, 4, 6, 3],  aa_layer=nn.AvgPool2d, stem_width=32, stem_type='deep', avg_down=True)\n    return _create_resnet('resnetaa34d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetaa50(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-50 model with avgpool anti-aliasing\n    \"\"\"\n    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], aa_layer=nn.AvgPool2d)\n    return _create_resnet('resnetaa50', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetaa50d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-50-D model with avgpool anti-aliasing\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 6, 3], aa_layer=nn.AvgPool2d,\n        stem_width=32, stem_type='deep', avg_down=True)\n    return _create_resnet('resnetaa50d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetaa101d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-101-D model with avgpool anti-aliasing\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 23, 3], aa_layer=nn.AvgPool2d,\n        stem_width=32, stem_type='deep', avg_down=True)\n    return _create_resnet('resnetaa101d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnetaa50d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a SE=ResNet-50-D model with avgpool anti-aliasing\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 6, 3], aa_layer=nn.AvgPool2d,\n        stem_width=32, stem_type='deep', avg_down=True, block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnetaa50d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnextaa101d_32x8d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a SE=ResNeXt-101-D 32x8d model with avgpool anti-aliasing\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 23, 3], cardinality=32, base_width=8,\n        stem_width=32, stem_type='deep', avg_down=True, aa_layer=nn.AvgPool2d,\n        block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnextaa101d_32x8d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef seresnextaa201d_32x8d(pretrained=False, **kwargs):\n    \"\"\"Constructs a SE=ResNeXt-101-D 32x8d model with avgpool anti-aliasing\n    \"\"\"\n    model_args = dict(\n        block=Bottleneck, layers=[3, 24, 36, 4], cardinality=32, base_width=8,\n        stem_width=64, stem_type='deep', avg_down=True, aa_layer=nn.AvgPool2d,\n        block_args=dict(attn_layer='se'))\n    return _create_resnet('seresnextaa201d_32x8d', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetrs50(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-RS-50 model.\n    Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579\n    Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs\n    \"\"\"\n    attn_layer = partial(get_attn('se'), rd_ratio=0.25)\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 6, 3], stem_width=32, stem_type='deep', replace_stem_pool=True,\n        avg_down=True,  block_args=dict(attn_layer=attn_layer))\n    return _create_resnet('resnetrs50', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetrs101(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-RS-101 model.\n    Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579\n    Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs\n    \"\"\"\n    attn_layer = partial(get_attn('se'), rd_ratio=0.25)\n    model_args = dict(\n        block=Bottleneck, layers=[3, 4, 23, 3], stem_width=32, stem_type='deep', replace_stem_pool=True,\n        avg_down=True,  block_args=dict(attn_layer=attn_layer))\n    return _create_resnet('resnetrs101', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetrs152(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-RS-152 model.\n    Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579\n    Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs\n    \"\"\"\n    attn_layer = partial(get_attn('se'), rd_ratio=0.25)\n    model_args = dict(\n        block=Bottleneck, layers=[3, 8, 36, 3], stem_width=32, stem_type='deep', replace_stem_pool=True,\n        avg_down=True,  block_args=dict(attn_layer=attn_layer))\n    return _create_resnet('resnetrs152', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetrs200(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-RS-200 model.\n    Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579\n    Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs\n    \"\"\"\n    attn_layer = partial(get_attn('se'), rd_ratio=0.25)\n    model_args = dict(\n        block=Bottleneck, layers=[3, 24, 36, 3], stem_width=32, stem_type='deep', replace_stem_pool=True,\n        avg_down=True,  block_args=dict(attn_layer=attn_layer))\n    return _create_resnet('resnetrs200', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetrs270(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-RS-270 model.\n    Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579\n    Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs\n    \"\"\"\n    attn_layer = partial(get_attn('se'), rd_ratio=0.25)\n    model_args = dict(\n        block=Bottleneck, layers=[4, 29, 53, 4], stem_width=32, stem_type='deep', replace_stem_pool=True,\n        avg_down=True,  block_args=dict(attn_layer=attn_layer))\n    return _create_resnet('resnetrs270', pretrained, **dict(model_args, **kwargs))\n\n\n\n@register_model\ndef resnetrs350(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-RS-350 model.\n    Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579\n    Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs\n    \"\"\"\n    attn_layer = partial(get_attn('se'), rd_ratio=0.25)\n    model_args = dict(\n        block=Bottleneck, layers=[4, 36, 72, 4], stem_width=32, stem_type='deep', replace_stem_pool=True,\n        avg_down=True,  block_args=dict(attn_layer=attn_layer))\n    return _create_resnet('resnetrs350', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetrs420(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a ResNet-RS-420 model\n    Paper: Revisiting ResNets - https://arxiv.org/abs/2103.07579\n    Pretrained weights from https://github.com/tensorflow/tpu/tree/bee9c4f6/models/official/resnet/resnet_rs\n    \"\"\"\n    attn_layer = partial(get_attn('se'), rd_ratio=0.25)\n    model_args = dict(\n        block=Bottleneck, layers=[4, 44, 87, 4], stem_width=32, stem_type='deep', replace_stem_pool=True,\n        avg_down=True,  block_args=dict(attn_layer=attn_layer))\n    return _create_resnet('resnetrs420', pretrained, **dict(model_args, **kwargs))\n\n\nregister_model_deprecations(__name__, {\n    'tv_resnet34': 'resnet34.tv_in1k',\n    'tv_resnet50': 'resnet50.tv_in1k',\n    'tv_resnet101': 'resnet101.tv_in1k',\n    'tv_resnet152': 'resnet152.tv_in1k',\n    'tv_resnext50_32x4d' : 'resnext50_32x4d.tv_in1k',\n    'ig_resnext101_32x8d': 'resnext101_32x8d.fb_wsl_ig1b_ft_in1k',\n    'ig_resnext101_32x16d': 'resnext101_32x8d.fb_wsl_ig1b_ft_in1k',\n    'ig_resnext101_32x32d': 'resnext101_32x8d.fb_wsl_ig1b_ft_in1k',\n    'ig_resnext101_32x48d': 'resnext101_32x8d.fb_wsl_ig1b_ft_in1k',\n    'ssl_resnet18': 'resnet18.fb_ssl_yfcc100m_ft_in1k',\n    'ssl_resnet50': 'resnet50.fb_ssl_yfcc100m_ft_in1k',\n    'ssl_resnext50_32x4d': 'resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k',\n    'ssl_resnext101_32x4d': 'resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k',\n    'ssl_resnext101_32x8d': 'resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k',\n    'ssl_resnext101_32x16d': 'resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k',\n    'swsl_resnet18': 'resnet18.fb_swsl_ig1b_ft_in1k',\n    'swsl_resnet50': 'resnet50.fb_swsl_ig1b_ft_in1k',\n    'swsl_resnext50_32x4d': 'resnext50_32x4d.fb_swsl_ig1b_ft_in1k',\n    'swsl_resnext101_32x4d': 'resnext101_32x4d.fb_swsl_ig1b_ft_in1k',\n    'swsl_resnext101_32x8d': 'resnext101_32x8d.fb_swsl_ig1b_ft_in1k',\n    'swsl_resnext101_32x16d': 'resnext101_32x16d.fb_swsl_ig1b_ft_in1k',\n    'gluon_resnet18_v1b': 'resnet18.gluon_in1k',\n    'gluon_resnet34_v1b': 'resnet34.gluon_in1k',\n    'gluon_resnet50_v1b': 'resnet50.gluon_in1k',\n    'gluon_resnet101_v1b': 'resnet101.gluon_in1k',\n    'gluon_resnet152_v1b': 'resnet152.gluon_in1k',\n    'gluon_resnet50_v1c': 'resnet50c.gluon_in1k',\n    'gluon_resnet101_v1c': 'resnet101c.gluon_in1k',\n    'gluon_resnet152_v1c': 'resnet152c.gluon_in1k',\n    'gluon_resnet50_v1d': 'resnet50d.gluon_in1k',\n    'gluon_resnet101_v1d': 'resnet101d.gluon_in1k',\n    'gluon_resnet152_v1d': 'resnet152d.gluon_in1k',\n    'gluon_resnet50_v1s': 'resnet50s.gluon_in1k',\n    'gluon_resnet101_v1s': 'resnet101s.gluon_in1k',\n    'gluon_resnet152_v1s': 'resnet152s.gluon_in1k',\n    'gluon_resnext50_32x4d': 'resnext50_32x4d.gluon_in1k',\n    'gluon_resnext101_32x4d': 'resnext101_32x4d.gluon_in1k',\n    'gluon_resnext101_64x4d': 'resnext101_64x4d.gluon_in1k',\n    'gluon_seresnext50_32x4d': 'seresnext50_32x4d.gluon_in1k',\n    'gluon_seresnext101_32x4d': 'seresnext101_32x4d.gluon_in1k',\n    'gluon_seresnext101_64x4d': 'seresnext101_64x4d.gluon_in1k',\n    'gluon_senet154': 'senet154.gluon_in1k',\n})\n",
  "\"\"\"\nPorted to pytorch thanks to [tstandley](https://github.com/tstandley/Xception-PyTorch)\n\n@author: tstandley\nAdapted by cadene\n\nCreates an Xception Model as defined in:\n\nFrancois Chollet\nXception: Deep Learning with Depthwise Separable Convolutions\nhttps://arxiv.org/pdf/1610.02357.pdf\n\nThis weights ported from the Keras implementation. Achieves the following performance on the validation set:\n\nLoss:0.9173 Prec@1:78.892 Prec@5:94.292\n\nREMEMBER to set your image size to 3x299x299 for both test and validation\n\nnormalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                  std=[0.5, 0.5, 0.5])\n\nThe resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n\"\"\"\nimport torch.jit\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.layers import create_classifier\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs, register_model_deprecations\n\n__all__ = ['Xception']\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1):\n        super(SeparableConv2d, self).__init__()\n\n        self.conv1 = nn.Conv2d(\n            in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels, bias=False)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0, 1, 1, bias=False)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self, in_channels, out_channels, reps, strides=1, start_with_relu=True, grow_first=True):\n        super(Block, self).__init__()\n\n        if out_channels != in_channels or strides != 1:\n            self.skip = nn.Conv2d(in_channels, out_channels, 1, stride=strides, bias=False)\n            self.skipbn = nn.BatchNorm2d(out_channels)\n        else:\n            self.skip = None\n\n        rep = []\n        for i in range(reps):\n            if grow_first:\n                inc = in_channels if i == 0 else out_channels\n                outc = out_channels\n            else:\n                inc = in_channels\n                outc = in_channels if i < (reps - 1) else out_channels\n            rep.append(nn.ReLU(inplace=True))\n            rep.append(SeparableConv2d(inc, outc, 3, stride=1, padding=1))\n            rep.append(nn.BatchNorm2d(outc))\n\n        if not start_with_relu:\n            rep = rep[1:]\n        else:\n            rep[0] = nn.ReLU(inplace=False)\n\n        if strides != 1:\n            rep.append(nn.MaxPool2d(3, strides, 1))\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self, inp):\n        x = self.rep(inp)\n\n        if self.skip is not None:\n            skip = self.skip(inp)\n            skip = self.skipbn(skip)\n        else:\n            skip = inp\n\n        x += skip\n        return x\n\n\nclass Xception(nn.Module):\n    \"\"\"\n    Xception optimized for the ImageNet dataset, as specified in\n    https://arxiv.org/pdf/1610.02357.pdf\n    \"\"\"\n\n    def __init__(self, num_classes=1000, in_chans=3, drop_rate=0., global_pool='avg'):\n        \"\"\" Constructor\n        Args:\n            num_classes: number of classes\n        \"\"\"\n        super(Xception, self).__init__()\n        self.drop_rate = drop_rate\n        self.global_pool = global_pool\n        self.num_classes = num_classes\n        self.num_features = 2048\n\n        self.conv1 = nn.Conv2d(in_chans, 32, 3, 2, 0, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.act1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(32, 64, 3, bias=False)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.act2 = nn.ReLU(inplace=True)\n\n        self.block1 = Block(64, 128, 2, 2, start_with_relu=False)\n        self.block2 = Block(128, 256, 2, 2)\n        self.block3 = Block(256, 728, 2, 2)\n\n        self.block4 = Block(728, 728, 3, 1)\n        self.block5 = Block(728, 728, 3, 1)\n        self.block6 = Block(728, 728, 3, 1)\n        self.block7 = Block(728, 728, 3, 1)\n\n        self.block8 = Block(728, 728, 3, 1)\n        self.block9 = Block(728, 728, 3, 1)\n        self.block10 = Block(728, 728, 3, 1)\n        self.block11 = Block(728, 728, 3, 1)\n\n        self.block12 = Block(728, 1024, 2, 2, grow_first=False)\n\n        self.conv3 = SeparableConv2d(1024, 1536, 3, 1, 1)\n        self.bn3 = nn.BatchNorm2d(1536)\n        self.act3 = nn.ReLU(inplace=True)\n\n        self.conv4 = SeparableConv2d(1536, self.num_features, 3, 1, 1)\n        self.bn4 = nn.BatchNorm2d(self.num_features)\n        self.act4 = nn.ReLU(inplace=True)\n        self.feature_info = [\n            dict(num_chs=64, reduction=2, module='act2'),\n            dict(num_chs=128, reduction=4, module='block2.rep.0'),\n            dict(num_chs=256, reduction=8, module='block3.rep.0'),\n            dict(num_chs=728, reduction=16, module='block12.rep.0'),\n            dict(num_chs=2048, reduction=32, module='act4'),\n        ]\n\n        self.global_pool, self.fc = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)\n\n        # #------- init weights --------\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^conv[12]|bn[12]',\n            blocks=[\n                (r'^block(\\d+)', None),\n                (r'^conv[34]|bn[34]', (99,)),\n            ],\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, \"gradient checkpointing not supported\"\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.fc\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.global_pool, self.fc = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.act2(x)\n\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = self.block10(x)\n        x = self.block11(x)\n        x = self.block12(x)\n\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.act3(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.act4(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        x = self.global_pool(x)\n        if self.drop_rate:\n            F.dropout(x, self.drop_rate, training=self.training)\n        return x if pre_logits else self.fc(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _xception(variant, pretrained=False, **kwargs):\n    return build_model_with_cfg(\n        Xception, variant, pretrained,\n        feature_cfg=dict(feature_cls='hook'),\n        **kwargs)\n\n\ndefault_cfgs = generate_default_cfgs({\n    'legacy_xception.tf_in1k': {\n        'url': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/xception-43020ad28.pth',\n        'input_size': (3, 299, 299),\n        'pool_size': (10, 10),\n        'crop_pct': 0.8975,\n        'interpolation': 'bicubic',\n        'mean': (0.5, 0.5, 0.5),\n        'std': (0.5, 0.5, 0.5),\n        'num_classes': 1000,\n        'first_conv': 'conv1',\n        'classifier': 'fc'\n        # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n    }\n})\n\n\n@register_model\ndef legacy_xception(pretrained=False, **kwargs) -> Xception:\n    return _xception('legacy_xception', pretrained=pretrained, **kwargs)\n\n\nregister_model_deprecations(__name__, {\n    'xception': 'legacy_xception',\n})\n",
  "\"\"\"RegNet X, Y, Z, and more\n\nPaper: `Designing Network Design Spaces` - https://arxiv.org/abs/2003.13678\nOriginal Impl: https://github.com/facebookresearch/pycls/blob/master/pycls/models/regnet.py\n\nPaper: `Fast and Accurate Model Scaling` - https://arxiv.org/abs/2103.06877\nOriginal Impl: None\n\nBased on original PyTorch impl linked above, but re-wrote to use my own blocks (adapted from ResNet here)\nand cleaned up with more descriptive variable names.\n\nWeights from original pycls impl have been modified:\n* first layer from BGR -> RGB as most PyTorch models are\n* removed training specific dict entries from checkpoints and keep model state_dict only\n* remap names to match the ones here\n\nSupports weight loading from torchvision and classy-vision (incl VISSL SEER)\n\nA number of custom timm model definitions additions including:\n* stochastic depth, gradient checkpointing, layer-decay, configurable dilation\n* a pre-activation 'V' variant\n* only known RegNet-Z model definitions with pretrained weights\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nfrom dataclasses import dataclass, replace\nfrom functools import partial\nfrom typing import Optional, Union, Callable\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import ClassifierHead, AvgPool2dSame, ConvNormAct, SEModule, DropPath, GroupNormAct\nfrom timm.layers import get_act_layer, get_norm_act_layer, create_conv2d, make_divisible\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq, named_apply\nfrom ._registry import generate_default_cfgs, register_model, register_model_deprecations\n\n__all__ = ['RegNet', 'RegNetCfg']  # model_registry will add each entrypoint fn to this\n\n\n@dataclass\nclass RegNetCfg:\n    depth: int = 21\n    w0: int = 80\n    wa: float = 42.63\n    wm: float = 2.66\n    group_size: int = 24\n    bottle_ratio: float = 1.\n    se_ratio: float = 0.\n    group_min_ratio: float = 0.\n    stem_width: int = 32\n    downsample: Optional[str] = 'conv1x1'\n    linear_out: bool = False\n    preact: bool = False\n    num_features: int = 0\n    act_layer: Union[str, Callable] = 'relu'\n    norm_layer: Union[str, Callable] = 'batchnorm'\n\n\ndef quantize_float(f, q):\n    \"\"\"Converts a float to the closest non-zero int divisible by q.\"\"\"\n    return int(round(f / q) * q)\n\n\ndef adjust_widths_groups_comp(widths, bottle_ratios, groups, min_ratio=0.):\n    \"\"\"Adjusts the compatibility of widths and groups.\"\"\"\n    bottleneck_widths = [int(w * b) for w, b in zip(widths, bottle_ratios)]\n    groups = [min(g, w_bot) for g, w_bot in zip(groups, bottleneck_widths)]\n    if min_ratio:\n        # torchvision uses a different rounding scheme for ensuring bottleneck widths divisible by group widths\n        bottleneck_widths = [make_divisible(w_bot, g, min_ratio) for w_bot, g in zip(bottleneck_widths, groups)]\n    else:\n        bottleneck_widths = [quantize_float(w_bot, g) for w_bot, g in zip(bottleneck_widths, groups)]\n    widths = [int(w_bot / b) for w_bot, b in zip(bottleneck_widths, bottle_ratios)]\n    return widths, groups\n\n\ndef generate_regnet(width_slope, width_initial, width_mult, depth, group_size, quant=8):\n    \"\"\"Generates per block widths from RegNet parameters.\"\"\"\n    assert width_slope >= 0 and width_initial > 0 and width_mult > 1 and width_initial % quant == 0\n    # TODO dWr scaling?\n    # depth = int(depth * (scale ** 0.1))\n    # width_scale = scale ** 0.4  # dWr scale, exp 0.8 / 2, applied to both group and layer widths\n    widths_cont = np.arange(depth) * width_slope + width_initial\n    width_exps = np.round(np.log(widths_cont / width_initial) / np.log(width_mult))\n    widths = np.round(np.divide(width_initial * np.power(width_mult, width_exps), quant)) * quant\n    num_stages, max_stage = len(np.unique(widths)), width_exps.max() + 1\n    groups = np.array([group_size for _ in range(num_stages)])\n    return widths.astype(int).tolist(), num_stages, groups.astype(int).tolist()\n\n\ndef downsample_conv(\n        in_chs,\n        out_chs,\n        kernel_size=1,\n        stride=1,\n        dilation=1,\n        norm_layer=None,\n        preact=False,\n):\n    norm_layer = norm_layer or nn.BatchNorm2d\n    kernel_size = 1 if stride == 1 and dilation == 1 else kernel_size\n    dilation = dilation if kernel_size > 1 else 1\n    if preact:\n        return create_conv2d(\n            in_chs,\n            out_chs,\n            kernel_size,\n            stride=stride,\n            dilation=dilation,\n        )\n    else:\n        return ConvNormAct(\n            in_chs,\n            out_chs,\n            kernel_size,\n            stride=stride,\n            dilation=dilation,\n            norm_layer=norm_layer,\n            apply_act=False,\n        )\n\n\ndef downsample_avg(\n        in_chs,\n        out_chs,\n        kernel_size=1,\n        stride=1,\n        dilation=1,\n        norm_layer=None,\n        preact=False,\n):\n    \"\"\" AvgPool Downsampling as in 'D' ResNet variants. This is not in RegNet space but I might experiment.\"\"\"\n    norm_layer = norm_layer or nn.BatchNorm2d\n    avg_stride = stride if dilation == 1 else 1\n    pool = nn.Identity()\n    if stride > 1 or dilation > 1:\n        avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d\n        pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)\n    if preact:\n        conv = create_conv2d(in_chs, out_chs, 1, stride=1)\n    else:\n        conv = ConvNormAct(in_chs, out_chs, 1, stride=1, norm_layer=norm_layer, apply_act=False)\n    return nn.Sequential(*[pool, conv])\n\n\ndef create_shortcut(\n        downsample_type,\n        in_chs,\n        out_chs,\n        kernel_size,\n        stride,\n        dilation=(1, 1),\n        norm_layer=None,\n        preact=False,\n):\n    assert downsample_type in ('avg', 'conv1x1', '', None)\n    if in_chs != out_chs or stride != 1 or dilation[0] != dilation[1]:\n        dargs = dict(stride=stride, dilation=dilation[0], norm_layer=norm_layer, preact=preact)\n        if not downsample_type:\n            return None  # no shortcut, no downsample\n        elif downsample_type == 'avg':\n            return downsample_avg(in_chs, out_chs, **dargs)\n        else:\n            return downsample_conv(in_chs, out_chs, kernel_size=kernel_size, **dargs)\n    else:\n        return nn.Identity()  # identity shortcut (no downsample)\n\n\nclass Bottleneck(nn.Module):\n    \"\"\" RegNet Bottleneck\n\n    This is almost exactly the same as a ResNet Bottlneck. The main difference is the SE block is moved from\n    after conv3 to after conv2. Otherwise, it's just redefining the arguments for groups/bottleneck channels.\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            stride=1,\n            dilation=(1, 1),\n            bottle_ratio=1,\n            group_size=1,\n            se_ratio=0.25,\n            downsample='conv1x1',\n            linear_out=False,\n            act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d,\n            drop_block=None,\n            drop_path_rate=0.,\n    ):\n        super(Bottleneck, self).__init__()\n        act_layer = get_act_layer(act_layer)\n        bottleneck_chs = int(round(out_chs * bottle_ratio))\n        groups = bottleneck_chs // group_size\n\n        cargs = dict(act_layer=act_layer, norm_layer=norm_layer)\n        self.conv1 = ConvNormAct(in_chs, bottleneck_chs, kernel_size=1, **cargs)\n        self.conv2 = ConvNormAct(\n            bottleneck_chs,\n            bottleneck_chs,\n            kernel_size=3,\n            stride=stride,\n            dilation=dilation[0],\n            groups=groups,\n            drop_layer=drop_block,\n            **cargs,\n        )\n        if se_ratio:\n            se_channels = int(round(in_chs * se_ratio))\n            self.se = SEModule(bottleneck_chs, rd_channels=se_channels, act_layer=act_layer)\n        else:\n            self.se = nn.Identity()\n        self.conv3 = ConvNormAct(bottleneck_chs, out_chs, kernel_size=1, apply_act=False, **cargs)\n        self.act3 = nn.Identity() if linear_out else act_layer()\n        self.downsample = create_shortcut(\n            downsample,\n            in_chs,\n            out_chs,\n            kernel_size=1,\n            stride=stride,\n            dilation=dilation,\n            norm_layer=norm_layer,\n        )\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()\n\n    def zero_init_last(self):\n        nn.init.zeros_(self.conv3.bn.weight)\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.se(x)\n        x = self.conv3(x)\n        if self.downsample is not None:\n            # NOTE stuck with downsample as the attr name due to weight compatibility\n            # now represents the shortcut, no shortcut if None, and non-downsample shortcut == nn.Identity()\n            x = self.drop_path(x) + self.downsample(shortcut)\n        x = self.act3(x)\n        return x\n\n\nclass PreBottleneck(nn.Module):\n    \"\"\" RegNet Bottleneck\n\n    This is almost exactly the same as a ResNet Bottlneck. The main difference is the SE block is moved from\n    after conv3 to after conv2. Otherwise, it's just redefining the arguments for groups/bottleneck channels.\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            stride=1,\n            dilation=(1, 1),\n            bottle_ratio=1,\n            group_size=1,\n            se_ratio=0.25,\n            downsample='conv1x1',\n            linear_out=False,\n            act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d,\n            drop_block=None,\n            drop_path_rate=0.,\n    ):\n        super(PreBottleneck, self).__init__()\n        norm_act_layer = get_norm_act_layer(norm_layer, act_layer)\n        bottleneck_chs = int(round(out_chs * bottle_ratio))\n        groups = bottleneck_chs // group_size\n\n        self.norm1 = norm_act_layer(in_chs)\n        self.conv1 = create_conv2d(in_chs, bottleneck_chs, kernel_size=1)\n        self.norm2 = norm_act_layer(bottleneck_chs)\n        self.conv2 = create_conv2d(\n            bottleneck_chs,\n            bottleneck_chs,\n            kernel_size=3,\n            stride=stride,\n            dilation=dilation[0],\n            groups=groups,\n        )\n        if se_ratio:\n            se_channels = int(round(in_chs * se_ratio))\n            self.se = SEModule(bottleneck_chs, rd_channels=se_channels, act_layer=act_layer)\n        else:\n            self.se = nn.Identity()\n        self.norm3 = norm_act_layer(bottleneck_chs)\n        self.conv3 = create_conv2d(bottleneck_chs, out_chs, kernel_size=1)\n        self.downsample = create_shortcut(\n            downsample,\n            in_chs,\n            out_chs,\n            kernel_size=1,\n            stride=stride,\n            dilation=dilation,\n            preact=True,\n        )\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()\n\n    def zero_init_last(self):\n        pass\n\n    def forward(self, x):\n        x = self.norm1(x)\n        shortcut = x\n        x = self.conv1(x)\n        x = self.norm2(x)\n        x = self.conv2(x)\n        x = self.se(x)\n        x = self.norm3(x)\n        x = self.conv3(x)\n        if self.downsample is not None:\n            # NOTE stuck with downsample as the attr name due to weight compatibility\n            # now represents the shortcut, no shortcut if None, and non-downsample shortcut == nn.Identity()\n            x = self.drop_path(x) + self.downsample(shortcut)\n        return x\n\n\nclass RegStage(nn.Module):\n    \"\"\"Stage (sequence of blocks w/ the same output shape).\"\"\"\n\n    def __init__(\n            self,\n            depth,\n            in_chs,\n            out_chs,\n            stride,\n            dilation,\n            drop_path_rates=None,\n            block_fn=Bottleneck,\n            **block_kwargs,\n    ):\n        super(RegStage, self).__init__()\n        self.grad_checkpointing = False\n\n        first_dilation = 1 if dilation in (1, 2) else 2\n        for i in range(depth):\n            block_stride = stride if i == 0 else 1\n            block_in_chs = in_chs if i == 0 else out_chs\n            block_dilation = (first_dilation, dilation)\n            dpr = drop_path_rates[i] if drop_path_rates is not None else 0.\n            name = \"b{}\".format(i + 1)\n            self.add_module(\n                name,\n                block_fn(\n                    block_in_chs,\n                    out_chs,\n                    stride=block_stride,\n                    dilation=block_dilation,\n                    drop_path_rate=dpr,\n                    **block_kwargs,\n                )\n            )\n            first_dilation = dilation\n\n    def forward(self, x):\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.children(), x)\n        else:\n            for block in self.children():\n                x = block(x)\n        return x\n\n\nclass RegNet(nn.Module):\n    \"\"\"RegNet-X, Y, and Z Models\n\n    Paper: https://arxiv.org/abs/2003.13678\n    Original Impl: https://github.com/facebookresearch/pycls/blob/master/pycls/models/regnet.py\n    \"\"\"\n\n    def __init__(\n            self,\n            cfg: RegNetCfg,\n            in_chans=3,\n            num_classes=1000,\n            output_stride=32,\n            global_pool='avg',\n            drop_rate=0.,\n            drop_path_rate=0.,\n            zero_init_last=True,\n            **kwargs,\n    ):\n        \"\"\"\n\n        Args:\n            cfg (RegNetCfg): Model architecture configuration\n            in_chans (int): Number of input channels (default: 3)\n            num_classes (int): Number of classifier classes (default: 1000)\n            output_stride (int): Output stride of network, one of (8, 16, 32) (default: 32)\n            global_pool (str): Global pooling type (default: 'avg')\n            drop_rate (float): Dropout rate (default: 0.)\n            drop_path_rate (float): Stochastic depth drop-path rate (default: 0.)\n            zero_init_last (bool): Zero-init last weight of residual path\n            kwargs (dict): Extra kwargs overlayed onto cfg\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        assert output_stride in (8, 16, 32)\n        cfg = replace(cfg, **kwargs)  # update cfg with extra passed kwargs\n\n        # Construct the stem\n        stem_width = cfg.stem_width\n        na_args = dict(act_layer=cfg.act_layer, norm_layer=cfg.norm_layer)\n        if cfg.preact:\n            self.stem = create_conv2d(in_chans, stem_width, 3, stride=2)\n        else:\n            self.stem = ConvNormAct(in_chans, stem_width, 3, stride=2, **na_args)\n        self.feature_info = [dict(num_chs=stem_width, reduction=2, module='stem')]\n\n        # Construct the stages\n        prev_width = stem_width\n        curr_stride = 2\n        per_stage_args, common_args = self._get_stage_args(\n            cfg,\n            output_stride=output_stride,\n            drop_path_rate=drop_path_rate,\n        )\n        assert len(per_stage_args) == 4\n        block_fn = PreBottleneck if cfg.preact else Bottleneck\n        for i, stage_args in enumerate(per_stage_args):\n            stage_name = \"s{}\".format(i + 1)\n            self.add_module(\n                stage_name,\n                RegStage(\n                    in_chs=prev_width,\n                    block_fn=block_fn,\n                    **stage_args,\n                    **common_args,\n                )\n            )\n            prev_width = stage_args['out_chs']\n            curr_stride *= stage_args['stride']\n            self.feature_info += [dict(num_chs=prev_width, reduction=curr_stride, module=stage_name)]\n\n        # Construct the head\n        if cfg.num_features:\n            self.final_conv = ConvNormAct(prev_width, cfg.num_features, kernel_size=1, **na_args)\n            self.num_features = cfg.num_features\n        else:\n            final_act = cfg.linear_out or cfg.preact\n            self.final_conv = get_act_layer(cfg.act_layer)() if final_act else nn.Identity()\n            self.num_features = prev_width\n        self.head = ClassifierHead(\n            in_features=self.num_features,\n            num_classes=num_classes,\n            pool_type=global_pool,\n            drop_rate=drop_rate,\n        )\n\n        named_apply(partial(_init_weights, zero_init_last=zero_init_last), self)\n\n    def _get_stage_args(self, cfg: RegNetCfg, default_stride=2, output_stride=32, drop_path_rate=0.):\n        # Generate RegNet ws per block\n        widths, num_stages, stage_gs = generate_regnet(cfg.wa, cfg.w0, cfg.wm, cfg.depth, cfg.group_size)\n\n        # Convert to per stage format\n        stage_widths, stage_depths = np.unique(widths, return_counts=True)\n        stage_br = [cfg.bottle_ratio for _ in range(num_stages)]\n        stage_strides = []\n        stage_dilations = []\n        net_stride = 2\n        dilation = 1\n        for _ in range(num_stages):\n            if net_stride >= output_stride:\n                dilation *= default_stride\n                stride = 1\n            else:\n                stride = default_stride\n                net_stride *= stride\n            stage_strides.append(stride)\n            stage_dilations.append(dilation)\n        stage_dpr = np.split(np.linspace(0, drop_path_rate, sum(stage_depths)), np.cumsum(stage_depths[:-1]))\n\n        # Adjust the compatibility of ws and gws\n        stage_widths, stage_gs = adjust_widths_groups_comp(\n            stage_widths, stage_br, stage_gs, min_ratio=cfg.group_min_ratio)\n        arg_names = ['out_chs', 'stride', 'dilation', 'depth', 'bottle_ratio', 'group_size', 'drop_path_rates']\n        per_stage_args = [\n            dict(zip(arg_names, params)) for params in\n            zip(stage_widths, stage_strides, stage_dilations, stage_depths, stage_br, stage_gs, stage_dpr)\n        ]\n        common_args = dict(\n            downsample=cfg.downsample,\n            se_ratio=cfg.se_ratio,\n            linear_out=cfg.linear_out,\n            act_layer=cfg.act_layer,\n            norm_layer=cfg.norm_layer,\n        )\n        return per_stage_args, common_args\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^stem',\n            blocks=r'^s(\\d+)' if coarse else r'^s(\\d+)\\.b(\\d+)',\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        for s in list(self.children())[1:-1]:\n            s.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.head.reset(num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        x = self.s1(x)\n        x = self.s2(x)\n        x = self.s3(x)\n        x = self.s4(x)\n        x = self.final_conv(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=pre_logits)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _init_weights(module, name='', zero_init_last=False):\n    if isinstance(module, nn.Conv2d):\n        fan_out = module.kernel_size[0] * module.kernel_size[1] * module.out_channels\n        fan_out //= module.groups\n        module.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n        if module.bias is not None:\n            module.bias.data.zero_()\n    elif isinstance(module, nn.Linear):\n        nn.init.normal_(module.weight, mean=0.0, std=0.01)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif zero_init_last and hasattr(module, 'zero_init_last'):\n        module.zero_init_last()\n\n\ndef _filter_fn(state_dict):\n    state_dict = state_dict.get('model', state_dict)\n    replaces = [\n        ('f.a.0', 'conv1.conv'),\n        ('f.a.1', 'conv1.bn'),\n        ('f.b.0', 'conv2.conv'),\n        ('f.b.1', 'conv2.bn'),\n        ('f.final_bn', 'conv3.bn'),\n        ('f.se.excitation.0', 'se.fc1'),\n        ('f.se.excitation.2', 'se.fc2'),\n        ('f.se', 'se'),\n        ('f.c.0', 'conv3.conv'),\n        ('f.c.1', 'conv3.bn'),\n        ('f.c', 'conv3.conv'),\n        ('proj.0', 'downsample.conv'),\n        ('proj.1', 'downsample.bn'),\n        ('proj', 'downsample.conv'),\n    ]\n    if 'classy_state_dict' in state_dict:\n        # classy-vision & vissl (SEER) weights\n        import re\n        state_dict = state_dict['classy_state_dict']['base_model']['model']\n        out = {}\n        for k, v in state_dict['trunk'].items():\n            k = k.replace('_feature_blocks.conv1.stem.0', 'stem.conv')\n            k = k.replace('_feature_blocks.conv1.stem.1', 'stem.bn')\n            k = re.sub(\n                r'^_feature_blocks.res\\d.block(\\d)-(\\d+)',\n                lambda x: f's{int(x.group(1))}.b{int(x.group(2)) + 1}', k)\n            k = re.sub(r's(\\d)\\.b(\\d+)\\.bn', r's\\1.b\\2.downsample.bn', k)\n            for s, r in replaces:\n                k = k.replace(s, r)\n            out[k] = v\n        for k, v in state_dict['heads'].items():\n            if 'projection_head' in k or 'prototypes' in k:\n                continue\n            k = k.replace('0.clf.0', 'head.fc')\n            out[k] = v\n        return out\n    if 'stem.0.weight' in state_dict:\n        # torchvision weights\n        import re\n        out = {}\n        for k, v in state_dict.items():\n            k = k.replace('stem.0', 'stem.conv')\n            k = k.replace('stem.1', 'stem.bn')\n            k = re.sub(\n                r'trunk_output.block(\\d)\\.block(\\d+)\\-(\\d+)',\n                lambda x: f's{int(x.group(1))}.b{int(x.group(3)) + 1}', k)\n            for s, r in replaces:\n                k = k.replace(s, r)\n            k = k.replace('fc.', 'head.fc.')\n            out[k] = v\n        return out\n    return state_dict\n\n\n# Model FLOPS = three trailing digits * 10^8\nmodel_cfgs = dict(\n    # RegNet-X\n    regnetx_002=RegNetCfg(w0=24, wa=36.44, wm=2.49, group_size=8, depth=13),\n    regnetx_004=RegNetCfg(w0=24, wa=24.48, wm=2.54, group_size=16, depth=22),\n    regnetx_004_tv=RegNetCfg(w0=24, wa=24.48, wm=2.54, group_size=16, depth=22, group_min_ratio=0.9),\n    regnetx_006=RegNetCfg(w0=48, wa=36.97, wm=2.24, group_size=24, depth=16),\n    regnetx_008=RegNetCfg(w0=56, wa=35.73, wm=2.28, group_size=16, depth=16),\n    regnetx_016=RegNetCfg(w0=80, wa=34.01, wm=2.25, group_size=24, depth=18),\n    regnetx_032=RegNetCfg(w0=88, wa=26.31, wm=2.25, group_size=48, depth=25),\n    regnetx_040=RegNetCfg(w0=96, wa=38.65, wm=2.43, group_size=40, depth=23),\n    regnetx_064=RegNetCfg(w0=184, wa=60.83, wm=2.07, group_size=56, depth=17),\n    regnetx_080=RegNetCfg(w0=80, wa=49.56, wm=2.88, group_size=120, depth=23),\n    regnetx_120=RegNetCfg(w0=168, wa=73.36, wm=2.37, group_size=112, depth=19),\n    regnetx_160=RegNetCfg(w0=216, wa=55.59, wm=2.1, group_size=128, depth=22),\n    regnetx_320=RegNetCfg(w0=320, wa=69.86, wm=2.0, group_size=168, depth=23),\n\n    # RegNet-Y\n    regnety_002=RegNetCfg(w0=24, wa=36.44, wm=2.49, group_size=8, depth=13, se_ratio=0.25),\n    regnety_004=RegNetCfg(w0=48, wa=27.89, wm=2.09, group_size=8, depth=16, se_ratio=0.25),\n    regnety_006=RegNetCfg(w0=48, wa=32.54, wm=2.32, group_size=16, depth=15, se_ratio=0.25),\n    regnety_008=RegNetCfg(w0=56, wa=38.84, wm=2.4, group_size=16, depth=14, se_ratio=0.25),\n    regnety_008_tv=RegNetCfg(w0=56, wa=38.84, wm=2.4, group_size=16, depth=14, se_ratio=0.25, group_min_ratio=0.9),\n    regnety_016=RegNetCfg(w0=48, wa=20.71, wm=2.65, group_size=24, depth=27, se_ratio=0.25),\n    regnety_032=RegNetCfg(w0=80, wa=42.63, wm=2.66, group_size=24, depth=21, se_ratio=0.25),\n    regnety_040=RegNetCfg(w0=96, wa=31.41, wm=2.24, group_size=64, depth=22, se_ratio=0.25),\n    regnety_064=RegNetCfg(w0=112, wa=33.22, wm=2.27, group_size=72, depth=25, se_ratio=0.25),\n    regnety_080=RegNetCfg(w0=192, wa=76.82, wm=2.19, group_size=56, depth=17, se_ratio=0.25),\n    regnety_080_tv=RegNetCfg(w0=192, wa=76.82, wm=2.19, group_size=56, depth=17, se_ratio=0.25, group_min_ratio=0.9),\n    regnety_120=RegNetCfg(w0=168, wa=73.36, wm=2.37, group_size=112, depth=19, se_ratio=0.25),\n    regnety_160=RegNetCfg(w0=200, wa=106.23, wm=2.48, group_size=112, depth=18, se_ratio=0.25),\n    regnety_320=RegNetCfg(w0=232, wa=115.89, wm=2.53, group_size=232, depth=20, se_ratio=0.25),\n    regnety_640=RegNetCfg(w0=352, wa=147.48, wm=2.4, group_size=328, depth=20, se_ratio=0.25),\n    regnety_1280=RegNetCfg(w0=456, wa=160.83, wm=2.52, group_size=264, depth=27, se_ratio=0.25),\n    regnety_2560=RegNetCfg(w0=640, wa=230.83, wm=2.53, group_size=373, depth=27, se_ratio=0.25),\n    #regnety_2560=RegNetCfg(w0=640, wa=124.47, wm=2.04, group_size=848, depth=27, se_ratio=0.25),\n\n    # Experimental\n    regnety_040_sgn=RegNetCfg(\n        w0=96, wa=31.41, wm=2.24, group_size=64, depth=22, se_ratio=0.25,\n        act_layer='silu', norm_layer=partial(GroupNormAct, group_size=16)),\n\n    # regnetv = 'preact regnet y'\n    regnetv_040=RegNetCfg(\n        depth=22, w0=96, wa=31.41, wm=2.24, group_size=64, se_ratio=0.25, preact=True, act_layer='silu'),\n    regnetv_064=RegNetCfg(\n        depth=25, w0=112, wa=33.22, wm=2.27, group_size=72, se_ratio=0.25, preact=True, act_layer='silu',\n        downsample='avg'),\n\n    # RegNet-Z (unverified)\n    regnetz_005=RegNetCfg(\n        depth=21, w0=16, wa=10.7, wm=2.51, group_size=4, bottle_ratio=4.0, se_ratio=0.25,\n        downsample=None, linear_out=True, num_features=1024, act_layer='silu',\n    ),\n    regnetz_040=RegNetCfg(\n        depth=28, w0=48, wa=14.5, wm=2.226, group_size=8, bottle_ratio=4.0, se_ratio=0.25,\n        downsample=None, linear_out=True, num_features=0, act_layer='silu',\n    ),\n    regnetz_040_h=RegNetCfg(\n        depth=28, w0=48, wa=14.5, wm=2.226, group_size=8, bottle_ratio=4.0, se_ratio=0.25,\n        downsample=None, linear_out=True, num_features=1536, act_layer='silu',\n    ),\n)\n\n\ndef _create_regnet(variant, pretrained, **kwargs):\n    return build_model_with_cfg(\n        RegNet, variant, pretrained,\n        model_cfg=model_cfgs[variant],\n        pretrained_filter_fn=_filter_fn,\n        **kwargs)\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'test_input_size': (3, 288, 288), 'crop_pct': 0.95, 'test_crop_pct': 1.0,\n        'interpolation': 'bicubic', 'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.conv', 'classifier': 'head.fc',\n        **kwargs\n    }\n\n\ndef _cfgpyc(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.conv', 'classifier': 'head.fc',\n        'license': 'mit', 'origin_url': 'https://github.com/facebookresearch/pycls', **kwargs\n    }\n\n\ndef _cfgtv2(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.965, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.conv', 'classifier': 'head.fc',\n        'license': 'bsd-3-clause', 'origin_url': 'https://github.com/pytorch/vision', **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    # timm trained models\n    'regnety_032.ra_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/regnety_032_ra-7f2439f9.pth'),\n    'regnety_040.ra3_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-tpu-weights/regnety_040_ra3-670e1166.pth'),\n    'regnety_064.ra3_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-tpu-weights/regnety_064_ra3-aa26dc7d.pth'),\n    'regnety_080.ra3_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-tpu-weights/regnety_080_ra3-1fdc4344.pth'),\n    'regnety_120.sw_in12k_ft_in1k': _cfg(hf_hub_id='timm/'),\n    'regnety_160.sw_in12k_ft_in1k': _cfg(hf_hub_id='timm/'),\n    'regnety_160.lion_in12k_ft_in1k': _cfg(hf_hub_id='timm/'),\n\n    # timm in12k pretrain\n    'regnety_120.sw_in12k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=11821),\n    'regnety_160.sw_in12k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=11821),\n\n    # timm custom arch (v and z guess) + trained models\n    'regnety_040_sgn.untrained': _cfg(url=''),\n    'regnetv_040.ra3_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetv_040_ra3-c248f51f.pth',\n        first_conv='stem'),\n    'regnetv_064.ra3_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetv_064_ra3-530616c2.pth',\n        first_conv='stem'),\n\n    'regnetz_005.untrained': _cfg(url=''),\n    'regnetz_040.ra3_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetz_040_ra3-9007edf5.pth',\n        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, test_input_size=(3, 320, 320)),\n    'regnetz_040_h.ra3_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetz_040h_ra3-f594343b.pth',\n        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, test_input_size=(3, 320, 320)),\n\n    # used in DeiT for distillation (from Facebook DeiT GitHub repository)\n    'regnety_160.deit_in1k': _cfg(\n        hf_hub_id='timm/', url='https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth'),\n\n    'regnetx_004_tv.tv2_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_x_400mf-62229a5f.pth'),\n    'regnetx_008.tv2_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_x_800mf-94a99ebd.pth'),\n    'regnetx_016.tv2_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_x_1_6gf-a12f2b72.pth'),\n    'regnetx_032.tv2_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_x_3_2gf-7071aa85.pth'),\n    'regnetx_080.tv2_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_x_8gf-2b70d774.pth'),\n    'regnetx_160.tv2_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_x_16gf-ba3796d7.pth'),\n    'regnetx_320.tv2_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_x_32gf-6eb8fdc6.pth'),\n\n    'regnety_004.tv2_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_y_400mf-e6988f5f.pth'),\n    'regnety_008_tv.tv2_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_y_800mf-58fc7688.pth'),\n    'regnety_016.tv2_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_y_1_6gf-0d7bc02a.pth'),\n    'regnety_032.tv2_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_y_3_2gf-9180c971.pth'),\n    'regnety_080_tv.tv2_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_y_8gf-dc2b1b54.pth'),\n    'regnety_160.tv2_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_y_16gf-3e4a00f9.pth'),\n    'regnety_320.tv2_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_y_32gf-8db6d4b5.pth'),\n\n    'regnety_160.swag_ft_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_y_16gf_swag-43afe44d.pth', license='cc-by-nc-4.0',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),\n    'regnety_320.swag_ft_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_y_32gf_swag-04fdfa75.pth', license='cc-by-nc-4.0',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),\n    'regnety_1280.swag_ft_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_y_128gf_swag-c8ce3e52.pth', license='cc-by-nc-4.0',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),\n\n    'regnety_160.swag_lc_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_y_16gf_lc_swag-f3ec0043.pth', license='cc-by-nc-4.0'),\n    'regnety_320.swag_lc_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_y_32gf_lc_swag-e1583746.pth', license='cc-by-nc-4.0'),\n    'regnety_1280.swag_lc_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/regnet_y_128gf_lc_swag-cbe8ce12.pth', license='cc-by-nc-4.0'),\n\n    'regnety_320.seer_ft_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        license='other', origin_url='https://github.com/facebookresearch/vissl',\n        url='https://dl.fbaipublicfiles.com/vissl/model_zoo/seer_finetuned/seer_regnet32_finetuned_in1k_model_final_checkpoint_phase78.torch',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),\n    'regnety_640.seer_ft_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        license='other', origin_url='https://github.com/facebookresearch/vissl',\n        url='https://dl.fbaipublicfiles.com/vissl/model_zoo/seer_finetuned/seer_regnet64_finetuned_in1k_model_final_checkpoint_phase78.torch',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),\n    'regnety_1280.seer_ft_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        license='other', origin_url='https://github.com/facebookresearch/vissl',\n        url='https://dl.fbaipublicfiles.com/vissl/model_zoo/seer_finetuned/seer_regnet128_finetuned_in1k_model_final_checkpoint_phase78.torch',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),\n    'regnety_2560.seer_ft_in1k': _cfgtv2(\n        hf_hub_id='timm/',\n        license='other', origin_url='https://github.com/facebookresearch/vissl',\n        url='https://dl.fbaipublicfiles.com/vissl/model_zoo/seer_finetuned/seer_regnet256_finetuned_in1k_model_final_checkpoint_phase38.torch',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),\n\n    'regnety_320.seer': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/vissl/model_zoo/seer_regnet32d/seer_regnet32gf_model_iteration244000.torch',\n        num_classes=0, license='other', origin_url='https://github.com/facebookresearch/vissl'),\n    'regnety_640.seer': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/vissl/model_zoo/seer_regnet64/seer_regnet64gf_model_final_checkpoint_phase0.torch',\n        num_classes=0, license='other', origin_url='https://github.com/facebookresearch/vissl'),\n    'regnety_1280.seer': _cfgtv2(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/vissl/model_zoo/swav_ig1b_regnet128Gf_cnstant_bs32_node16_sinkhorn10_proto16k_syncBN64_warmup8k/model_final_checkpoint_phase0.torch',\n        num_classes=0, license='other', origin_url='https://github.com/facebookresearch/vissl'),\n    # FIXME invalid weight <-> model match, mistake on their end\n    #'regnety_2560.seer': _cfgtv2(\n    #    url='https://dl.fbaipublicfiles.com/vissl/model_zoo/swav_ig1b_cosine_rg256gf_noBNhead_wd1e5_fairstore_bs16_node64_sinkhorn10_proto16k_apex_syncBN64_warmup8k/model_final_checkpoint_phase0.torch',\n    #    num_classes=0, license='other', origin_url='https://github.com/facebookresearch/vissl'),\n\n    'regnetx_002.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnetx_004.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnetx_006.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnetx_008.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnetx_016.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnetx_032.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnetx_040.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnetx_064.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnetx_080.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnetx_120.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnetx_160.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnetx_320.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n\n    'regnety_002.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnety_004.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnety_006.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnety_008.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnety_016.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnety_032.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnety_040.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnety_064.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnety_080.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnety_120.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnety_160.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n    'regnety_320.pycls_in1k': _cfgpyc(hf_hub_id='timm/'),\n})\n\n\n@register_model\ndef regnetx_002(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetX-200MF\"\"\"\n    return _create_regnet('regnetx_002', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_004(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetX-400MF\"\"\"\n    return _create_regnet('regnetx_004', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_004_tv(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetX-400MF w/ torchvision group rounding\"\"\"\n    return _create_regnet('regnetx_004_tv', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_006(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetX-600MF\"\"\"\n    return _create_regnet('regnetx_006', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_008(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetX-800MF\"\"\"\n    return _create_regnet('regnetx_008', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_016(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetX-1.6GF\"\"\"\n    return _create_regnet('regnetx_016', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_032(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetX-3.2GF\"\"\"\n    return _create_regnet('regnetx_032', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_040(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetX-4.0GF\"\"\"\n    return _create_regnet('regnetx_040', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_064(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetX-6.4GF\"\"\"\n    return _create_regnet('regnetx_064', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_080(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetX-8.0GF\"\"\"\n    return _create_regnet('regnetx_080', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_120(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetX-12GF\"\"\"\n    return _create_regnet('regnetx_120', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_160(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetX-16GF\"\"\"\n    return _create_regnet('regnetx_160', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_320(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetX-32GF\"\"\"\n    return _create_regnet('regnetx_320', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_002(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-200MF\"\"\"\n    return _create_regnet('regnety_002', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_004(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-400MF\"\"\"\n    return _create_regnet('regnety_004', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_006(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-600MF\"\"\"\n    return _create_regnet('regnety_006', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_008(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-800MF\"\"\"\n    return _create_regnet('regnety_008', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_008_tv(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-800MF w/ torchvision group rounding\"\"\"\n    return _create_regnet('regnety_008_tv', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_016(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-1.6GF\"\"\"\n    return _create_regnet('regnety_016', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_032(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-3.2GF\"\"\"\n    return _create_regnet('regnety_032', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_040(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-4.0GF\"\"\"\n    return _create_regnet('regnety_040', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_064(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-6.4GF\"\"\"\n    return _create_regnet('regnety_064', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_080(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-8.0GF\"\"\"\n    return _create_regnet('regnety_080', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_080_tv(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-8.0GF w/ torchvision group rounding\"\"\"\n    return _create_regnet('regnety_080_tv', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_120(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-12GF\"\"\"\n    return _create_regnet('regnety_120', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_160(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-16GF\"\"\"\n    return _create_regnet('regnety_160', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_320(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-32GF\"\"\"\n    return _create_regnet('regnety_320', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_640(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-64GF\"\"\"\n    return _create_regnet('regnety_640', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_1280(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-128GF\"\"\"\n    return _create_regnet('regnety_1280', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_2560(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-256GF\"\"\"\n    return _create_regnet('regnety_2560', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_040_sgn(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetY-4.0GF w/ GroupNorm \"\"\"\n    return _create_regnet('regnety_040_sgn', pretrained, **kwargs)\n\n\n@register_model\ndef regnetv_040(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetV-4.0GF (pre-activation)\"\"\"\n    return _create_regnet('regnetv_040', pretrained, **kwargs)\n\n\n@register_model\ndef regnetv_064(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetV-6.4GF (pre-activation)\"\"\"\n    return _create_regnet('regnetv_064', pretrained, **kwargs)\n\n\n@register_model\ndef regnetz_005(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetZ-500MF\n    NOTE: config found in https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/models/regnet.py\n    but it's not clear it is equivalent to paper model as not detailed in the paper.\n    \"\"\"\n    return _create_regnet('regnetz_005', pretrained, zero_init_last=False, **kwargs)\n\n\n@register_model\ndef regnetz_040(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetZ-4.0GF\n    NOTE: config found in https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/models/regnet.py\n    but it's not clear it is equivalent to paper model as not detailed in the paper.\n    \"\"\"\n    return _create_regnet('regnetz_040', pretrained, zero_init_last=False, **kwargs)\n\n\n@register_model\ndef regnetz_040_h(pretrained=False, **kwargs) -> RegNet:\n    \"\"\"RegNetZ-4.0GF\n    NOTE: config found in https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/models/regnet.py\n    but it's not clear it is equivalent to paper model as not detailed in the paper.\n    \"\"\"\n    return _create_regnet('regnetz_040_h', pretrained, zero_init_last=False, **kwargs)\n\n\nregister_model_deprecations(__name__, {\n    'regnetz_040h': 'regnetz_040_h',\n})",
  "\"\"\" Model creation / weight loading / state_dict helpers\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport logging\nimport os\nfrom collections import OrderedDict\nfrom typing import Any, Callable, Dict, Optional, Union\n\nimport torch\ntry:\n    import safetensors.torch\n    _has_safetensors = True\nexcept ImportError:\n    _has_safetensors = False\n\n_logger = logging.getLogger(__name__)\n\n__all__ = ['clean_state_dict', 'load_state_dict', 'load_checkpoint', 'remap_state_dict', 'resume_checkpoint']\n\n\ndef clean_state_dict(state_dict: Dict[str, Any]) -> Dict[str, Any]:\n    # 'clean' checkpoint by removing .module prefix from state dict if it exists from parallel training\n    cleaned_state_dict = {}\n    for k, v in state_dict.items():\n        name = k[7:] if k.startswith('module.') else k\n        cleaned_state_dict[name] = v\n    return cleaned_state_dict\n\n\ndef load_state_dict(\n        checkpoint_path: str,\n        use_ema: bool = True,\n        device: Union[str, torch.device] = 'cpu',\n) -> Dict[str, Any]:\n    if checkpoint_path and os.path.isfile(checkpoint_path):\n        # Check if safetensors or not and load weights accordingly\n        if str(checkpoint_path).endswith(\".safetensors\"):\n            assert _has_safetensors, \"`pip install safetensors` to use .safetensors\"\n            checkpoint = safetensors.torch.load_file(checkpoint_path, device=device)\n        else:\n            checkpoint = torch.load(checkpoint_path, map_location=device)\n\n        state_dict_key = ''\n        if isinstance(checkpoint, dict):\n            if use_ema and checkpoint.get('state_dict_ema', None) is not None:\n                state_dict_key = 'state_dict_ema'\n            elif use_ema and checkpoint.get('model_ema', None) is not None:\n                state_dict_key = 'model_ema'\n            elif 'state_dict' in checkpoint:\n                state_dict_key = 'state_dict'\n            elif 'model' in checkpoint:\n                state_dict_key = 'model'\n        state_dict = clean_state_dict(checkpoint[state_dict_key] if state_dict_key else checkpoint)\n        _logger.info(\"Loaded {} from checkpoint '{}'\".format(state_dict_key, checkpoint_path))\n        return state_dict\n    else:\n        _logger.error(\"No checkpoint found at '{}'\".format(checkpoint_path))\n        raise FileNotFoundError()\n\n\ndef load_checkpoint(\n        model: torch.nn.Module,\n        checkpoint_path: str,\n        use_ema: bool = True,\n        device: Union[str, torch.device] = 'cpu',\n        strict: bool = True,\n        remap: bool = False,\n        filter_fn: Optional[Callable] = None,\n):\n    if os.path.splitext(checkpoint_path)[-1].lower() in ('.npz', '.npy'):\n        # numpy checkpoint, try to load via model specific load_pretrained fn\n        if hasattr(model, 'load_pretrained'):\n            model.load_pretrained(checkpoint_path)\n        else:\n            raise NotImplementedError('Model cannot load numpy checkpoint')\n        return\n\n    state_dict = load_state_dict(checkpoint_path, use_ema, device=device)\n    if remap:\n        state_dict = remap_state_dict(state_dict, model)\n    elif filter_fn:\n        state_dict = filter_fn(state_dict, model)\n    incompatible_keys = model.load_state_dict(state_dict, strict=strict)\n    return incompatible_keys\n\n\ndef remap_state_dict(\n        state_dict: Dict[str, Any],\n        model: torch.nn.Module,\n        allow_reshape: bool = True\n):\n    \"\"\" remap checkpoint by iterating over state dicts in order (ignoring original keys).\n    This assumes models (and originating state dict) were created with params registered in same order.\n    \"\"\"\n    out_dict = {}\n    for (ka, va), (kb, vb) in zip(model.state_dict().items(), state_dict.items()):\n        assert va.numel() == vb.numel(), f'Tensor size mismatch {ka}: {va.shape} vs {kb}: {vb.shape}. Remap failed.'\n        if va.shape != vb.shape:\n            if allow_reshape:\n                vb = vb.reshape(va.shape)\n            else:\n                assert False,  f'Tensor shape mismatch {ka}: {va.shape} vs {kb}: {vb.shape}. Remap failed.'\n        out_dict[ka] = vb\n    return out_dict\n\n\ndef resume_checkpoint(\n        model: torch.nn.Module,\n        checkpoint_path: str,\n        optimizer: torch.optim.Optimizer = None,\n        loss_scaler: Any = None,\n        log_info: bool = True,\n):\n    resume_epoch = None\n    if os.path.isfile(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n        if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n            if log_info:\n                _logger.info('Restoring model state from checkpoint...')\n            state_dict = clean_state_dict(checkpoint['state_dict'])\n            model.load_state_dict(state_dict)\n\n            if optimizer is not None and 'optimizer' in checkpoint:\n                if log_info:\n                    _logger.info('Restoring optimizer state from checkpoint...')\n                optimizer.load_state_dict(checkpoint['optimizer'])\n\n            if loss_scaler is not None and loss_scaler.state_dict_key in checkpoint:\n                if log_info:\n                    _logger.info('Restoring AMP loss scaler state from checkpoint...')\n                loss_scaler.load_state_dict(checkpoint[loss_scaler.state_dict_key])\n\n            if 'epoch' in checkpoint:\n                resume_epoch = checkpoint['epoch']\n                if 'version' in checkpoint and checkpoint['version'] > 1:\n                    resume_epoch += 1  # start at the next epoch, old checkpoints incremented before save\n\n            if log_info:\n                _logger.info(\"Loaded checkpoint '{}' (epoch {})\".format(checkpoint_path, checkpoint['epoch']))\n        else:\n            model.load_state_dict(checkpoint)\n            if log_info:\n                _logger.info(\"Loaded checkpoint '{}'\".format(checkpoint_path))\n        return resume_epoch\n    else:\n        _logger.error(\"No checkpoint found at '{}'\".format(checkpoint_path))\n        raise FileNotFoundError()\n\n\n",
  "\"\"\" Swin Transformer\nA PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`\n    - https://arxiv.org/pdf/2103.14030\n\nCode/weights from https://github.com/microsoft/Swin-Transformer, original copyright/license info below\n\nS3 (AutoFormerV2, https://arxiv.org/abs/2111.14725) Swin weights from\n    - https://github.com/microsoft/Cream/tree/main/AutoFormerV2\n\nModifications and additions for timm hacked together by / Copyright 2021, Ross Wightman\n\"\"\"\n# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\nimport logging\nimport math\nfrom typing import Callable, List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import PatchEmbed, Mlp, DropPath, ClassifierHead, to_2tuple, to_ntuple, trunc_normal_, \\\n    _assert, use_fused_attn\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_function\nfrom ._manipulate import checkpoint_seq, named_apply\nfrom ._registry import generate_default_cfgs, register_model, register_model_deprecations\nfrom .vision_transformer import get_init_weights_vit\n\n__all__ = ['SwinTransformer']  # model_registry will add each entrypoint fn to this\n\n_logger = logging.getLogger(__name__)\n\n_int_or_tuple_2_t = Union[int, Tuple[int, int]]\n\n\ndef window_partition(x, window_size: int):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\n\n@register_notrace_function  # reason: int argument is a Proxy\ndef window_reverse(windows, window_size: int, H: int, W: int):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    C = windows.shape[-1]\n    x = windows.view(-1, H // window_size, W // window_size, window_size, window_size, C)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, C)\n    return x\n\n\ndef get_relative_position_index(win_h: int, win_w: int):\n    # get pair-wise relative position index for each token inside the window\n    coords = torch.stack(torch.meshgrid([torch.arange(win_h), torch.arange(win_w)]))  # 2, Wh, Ww\n    coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n    relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n    relative_coords[:, :, 0] += win_h - 1  # shift to start from 0\n    relative_coords[:, :, 1] += win_w - 1\n    relative_coords[:, :, 0] *= 2 * win_w - 1\n    return relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n\n\nclass WindowAttention(nn.Module):\n    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports shifted and non-shifted windows.\n    \"\"\"\n    fused_attn: torch.jit.Final[bool]\n\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int,\n            head_dim: Optional[int] = None,\n            window_size: _int_or_tuple_2_t = 7,\n            qkv_bias: bool = True,\n            attn_drop: float = 0.,\n            proj_drop: float = 0.,\n    ):\n        \"\"\"\n        Args:\n            dim: Number of input channels.\n            num_heads: Number of attention heads.\n            head_dim: Number of channels per head (dim // num_heads if not set)\n            window_size: The height and width of the window.\n            qkv_bias:  If True, add a learnable bias to query, key, value.\n            attn_drop: Dropout ratio of attention weight.\n            proj_drop: Dropout ratio of output.\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n        self.window_size = to_2tuple(window_size)  # Wh, Ww\n        win_h, win_w = self.window_size\n        self.window_area = win_h * win_w\n        self.num_heads = num_heads\n        head_dim = head_dim or dim // num_heads\n        attn_dim = head_dim * num_heads\n        self.scale = head_dim ** -0.5\n        self.fused_attn = use_fused_attn(experimental=True)  # NOTE not tested for prime-time yet\n\n        # define a parameter table of relative position bias, shape: 2*Wh-1 * 2*Ww-1, nH\n        self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * win_h - 1) * (2 * win_w - 1), num_heads))\n\n        # get pair-wise relative position index for each token inside the window\n        self.register_buffer(\"relative_position_index\", get_relative_position_index(win_h, win_w))\n\n        self.qkv = nn.Linear(dim, attn_dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(attn_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def _get_rel_pos_bias(self) -> torch.Tensor:\n        relative_position_bias = self.relative_position_bias_table[\n            self.relative_position_index.view(-1)].view(self.window_area, self.window_area, -1)  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        return relative_position_bias.unsqueeze(0)\n\n    def forward(self, x, mask: Optional[torch.Tensor] = None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)\n\n        if self.fused_attn:\n            attn_mask = self._get_rel_pos_bias()\n            if mask is not None:\n                num_win = mask.shape[0]\n                mask = mask.view(1, num_win, 1, N, N).expand(B_ // num_win, -1, self.num_heads, -1, -1)\n                attn_mask = attn_mask + mask.reshape(-1, self.num_heads, N, N)\n            x = torch.nn.functional.scaled_dot_product_attention(\n                q, k, v,\n                attn_mask=attn_mask,\n                dropout_p=self.attn_drop.p,\n            )\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n            attn = attn + self._get_rel_pos_bias()\n            if mask is not None:\n                num_win = mask.shape[0]\n                attn = attn.view(-1, num_win, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n                attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n            attn = self.attn_drop(attn)\n            x = attn @ v\n\n        x = x.transpose(1, 2).reshape(B_, N, -1)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass SwinTransformerBlock(nn.Module):\n    \"\"\" Swin Transformer Block.\n    \"\"\"\n\n    def __init__(\n            self,\n            dim: int,\n            input_resolution: _int_or_tuple_2_t,\n            num_heads: int = 4,\n            head_dim: Optional[int] = None,\n            window_size: _int_or_tuple_2_t = 7,\n            shift_size: int = 0,\n            mlp_ratio: float = 4.,\n            qkv_bias: bool = True,\n            proj_drop: float = 0.,\n            attn_drop: float = 0.,\n            drop_path: float = 0.,\n            act_layer: Callable = nn.GELU,\n            norm_layer: Callable = nn.LayerNorm,\n    ):\n        \"\"\"\n        Args:\n            dim: Number of input channels.\n            input_resolution: Input resolution.\n            window_size: Window size.\n            num_heads: Number of attention heads.\n            head_dim: Enforce the number of channels per head\n            shift_size: Shift size for SW-MSA.\n            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n            qkv_bias: If True, add a learnable bias to query, key, value.\n            proj_drop: Dropout rate.\n            attn_drop: Attention dropout rate.\n            drop_path: Stochastic depth rate.\n            act_layer: Activation layer.\n            norm_layer: Normalization layer.\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim,\n            num_heads=num_heads,\n            head_dim=head_dim,\n            window_size=to_2tuple(self.window_size),\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n        )\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n\n        if self.shift_size > 0:\n            # calculate attention mask for SW-MSA\n            H, W = self.input_resolution\n            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n            cnt = 0\n            for h in (\n                    slice(0, -self.window_size),\n                    slice(-self.window_size, -self.shift_size),\n                    slice(-self.shift_size, None)):\n                for w in (\n                        slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None)):\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n            mask_windows = window_partition(img_mask, self.window_size)  # num_win, window_size, window_size, 1\n            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n        else:\n            attn_mask = None\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n    def forward(self, x):\n        B, H, W, C = x.shape\n        _assert(H == self.input_resolution[0], \"input feature has wrong size\")\n        _assert(W == self.input_resolution[1], \"input feature has wrong size\")\n\n        shortcut = x\n        x = self.norm1(x)\n\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)  # num_win*B, window_size, window_size, C\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # num_win*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # num_win*B, window_size*window_size, C\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n        else:\n            x = shifted_x\n\n        # FFN\n        x = shortcut + self.drop_path(x)\n\n        x = x.reshape(B, -1, C)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        x = x.reshape(B, H, W, C)\n        return x\n\n\nclass PatchMerging(nn.Module):\n    \"\"\" Patch Merging Layer.\n    \"\"\"\n\n    def __init__(\n            self,\n            dim: int,\n            out_dim: Optional[int] = None,\n            norm_layer: Callable = nn.LayerNorm,\n    ):\n        \"\"\"\n        Args:\n            dim: Number of input channels.\n            out_dim: Number of output channels (or 2 * dim if None)\n            norm_layer: Normalization layer.\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n        self.out_dim = out_dim or 2 * dim\n        self.norm = norm_layer(4 * dim)\n        self.reduction = nn.Linear(4 * dim, self.out_dim, bias=False)\n\n    def forward(self, x):\n        B, H, W, C = x.shape\n        _assert(H % 2 == 0, f\"x height ({H}) is not even.\")\n        _assert(W % 2 == 0, f\"x width ({W}) is not even.\")\n        x = x.reshape(B, H // 2, 2, W // 2, 2, C).permute(0, 1, 3, 4, 2, 5).flatten(3)\n        x = self.norm(x)\n        x = self.reduction(x)\n        return x\n\n\nclass SwinTransformerStage(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n    \"\"\"\n\n    def __init__(\n            self,\n            dim: int,\n            out_dim: int,\n            input_resolution: Tuple[int, int],\n            depth: int,\n            downsample: bool = True,\n            num_heads: int = 4,\n            head_dim: Optional[int] = None,\n            window_size: _int_or_tuple_2_t = 7,\n            mlp_ratio: float = 4.,\n            qkv_bias: bool = True,\n            proj_drop: float = 0.,\n            attn_drop: float = 0.,\n            drop_path: Union[List[float], float] = 0.,\n            norm_layer: Callable = nn.LayerNorm,\n    ):\n        \"\"\"\n        Args:\n            dim: Number of input channels.\n            input_resolution: Input resolution.\n            depth: Number of blocks.\n            downsample: Downsample layer at the end of the layer.\n            num_heads: Number of attention heads.\n            head_dim: Channels per head (dim // num_heads if not set)\n            window_size: Local window size.\n            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n            qkv_bias: If True, add a learnable bias to query, key, value.\n            proj_drop: Projection dropout rate.\n            attn_drop: Attention dropout rate.\n            drop_path: Stochastic depth rate.\n            norm_layer: Normalization layer.\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.output_resolution = tuple(i // 2 for i in input_resolution) if downsample else input_resolution\n        self.depth = depth\n        self.grad_checkpointing = False\n\n        # patch merging layer\n        if downsample:\n            self.downsample = PatchMerging(\n                dim=dim,\n                out_dim=out_dim,\n                norm_layer=norm_layer,\n            )\n        else:\n            assert dim == out_dim\n            self.downsample = nn.Identity()\n\n        # build blocks\n        self.blocks = nn.Sequential(*[\n            SwinTransformerBlock(\n                dim=out_dim,\n                input_resolution=self.output_resolution,\n                num_heads=num_heads,\n                head_dim=head_dim,\n                window_size=window_size,\n                shift_size=0 if (i % 2 == 0) else window_size // 2,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                proj_drop=proj_drop,\n                attn_drop=attn_drop,\n                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                norm_layer=norm_layer,\n            )\n            for i in range(depth)])\n\n    def forward(self, x):\n        x = self.downsample(x)\n\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        return x\n\n\nclass SwinTransformer(nn.Module):\n    \"\"\" Swin Transformer\n\n    A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n    \"\"\"\n\n    def __init__(\n            self,\n            img_size: _int_or_tuple_2_t = 224,\n            patch_size: int = 4,\n            in_chans: int = 3,\n            num_classes: int = 1000,\n            global_pool: str = 'avg',\n            embed_dim: int = 96,\n            depths: Tuple[int, ...] = (2, 2, 6, 2),\n            num_heads: Tuple[int, ...] = (3, 6, 12, 24),\n            head_dim: Optional[int] = None,\n            window_size: _int_or_tuple_2_t = 7,\n            mlp_ratio: float = 4.,\n            qkv_bias: bool = True,\n            drop_rate: float = 0.,\n            proj_drop_rate: float = 0.,\n            attn_drop_rate: float = 0.,\n            drop_path_rate: float = 0.1,\n            norm_layer: Union[str, Callable] = nn.LayerNorm,\n            weight_init: str = '',\n            **kwargs,\n    ):\n        \"\"\"\n        Args:\n            img_size: Input image size.\n            patch_size: Patch size.\n            in_chans: Number of input image channels.\n            num_classes: Number of classes for classification head.\n            embed_dim: Patch embedding dimension.\n            depths: Depth of each Swin Transformer layer.\n            num_heads: Number of attention heads in different layers.\n            head_dim: Dimension of self-attention heads.\n            window_size: Window size.\n            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n            qkv_bias: If True, add a learnable bias to query, key, value.\n            drop_rate: Dropout rate.\n            attn_drop_rate (float): Attention dropout rate.\n            drop_path_rate (float): Stochastic depth rate.\n            norm_layer (nn.Module): Normalization layer.\n        \"\"\"\n        super().__init__()\n        assert global_pool in ('', 'avg')\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.output_fmt = 'NHWC'\n\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n        self.feature_info = []\n\n        if not isinstance(embed_dim, (tuple, list)):\n            embed_dim = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim[0],\n            norm_layer=norm_layer,\n            output_fmt='NHWC',\n        )\n        self.patch_grid = self.patch_embed.grid_size\n\n        # build layers\n        head_dim = to_ntuple(self.num_layers)(head_dim)\n        window_size = to_ntuple(self.num_layers)(window_size)\n        mlp_ratio = to_ntuple(self.num_layers)(mlp_ratio)\n        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n        layers = []\n        in_dim = embed_dim[0]\n        scale = 1\n        for i in range(self.num_layers):\n            out_dim = embed_dim[i]\n            layers += [SwinTransformerStage(\n                dim=in_dim,\n                out_dim=out_dim,\n                input_resolution=(\n                    self.patch_grid[0] // scale,\n                    self.patch_grid[1] // scale\n                ),\n                depth=depths[i],\n                downsample=i > 0,\n                num_heads=num_heads[i],\n                head_dim=head_dim[i],\n                window_size=window_size[i],\n                mlp_ratio=mlp_ratio[i],\n                qkv_bias=qkv_bias,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n            )]\n            in_dim = out_dim\n            if i > 0:\n                scale *= 2\n            self.feature_info += [dict(num_chs=out_dim, reduction=4 * scale, module=f'layers.{i}')]\n        self.layers = nn.Sequential(*layers)\n\n        self.norm = norm_layer(self.num_features)\n        self.head = ClassifierHead(\n            self.num_features,\n            num_classes,\n            pool_type=global_pool,\n            drop_rate=drop_rate,\n            input_fmt=self.output_fmt,\n        )\n        if weight_init != 'skip':\n            self.init_weights(weight_init)\n\n    @torch.jit.ignore\n    def init_weights(self, mode=''):\n        assert mode in ('jax', 'jax_nlhb', 'moco', '')\n        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.\n        named_apply(get_init_weights_vit(mode, head_bias=head_bias), self)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        nwd = set()\n        for n, _ in self.named_parameters():\n            if 'relative_position_bias_table' in n:\n                nwd.add(n)\n        return nwd\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^patch_embed',  # stem and embed\n            blocks=r'^layers\\.(\\d+)' if coarse else [\n                (r'^layers\\.(\\d+).downsample', (0,)),\n                (r'^layers\\.(\\d+)\\.\\w+\\.(\\d+)', None),\n                (r'^norm', (99999,)),\n            ]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        for l in self.layers:\n            l.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        self.head.reset(num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = self.layers(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=True) if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n    if 'head.fc.weight' in state_dict:\n        return state_dict\n    import re\n    out_dict = {}\n    state_dict = state_dict.get('model', state_dict)\n    state_dict = state_dict.get('state_dict', state_dict)\n    for k, v in state_dict.items():\n        k = re.sub(r'layers.(\\d+).downsample', lambda x: f'layers.{int(x.group(1)) + 1}.downsample', k)\n        k = k.replace('head.', 'head.fc.')\n        out_dict[k] = v\n    return out_dict\n\n\ndef _create_swin_transformer(variant, pretrained=False, **kwargs):\n    default_out_indices = tuple(i for i, _ in enumerate(kwargs.get('depths', (1, 1, 3, 1))))\n    out_indices = kwargs.pop('out_indices', default_out_indices)\n\n    model = build_model_with_cfg(\n        SwinTransformer, variant, pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),\n        **kwargs)\n\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'patch_embed.proj', 'classifier': 'head.fc',\n        'license': 'mit', **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'swin_small_patch4_window7_224.ms_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_small_patch4_window7_224_22kto1k_finetune.pth', ),\n    'swin_base_patch4_window7_224.ms_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22kto1k.pth',),\n    'swin_base_patch4_window12_384.ms_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22kto1k.pth',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),\n    'swin_large_patch4_window7_224.ms_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22kto1k.pth',),\n    'swin_large_patch4_window12_384.ms_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22kto1k.pth',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),\n\n    'swin_tiny_patch4_window7_224.ms_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth',),\n    'swin_small_patch4_window7_224.ms_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth',),\n    'swin_base_patch4_window7_224.ms_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224.pth',),\n    'swin_base_patch4_window12_384.ms_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384.pth',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),\n\n    # tiny 22k pretrain is worse than 1k, so moved after (untagged priority is based on order)\n    'swin_tiny_patch4_window7_224.ms_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_tiny_patch4_window7_224_22kto1k_finetune.pth',),\n\n    'swin_tiny_patch4_window7_224.ms_in22k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_tiny_patch4_window7_224_22k.pth',\n        num_classes=21841),\n    'swin_small_patch4_window7_224.ms_in22k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_small_patch4_window7_224_22k.pth',\n        num_classes=21841),\n    'swin_base_patch4_window7_224.ms_in22k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22k.pth',\n        num_classes=21841),\n    'swin_base_patch4_window12_384.ms_in22k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22k.pth',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, num_classes=21841),\n    'swin_large_patch4_window7_224.ms_in22k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22k.pth',\n        num_classes=21841),\n    'swin_large_patch4_window12_384.ms_in22k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, num_classes=21841),\n\n    'swin_s3_tiny_224.ms_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/s3_t-1d53f6a8.pth'),\n    'swin_s3_small_224.ms_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/s3_s-3bb4c69d.pth'),\n    'swin_s3_base_224.ms_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/s3_b-a1e95db4.pth'),\n})\n\n\n@register_model\ndef swin_tiny_patch4_window7_224(pretrained=False, **kwargs) -> SwinTransformer:\n    \"\"\" Swin-T @ 224x224, trained ImageNet-1k\n    \"\"\"\n    model_args = dict(patch_size=4, window_size=7, embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24))\n    return _create_swin_transformer(\n        'swin_tiny_patch4_window7_224', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swin_small_patch4_window7_224(pretrained=False, **kwargs) -> SwinTransformer:\n    \"\"\" Swin-S @ 224x224\n    \"\"\"\n    model_args = dict(patch_size=4, window_size=7, embed_dim=96, depths=(2, 2, 18, 2), num_heads=(3, 6, 12, 24))\n    return _create_swin_transformer(\n        'swin_small_patch4_window7_224', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swin_base_patch4_window7_224(pretrained=False, **kwargs) -> SwinTransformer:\n    \"\"\" Swin-B @ 224x224\n    \"\"\"\n    model_args = dict(patch_size=4, window_size=7, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32))\n    return _create_swin_transformer(\n        'swin_base_patch4_window7_224', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swin_base_patch4_window12_384(pretrained=False, **kwargs) -> SwinTransformer:\n    \"\"\" Swin-B @ 384x384\n    \"\"\"\n    model_args = dict(patch_size=4, window_size=12, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32))\n    return _create_swin_transformer(\n        'swin_base_patch4_window12_384', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swin_large_patch4_window7_224(pretrained=False, **kwargs) -> SwinTransformer:\n    \"\"\" Swin-L @ 224x224\n    \"\"\"\n    model_args = dict(patch_size=4, window_size=7, embed_dim=192, depths=(2, 2, 18, 2), num_heads=(6, 12, 24, 48))\n    return _create_swin_transformer(\n        'swin_large_patch4_window7_224', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swin_large_patch4_window12_384(pretrained=False, **kwargs) -> SwinTransformer:\n    \"\"\" Swin-L @ 384x384\n    \"\"\"\n    model_args = dict(patch_size=4, window_size=12, embed_dim=192, depths=(2, 2, 18, 2), num_heads=(6, 12, 24, 48))\n    return _create_swin_transformer(\n        'swin_large_patch4_window12_384', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swin_s3_tiny_224(pretrained=False, **kwargs) -> SwinTransformer:\n    \"\"\" Swin-S3-T @ 224x224, https://arxiv.org/abs/2111.14725\n    \"\"\"\n    model_args = dict(\n        patch_size=4, window_size=(7, 7, 14, 7), embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24))\n    return _create_swin_transformer('swin_s3_tiny_224', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swin_s3_small_224(pretrained=False, **kwargs) -> SwinTransformer:\n    \"\"\" Swin-S3-S @ 224x224, https://arxiv.org/abs/2111.14725\n    \"\"\"\n    model_args = dict(\n        patch_size=4, window_size=(14, 14, 14, 7), embed_dim=96, depths=(2, 2, 18, 2), num_heads=(3, 6, 12, 24))\n    return _create_swin_transformer('swin_s3_small_224', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swin_s3_base_224(pretrained=False, **kwargs) -> SwinTransformer:\n    \"\"\" Swin-S3-B @ 224x224, https://arxiv.org/abs/2111.14725\n    \"\"\"\n    model_args = dict(\n        patch_size=4, window_size=(7, 7, 14, 7), embed_dim=96, depths=(2, 2, 30, 2), num_heads=(3, 6, 12, 24))\n    return _create_swin_transformer('swin_s3_base_224', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\nregister_model_deprecations(__name__, {\n    'swin_base_patch4_window7_224_in22k': 'swin_base_patch4_window7_224.ms_in22k',\n    'swin_base_patch4_window12_384_in22k': 'swin_base_patch4_window12_384.ms_in22k',\n    'swin_large_patch4_window7_224_in22k': 'swin_large_patch4_window7_224.ms_in22k',\n    'swin_large_patch4_window12_384_in22k': 'swin_large_patch4_window12_384.ms_in22k',\n})\n",
  "\"\"\" PyTorch FX Based Feature Extraction Helpers\nUsing https://pytorch.org/vision/stable/feature_extraction.html\n\"\"\"\nfrom typing import Callable, List, Dict, Union, Type\n\nimport torch\nfrom torch import nn\n\nfrom ._features import _get_feature_info, _get_return_layers\n\ntry:\n    from torchvision.models.feature_extraction import create_feature_extractor as _create_feature_extractor\n    has_fx_feature_extraction = True\nexcept ImportError:\n    has_fx_feature_extraction = False\n\n# Layers we went to treat as leaf modules\nfrom timm.layers import Conv2dSame, ScaledStdConv2dSame, CondConv2d, StdConv2dSame\nfrom timm.layers.non_local_attn import BilinearAttnTransform\nfrom timm.layers.pool2d_same import MaxPool2dSame, AvgPool2dSame\n\n__all__ = ['register_notrace_module', 'is_notrace_module', 'get_notrace_modules',\n           'register_notrace_function', 'is_notrace_function', 'get_notrace_functions',\n           'create_feature_extractor', 'FeatureGraphNet', 'GraphExtractNet']\n\n\n# NOTE: By default, any modules from timm.models.layers that we want to treat as leaf modules go here\n# BUT modules from timm.models should use the registration mechanism below\n_leaf_modules = {\n    BilinearAttnTransform,  # reason: flow control t <= 1\n    # Reason: get_same_padding has a max which raises a control flow error\n    Conv2dSame, MaxPool2dSame, ScaledStdConv2dSame, StdConv2dSame, AvgPool2dSame,\n    CondConv2d,  # reason: TypeError: F.conv2d received Proxy in groups=self.groups * B (because B = x.shape[0])\n}\n\ntry:\n    from timm.layers import InplaceAbn\n    _leaf_modules.add(InplaceAbn)\nexcept ImportError:\n    pass\n\n\ndef register_notrace_module(module: Type[nn.Module]):\n    \"\"\"\n    Any module not under timm.models.layers should get this decorator if we don't want to trace through it.\n    \"\"\"\n    _leaf_modules.add(module)\n    return module\n\n\ndef is_notrace_module(module: Type[nn.Module]):\n    return module in _leaf_modules\n\n\ndef get_notrace_modules():\n    return list(_leaf_modules)\n\n\n# Functions we want to autowrap (treat them as leaves)\n_autowrap_functions = set()\n\n\ndef register_notrace_function(func: Callable):\n    \"\"\"\n    Decorator for functions which ought not to be traced through\n    \"\"\"\n    _autowrap_functions.add(func)\n    return func\n\n\ndef is_notrace_function(func: Callable):\n    return func in _autowrap_functions\n\n\ndef get_notrace_functions():\n    return list(_autowrap_functions)\n\n\ndef create_feature_extractor(model: nn.Module, return_nodes: Union[Dict[str, str], List[str]]):\n    assert has_fx_feature_extraction, 'Please update to PyTorch 1.10+, torchvision 0.11+ for FX feature extraction'\n    return _create_feature_extractor(\n        model, return_nodes,\n        tracer_kwargs={'leaf_modules': list(_leaf_modules), 'autowrap_functions': list(_autowrap_functions)}\n    )\n\n\nclass FeatureGraphNet(nn.Module):\n    \"\"\" A FX Graph based feature extractor that works with the model feature_info metadata\n    \"\"\"\n    def __init__(self, model, out_indices, out_map=None):\n        super().__init__()\n        assert has_fx_feature_extraction, 'Please update to PyTorch 1.10+, torchvision 0.11+ for FX feature extraction'\n        self.feature_info = _get_feature_info(model, out_indices)\n        if out_map is not None:\n            assert len(out_map) == len(out_indices)\n        return_nodes = _get_return_layers(self.feature_info, out_map)\n        self.graph_module = create_feature_extractor(model, return_nodes)\n\n    def forward(self, x):\n        return list(self.graph_module(x).values())\n\n\nclass GraphExtractNet(nn.Module):\n    \"\"\" A standalone feature extraction wrapper that maps dict -> list or single tensor\n    NOTE:\n      * one can use feature_extractor directly if dictionary output is desired\n      * unlike FeatureGraphNet, this is intended to be used standalone and not with model feature_info\n      metadata for builtin feature extraction mode\n      * create_feature_extractor can be used directly if dictionary output is acceptable\n\n    Args:\n        model: model to extract features from\n        return_nodes: node names to return features from (dict or list)\n        squeeze_out: if only one output, and output in list format, flatten to single tensor\n    \"\"\"\n    def __init__(self, model, return_nodes: Union[Dict[str, str], List[str]], squeeze_out: bool = True):\n        super().__init__()\n        self.squeeze_out = squeeze_out\n        self.graph_module = create_feature_extractor(model, return_nodes)\n\n    def forward(self, x) -> Union[List[torch.Tensor], torch.Tensor]:\n        out = list(self.graph_module(x).values())\n        if self.squeeze_out and len(out) == 1:\n            return out[0]\n        return out\n",
  "\"\"\" MaxVit and CoAtNet Vision Transformer - CNN Hybrids in PyTorch\n\nThis is a from-scratch implementation of both CoAtNet and MaxVit in PyTorch.\n\n99% of the implementation was done from papers, however last minute some adjustments were made\nbased on the (as yet unfinished?) public code release https://github.com/google-research/maxvit\n\nThere are multiple sets of models defined for both architectures. Typically, names with a\n `_rw` suffix are my own original configs prior to referencing https://github.com/google-research/maxvit.\nThese configs work well and appear to be a bit faster / lower resource than the paper.\n\nThe models without extra prefix / suffix' (coatnet_0_224, maxvit_tiny_224, etc), are intended to\nmatch paper, BUT, without any official pretrained weights it's difficult to confirm a 100% match.\n\nPapers:\n\nMaxViT: Multi-Axis Vision Transformer - https://arxiv.org/abs/2204.01697\n@article{tu2022maxvit,\n  title={MaxViT: Multi-Axis Vision Transformer},\n  author={Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and Yang, Feng and Milanfar, Peyman and Bovik, Alan and Li, Yinxiao},\n  journal={ECCV},\n  year={2022},\n}\n\nCoAtNet: Marrying Convolution and Attention for All Data Sizes - https://arxiv.org/abs/2106.04803\n@article{DBLP:journals/corr/abs-2106-04803,\n  author    = {Zihang Dai and Hanxiao Liu and Quoc V. Le and Mingxing Tan},\n  title     = {CoAtNet: Marrying Convolution and Attention for All Data Sizes},\n  journal   = {CoRR},\n  volume    = {abs/2106.04803},\n  year      = {2021}\n}\n\nHacked together by / Copyright 2022, Ross Wightman\n\"\"\"\n\nimport math\nfrom collections import OrderedDict\nfrom dataclasses import dataclass, replace, field\nfrom functools import partial\nfrom typing import Callable, Optional, Union, Tuple, List\n\nimport torch\nfrom torch import nn\nfrom torch.jit import Final\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import Mlp, ConvMlp, DropPath, LayerNorm, ClassifierHead, NormMlpClassifierHead\nfrom timm.layers import create_attn, get_act_layer, get_norm_layer, get_norm_act_layer, create_conv2d, create_pool2d\nfrom timm.layers import trunc_normal_tf_, to_2tuple, extend_tuple, make_divisible, _assert\nfrom timm.layers import RelPosMlp, RelPosBias, RelPosBiasTf, use_fused_attn\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_function\nfrom ._manipulate import named_apply, checkpoint_seq\nfrom ._registry import generate_default_cfgs, register_model\n\n__all__ = ['MaxxVitCfg', 'MaxxVitConvCfg', 'MaxxVitTransformerCfg', 'MaxxVit']\n\n\n@dataclass\nclass MaxxVitTransformerCfg:\n    dim_head: int = 32\n    head_first: bool = True  # head ordering in qkv channel dim\n    expand_ratio: float = 4.0\n    expand_first: bool = True\n    shortcut_bias: bool = True\n    attn_bias: bool = True\n    attn_drop: float = 0.\n    proj_drop: float = 0.\n    pool_type: str = 'avg2'\n    rel_pos_type: str = 'bias'\n    rel_pos_dim: int = 512  # for relative position types w/ MLP\n    partition_ratio: int = 32\n    window_size: Optional[Tuple[int, int]] = None\n    grid_size: Optional[Tuple[int, int]] = None\n    no_block_attn: bool = False  # disable window block attention for maxvit (ie only grid)\n    use_nchw_attn: bool = False  # for MaxViT variants (not used for CoAt), keep tensors in NCHW order\n    init_values: Optional[float] = None\n    act_layer: str = 'gelu'\n    norm_layer: str = 'layernorm2d'\n    norm_layer_cl: str = 'layernorm'\n    norm_eps: float = 1e-6\n\n    def __post_init__(self):\n        if self.grid_size is not None:\n            self.grid_size = to_2tuple(self.grid_size)\n        if self.window_size is not None:\n            self.window_size = to_2tuple(self.window_size)\n            if self.grid_size is None:\n                self.grid_size = self.window_size\n\n\n@dataclass\nclass MaxxVitConvCfg:\n    block_type: str = 'mbconv'\n    expand_ratio: float = 4.0\n    expand_output: bool = True  # calculate expansion channels from output (vs input chs)\n    kernel_size: int = 3\n    group_size: int = 1  # 1 == depthwise\n    pre_norm_act: bool = False  # activation after pre-norm\n    output_bias: bool = True  # bias for shortcut + final 1x1 projection conv\n    stride_mode: str = 'dw'  # stride done via one of 'pool', '1x1', 'dw'\n    pool_type: str = 'avg2'\n    downsample_pool_type: str = 'avg2'\n    padding: str = ''\n    attn_early: bool = False  # apply attn between conv2 and norm2, instead of after norm2\n    attn_layer: str = 'se'\n    attn_act_layer: str = 'silu'\n    attn_ratio: float = 0.25\n    init_values: Optional[float] = 1e-6  # for ConvNeXt block, ignored by MBConv\n    act_layer: str = 'gelu'\n    norm_layer: str = ''\n    norm_layer_cl: str = ''\n    norm_eps: Optional[float] = None\n\n    def __post_init__(self):\n        # mbconv vs convnext blocks have different defaults, set in post_init to avoid explicit config args\n        assert self.block_type in ('mbconv', 'convnext')\n        use_mbconv = self.block_type == 'mbconv'\n        if not self.norm_layer:\n            self.norm_layer = 'batchnorm2d' if use_mbconv else 'layernorm2d'\n        if not self.norm_layer_cl and not use_mbconv:\n            self.norm_layer_cl = 'layernorm'\n        if self.norm_eps is None:\n            self.norm_eps = 1e-5 if use_mbconv else 1e-6\n        self.downsample_pool_type = self.downsample_pool_type or self.pool_type\n\n\n@dataclass\nclass MaxxVitCfg:\n    embed_dim: Tuple[int, ...] = (96, 192, 384, 768)\n    depths: Tuple[int, ...] = (2, 3, 5, 2)\n    block_type: Tuple[Union[str, Tuple[str, ...]], ...] = ('C', 'C', 'T', 'T')\n    stem_width: Union[int, Tuple[int, int]] = 64\n    stem_bias: bool = False\n    conv_cfg: MaxxVitConvCfg = field(default_factory=MaxxVitConvCfg)\n    transformer_cfg: MaxxVitTransformerCfg = field(default_factory=MaxxVitTransformerCfg)\n    head_hidden_size: int = None\n    weight_init: str = 'vit_eff'\n\n\nclass Attention2d(nn.Module):\n    fused_attn: Final[bool]\n\n    \"\"\" multi-head attention for 2D NCHW tensors\"\"\"\n    def __init__(\n            self,\n            dim: int,\n            dim_out: Optional[int] = None,\n            dim_head: int = 32,\n            bias: bool = True,\n            expand_first: bool = True,\n            head_first: bool = True,\n            rel_pos_cls: Callable = None,\n            attn_drop: float = 0.,\n            proj_drop: float = 0.\n    ):\n        super().__init__()\n        dim_out = dim_out or dim\n        dim_attn = dim_out if expand_first else dim\n        self.num_heads = dim_attn // dim_head\n        self.dim_head = dim_head\n        self.head_first = head_first\n        self.scale = dim_head ** -0.5\n        self.fused_attn = use_fused_attn()\n\n        self.qkv = nn.Conv2d(dim, dim_attn * 3, 1, bias=bias)\n        self.rel_pos = rel_pos_cls(num_heads=self.num_heads) if rel_pos_cls else None\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Conv2d(dim_attn, dim_out, 1, bias=bias)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, shared_rel_pos: Optional[torch.Tensor] = None):\n        B, C, H, W = x.shape\n\n        if self.head_first:\n            q, k, v = self.qkv(x).view(B, self.num_heads, self.dim_head * 3, -1).chunk(3, dim=2)\n        else:\n            q, k, v = self.qkv(x).reshape(B, 3, self.num_heads, self.dim_head, -1).unbind(1)\n\n        if self.fused_attn:\n            attn_bias = None\n            if self.rel_pos is not None:\n                attn_bias = self.rel_pos.get_bias()\n            elif shared_rel_pos is not None:\n                attn_bias = shared_rel_pos\n\n            x = torch.nn.functional.scaled_dot_product_attention(\n                q.transpose(-1, -2),\n                k.transpose(-1, -2),\n                v.transpose(-1, -2),\n                attn_mask=attn_bias,\n                dropout_p=self.attn_drop.p,\n            ).transpose(-1, -2).reshape(B, -1, H, W)\n        else:\n            q = q * self.scale\n            attn = q.transpose(-2, -1) @ k\n            if self.rel_pos is not None:\n                attn = self.rel_pos(attn)\n            elif shared_rel_pos is not None:\n                attn = attn + shared_rel_pos\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            x = (v @ attn.transpose(-2, -1)).view(B, -1, H, W)\n\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass AttentionCl(nn.Module):\n    \"\"\" Channels-last multi-head attention (B, ..., C) \"\"\"\n    fused_attn: Final[bool]\n\n    def __init__(\n            self,\n            dim: int,\n            dim_out: Optional[int] = None,\n            dim_head: int = 32,\n            bias: bool = True,\n            expand_first: bool = True,\n            head_first: bool = True,\n            rel_pos_cls: Callable = None,\n            attn_drop: float = 0.,\n            proj_drop: float = 0.\n    ):\n        super().__init__()\n        dim_out = dim_out or dim\n        dim_attn = dim_out if expand_first and dim_out > dim else dim\n        assert dim_attn % dim_head == 0, 'attn dim should be divisible by head_dim'\n        self.num_heads = dim_attn // dim_head\n        self.dim_head = dim_head\n        self.head_first = head_first\n        self.scale = dim_head ** -0.5\n        self.fused_attn = use_fused_attn()\n\n        self.qkv = nn.Linear(dim, dim_attn * 3, bias=bias)\n        self.rel_pos = rel_pos_cls(num_heads=self.num_heads) if rel_pos_cls else None\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim_attn, dim_out, bias=bias)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, shared_rel_pos: Optional[torch.Tensor] = None):\n        B = x.shape[0]\n        restore_shape = x.shape[:-1]\n\n        if self.head_first:\n            q, k, v = self.qkv(x).view(B, -1, self.num_heads, self.dim_head * 3).transpose(1, 2).chunk(3, dim=3)\n        else:\n            q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n\n        if self.fused_attn:\n            attn_bias = None\n            if self.rel_pos is not None:\n                attn_bias = self.rel_pos.get_bias()\n            elif shared_rel_pos is not None:\n                attn_bias = shared_rel_pos\n\n            x = torch.nn.functional.scaled_dot_product_attention(\n                q, k, v,\n                attn_mask=attn_bias,\n                dropout_p=self.attn_drop.p,\n            )\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n            if self.rel_pos is not None:\n                attn = self.rel_pos(attn, shared_rel_pos=shared_rel_pos)\n            elif shared_rel_pos is not None:\n                attn = attn + shared_rel_pos\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            x = attn @ v\n\n        x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass LayerScale(nn.Module):\n    def __init__(self, dim, init_values=1e-5, inplace=False):\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        gamma = self.gamma\n        return x.mul_(gamma) if self.inplace else x * gamma\n\n\nclass LayerScale2d(nn.Module):\n    def __init__(self, dim, init_values=1e-5, inplace=False):\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        gamma = self.gamma.view(1, -1, 1, 1)\n        return x.mul_(gamma) if self.inplace else x * gamma\n\n\nclass Downsample2d(nn.Module):\n    \"\"\" A downsample pooling module supporting several maxpool and avgpool modes\n    * 'max' - MaxPool2d w/ kernel_size 3, stride 2, padding 1\n    * 'max2' - MaxPool2d w/ kernel_size = stride = 2\n    * 'avg' - AvgPool2d w/ kernel_size 3, stride 2, padding 1\n    * 'avg2' - AvgPool2d w/ kernel_size = stride = 2\n    \"\"\"\n\n    def __init__(\n            self,\n            dim: int,\n            dim_out: int,\n            pool_type: str = 'avg2',\n            padding: str = '',\n            bias: bool = True,\n    ):\n        super().__init__()\n        assert pool_type in ('max', 'max2', 'avg', 'avg2')\n        if pool_type == 'max':\n            self.pool = create_pool2d('max', kernel_size=3, stride=2, padding=padding or 1)\n        elif pool_type == 'max2':\n            self.pool = create_pool2d('max', 2, padding=padding or 0)  # kernel_size == stride == 2\n        elif pool_type == 'avg':\n            self.pool = create_pool2d(\n                'avg', kernel_size=3, stride=2, count_include_pad=False, padding=padding or 1)\n        else:\n            self.pool = create_pool2d('avg', 2, padding=padding or 0)\n\n        if dim != dim_out:\n            self.expand = nn.Conv2d(dim, dim_out, 1, bias=bias)\n        else:\n            self.expand = nn.Identity()\n\n    def forward(self, x):\n        x = self.pool(x)  # spatial downsample\n        x = self.expand(x)  # expand chs\n        return x\n\n\ndef _init_transformer(module, name, scheme=''):\n    if isinstance(module, (nn.Conv2d, nn.Linear)):\n        if scheme == 'normal':\n            nn.init.normal_(module.weight, std=.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif scheme == 'trunc_normal':\n            trunc_normal_tf_(module.weight, std=.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif scheme == 'xavier_normal':\n            nn.init.xavier_normal_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        else:\n            # vit like\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                if 'mlp' in name:\n                    nn.init.normal_(module.bias, std=1e-6)\n                else:\n                    nn.init.zeros_(module.bias)\n\n\nclass TransformerBlock2d(nn.Module):\n    \"\"\" Transformer block with 2D downsampling\n    '2D' NCHW tensor layout\n\n    Some gains can be seen on GPU using a 1D / CL block, BUT w/ the need to switch back/forth to NCHW\n    for spatial pooling, the benefit is minimal so ended up using just this variant for CoAt configs.\n\n    This impl was faster on TPU w/ PT XLA than the 1D experiment.\n    \"\"\"\n\n    def __init__(\n            self,\n            dim: int,\n            dim_out: int,\n            stride: int = 1,\n            rel_pos_cls: Callable = None,\n            cfg: MaxxVitTransformerCfg = MaxxVitTransformerCfg(),\n            drop_path: float = 0.,\n    ):\n        super().__init__()\n        norm_layer = partial(get_norm_layer(cfg.norm_layer), eps=cfg.norm_eps)\n        act_layer = get_act_layer(cfg.act_layer)\n\n        if stride == 2:\n            self.shortcut = Downsample2d(dim, dim_out, pool_type=cfg.pool_type, bias=cfg.shortcut_bias)\n            self.norm1 = nn.Sequential(OrderedDict([\n                ('norm', norm_layer(dim)),\n                ('down', Downsample2d(dim, dim, pool_type=cfg.pool_type)),\n            ]))\n        else:\n            assert dim == dim_out\n            self.shortcut = nn.Identity()\n            self.norm1 = norm_layer(dim)\n\n        self.attn = Attention2d(\n            dim,\n            dim_out,\n            dim_head=cfg.dim_head,\n            expand_first=cfg.expand_first,\n            bias=cfg.attn_bias,\n            rel_pos_cls=rel_pos_cls,\n            attn_drop=cfg.attn_drop,\n            proj_drop=cfg.proj_drop\n        )\n        self.ls1 = LayerScale2d(dim_out, init_values=cfg.init_values) if cfg.init_values else nn.Identity()\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm2 = norm_layer(dim_out)\n        self.mlp = ConvMlp(\n            in_features=dim_out,\n            hidden_features=int(dim_out * cfg.expand_ratio),\n            act_layer=act_layer,\n            drop=cfg.proj_drop)\n        self.ls2 = LayerScale2d(dim_out, init_values=cfg.init_values) if cfg.init_values else nn.Identity()\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def init_weights(self, scheme=''):\n        named_apply(partial(_init_transformer, scheme=scheme), self)\n\n    def forward(self, x, shared_rel_pos: Optional[torch.Tensor] = None):\n        x = self.shortcut(x) + self.drop_path1(self.ls1(self.attn(self.norm1(x), shared_rel_pos=shared_rel_pos)))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x\n\n\ndef _init_conv(module, name, scheme=''):\n    if isinstance(module, nn.Conv2d):\n        if scheme == 'normal':\n            nn.init.normal_(module.weight, std=.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif scheme == 'trunc_normal':\n            trunc_normal_tf_(module.weight, std=.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif scheme == 'xavier_normal':\n            nn.init.xavier_normal_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        else:\n            # efficientnet like\n            fan_out = module.kernel_size[0] * module.kernel_size[1] * module.out_channels\n            fan_out //= module.groups\n            nn.init.normal_(module.weight, 0, math.sqrt(2.0 / fan_out))\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n\ndef num_groups(group_size, channels):\n    if not group_size:  # 0 or None\n        return 1  # normal conv with 1 group\n    else:\n        # NOTE group_size == 1 -> depthwise conv\n        assert channels % group_size == 0\n        return channels // group_size\n\n\nclass MbConvBlock(nn.Module):\n    \"\"\" Pre-Norm Conv Block - 1x1 - kxk - 1x1, w/ inverted bottleneck (expand)\n    \"\"\"\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: int,\n            stride: int = 1,\n            dilation: Tuple[int, int] = (1, 1),\n            cfg: MaxxVitConvCfg = MaxxVitConvCfg(),\n            drop_path: float = 0.\n    ):\n        super(MbConvBlock, self).__init__()\n        norm_act_layer = partial(get_norm_act_layer(cfg.norm_layer, cfg.act_layer), eps=cfg.norm_eps)\n        mid_chs = make_divisible((out_chs if cfg.expand_output else in_chs) * cfg.expand_ratio)\n        groups = num_groups(cfg.group_size, mid_chs)\n\n        if stride == 2:\n            self.shortcut = Downsample2d(\n                in_chs, out_chs, pool_type=cfg.pool_type, bias=cfg.output_bias, padding=cfg.padding)\n        else:\n            self.shortcut = nn.Identity()\n\n        assert cfg.stride_mode in ('pool', '1x1', 'dw')\n        stride_pool, stride_1, stride_2 = 1, 1, 1\n        if cfg.stride_mode == 'pool':\n            # NOTE this is not described in paper, experiment to find faster option that doesn't stride in 1x1\n            stride_pool, dilation_2 = stride, dilation[1]\n            # FIXME handle dilation of avg pool\n        elif cfg.stride_mode == '1x1':\n            # NOTE I don't like this option described in paper, 1x1 w/ stride throws info away\n            stride_1, dilation_2 = stride, dilation[1]\n        else:\n            stride_2, dilation_2 = stride, dilation[0]\n\n        self.pre_norm = norm_act_layer(in_chs, apply_act=cfg.pre_norm_act)\n        if stride_pool > 1:\n            self.down = Downsample2d(in_chs, in_chs, pool_type=cfg.downsample_pool_type, padding=cfg.padding)\n        else:\n            self.down = nn.Identity()\n        self.conv1_1x1 = create_conv2d(in_chs, mid_chs, 1, stride=stride_1)\n        self.norm1 = norm_act_layer(mid_chs)\n\n        self.conv2_kxk = create_conv2d(\n            mid_chs, mid_chs, cfg.kernel_size,\n            stride=stride_2, dilation=dilation_2, groups=groups, padding=cfg.padding)\n\n        attn_kwargs = {}\n        if isinstance(cfg.attn_layer, str):\n            if cfg.attn_layer == 'se' or cfg.attn_layer == 'eca':\n                attn_kwargs['act_layer'] = cfg.attn_act_layer\n                attn_kwargs['rd_channels'] = int(cfg.attn_ratio * (out_chs if cfg.expand_output else mid_chs))\n\n        # two different orderings for SE and norm2 (due to some weights and trials using SE before norm2)\n        if cfg.attn_early:\n            self.se_early = create_attn(cfg.attn_layer, mid_chs, **attn_kwargs)\n            self.norm2 = norm_act_layer(mid_chs)\n            self.se = None\n        else:\n            self.se_early = None\n            self.norm2 = norm_act_layer(mid_chs)\n            self.se = create_attn(cfg.attn_layer, mid_chs, **attn_kwargs)\n\n        self.conv3_1x1 = create_conv2d(mid_chs, out_chs, 1, bias=cfg.output_bias)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def init_weights(self, scheme=''):\n        named_apply(partial(_init_conv, scheme=scheme), self)\n\n    def forward(self, x):\n        shortcut = self.shortcut(x)\n        x = self.pre_norm(x)\n        x = self.down(x)\n\n        # 1x1 expansion conv & norm-act\n        x = self.conv1_1x1(x)\n        x = self.norm1(x)\n\n        # depthwise / grouped 3x3 conv w/ SE (or other) channel attention & norm-act\n        x = self.conv2_kxk(x)\n        if self.se_early is not None:\n            x = self.se_early(x)\n        x = self.norm2(x)\n        if self.se is not None:\n            x = self.se(x)\n\n        # 1x1 linear projection to output width\n        x = self.conv3_1x1(x)\n        x = self.drop_path(x) + shortcut\n        return x\n\n\nclass ConvNeXtBlock(nn.Module):\n    \"\"\" ConvNeXt Block\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: Optional[int] = None,\n            kernel_size: int = 7,\n            stride: int = 1,\n            dilation: Tuple[int, int] = (1, 1),\n            cfg: MaxxVitConvCfg = MaxxVitConvCfg(),\n            conv_mlp: bool = True,\n            drop_path: float = 0.\n    ):\n        super().__init__()\n        out_chs = out_chs or in_chs\n        act_layer = get_act_layer(cfg.act_layer)\n        if conv_mlp:\n            norm_layer = partial(get_norm_layer(cfg.norm_layer), eps=cfg.norm_eps)\n            mlp_layer = ConvMlp\n        else:\n            assert 'layernorm' in cfg.norm_layer\n            norm_layer = LayerNorm\n            mlp_layer = Mlp\n        self.use_conv_mlp = conv_mlp\n\n        if stride == 2:\n            self.shortcut = Downsample2d(in_chs, out_chs)\n        elif in_chs != out_chs:\n            self.shortcut = nn.Conv2d(in_chs, out_chs, kernel_size=1, bias=cfg.output_bias)\n        else:\n            self.shortcut = nn.Identity()\n\n        assert cfg.stride_mode in ('pool', 'dw')\n        stride_pool, stride_dw = 1, 1\n        # FIXME handle dilation?\n        if cfg.stride_mode == 'pool':\n            stride_pool = stride\n        else:\n            stride_dw = stride\n\n        if stride_pool == 2:\n            self.down = Downsample2d(in_chs, in_chs, pool_type=cfg.downsample_pool_type)\n        else:\n            self.down = nn.Identity()\n\n        self.conv_dw = create_conv2d(\n            in_chs, out_chs, kernel_size=kernel_size, stride=stride_dw, dilation=dilation[1],\n            depthwise=True, bias=cfg.output_bias)\n        self.norm = norm_layer(out_chs)\n        self.mlp = mlp_layer(out_chs, int(cfg.expand_ratio * out_chs), bias=cfg.output_bias, act_layer=act_layer)\n        if conv_mlp:\n            self.ls = LayerScale2d(out_chs, cfg.init_values) if cfg.init_values else nn.Identity()\n        else:\n            self.ls = LayerScale(out_chs, cfg.init_values) if cfg.init_values else nn.Identity()\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        shortcut = self.shortcut(x)\n        x = self.down(x)\n        x = self.conv_dw(x)\n        if self.use_conv_mlp:\n            x = self.norm(x)\n            x = self.mlp(x)\n            x = self.ls(x)\n        else:\n            x = x.permute(0, 2, 3, 1)\n            x = self.norm(x)\n            x = self.mlp(x)\n            x = self.ls(x)\n            x = x.permute(0, 3, 1, 2)\n\n        x = self.drop_path(x) + shortcut\n        return x\n\n\ndef window_partition(x, window_size: List[int]):\n    B, H, W, C = x.shape\n    _assert(H % window_size[0] == 0, f'height ({H}) must be divisible by window ({window_size[0]})')\n    _assert(W % window_size[1] == 0, '')\n    x = x.view(B, H // window_size[0], window_size[0], W // window_size[1], window_size[1], C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0], window_size[1], C)\n    return windows\n\n\n@register_notrace_function  # reason: int argument is a Proxy\ndef window_reverse(windows, window_size: List[int], img_size: List[int]):\n    H, W = img_size\n    C = windows.shape[-1]\n    x = windows.view(-1, H // window_size[0], W // window_size[1], window_size[0], window_size[1], C)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, C)\n    return x\n\n\ndef grid_partition(x, grid_size: List[int]):\n    B, H, W, C = x.shape\n    _assert(H % grid_size[0] == 0, f'height {H} must be divisible by grid {grid_size[0]}')\n    _assert(W % grid_size[1] == 0, '')\n    x = x.view(B, grid_size[0], H // grid_size[0], grid_size[1], W // grid_size[1], C)\n    windows = x.permute(0, 2, 4, 1, 3, 5).contiguous().view(-1, grid_size[0], grid_size[1], C)\n    return windows\n\n\n@register_notrace_function  # reason: int argument is a Proxy\ndef grid_reverse(windows, grid_size: List[int], img_size: List[int]):\n    H, W = img_size\n    C = windows.shape[-1]\n    x = windows.view(-1, H // grid_size[0], W // grid_size[1], grid_size[0], grid_size[1], C)\n    x = x.permute(0, 3, 1, 4, 2, 5).contiguous().view(-1, H, W, C)\n    return x\n\n\ndef get_rel_pos_cls(cfg: MaxxVitTransformerCfg, window_size):\n    rel_pos_cls = None\n    if cfg.rel_pos_type == 'mlp':\n        rel_pos_cls = partial(RelPosMlp, window_size=window_size, hidden_dim=cfg.rel_pos_dim)\n    elif cfg.rel_pos_type == 'bias':\n        rel_pos_cls = partial(RelPosBias, window_size=window_size)\n    elif cfg.rel_pos_type == 'bias_tf':\n        rel_pos_cls = partial(RelPosBiasTf, window_size=window_size)\n    return rel_pos_cls\n\n\nclass PartitionAttentionCl(nn.Module):\n    \"\"\" Grid or Block partition + Attn + FFN.\n    NxC 'channels last' tensor layout.\n    \"\"\"\n\n    def __init__(\n            self,\n            dim: int,\n            partition_type: str = 'block',\n            cfg: MaxxVitTransformerCfg = MaxxVitTransformerCfg(),\n            drop_path: float = 0.,\n    ):\n        super().__init__()\n        norm_layer = partial(get_norm_layer(cfg.norm_layer_cl), eps=cfg.norm_eps)  # NOTE this block is channels-last\n        act_layer = get_act_layer(cfg.act_layer)\n\n        self.partition_block = partition_type == 'block'\n        self.partition_size = to_2tuple(cfg.window_size if self.partition_block else cfg.grid_size)\n        rel_pos_cls = get_rel_pos_cls(cfg, self.partition_size)\n\n        self.norm1 = norm_layer(dim)\n        self.attn = AttentionCl(\n            dim,\n            dim,\n            dim_head=cfg.dim_head,\n            bias=cfg.attn_bias,\n            head_first=cfg.head_first,\n            rel_pos_cls=rel_pos_cls,\n            attn_drop=cfg.attn_drop,\n            proj_drop=cfg.proj_drop,\n        )\n        self.ls1 = LayerScale(dim, init_values=cfg.init_values) if cfg.init_values else nn.Identity()\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * cfg.expand_ratio),\n            act_layer=act_layer,\n            drop=cfg.proj_drop)\n        self.ls2 = LayerScale(dim, init_values=cfg.init_values) if cfg.init_values else nn.Identity()\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def _partition_attn(self, x):\n        img_size = x.shape[1:3]\n        if self.partition_block:\n            partitioned = window_partition(x, self.partition_size)\n        else:\n            partitioned = grid_partition(x, self.partition_size)\n\n        partitioned = self.attn(partitioned)\n\n        if self.partition_block:\n            x = window_reverse(partitioned, self.partition_size, img_size)\n        else:\n            x = grid_reverse(partitioned, self.partition_size, img_size)\n        return x\n\n    def forward(self, x):\n        x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x\n\n\nclass ParallelPartitionAttention(nn.Module):\n    \"\"\" Experimental. Grid and Block partition + single FFN\n    NxC tensor layout.\n    \"\"\"\n\n    def __init__(\n            self,\n            dim: int,\n            cfg: MaxxVitTransformerCfg = MaxxVitTransformerCfg(),\n            drop_path: float = 0.,\n    ):\n        super().__init__()\n        assert dim % 2 == 0\n        norm_layer = partial(get_norm_layer(cfg.norm_layer_cl), eps=cfg.norm_eps)  # NOTE this block is channels-last\n        act_layer = get_act_layer(cfg.act_layer)\n\n        assert cfg.window_size == cfg.grid_size\n        self.partition_size = to_2tuple(cfg.window_size)\n        rel_pos_cls = get_rel_pos_cls(cfg, self.partition_size)\n\n        self.norm1 = norm_layer(dim)\n        self.attn_block = AttentionCl(\n            dim,\n            dim // 2,\n            dim_head=cfg.dim_head,\n            bias=cfg.attn_bias,\n            head_first=cfg.head_first,\n            rel_pos_cls=rel_pos_cls,\n            attn_drop=cfg.attn_drop,\n            proj_drop=cfg.proj_drop,\n        )\n        self.attn_grid = AttentionCl(\n            dim,\n            dim // 2,\n            dim_head=cfg.dim_head,\n            bias=cfg.attn_bias,\n            head_first=cfg.head_first,\n            rel_pos_cls=rel_pos_cls,\n            attn_drop=cfg.attn_drop,\n            proj_drop=cfg.proj_drop,\n        )\n        self.ls1 = LayerScale(dim, init_values=cfg.init_values) if cfg.init_values else nn.Identity()\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * cfg.expand_ratio),\n            out_features=dim,\n            act_layer=act_layer,\n            drop=cfg.proj_drop)\n        self.ls2 = LayerScale(dim, init_values=cfg.init_values) if cfg.init_values else nn.Identity()\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def _partition_attn(self, x):\n        img_size = x.shape[1:3]\n\n        partitioned_block = window_partition(x, self.partition_size)\n        partitioned_block = self.attn_block(partitioned_block)\n        x_window = window_reverse(partitioned_block, self.partition_size, img_size)\n\n        partitioned_grid = grid_partition(x, self.partition_size)\n        partitioned_grid = self.attn_grid(partitioned_grid)\n        x_grid = grid_reverse(partitioned_grid, self.partition_size, img_size)\n\n        return torch.cat([x_window, x_grid], dim=-1)\n\n    def forward(self, x):\n        x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x\n\n\ndef window_partition_nchw(x, window_size: List[int]):\n    B, C, H, W = x.shape\n    _assert(H % window_size[0] == 0, f'height ({H}) must be divisible by window ({window_size[0]})')\n    _assert(W % window_size[1] == 0, '')\n    x = x.view(B, C, H // window_size[0], window_size[0], W // window_size[1], window_size[1])\n    windows = x.permute(0, 2, 4, 1, 3, 5).contiguous().view(-1, C, window_size[0], window_size[1])\n    return windows\n\n\n@register_notrace_function  # reason: int argument is a Proxy\ndef window_reverse_nchw(windows, window_size: List[int], img_size: List[int]):\n    H, W = img_size\n    C = windows.shape[1]\n    x = windows.view(-1, H // window_size[0], W // window_size[1], C, window_size[0], window_size[1])\n    x = x.permute(0, 3, 1, 4, 2, 5).contiguous().view(-1, C, H, W)\n    return x\n\n\ndef grid_partition_nchw(x, grid_size: List[int]):\n    B, C, H, W = x.shape\n    _assert(H % grid_size[0] == 0, f'height {H} must be divisible by grid {grid_size[0]}')\n    _assert(W % grid_size[1] == 0, '')\n    x = x.view(B, C, grid_size[0], H // grid_size[0], grid_size[1], W // grid_size[1])\n    windows = x.permute(0, 3, 5, 1, 2, 4).contiguous().view(-1, C, grid_size[0], grid_size[1])\n    return windows\n\n\n@register_notrace_function  # reason: int argument is a Proxy\ndef grid_reverse_nchw(windows, grid_size: List[int], img_size: List[int]):\n    H, W = img_size\n    C = windows.shape[1]\n    x = windows.view(-1, H // grid_size[0], W // grid_size[1], C, grid_size[0], grid_size[1])\n    x = x.permute(0, 3, 4, 1, 5, 2).contiguous().view(-1, C, H, W)\n    return x\n\n\nclass PartitionAttention2d(nn.Module):\n    \"\"\" Grid or Block partition + Attn + FFN\n\n    '2D' NCHW tensor layout.\n    \"\"\"\n\n    def __init__(\n            self,\n            dim: int,\n            partition_type: str = 'block',\n            cfg: MaxxVitTransformerCfg = MaxxVitTransformerCfg(),\n            drop_path: float = 0.,\n    ):\n        super().__init__()\n        norm_layer = partial(get_norm_layer(cfg.norm_layer), eps=cfg.norm_eps)  # NOTE this block is channels-last\n        act_layer = get_act_layer(cfg.act_layer)\n\n        self.partition_block = partition_type == 'block'\n        self.partition_size = to_2tuple(cfg.window_size if self.partition_block else cfg.grid_size)\n        rel_pos_cls = get_rel_pos_cls(cfg, self.partition_size)\n\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention2d(\n            dim,\n            dim,\n            dim_head=cfg.dim_head,\n            bias=cfg.attn_bias,\n            head_first=cfg.head_first,\n            rel_pos_cls=rel_pos_cls,\n            attn_drop=cfg.attn_drop,\n            proj_drop=cfg.proj_drop,\n        )\n        self.ls1 = LayerScale2d(dim, init_values=cfg.init_values) if cfg.init_values else nn.Identity()\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        self.mlp = ConvMlp(\n            in_features=dim,\n            hidden_features=int(dim * cfg.expand_ratio),\n            act_layer=act_layer,\n            drop=cfg.proj_drop)\n        self.ls2 = LayerScale2d(dim, init_values=cfg.init_values) if cfg.init_values else nn.Identity()\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def _partition_attn(self, x):\n        img_size = x.shape[-2:]\n        if self.partition_block:\n            partitioned = window_partition_nchw(x, self.partition_size)\n        else:\n            partitioned = grid_partition_nchw(x, self.partition_size)\n\n        partitioned = self.attn(partitioned)\n\n        if self.partition_block:\n            x = window_reverse_nchw(partitioned, self.partition_size, img_size)\n        else:\n            x = grid_reverse_nchw(partitioned, self.partition_size, img_size)\n        return x\n\n    def forward(self, x):\n        x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x\n\n\nclass MaxxVitBlock(nn.Module):\n    \"\"\" MaxVit conv, window partition + FFN , grid partition + FFN\n    \"\"\"\n\n    def __init__(\n            self,\n            dim: int,\n            dim_out: int,\n            stride: int = 1,\n            conv_cfg: MaxxVitConvCfg = MaxxVitConvCfg(),\n            transformer_cfg: MaxxVitTransformerCfg = MaxxVitTransformerCfg(),\n            drop_path: float = 0.,\n    ):\n        super().__init__()\n        self.nchw_attn = transformer_cfg.use_nchw_attn\n\n        conv_cls = ConvNeXtBlock if conv_cfg.block_type == 'convnext' else MbConvBlock\n        self.conv = conv_cls(dim, dim_out, stride=stride, cfg=conv_cfg, drop_path=drop_path)\n\n        attn_kwargs = dict(dim=dim_out, cfg=transformer_cfg, drop_path=drop_path)\n        partition_layer = PartitionAttention2d if self.nchw_attn else PartitionAttentionCl\n        self.attn_block = None if transformer_cfg.no_block_attn else partition_layer(**attn_kwargs)\n        self.attn_grid = partition_layer(partition_type='grid', **attn_kwargs)\n\n    def init_weights(self, scheme=''):\n        if self.attn_block is not None:\n            named_apply(partial(_init_transformer, scheme=scheme), self.attn_block)\n        named_apply(partial(_init_transformer, scheme=scheme), self.attn_grid)\n        named_apply(partial(_init_conv, scheme=scheme), self.conv)\n\n    def forward(self, x):\n        # NCHW format\n        x = self.conv(x)\n\n        if not self.nchw_attn:\n            x = x.permute(0, 2, 3, 1)  # to NHWC (channels-last)\n        if self.attn_block is not None:\n            x = self.attn_block(x)\n        x = self.attn_grid(x)\n        if not self.nchw_attn:\n            x = x.permute(0, 3, 1, 2)  # back to NCHW\n        return x\n\n\nclass ParallelMaxxVitBlock(nn.Module):\n    \"\"\" MaxVit block with parallel cat(window + grid), one FF\n    Experimental timm block.\n    \"\"\"\n\n    def __init__(\n            self,\n            dim,\n            dim_out,\n            stride=1,\n            num_conv=2,\n            conv_cfg: MaxxVitConvCfg = MaxxVitConvCfg(),\n            transformer_cfg: MaxxVitTransformerCfg = MaxxVitTransformerCfg(),\n            drop_path=0.,\n    ):\n        super().__init__()\n\n        conv_cls = ConvNeXtBlock if conv_cfg.block_type == 'convnext' else MbConvBlock\n        if num_conv > 1:\n            convs = [conv_cls(dim, dim_out, stride=stride, cfg=conv_cfg, drop_path=drop_path)]\n            convs += [conv_cls(dim_out, dim_out, cfg=conv_cfg, drop_path=drop_path)] * (num_conv - 1)\n            self.conv = nn.Sequential(*convs)\n        else:\n            self.conv = conv_cls(dim, dim_out, stride=stride, cfg=conv_cfg, drop_path=drop_path)\n        self.attn = ParallelPartitionAttention(dim=dim_out, cfg=transformer_cfg, drop_path=drop_path)\n\n    def init_weights(self, scheme=''):\n        named_apply(partial(_init_transformer, scheme=scheme), self.attn)\n        named_apply(partial(_init_conv, scheme=scheme), self.conv)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.permute(0, 2, 3, 1)\n        x = self.attn(x)\n        x = x.permute(0, 3, 1, 2)\n        return x\n\n\nclass MaxxVitStage(nn.Module):\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: int,\n            stride: int = 2,\n            depth: int = 4,\n            feat_size: Tuple[int, int] = (14, 14),\n            block_types: Union[str, Tuple[str]] = 'C',\n            transformer_cfg: MaxxVitTransformerCfg = MaxxVitTransformerCfg(),\n            conv_cfg: MaxxVitConvCfg = MaxxVitConvCfg(),\n            drop_path: Union[float, List[float]] = 0.,\n    ):\n        super().__init__()\n        self.grad_checkpointing = False\n\n        block_types = extend_tuple(block_types, depth)\n        blocks = []\n        for i, t in enumerate(block_types):\n            block_stride = stride if i == 0 else 1\n            assert t in ('C', 'T', 'M', 'PM')\n            if t == 'C':\n                conv_cls = ConvNeXtBlock if conv_cfg.block_type == 'convnext' else MbConvBlock\n                blocks += [conv_cls(\n                    in_chs,\n                    out_chs,\n                    stride=block_stride,\n                    cfg=conv_cfg,\n                    drop_path=drop_path[i],\n                )]\n            elif t == 'T':\n                rel_pos_cls = get_rel_pos_cls(transformer_cfg, feat_size)\n                blocks += [TransformerBlock2d(\n                    in_chs,\n                    out_chs,\n                    stride=block_stride,\n                    rel_pos_cls=rel_pos_cls,\n                    cfg=transformer_cfg,\n                    drop_path=drop_path[i],\n                )]\n            elif t == 'M':\n                blocks += [MaxxVitBlock(\n                    in_chs,\n                    out_chs,\n                    stride=block_stride,\n                    conv_cfg=conv_cfg,\n                    transformer_cfg=transformer_cfg,\n                    drop_path=drop_path[i],\n                )]\n            elif t == 'PM':\n                blocks += [ParallelMaxxVitBlock(\n                    in_chs,\n                    out_chs,\n                    stride=block_stride,\n                    conv_cfg=conv_cfg,\n                    transformer_cfg=transformer_cfg,\n                    drop_path=drop_path[i],\n                )]\n            in_chs = out_chs\n        self.blocks = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        return x\n\n\nclass Stem(nn.Module):\n\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: int,\n            kernel_size: int = 3,\n            padding: str = '',\n            bias: bool = False,\n            act_layer: str = 'gelu',\n            norm_layer: str = 'batchnorm2d',\n            norm_eps: float = 1e-5,\n    ):\n        super().__init__()\n        if not isinstance(out_chs, (list, tuple)):\n            out_chs = to_2tuple(out_chs)\n\n        norm_act_layer = partial(get_norm_act_layer(norm_layer, act_layer), eps=norm_eps)\n        self.out_chs = out_chs[-1]\n        self.stride = 2\n\n        self.conv1 = create_conv2d(in_chs, out_chs[0], kernel_size, stride=2, padding=padding, bias=bias)\n        self.norm1 = norm_act_layer(out_chs[0])\n        self.conv2 = create_conv2d(out_chs[0], out_chs[1], kernel_size, stride=1, padding=padding, bias=bias)\n\n    def init_weights(self, scheme=''):\n        named_apply(partial(_init_conv, scheme=scheme), self)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.conv2(x)\n        return x\n\n\ndef cfg_window_size(cfg: MaxxVitTransformerCfg, img_size: Tuple[int, int]):\n    if cfg.window_size is not None:\n        assert cfg.grid_size\n        return cfg\n    partition_size = img_size[0] // cfg.partition_ratio, img_size[1] // cfg.partition_ratio\n    cfg = replace(cfg, window_size=partition_size, grid_size=partition_size)\n    return cfg\n\n\ndef _overlay_kwargs(cfg: MaxxVitCfg, **kwargs):\n    transformer_kwargs = {}\n    conv_kwargs = {}\n    base_kwargs = {}\n    for k, v in kwargs.items():\n        if k.startswith('transformer_'):\n            transformer_kwargs[k.replace('transformer_', '')] = v\n        elif k.startswith('conv_'):\n            conv_kwargs[k.replace('conv_', '')] = v\n        else:\n            base_kwargs[k] = v\n    cfg = replace(\n        cfg,\n        transformer_cfg=replace(cfg.transformer_cfg, **transformer_kwargs),\n        conv_cfg=replace(cfg.conv_cfg, **conv_kwargs),\n        **base_kwargs\n    )\n    return cfg\n\n\nclass MaxxVit(nn.Module):\n    \"\"\" CoaTNet + MaxVit base model.\n\n    Highly configurable for different block compositions, tensor layouts, pooling types.\n    \"\"\"\n\n    def __init__(\n            self,\n            cfg: MaxxVitCfg,\n            img_size: Union[int, Tuple[int, int]] = 224,\n            in_chans: int = 3,\n            num_classes: int = 1000,\n            global_pool: str = 'avg',\n            drop_rate: float = 0.,\n            drop_path_rate: float = 0.,\n            **kwargs,\n    ):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        if kwargs:\n            cfg = _overlay_kwargs(cfg, **kwargs)\n        transformer_cfg = cfg_window_size(cfg.transformer_cfg, img_size)\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.num_features = self.embed_dim = cfg.embed_dim[-1]\n        self.drop_rate = drop_rate\n        self.grad_checkpointing = False\n        self.feature_info = []\n\n        self.stem = Stem(\n            in_chs=in_chans,\n            out_chs=cfg.stem_width,\n            padding=cfg.conv_cfg.padding,\n            bias=cfg.stem_bias,\n            act_layer=cfg.conv_cfg.act_layer,\n            norm_layer=cfg.conv_cfg.norm_layer,\n            norm_eps=cfg.conv_cfg.norm_eps,\n        )\n        stride = self.stem.stride\n        self.feature_info += [dict(num_chs=self.stem.out_chs, reduction=2, module='stem')]\n        feat_size = tuple([i // s for i, s in zip(img_size, to_2tuple(stride))])\n\n        num_stages = len(cfg.embed_dim)\n        assert len(cfg.depths) == num_stages\n        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(cfg.depths)).split(cfg.depths)]\n        in_chs = self.stem.out_chs\n        stages = []\n        for i in range(num_stages):\n            stage_stride = 2\n            out_chs = cfg.embed_dim[i]\n            feat_size = tuple([(r - 1) // stage_stride + 1 for r in feat_size])\n            stages += [MaxxVitStage(\n                in_chs,\n                out_chs,\n                depth=cfg.depths[i],\n                block_types=cfg.block_type[i],\n                conv_cfg=cfg.conv_cfg,\n                transformer_cfg=transformer_cfg,\n                feat_size=feat_size,\n                drop_path=dpr[i],\n            )]\n            stride *= stage_stride\n            in_chs = out_chs\n            self.feature_info += [dict(num_chs=out_chs, reduction=stride, module=f'stages.{i}')]\n        self.stages = nn.Sequential(*stages)\n\n        final_norm_layer = partial(get_norm_layer(cfg.transformer_cfg.norm_layer), eps=cfg.transformer_cfg.norm_eps)\n        self.head_hidden_size = cfg.head_hidden_size\n        if self.head_hidden_size:\n            self.norm = nn.Identity()\n            self.head = NormMlpClassifierHead(\n                self.num_features,\n                num_classes,\n                hidden_size=self.head_hidden_size,\n                pool_type=global_pool,\n                drop_rate=drop_rate,\n                norm_layer=final_norm_layer,\n            )\n        else:\n            # standard classifier head w/ norm, pooling, fc classifier\n            self.norm = final_norm_layer(self.num_features)\n            self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=drop_rate)\n\n        # Weight init (default PyTorch init works well for AdamW if scheme not set)\n        assert cfg.weight_init in ('', 'normal', 'trunc_normal', 'xavier_normal', 'vit_eff')\n        if cfg.weight_init:\n            named_apply(partial(self._init_weights, scheme=cfg.weight_init), self)\n\n    def _init_weights(self, module, name, scheme=''):\n        if hasattr(module, 'init_weights'):\n            try:\n                module.init_weights(scheme=scheme)\n            except TypeError:\n                module.init_weights()\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\n            k for k, _ in self.named_parameters()\n            if any(n in k for n in [\"relative_position_bias_table\", \"rel_pos.mlp\"])}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^stem',  # stem and embed\n            blocks=[(r'^stages\\.(\\d+)', None), (r'^norm', (99999,))]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        for s in self.stages:\n            s.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        self.head.reset(num_classes, global_pool)\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        x = self.stages(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=pre_logits)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _rw_coat_cfg(\n        stride_mode='pool',\n        pool_type='avg2',\n        conv_output_bias=False,\n        conv_attn_early=False,\n        conv_attn_act_layer='relu',\n        conv_norm_layer='',\n        transformer_shortcut_bias=True,\n        transformer_norm_layer='layernorm2d',\n        transformer_norm_layer_cl='layernorm',\n        init_values=None,\n        rel_pos_type='bias',\n        rel_pos_dim=512,\n):\n    # 'RW' timm variant models were created and trained before seeing https://github.com/google-research/maxvit\n    # Common differences for initial timm models:\n    # - pre-norm layer in MZBConv included an activation after norm\n    # - mbconv expansion calculated from input instead of output chs\n    # - mbconv shortcut and final 1x1 conv did not have a bias\n    # - SE act layer was relu, not silu\n    # - mbconv uses silu in timm, not gelu\n    # - expansion in attention block done via output proj, not input proj\n    # Variable differences (evolved over training initial models):\n    # - avg pool with kernel_size=2 favoured downsampling (instead of maxpool for coat)\n    # - SE attention was between conv2 and norm/act\n    # - default to avg pool for mbconv downsample instead of 1x1 or dw conv\n    # - transformer block shortcut has no bias\n    return dict(\n        conv_cfg=MaxxVitConvCfg(\n            stride_mode=stride_mode,\n            pool_type=pool_type,\n            pre_norm_act=True,\n            expand_output=False,\n            output_bias=conv_output_bias,\n            attn_early=conv_attn_early,\n            attn_act_layer=conv_attn_act_layer,\n            act_layer='silu',\n            norm_layer=conv_norm_layer,\n        ),\n        transformer_cfg=MaxxVitTransformerCfg(\n            expand_first=False,\n            shortcut_bias=transformer_shortcut_bias,\n            pool_type=pool_type,\n            init_values=init_values,\n            norm_layer=transformer_norm_layer,\n            norm_layer_cl=transformer_norm_layer_cl,\n            rel_pos_type=rel_pos_type,\n            rel_pos_dim=rel_pos_dim,\n        ),\n    )\n\n\ndef _rw_max_cfg(\n        stride_mode='dw',\n        pool_type='avg2',\n        conv_output_bias=False,\n        conv_attn_ratio=1 / 16,\n        conv_norm_layer='',\n        transformer_norm_layer='layernorm2d',\n        transformer_norm_layer_cl='layernorm',\n        window_size=None,\n        dim_head=32,\n        init_values=None,\n        rel_pos_type='bias',\n        rel_pos_dim=512,\n):\n    # 'RW' timm variant models were created and trained before seeing https://github.com/google-research/maxvit\n    # Differences of initial timm models:\n    # - mbconv expansion calculated from input instead of output chs\n    # - mbconv shortcut and final 1x1 conv did not have a bias\n    # - mbconv uses silu in timm, not gelu\n    # - expansion in attention block done via output proj, not input proj\n    return dict(\n        conv_cfg=MaxxVitConvCfg(\n            stride_mode=stride_mode,\n            pool_type=pool_type,\n            expand_output=False,\n            output_bias=conv_output_bias,\n            attn_ratio=conv_attn_ratio,\n            act_layer='silu',\n            norm_layer=conv_norm_layer,\n        ),\n        transformer_cfg=MaxxVitTransformerCfg(\n            expand_first=False,\n            pool_type=pool_type,\n            dim_head=dim_head,\n            window_size=window_size,\n            init_values=init_values,\n            norm_layer=transformer_norm_layer,\n            norm_layer_cl=transformer_norm_layer_cl,\n            rel_pos_type=rel_pos_type,\n            rel_pos_dim=rel_pos_dim,\n        ),\n    )\n\n\ndef _next_cfg(\n        stride_mode='dw',\n        pool_type='avg2',\n        conv_norm_layer='layernorm2d',\n        conv_norm_layer_cl='layernorm',\n        transformer_norm_layer='layernorm2d',\n        transformer_norm_layer_cl='layernorm',\n        window_size=None,\n        no_block_attn=False,\n        init_values=1e-6,\n        rel_pos_type='mlp',  # MLP by default for maxxvit\n        rel_pos_dim=512,\n):\n    # For experimental models with convnext instead of mbconv\n    init_values = to_2tuple(init_values)\n    return dict(\n        conv_cfg=MaxxVitConvCfg(\n            block_type='convnext',\n            stride_mode=stride_mode,\n            pool_type=pool_type,\n            expand_output=False,\n            init_values=init_values[0],\n            norm_layer=conv_norm_layer,\n            norm_layer_cl=conv_norm_layer_cl,\n        ),\n        transformer_cfg=MaxxVitTransformerCfg(\n            expand_first=False,\n            pool_type=pool_type,\n            window_size=window_size,\n            no_block_attn=no_block_attn,  # enabled for MaxxViT-V2\n            init_values=init_values[1],\n            norm_layer=transformer_norm_layer,\n            norm_layer_cl=transformer_norm_layer_cl,\n            rel_pos_type=rel_pos_type,\n            rel_pos_dim=rel_pos_dim,\n        ),\n    )\n\n\ndef _tf_cfg():\n    return dict(\n        conv_cfg=MaxxVitConvCfg(\n            norm_eps=1e-3,\n            act_layer='gelu_tanh',\n            padding='same',\n        ),\n        transformer_cfg=MaxxVitTransformerCfg(\n            norm_eps=1e-5,\n            act_layer='gelu_tanh',\n            head_first=False,  # heads are interleaved (q_nh, q_hdim, k_nh, q_hdim, ....)\n            rel_pos_type='bias_tf',\n        ),\n    )\n\n\nmodel_cfgs = dict(\n    # timm specific CoAtNet configs\n    coatnet_pico_rw=MaxxVitCfg(\n        embed_dim=(64, 128, 256, 512),\n        depths=(2, 3, 5, 2),\n        stem_width=(32, 64),\n        **_rw_max_cfg(  # using newer max defaults here\n            conv_output_bias=True,\n            conv_attn_ratio=0.25,\n        ),\n    ),\n    coatnet_nano_rw=MaxxVitCfg(\n        embed_dim=(64, 128, 256, 512),\n        depths=(3, 4, 6, 3),\n        stem_width=(32, 64),\n        **_rw_max_cfg(  # using newer max defaults here\n            stride_mode='pool',\n            conv_output_bias=True,\n            conv_attn_ratio=0.25,\n        ),\n    ),\n    coatnet_0_rw=MaxxVitCfg(\n        embed_dim=(96, 192, 384, 768),\n        depths=(2, 3, 7, 2),  # deeper than paper '0' model\n        stem_width=(32, 64),\n        **_rw_coat_cfg(\n            conv_attn_early=True,\n            transformer_shortcut_bias=False,\n        ),\n    ),\n    coatnet_1_rw=MaxxVitCfg(\n        embed_dim=(96, 192, 384, 768),\n        depths=(2, 6, 14, 2),\n        stem_width=(32, 64),\n        **_rw_coat_cfg(\n            stride_mode='dw',\n            conv_attn_early=True,\n            transformer_shortcut_bias=False,\n        )\n    ),\n    coatnet_2_rw=MaxxVitCfg(\n        embed_dim=(128, 256, 512, 1024),\n        depths=(2, 6, 14, 2),\n        stem_width=(64, 128),\n        **_rw_coat_cfg(\n            stride_mode='dw',\n            conv_attn_act_layer='silu',\n            #init_values=1e-6,\n        ),\n    ),\n    coatnet_3_rw=MaxxVitCfg(\n        embed_dim=(192, 384, 768, 1536),\n        depths=(2, 6, 14, 2),\n        stem_width=(96, 192),\n        **_rw_coat_cfg(\n            stride_mode='dw',\n            conv_attn_act_layer='silu',\n            init_values=1e-6,\n        ),\n    ),\n\n    # Experimental CoAtNet configs w/ ImageNet-1k train (different norm layers, MLP rel-pos)\n    coatnet_bn_0_rw=MaxxVitCfg(\n        embed_dim=(96, 192, 384, 768),\n        depths=(2, 3, 7, 2),  # deeper than paper '0' model\n        stem_width=(32, 64),\n        **_rw_coat_cfg(\n            stride_mode='dw',\n            conv_attn_early=True,\n            transformer_shortcut_bias=False,\n            transformer_norm_layer='batchnorm2d',\n        )\n    ),\n    coatnet_rmlp_nano_rw=MaxxVitCfg(\n        embed_dim=(64, 128, 256, 512),\n        depths=(3, 4, 6, 3),\n        stem_width=(32, 64),\n        **_rw_max_cfg(\n            conv_output_bias=True,\n            conv_attn_ratio=0.25,\n            rel_pos_type='mlp',\n            rel_pos_dim=384,\n        ),\n    ),\n    coatnet_rmlp_0_rw=MaxxVitCfg(\n        embed_dim=(96, 192, 384, 768),\n        depths=(2, 3, 7, 2),  # deeper than paper '0' model\n        stem_width=(32, 64),\n        **_rw_coat_cfg(\n            stride_mode='dw',\n            rel_pos_type='mlp',\n        ),\n    ),\n    coatnet_rmlp_1_rw=MaxxVitCfg(\n        embed_dim=(96, 192, 384, 768),\n        depths=(2, 6, 14, 2),\n        stem_width=(32, 64),\n        **_rw_coat_cfg(\n            pool_type='max',\n            conv_attn_early=True,\n            transformer_shortcut_bias=False,\n            rel_pos_type='mlp',\n            rel_pos_dim=384,  # was supposed to be 512, woops\n        ),\n    ),\n    coatnet_rmlp_1_rw2=MaxxVitCfg(\n        embed_dim=(96, 192, 384, 768),\n        depths=(2, 6, 14, 2),\n        stem_width=(32, 64),\n        **_rw_coat_cfg(\n            stride_mode='dw',\n            rel_pos_type='mlp',\n            rel_pos_dim=512,  # was supposed to be 512, woops\n        ),\n    ),\n    coatnet_rmlp_2_rw=MaxxVitCfg(\n        embed_dim=(128, 256, 512, 1024),\n        depths=(2, 6, 14, 2),\n        stem_width=(64, 128),\n        **_rw_coat_cfg(\n            stride_mode='dw',\n            conv_attn_act_layer='silu',\n            init_values=1e-6,\n            rel_pos_type='mlp'\n        ),\n    ),\n    coatnet_rmlp_3_rw=MaxxVitCfg(\n        embed_dim=(192, 384, 768, 1536),\n        depths=(2, 6, 14, 2),\n        stem_width=(96, 192),\n        **_rw_coat_cfg(\n            stride_mode='dw',\n            conv_attn_act_layer='silu',\n            init_values=1e-6,\n            rel_pos_type='mlp'\n        ),\n    ),\n\n    coatnet_nano_cc=MaxxVitCfg(\n        embed_dim=(64, 128, 256, 512),\n        depths=(3, 4, 6, 3),\n        stem_width=(32, 64),\n        block_type=('C', 'C', ('C', 'T'), ('C', 'T')),\n        **_rw_coat_cfg(),\n    ),\n    coatnext_nano_rw=MaxxVitCfg(\n        embed_dim=(64, 128, 256, 512),\n        depths=(3, 4, 6, 3),\n        stem_width=(32, 64),\n        weight_init='normal',\n        **_next_cfg(\n            rel_pos_type='bias',\n            init_values=(1e-5, None)\n        ),\n    ),\n\n    # Trying to be like the CoAtNet paper configs\n    coatnet_0=MaxxVitCfg(\n        embed_dim=(96, 192, 384, 768),\n        depths=(2, 3, 5, 2),\n        stem_width=64,\n        head_hidden_size=768,\n    ),\n    coatnet_1=MaxxVitCfg(\n        embed_dim=(96, 192, 384, 768),\n        depths=(2, 6, 14, 2),\n        stem_width=64,\n        head_hidden_size=768,\n    ),\n    coatnet_2=MaxxVitCfg(\n        embed_dim=(128, 256, 512, 1024),\n        depths=(2, 6, 14, 2),\n        stem_width=128,\n        head_hidden_size=1024,\n    ),\n    coatnet_3=MaxxVitCfg(\n        embed_dim=(192, 384, 768, 1536),\n        depths=(2, 6, 14, 2),\n        stem_width=192,\n        head_hidden_size=1536,\n    ),\n    coatnet_4=MaxxVitCfg(\n        embed_dim=(192, 384, 768, 1536),\n        depths=(2, 12, 28, 2),\n        stem_width=192,\n        head_hidden_size=1536,\n    ),\n    coatnet_5=MaxxVitCfg(\n        embed_dim=(256, 512, 1280, 2048),\n        depths=(2, 12, 28, 2),\n        stem_width=192,\n        head_hidden_size=2048,\n    ),\n\n    # Experimental MaxVit configs\n    maxvit_pico_rw=MaxxVitCfg(\n        embed_dim=(32, 64, 128, 256),\n        depths=(2, 2, 5, 2),\n        block_type=('M',) * 4,\n        stem_width=(24, 32),\n        **_rw_max_cfg(),\n    ),\n    maxvit_nano_rw=MaxxVitCfg(\n        embed_dim=(64, 128, 256, 512),\n        depths=(1, 2, 3, 1),\n        block_type=('M',) * 4,\n        stem_width=(32, 64),\n        **_rw_max_cfg(),\n    ),\n    maxvit_tiny_rw=MaxxVitCfg(\n        embed_dim=(64, 128, 256, 512),\n        depths=(2, 2, 5, 2),\n        block_type=('M',) * 4,\n        stem_width=(32, 64),\n        **_rw_max_cfg(),\n    ),\n    maxvit_tiny_pm=MaxxVitCfg(\n        embed_dim=(64, 128, 256, 512),\n        depths=(2, 2, 5, 2),\n        block_type=('PM',) * 4,\n        stem_width=(32, 64),\n        **_rw_max_cfg(),\n    ),\n\n    maxvit_rmlp_pico_rw=MaxxVitCfg(\n        embed_dim=(32, 64, 128, 256),\n        depths=(2, 2, 5, 2),\n        block_type=('M',) * 4,\n        stem_width=(24, 32),\n        **_rw_max_cfg(rel_pos_type='mlp'),\n    ),\n    maxvit_rmlp_nano_rw=MaxxVitCfg(\n        embed_dim=(64, 128, 256, 512),\n        depths=(1, 2, 3, 1),\n        block_type=('M',) * 4,\n        stem_width=(32, 64),\n        **_rw_max_cfg(rel_pos_type='mlp'),\n    ),\n    maxvit_rmlp_tiny_rw=MaxxVitCfg(\n        embed_dim=(64, 128, 256, 512),\n        depths=(2, 2, 5, 2),\n        block_type=('M',) * 4,\n        stem_width=(32, 64),\n        **_rw_max_cfg(rel_pos_type='mlp'),\n    ),\n    maxvit_rmlp_small_rw=MaxxVitCfg(\n        embed_dim=(96, 192, 384, 768),\n        depths=(2, 2, 5, 2),\n        block_type=('M',) * 4,\n        stem_width=(32, 64),\n        **_rw_max_cfg(\n            rel_pos_type='mlp',\n            init_values=1e-6,\n        ),\n    ),\n    maxvit_rmlp_base_rw=MaxxVitCfg(\n        embed_dim=(96, 192, 384, 768),\n        depths=(2, 6, 14, 2),\n        block_type=('M',) * 4,\n        stem_width=(32, 64),\n        head_hidden_size=768,\n        **_rw_max_cfg(\n            rel_pos_type='mlp',\n        ),\n    ),\n\n    maxxvit_rmlp_nano_rw=MaxxVitCfg(\n        embed_dim=(64, 128, 256, 512),\n        depths=(1, 2, 3, 1),\n        block_type=('M',) * 4,\n        stem_width=(32, 64),\n        weight_init='normal',\n        **_next_cfg(),\n    ),\n    maxxvit_rmlp_tiny_rw=MaxxVitCfg(\n        embed_dim=(64, 128, 256, 512),\n        depths=(2, 2, 5, 2),\n        block_type=('M',) * 4,\n        stem_width=(32, 64),\n        **_next_cfg(),\n    ),\n    maxxvit_rmlp_small_rw=MaxxVitCfg(\n        embed_dim=(96, 192, 384, 768),\n        depths=(2, 2, 5, 2),\n        block_type=('M',) * 4,\n        stem_width=(48, 96),\n        **_next_cfg(),\n    ),\n\n    maxxvitv2_nano_rw=MaxxVitCfg(\n        embed_dim=(96, 192, 384, 768),\n        depths=(1, 2, 3, 1),\n        block_type=('M',) * 4,\n        stem_width=(48, 96),\n        weight_init='normal',\n        **_next_cfg(\n            no_block_attn=True,\n            rel_pos_type='bias',\n        ),\n    ),\n    maxxvitv2_rmlp_base_rw=MaxxVitCfg(\n        embed_dim=(128, 256, 512, 1024),\n        depths=(2, 6, 12, 2),\n        block_type=('M',) * 4,\n        stem_width=(64, 128),\n        **_next_cfg(\n            no_block_attn=True,\n        ),\n    ),\n    maxxvitv2_rmlp_large_rw=MaxxVitCfg(\n        embed_dim=(160, 320, 640, 1280),\n        depths=(2, 6, 16, 2),\n        block_type=('M',) * 4,\n        stem_width=(80, 160),\n        head_hidden_size=1280,\n        **_next_cfg(\n            no_block_attn=True,\n        ),\n    ),\n\n    # Trying to be like the MaxViT paper configs\n    maxvit_tiny_tf=MaxxVitCfg(\n        embed_dim=(64, 128, 256, 512),\n        depths=(2, 2, 5, 2),\n        block_type=('M',) * 4,\n        stem_width=64,\n        stem_bias=True,\n        head_hidden_size=512,\n        **_tf_cfg(),\n    ),\n    maxvit_small_tf=MaxxVitCfg(\n        embed_dim=(96, 192, 384, 768),\n        depths=(2, 2, 5, 2),\n        block_type=('M',) * 4,\n        stem_width=64,\n        stem_bias=True,\n        head_hidden_size=768,\n        **_tf_cfg(),\n    ),\n    maxvit_base_tf=MaxxVitCfg(\n        embed_dim=(96, 192, 384, 768),\n        depths=(2, 6, 14, 2),\n        block_type=('M',) * 4,\n        stem_width=64,\n        stem_bias=True,\n        head_hidden_size=768,\n        **_tf_cfg(),\n    ),\n    maxvit_large_tf=MaxxVitCfg(\n        embed_dim=(128, 256, 512, 1024),\n        depths=(2, 6, 14, 2),\n        block_type=('M',) * 4,\n        stem_width=128,\n        stem_bias=True,\n        head_hidden_size=1024,\n        **_tf_cfg(),\n    ),\n    maxvit_xlarge_tf=MaxxVitCfg(\n        embed_dim=(192, 384, 768, 1536),\n        depths=(2, 6, 14, 2),\n        block_type=('M',) * 4,\n        stem_width=192,\n        stem_bias=True,\n        head_hidden_size=1536,\n        **_tf_cfg(),\n    ),\n)\n\n\ndef checkpoint_filter_fn(state_dict, model: nn.Module):\n    model_state_dict = model.state_dict()\n    out_dict = {}\n    for k, v in state_dict.items():\n        if k in model_state_dict and v.ndim != model_state_dict[k].ndim and v.numel() == model_state_dict[k].numel():\n            # adapt between conv2d / linear layers\n            assert v.ndim in (2, 4)\n            v = v.reshape(model_state_dict[k].shape)\n        out_dict[k] = v\n    return out_dict\n\n\ndef _create_maxxvit(variant, cfg_variant=None, pretrained=False, **kwargs):\n    if cfg_variant is None:\n        if variant in model_cfgs:\n            cfg_variant = variant\n        else:\n            cfg_variant = '_'.join(variant.split('_')[:-1])\n    return build_model_with_cfg(\n        MaxxVit, variant, pretrained,\n        model_cfg=model_cfgs[cfg_variant],\n        feature_cfg=dict(flatten_sequential=True),\n        pretrained_filter_fn=checkpoint_filter_fn,\n        **kwargs)\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.95, 'interpolation': 'bicubic',\n        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n        'first_conv': 'stem.conv1', 'classifier': 'head.fc',\n        'fixed_input_size': True,\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    # timm specific CoAtNet configs, ImageNet-1k pretrain, fixed rel-pos\n    'coatnet_pico_rw_224.untrained': _cfg(url=''),\n    'coatnet_nano_rw_224.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_nano_rw_224_sw-f53093b4.pth',\n        crop_pct=0.9),\n    'coatnet_0_rw_224.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_0_rw_224_sw-a6439706.pth'),\n    'coatnet_1_rw_224.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_1_rw_224_sw-5cae1ea8.pth'\n    ),\n\n    # timm specific CoAtNet configs, ImageNet-12k pretrain w/ 1k fine-tune, fixed rel-pos\n    'coatnet_2_rw_224.sw_in12k_ft_in1k': _cfg(\n        hf_hub_id='timm/'),\n    #'coatnet_3_rw_224.untrained': _cfg(url=''),\n\n    # Experimental CoAtNet configs w/ ImageNet-12k pretrain -> 1k fine-tune (different norm layers, MLP rel-pos)\n    'coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k': _cfg(\n        hf_hub_id='timm/'),\n    'coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k': _cfg(\n        hf_hub_id='timm/'),\n    'coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n\n    # Experimental CoAtNet configs w/ ImageNet-1k train (different norm layers, MLP rel-pos)\n    'coatnet_bn_0_rw_224.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_bn_0_rw_224_sw-c228e218.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD,\n        crop_pct=0.95),\n    'coatnet_rmlp_nano_rw_224.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_rmlp_nano_rw_224_sw-bd1d51b3.pth',\n        crop_pct=0.9),\n    'coatnet_rmlp_0_rw_224.untrained': _cfg(url=''),\n    'coatnet_rmlp_1_rw_224.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_rmlp_1_rw_224_sw-9051e6c3.pth'),\n    'coatnet_rmlp_2_rw_224.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_rmlp_2_rw_224_sw-5ccfac55.pth'),\n    'coatnet_rmlp_3_rw_224.untrained': _cfg(url=''),\n    'coatnet_nano_cc_224.untrained': _cfg(url=''),\n    'coatnext_nano_rw_224.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnext_nano_rw_224_ad-22cb71c2.pth',\n        crop_pct=0.9),\n\n    # ImagenNet-12k pretrain CoAtNet\n    'coatnet_2_rw_224.sw_in12k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=11821),\n    'coatnet_3_rw_224.sw_in12k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=11821),\n    'coatnet_rmlp_1_rw2_224.sw_in12k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=11821),\n    'coatnet_rmlp_2_rw_224.sw_in12k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=11821),\n\n    # Trying to be like the CoAtNet paper configs (will adapt if 'tf' weights are ever released)\n    'coatnet_0_224.untrained': _cfg(url=''),\n    'coatnet_1_224.untrained': _cfg(url=''),\n    'coatnet_2_224.untrained': _cfg(url=''),\n    'coatnet_3_224.untrained': _cfg(url=''),\n    'coatnet_4_224.untrained': _cfg(url=''),\n    'coatnet_5_224.untrained': _cfg(url=''),\n\n    # timm specific MaxVit configs, ImageNet-1k pretrain or untrained\n    'maxvit_pico_rw_256.untrained': _cfg(url='', input_size=(3, 256, 256), pool_size=(8, 8)),\n    'maxvit_nano_rw_256.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_nano_rw_256_sw-fb127241.pth',\n        input_size=(3, 256, 256), pool_size=(8, 8)),\n    'maxvit_tiny_rw_224.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_tiny_rw_224_sw-7d0dffeb.pth'),\n    'maxvit_tiny_rw_256.untrained': _cfg(\n        url='',\n        input_size=(3, 256, 256), pool_size=(8, 8)),\n    'maxvit_tiny_pm_256.untrained': _cfg(url='', input_size=(3, 256, 256), pool_size=(8, 8)),\n\n    # timm specific MaxVit w/ MLP rel-pos, ImageNet-1k pretrain\n    'maxvit_rmlp_pico_rw_256.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_rmlp_pico_rw_256_sw-8d82f2c6.pth',\n        input_size=(3, 256, 256), pool_size=(8, 8)),\n    'maxvit_rmlp_nano_rw_256.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_rmlp_nano_rw_256_sw-c17bb0d6.pth',\n        input_size=(3, 256, 256), pool_size=(8, 8)),\n    'maxvit_rmlp_tiny_rw_256.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_rmlp_tiny_rw_256_sw-bbef0ff5.pth',\n        input_size=(3, 256, 256), pool_size=(8, 8)),\n    'maxvit_rmlp_small_rw_224.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_rmlp_small_rw_224_sw-6ef0ae4f.pth',\n        crop_pct=0.9,\n    ),\n    'maxvit_rmlp_small_rw_256.untrained': _cfg(\n        url='',\n        input_size=(3, 256, 256), pool_size=(8, 8)),\n\n    # timm specific MaxVit w/ ImageNet-12k pretrain and 1k fine-tune\n    'maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n    'maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n\n    # timm specific MaxVit w/ ImageNet-12k pretrain\n    'maxvit_rmlp_base_rw_224.sw_in12k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=11821,\n    ),\n\n    # timm MaxxViT configs (ConvNeXt conv blocks mixed with MaxVit transformer blocks)\n    'maxxvit_rmlp_nano_rw_256.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxxvit_rmlp_nano_rw_256_sw-0325d459.pth',\n        input_size=(3, 256, 256), pool_size=(8, 8)),\n    'maxxvit_rmlp_tiny_rw_256.untrained': _cfg(url='', input_size=(3, 256, 256), pool_size=(8, 8)),\n    'maxxvit_rmlp_small_rw_256.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxxvit_rmlp_small_rw_256_sw-37e217ff.pth',\n        input_size=(3, 256, 256), pool_size=(8, 8)),\n\n    # timm MaxxViT-V2 configs (ConvNeXt conv blocks mixed with MaxVit transformer blocks, more width, no block attn)\n    'maxxvitv2_nano_rw_256.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 256, 256), pool_size=(8, 8)),\n    'maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k': _cfg(\n        hf_hub_id='timm/'),\n    'maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'maxxvitv2_rmlp_large_rw_224.untrained': _cfg(url=''),\n\n    'maxxvitv2_rmlp_base_rw_224.sw_in12k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=11821),\n\n    # MaxViT models ported from official Tensorflow impl\n    'maxvit_tiny_tf_224.in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n    'maxvit_tiny_tf_384.in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'maxvit_tiny_tf_512.in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),\n    'maxvit_small_tf_224.in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n    'maxvit_small_tf_384.in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'maxvit_small_tf_512.in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),\n    'maxvit_base_tf_224.in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n    'maxvit_base_tf_384.in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'maxvit_base_tf_512.in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),\n    'maxvit_large_tf_224.in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n    'maxvit_large_tf_384.in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'maxvit_large_tf_512.in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),\n\n    'maxvit_base_tf_224.in21k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=21843),\n    'maxvit_base_tf_384.in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'maxvit_base_tf_512.in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),\n    'maxvit_large_tf_224.in21k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=21843),\n    'maxvit_large_tf_384.in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'maxvit_large_tf_512.in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 512, 512), crop_pct=1.0, crop_mode='squash'),\n    'maxvit_xlarge_tf_224.in21k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=21843),\n    'maxvit_xlarge_tf_384.in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'maxvit_xlarge_tf_512.in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),\n})\n\n\n@register_model\ndef coatnet_pico_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_pico_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_nano_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_nano_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_0_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_0_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_1_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_1_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_2_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_2_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_3_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_3_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_bn_0_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_bn_0_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_rmlp_nano_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_rmlp_nano_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_rmlp_0_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_rmlp_0_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_rmlp_1_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_rmlp_1_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_rmlp_1_rw2_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_rmlp_1_rw2_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_rmlp_2_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_rmlp_2_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_rmlp_2_rw_384(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_rmlp_2_rw_384', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_rmlp_3_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_rmlp_3_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_nano_cc_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_nano_cc_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnext_nano_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnext_nano_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_0_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_0_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_1_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_1_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_2_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_2_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_3_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_3_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_4_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_4_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef coatnet_5_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('coatnet_5_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_pico_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_pico_rw_256', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_nano_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_nano_rw_256', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_tiny_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_tiny_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_tiny_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_tiny_rw_256', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_rmlp_pico_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_rmlp_pico_rw_256', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_rmlp_nano_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_rmlp_nano_rw_256', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_rmlp_tiny_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_rmlp_tiny_rw_256', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_rmlp_small_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_rmlp_small_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_rmlp_small_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_rmlp_small_rw_256', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_rmlp_base_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_rmlp_base_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_rmlp_base_rw_384(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_rmlp_base_rw_384', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_tiny_pm_256(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_tiny_pm_256', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxxvit_rmlp_nano_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxxvit_rmlp_nano_rw_256', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxxvit_rmlp_tiny_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxxvit_rmlp_tiny_rw_256', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxxvit_rmlp_small_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxxvit_rmlp_small_rw_256', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxxvitv2_nano_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxxvitv2_nano_rw_256', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxxvitv2_rmlp_base_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxxvitv2_rmlp_base_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxxvitv2_rmlp_base_rw_384(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxxvitv2_rmlp_base_rw_384', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxxvitv2_rmlp_large_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxxvitv2_rmlp_large_rw_224', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_tiny_tf_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_tiny_tf_224', 'maxvit_tiny_tf', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_tiny_tf_384(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_tiny_tf_384', 'maxvit_tiny_tf', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_tiny_tf_512(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_tiny_tf_512', 'maxvit_tiny_tf', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_small_tf_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_small_tf_224', 'maxvit_small_tf', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_small_tf_384(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_small_tf_384', 'maxvit_small_tf', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_small_tf_512(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_small_tf_512', 'maxvit_small_tf', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_base_tf_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_base_tf_224', 'maxvit_base_tf', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_base_tf_384(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_base_tf_384', 'maxvit_base_tf', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_base_tf_512(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_base_tf_512', 'maxvit_base_tf', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_large_tf_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_large_tf_224', 'maxvit_large_tf', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_large_tf_384(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_large_tf_384', 'maxvit_large_tf', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_large_tf_512(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_large_tf_512', 'maxvit_large_tf', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_xlarge_tf_224(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_xlarge_tf_224', 'maxvit_xlarge_tf', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_xlarge_tf_384(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_xlarge_tf_384', 'maxvit_xlarge_tf', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef maxvit_xlarge_tf_512(pretrained=False, **kwargs) -> MaxxVit:\n    return _create_maxxvit('maxvit_xlarge_tf_512', 'maxvit_xlarge_tf', pretrained=pretrained, **kwargs)\n",
  "\"\"\" Visformer\n\nPaper: Visformer: The Vision-friendly Transformer - https://arxiv.org/abs/2104.12533\n\nFrom original at https://github.com/danczs/Visformer\n\nModifications and additions for timm hacked together by / Copyright 2021, Ross Wightman\n\"\"\"\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import to_2tuple, trunc_normal_, DropPath, PatchEmbed, LayerNorm2d, create_classifier, use_fused_attn\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['Visformer']\n\n\nclass SpatialMlp(nn.Module):\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            drop=0.,\n            group=8,\n            spatial_conv=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        drop_probs = to_2tuple(drop)\n\n        self.in_features = in_features\n        self.out_features = out_features\n        self.spatial_conv = spatial_conv\n        if self.spatial_conv:\n            if group < 2:  # net setting\n                hidden_features = in_features * 5 // 6\n            else:\n                hidden_features = in_features * 2\n        self.hidden_features = hidden_features\n        self.group = group\n        self.conv1 = nn.Conv2d(in_features, hidden_features, 1, stride=1, padding=0, bias=False)\n        self.act1 = act_layer()\n        self.drop1 = nn.Dropout(drop_probs[0])\n        if self.spatial_conv:\n            self.conv2 = nn.Conv2d(\n                hidden_features, hidden_features, 3, stride=1, padding=1, groups=self.group, bias=False)\n            self.act2 = act_layer()\n        else:\n            self.conv2 = None\n            self.act2 = None\n        self.conv3 = nn.Conv2d(hidden_features, out_features, 1, stride=1, padding=0, bias=False)\n        self.drop3 = nn.Dropout(drop_probs[1])\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.act1(x)\n        x = self.drop1(x)\n        if self.conv2 is not None:\n            x = self.conv2(x)\n            x = self.act2(x)\n        x = self.conv3(x)\n        x = self.drop3(x)\n        return x\n\n\nclass Attention(nn.Module):\n    fused_attn: torch.jit.Final[bool]\n\n    def __init__(self, dim, num_heads=8, head_dim_ratio=1., attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = round(dim // num_heads * head_dim_ratio)\n        self.head_dim = head_dim\n        self.scale = head_dim ** -0.5\n        self.fused_attn = use_fused_attn(experimental=True)\n\n        self.qkv = nn.Conv2d(dim, head_dim * num_heads * 3, 1, stride=1, padding=0, bias=False)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Conv2d(self.head_dim * self.num_heads, dim, 1, stride=1, padding=0, bias=False)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = self.qkv(x).reshape(B, 3, self.num_heads, self.head_dim, -1).permute(1, 0, 2, 4, 3)\n        q, k, v = x.unbind(0)\n\n        if self.fused_attn:\n            x = torch.nn.functional.scaled_dot_product_attention(\n                q.contiguous(), k.contiguous(), v.contiguous(),\n                dropout_p=self.attn_drop.p,\n            )\n        else:\n            attn = (q @ k.transpose(-2, -1)) * self.scale\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            x = attn @ v\n\n        x = x.permute(0, 1, 3, 2).reshape(B, -1, H, W)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            head_dim_ratio=1.,\n            mlp_ratio=4.,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=LayerNorm2d,\n            group=8,\n            attn_disabled=False,\n            spatial_conv=False,\n    ):\n        super().__init__()\n        self.spatial_conv = spatial_conv\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        if attn_disabled:\n            self.norm1 = None\n            self.attn = None\n        else:\n            self.norm1 = norm_layer(dim)\n            self.attn = Attention(\n                dim,\n                num_heads=num_heads,\n                head_dim_ratio=head_dim_ratio,\n                attn_drop=attn_drop,\n                proj_drop=proj_drop,\n            )\n\n        self.norm2 = norm_layer(dim)\n        self.mlp = SpatialMlp(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            drop=proj_drop,\n            group=group,\n            spatial_conv=spatial_conv,\n        )\n\n    def forward(self, x):\n        if self.attn is not None:\n            x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass Visformer(nn.Module):\n    def __init__(\n            self,\n            img_size=224,\n            patch_size=16,\n            in_chans=3,\n            num_classes=1000,\n            init_channels=32,\n            embed_dim=384,\n            depth=12,\n            num_heads=6,\n            mlp_ratio=4.,\n            drop_rate=0.,\n            pos_drop_rate=0.,\n            proj_drop_rate=0.,\n            attn_drop_rate=0.,\n            drop_path_rate=0.,\n            norm_layer=LayerNorm2d,\n            attn_stage='111',\n            use_pos_embed=True,\n            spatial_conv='111',\n            vit_stem=False,\n            group=8,\n            global_pool='avg',\n            conv_init=False,\n            embed_norm=None,\n    ):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        self.num_classes = num_classes\n        self.embed_dim = embed_dim\n        self.init_channels = init_channels\n        self.img_size = img_size\n        self.vit_stem = vit_stem\n        self.conv_init = conv_init\n        if isinstance(depth, (list, tuple)):\n            self.stage_num1, self.stage_num2, self.stage_num3 = depth\n            depth = sum(depth)\n        else:\n            self.stage_num1 = self.stage_num3 = depth // 3\n            self.stage_num2 = depth - self.stage_num1 - self.stage_num3\n        self.use_pos_embed = use_pos_embed\n        self.grad_checkpointing = False\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n        # stage 1\n        if self.vit_stem:\n            self.stem = None\n            self.patch_embed1 = PatchEmbed(\n                img_size=img_size,\n                patch_size=patch_size,\n                in_chans=in_chans,\n                embed_dim=embed_dim,\n                norm_layer=embed_norm,\n                flatten=False,\n            )\n            img_size = [x // patch_size for x in img_size]\n        else:\n            if self.init_channels is None:\n                self.stem = None\n                self.patch_embed1 = PatchEmbed(\n                    img_size=img_size,\n                    patch_size=patch_size // 2,\n                    in_chans=in_chans,\n                    embed_dim=embed_dim // 2,\n                    norm_layer=embed_norm,\n                    flatten=False,\n                )\n                img_size = [x // (patch_size // 2) for x in img_size]\n            else:\n                self.stem = nn.Sequential(\n                    nn.Conv2d(in_chans, self.init_channels, 7, stride=2, padding=3, bias=False),\n                    nn.BatchNorm2d(self.init_channels),\n                    nn.ReLU(inplace=True)\n                )\n                img_size = [x // 2 for x in img_size]\n                self.patch_embed1 = PatchEmbed(\n                    img_size=img_size,\n                    patch_size=patch_size // 4,\n                    in_chans=self.init_channels,\n                    embed_dim=embed_dim // 2,\n                    norm_layer=embed_norm,\n                    flatten=False,\n                )\n                img_size = [x // (patch_size // 4) for x in img_size]\n\n        if self.use_pos_embed:\n            if self.vit_stem:\n                self.pos_embed1 = nn.Parameter(torch.zeros(1, embed_dim, *img_size))\n            else:\n                self.pos_embed1 = nn.Parameter(torch.zeros(1, embed_dim//2, *img_size))\n            self.pos_drop = nn.Dropout(p=pos_drop_rate)\n        else:\n            self.pos_embed1 = None\n\n        self.stage1 = nn.Sequential(*[\n            Block(\n                dim=embed_dim//2,\n                num_heads=num_heads,\n                head_dim_ratio=0.5,\n                mlp_ratio=mlp_ratio,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n                group=group,\n                attn_disabled=(attn_stage[0] == '0'),\n                spatial_conv=(spatial_conv[0] == '1'),\n            )\n            for i in range(self.stage_num1)\n        ])\n\n        # stage2\n        if not self.vit_stem:\n            self.patch_embed2 = PatchEmbed(\n                img_size=img_size,\n                patch_size=patch_size // 8,\n                in_chans=embed_dim // 2,\n                embed_dim=embed_dim,\n                norm_layer=embed_norm,\n                flatten=False,\n            )\n            img_size = [x // (patch_size // 8) for x in img_size]\n            if self.use_pos_embed:\n                self.pos_embed2 = nn.Parameter(torch.zeros(1, embed_dim, *img_size))\n            else:\n                self.pos_embed2 = None\n        else:\n            self.patch_embed2 = None\n        self.stage2 = nn.Sequential(*[\n            Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                head_dim_ratio=1.0,\n                mlp_ratio=mlp_ratio,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n                group=group,\n                attn_disabled=(attn_stage[1] == '0'),\n                spatial_conv=(spatial_conv[1] == '1'),\n            )\n            for i in range(self.stage_num1, self.stage_num1+self.stage_num2)\n        ])\n\n        # stage 3\n        if not self.vit_stem:\n            self.patch_embed3 = PatchEmbed(\n                img_size=img_size,\n                patch_size=patch_size // 8,\n                in_chans=embed_dim,\n                embed_dim=embed_dim * 2,\n                norm_layer=embed_norm,\n                flatten=False,\n            )\n            img_size = [x // (patch_size // 8) for x in img_size]\n            if self.use_pos_embed:\n                self.pos_embed3 = nn.Parameter(torch.zeros(1, embed_dim*2, *img_size))\n            else:\n                self.pos_embed3 = None\n        else:\n            self.patch_embed3 = None\n        self.stage3 = nn.Sequential(*[\n            Block(\n                dim=embed_dim * 2,\n                num_heads=num_heads,\n                head_dim_ratio=1.0,\n                mlp_ratio=mlp_ratio,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n                group=group,\n                attn_disabled=(attn_stage[2] == '0'),\n                spatial_conv=(spatial_conv[2] == '1'),\n            )\n            for i in range(self.stage_num1+self.stage_num2, depth)\n        ])\n\n        self.num_features = embed_dim if self.vit_stem else embed_dim * 2\n        self.norm = norm_layer(self.num_features)\n\n        # head\n        global_pool, head = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)\n        self.global_pool = global_pool\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = head\n\n        # weights init\n        if self.use_pos_embed:\n            trunc_normal_(self.pos_embed1, std=0.02)\n            if not self.vit_stem:\n                trunc_normal_(self.pos_embed2, std=0.02)\n                trunc_normal_(self.pos_embed3, std=0.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.Conv2d):\n            if self.conv_init:\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            else:\n                trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0.)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^patch_embed1|pos_embed1|stem',  # stem and embed\n            blocks=[\n                (r'^stage(\\d+)\\.(\\d+)' if coarse else r'^stage(\\d+)\\.(\\d+)', None),\n                (r'^(?:patch_embed|pos_embed)(\\d+)', (0,)),\n                (r'^norm', (99999,))\n            ]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.global_pool, self.head = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        if self.stem is not None:\n            x = self.stem(x)\n\n        # stage 1\n        x = self.patch_embed1(x)\n        if self.pos_embed1 is not None:\n            x = self.pos_drop(x + self.pos_embed1)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.stage1, x)\n        else:\n            x = self.stage1(x)\n\n        # stage 2\n        if self.patch_embed2 is not None:\n            x = self.patch_embed2(x)\n            if self.pos_embed2 is not None:\n                x = self.pos_drop(x + self.pos_embed2)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.stage2, x)\n        else:\n            x = self.stage2(x)\n\n        # stage3\n        if self.patch_embed3 is not None:\n            x = self.patch_embed3(x)\n            if self.pos_embed3 is not None:\n                x = self.pos_drop(x + self.pos_embed3)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.stage3, x)\n        else:\n            x = self.stage3(x)\n\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        x = self.global_pool(x)\n        x = self.head_drop(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_visformer(variant, pretrained=False, default_cfg=None, **kwargs):\n    if kwargs.get('features_only', None):\n        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n    model = build_model_with_cfg(Visformer, variant, pretrained, **kwargs)\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.0', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'visformer_tiny.in1k': _cfg(hf_hub_id='timm/'),\n    'visformer_small.in1k': _cfg(hf_hub_id='timm/'),\n})\n\n\n@register_model\ndef visformer_tiny(pretrained=False, **kwargs) -> Visformer:\n    model_cfg = dict(\n        init_channels=16, embed_dim=192, depth=(7, 4, 4), num_heads=3, mlp_ratio=4., group=8,\n        attn_stage='011', spatial_conv='100', norm_layer=nn.BatchNorm2d, conv_init=True,\n        embed_norm=nn.BatchNorm2d)\n    model = _create_visformer('visformer_tiny', pretrained=pretrained, **dict(model_cfg, **kwargs))\n    return model\n\n\n@register_model\ndef visformer_small(pretrained=False, **kwargs) -> Visformer:\n    model_cfg = dict(\n        init_channels=32, embed_dim=384, depth=(7, 4, 4), num_heads=6, mlp_ratio=4., group=8,\n        attn_stage='011', spatial_conv='100', norm_layer=nn.BatchNorm2d, conv_init=True,\n        embed_norm=nn.BatchNorm2d)\n    model = _create_visformer('visformer_small', pretrained=pretrained, **dict(model_cfg, **kwargs))\n    return model\n\n\n# @register_model\n# def visformer_net1(pretrained=False, **kwargs):\n#     model = Visformer(\n#         init_channels=None, embed_dim=384, depth=(0, 12, 0), num_heads=6, mlp_ratio=4., attn_stage='111',\n#         spatial_conv='000', vit_stem=True, conv_init=True, **kwargs)\n#     model.default_cfg = _cfg()\n#     return model\n#\n#\n# @register_model\n# def visformer_net2(pretrained=False, **kwargs):\n#     model = Visformer(\n#         init_channels=32, embed_dim=384, depth=(0, 12, 0), num_heads=6, mlp_ratio=4., attn_stage='111',\n#         spatial_conv='000', vit_stem=False, conv_init=True, **kwargs)\n#     model.default_cfg = _cfg()\n#     return model\n#\n#\n# @register_model\n# def visformer_net3(pretrained=False, **kwargs):\n#     model = Visformer(\n#         init_channels=32, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4., attn_stage='111',\n#         spatial_conv='000', vit_stem=False, conv_init=True, **kwargs)\n#     model.default_cfg = _cfg()\n#     return model\n#\n#\n# @register_model\n# def visformer_net4(pretrained=False, **kwargs):\n#     model = Visformer(\n#         init_channels=32, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4., attn_stage='111',\n#         spatial_conv='000', vit_stem=False, conv_init=True, **kwargs)\n#     model.default_cfg = _cfg()\n#     return model\n#\n#\n# @register_model\n# def visformer_net5(pretrained=False, **kwargs):\n#     model = Visformer(\n#         init_channels=32, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4., group=1, attn_stage='111',\n#         spatial_conv='111', vit_stem=False, conv_init=True, **kwargs)\n#     model.default_cfg = _cfg()\n#     return model\n#\n#\n# @register_model\n# def visformer_net6(pretrained=False, **kwargs):\n#     model = Visformer(\n#         init_channels=32, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4., group=1, attn_stage='111',\n#         pos_embed=False, spatial_conv='111', conv_init=True, **kwargs)\n#     model.default_cfg = _cfg()\n#     return model\n#\n#\n# @register_model\n# def visformer_net7(pretrained=False, **kwargs):\n#     model = Visformer(\n#         init_channels=32, embed_dim=384, depth=(6, 7, 7), num_heads=6, group=1, attn_stage='000',\n#         pos_embed=False, spatial_conv='111', conv_init=True, **kwargs)\n#     model.default_cfg = _cfg()\n#     return model\n\n\n\n\n",
  "\"\"\" EdgeNeXt\n\nPaper: `EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications`\n - https://arxiv.org/abs/2206.10589\n\nOriginal code and weights from https://github.com/mmaaz60/EdgeNeXt\n\nModifications and additions for timm by / Copyright 2022, Ross Wightman\n\"\"\"\nimport math\nfrom collections import OrderedDict\nfrom functools import partial\nfrom typing import Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import trunc_normal_tf_, DropPath, LayerNorm2d, Mlp, SelectAdaptivePool2d, create_conv2d, \\\n    use_fused_attn\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_module\nfrom ._manipulate import named_apply, checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['EdgeNeXt']  # model_registry will add each entrypoint fn to this\n\n\n@register_notrace_module  # reason: FX can't symbolically trace torch.arange in forward method\nclass PositionalEncodingFourier(nn.Module):\n    def __init__(self, hidden_dim=32, dim=768, temperature=10000):\n        super().__init__()\n        self.token_projection = nn.Conv2d(hidden_dim * 2, dim, kernel_size=1)\n        self.scale = 2 * math.pi\n        self.temperature = temperature\n        self.hidden_dim = hidden_dim\n        self.dim = dim\n\n    def forward(self, shape: Tuple[int, int, int]):\n        device = self.token_projection.weight.device\n        dtype = self.token_projection.weight.dtype\n        inv_mask = ~torch.zeros(shape).to(device=device, dtype=torch.bool)\n        y_embed = inv_mask.cumsum(1, dtype=dtype)\n        x_embed = inv_mask.cumsum(2, dtype=dtype)\n        eps = 1e-6\n        y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n        x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n\n        dim_t = torch.arange(self.hidden_dim, dtype=dtype, device=device)\n        dim_t = self.temperature ** (2 * torch.div(dim_t, 2, rounding_mode='floor') / self.hidden_dim)\n\n        pos_x = x_embed[:, :, :, None] / dim_t\n        pos_y = y_embed[:, :, :, None] / dim_t\n        pos_x = torch.stack(\n            (pos_x[:, :, :, 0::2].sin(),\n             pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n        pos_y = torch.stack(\n            (pos_y[:, :, :, 0::2].sin(),\n             pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n        pos = self.token_projection(pos)\n\n        return pos\n\n\nclass ConvBlock(nn.Module):\n    def __init__(\n            self,\n            dim,\n            dim_out=None,\n            kernel_size=7,\n            stride=1,\n            conv_bias=True,\n            expand_ratio=4,\n            ls_init_value=1e-6,\n            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n            act_layer=nn.GELU, drop_path=0.,\n    ):\n        super().__init__()\n        dim_out = dim_out or dim\n        self.shortcut_after_dw = stride > 1 or dim != dim_out\n\n        self.conv_dw = create_conv2d(\n            dim, dim_out, kernel_size=kernel_size, stride=stride, depthwise=True, bias=conv_bias)\n        self.norm = norm_layer(dim_out)\n        self.mlp = Mlp(dim_out, int(expand_ratio * dim_out), act_layer=act_layer)\n        self.gamma = nn.Parameter(ls_init_value * torch.ones(dim_out)) if ls_init_value > 0 else None\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv_dw(x)\n        if self.shortcut_after_dw:\n            shortcut = x\n\n        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n        x = self.norm(x)\n        x = self.mlp(x)\n        if self.gamma is not None:\n            x = self.gamma * x\n        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n\n        x = shortcut + self.drop_path(x)\n        return x\n\n\nclass CrossCovarianceAttn(nn.Module):\n    def __init__(\n            self,\n            dim,\n            num_heads=8,\n            qkv_bias=False,\n            attn_drop=0.,\n            proj_drop=0.\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 4, 1)\n        q, k, v = qkv.unbind(0)\n\n        # NOTE, this is NOT spatial attn, q, k, v are B, num_heads, C, L -->  C x C attn map\n        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)) * self.temperature\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        x = (attn @ v)\n\n        x = x.permute(0, 3, 1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'temperature'}\n\n\nclass SplitTransposeBlock(nn.Module):\n    def __init__(\n            self,\n            dim,\n            num_scales=1,\n            num_heads=8,\n            expand_ratio=4,\n            use_pos_emb=True,\n            conv_bias=True,\n            qkv_bias=True,\n            ls_init_value=1e-6,\n            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n            act_layer=nn.GELU,\n            drop_path=0.,\n            attn_drop=0.,\n            proj_drop=0.\n    ):\n        super().__init__()\n        width = max(int(math.ceil(dim / num_scales)), int(math.floor(dim // num_scales)))\n        self.width = width\n        self.num_scales = max(1, num_scales - 1)\n\n        convs = []\n        for i in range(self.num_scales):\n            convs.append(create_conv2d(width, width, kernel_size=3, depthwise=True, bias=conv_bias))\n        self.convs = nn.ModuleList(convs)\n\n        self.pos_embd = None\n        if use_pos_emb:\n            self.pos_embd = PositionalEncodingFourier(dim=dim)\n        self.norm_xca = norm_layer(dim)\n        self.gamma_xca = nn.Parameter(ls_init_value * torch.ones(dim)) if ls_init_value > 0 else None\n        self.xca = CrossCovarianceAttn(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=proj_drop)\n\n        self.norm = norm_layer(dim, eps=1e-6)\n        self.mlp = Mlp(dim, int(expand_ratio * dim), act_layer=act_layer)\n        self.gamma = nn.Parameter(ls_init_value * torch.ones(dim)) if ls_init_value > 0 else None\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        shortcut = x\n\n        # scales code re-written for torchscript as per my res2net fixes -rw\n        # NOTE torch.split(x, self.width, 1) causing issues with ONNX export\n        spx = x.chunk(len(self.convs) + 1, dim=1)\n        spo = []\n        sp = spx[0]\n        for i, conv in enumerate(self.convs):\n            if i > 0:\n                sp = sp + spx[i]\n            sp = conv(sp)\n            spo.append(sp)\n        spo.append(spx[-1])\n        x = torch.cat(spo, 1)\n\n        # XCA\n        B, C, H, W = x.shape\n        x = x.reshape(B, C, H * W).permute(0, 2, 1)\n        if self.pos_embd is not None:\n            pos_encoding = self.pos_embd((B, H, W)).reshape(B, -1, x.shape[1]).permute(0, 2, 1)\n            x = x + pos_encoding\n        x = x + self.drop_path(self.gamma_xca * self.xca(self.norm_xca(x)))\n        x = x.reshape(B, H, W, C)\n\n        # Inverted Bottleneck\n        x = self.norm(x)\n        x = self.mlp(x)\n        if self.gamma is not None:\n            x = self.gamma * x\n        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n\n        x = shortcut + self.drop_path(x)\n        return x\n\n\nclass EdgeNeXtStage(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            stride=2,\n            depth=2,\n            num_global_blocks=1,\n            num_heads=4,\n            scales=2,\n            kernel_size=7,\n            expand_ratio=4,\n            use_pos_emb=False,\n            downsample_block=False,\n            conv_bias=True,\n            ls_init_value=1.0,\n            drop_path_rates=None,\n            norm_layer=LayerNorm2d,\n            norm_layer_cl=partial(nn.LayerNorm, eps=1e-6),\n            act_layer=nn.GELU\n    ):\n        super().__init__()\n        self.grad_checkpointing = False\n\n        if downsample_block or stride == 1:\n            self.downsample = nn.Identity()\n        else:\n            self.downsample = nn.Sequential(\n                norm_layer(in_chs),\n                nn.Conv2d(in_chs, out_chs, kernel_size=2, stride=2, bias=conv_bias)\n            )\n            in_chs = out_chs\n\n        stage_blocks = []\n        for i in range(depth):\n            if i < depth - num_global_blocks:\n                stage_blocks.append(\n                    ConvBlock(\n                        dim=in_chs,\n                        dim_out=out_chs,\n                        stride=stride if downsample_block and i == 0 else 1,\n                        conv_bias=conv_bias,\n                        kernel_size=kernel_size,\n                        expand_ratio=expand_ratio,\n                        ls_init_value=ls_init_value,\n                        drop_path=drop_path_rates[i],\n                        norm_layer=norm_layer_cl,\n                        act_layer=act_layer,\n                    )\n                )\n            else:\n                stage_blocks.append(\n                    SplitTransposeBlock(\n                        dim=in_chs,\n                        num_scales=scales,\n                        num_heads=num_heads,\n                        expand_ratio=expand_ratio,\n                        use_pos_emb=use_pos_emb,\n                        conv_bias=conv_bias,\n                        ls_init_value=ls_init_value,\n                        drop_path=drop_path_rates[i],\n                        norm_layer=norm_layer_cl,\n                        act_layer=act_layer,\n                    )\n                )\n            in_chs = out_chs\n        self.blocks = nn.Sequential(*stage_blocks)\n\n    def forward(self, x):\n        x = self.downsample(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        return x\n\n\nclass EdgeNeXt(nn.Module):\n    def __init__(\n            self,\n            in_chans=3,\n            num_classes=1000,\n            global_pool='avg',\n            dims=(24, 48, 88, 168),\n            depths=(3, 3, 9, 3),\n            global_block_counts=(0, 1, 1, 1),\n            kernel_sizes=(3, 5, 7, 9),\n            heads=(8, 8, 8, 8),\n            d2_scales=(2, 2, 3, 4),\n            use_pos_emb=(False, True, False, False),\n            ls_init_value=1e-6,\n            head_init_scale=1.,\n            expand_ratio=4,\n            downsample_block=False,\n            conv_bias=True,\n            stem_type='patch',\n            head_norm_first=False,\n            act_layer=nn.GELU,\n            drop_path_rate=0.,\n            drop_rate=0.,\n    ):\n        super().__init__()\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.drop_rate = drop_rate\n        norm_layer = partial(LayerNorm2d, eps=1e-6)\n        norm_layer_cl = partial(nn.LayerNorm, eps=1e-6)\n        self.feature_info = []\n\n        assert stem_type in ('patch', 'overlap')\n        if stem_type == 'patch':\n            self.stem = nn.Sequential(\n                nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4, bias=conv_bias),\n                norm_layer(dims[0]),\n            )\n        else:\n            self.stem = nn.Sequential(\n                nn.Conv2d(in_chans, dims[0], kernel_size=9, stride=4, padding=9 // 2, bias=conv_bias),\n                norm_layer(dims[0]),\n            )\n\n        curr_stride = 4\n        stages = []\n        dp_rates = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n        in_chs = dims[0]\n        for i in range(4):\n            stride = 2 if curr_stride == 2 or i > 0 else 1\n            # FIXME support dilation / output_stride\n            curr_stride *= stride\n            stages.append(EdgeNeXtStage(\n                in_chs=in_chs,\n                out_chs=dims[i],\n                stride=stride,\n                depth=depths[i],\n                num_global_blocks=global_block_counts[i],\n                num_heads=heads[i],\n                drop_path_rates=dp_rates[i],\n                scales=d2_scales[i],\n                expand_ratio=expand_ratio,\n                kernel_size=kernel_sizes[i],\n                use_pos_emb=use_pos_emb[i],\n                ls_init_value=ls_init_value,\n                downsample_block=downsample_block,\n                conv_bias=conv_bias,\n                norm_layer=norm_layer,\n                norm_layer_cl=norm_layer_cl,\n                act_layer=act_layer,\n            ))\n            # NOTE feature_info use currently assumes stage 0 == stride 1, rest are stride 2\n            in_chs = dims[i]\n            self.feature_info += [dict(num_chs=in_chs, reduction=curr_stride, module=f'stages.{i}')]\n\n        self.stages = nn.Sequential(*stages)\n\n        self.num_features = dims[-1]\n        self.norm_pre = norm_layer(self.num_features) if head_norm_first else nn.Identity()\n        self.head = nn.Sequential(OrderedDict([\n                ('global_pool', SelectAdaptivePool2d(pool_type=global_pool)),\n                ('norm', nn.Identity() if head_norm_first else norm_layer(self.num_features)),\n                ('flatten', nn.Flatten(1) if global_pool else nn.Identity()),\n                ('drop', nn.Dropout(self.drop_rate)),\n                ('fc', nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity())]))\n\n        named_apply(partial(_init_weights, head_init_scale=head_init_scale), self)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^stem',\n            blocks=r'^stages\\.(\\d+)' if coarse else [\n                (r'^stages\\.(\\d+)\\.downsample', (0,)),  # blocks\n                (r'^stages\\.(\\d+)\\.blocks\\.(\\d+)', None),\n                (r'^norm_pre', (99999,))\n            ]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        for s in self.stages:\n            s.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes=0, global_pool=None):\n        if global_pool is not None:\n            self.head.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n            self.head.flatten = nn.Flatten(1) if global_pool else nn.Identity()\n        self.head.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        x = self.stages(x)\n        x = self.norm_pre(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        # NOTE nn.Sequential in head broken down since can't call head[:-1](x) in torchscript :(\n        x = self.head.global_pool(x)\n        x = self.head.norm(x)\n        x = self.head.flatten(x)\n        x = self.head.drop(x)\n        return x if pre_logits else self.head.fc(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _init_weights(module, name=None, head_init_scale=1.0):\n    if isinstance(module, nn.Conv2d):\n        trunc_normal_tf_(module.weight, std=.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Linear):\n        trunc_normal_tf_(module.weight, std=.02)\n        nn.init.zeros_(module.bias)\n        if name and 'head.' in name:\n            module.weight.data.mul_(head_init_scale)\n            module.bias.data.mul_(head_init_scale)\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    \"\"\" Remap FB checkpoints -> timm \"\"\"\n    if 'head.norm.weight' in state_dict or 'norm_pre.weight' in state_dict:\n        return state_dict  # non-FB checkpoint\n\n    # models were released as train checkpoints... :/\n    if 'model_ema' in state_dict:\n        state_dict = state_dict['model_ema']\n    elif 'model' in state_dict:\n        state_dict = state_dict['model']\n    elif 'state_dict' in state_dict:\n        state_dict = state_dict['state_dict']\n\n    out_dict = {}\n    import re\n    for k, v in state_dict.items():\n        k = k.replace('downsample_layers.0.', 'stem.')\n        k = re.sub(r'stages.([0-9]+).([0-9]+)', r'stages.\\1.blocks.\\2', k)\n        k = re.sub(r'downsample_layers.([0-9]+).([0-9]+)', r'stages.\\1.downsample.\\2', k)\n        k = k.replace('dwconv', 'conv_dw')\n        k = k.replace('pwconv', 'mlp.fc')\n        k = k.replace('head.', 'head.fc.')\n        if k.startswith('norm.'):\n            k = k.replace('norm', 'head.norm')\n        if v.ndim == 2 and 'head' not in k:\n            model_shape = model.state_dict()[k].shape\n            v = v.reshape(model_shape)\n        out_dict[k] = v\n    return out_dict\n\n\ndef _create_edgenext(variant, pretrained=False, **kwargs):\n    model = build_model_with_cfg(\n        EdgeNeXt, variant, pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        feature_cfg=dict(out_indices=(0, 1, 2, 3), flatten_sequential=True),\n        **kwargs)\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': (8, 8),\n        'crop_pct': 0.9, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.0', 'classifier': 'head.fc',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'edgenext_xx_small.in1k': _cfg(\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'edgenext_x_small.in1k': _cfg(\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'edgenext_small.usi_in1k': _cfg(  # USI weights\n        hf_hub_id='timm/',\n        crop_pct=0.95, test_input_size=(3, 320, 320), test_crop_pct=1.0,\n    ),\n    'edgenext_base.usi_in1k': _cfg(  # USI weights\n        hf_hub_id='timm/',\n        crop_pct=0.95, test_input_size=(3, 320, 320), test_crop_pct=1.0,\n    ),\n    'edgenext_base.in21k_ft_in1k': _cfg(  # USI weights\n        hf_hub_id='timm/',\n        crop_pct=0.95, test_input_size=(3, 320, 320), test_crop_pct=1.0,\n    ),\n    'edgenext_small_rw.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        test_input_size=(3, 320, 320), test_crop_pct=1.0,\n    ),\n})\n\n\n@register_model\ndef edgenext_xx_small(pretrained=False, **kwargs) -> EdgeNeXt:\n    # 1.33M & 260.58M @ 256 resolution\n    # 71.23% Top-1 accuracy\n    # No AA, Color Jitter=0.4, No Mixup & Cutmix, DropPath=0.0, BS=4096, lr=0.006, multi-scale-sampler\n    # Jetson FPS=51.66 versus 47.67 for MobileViT_XXS\n    # For A100: FPS @ BS=1: 212.13 & @ BS=256: 7042.06 versus FPS @ BS=1: 96.68 & @ BS=256: 4624.71 for MobileViT_XXS\n    model_kwargs = dict(depths=(2, 2, 6, 2), dims=(24, 48, 88, 168), heads=(4, 4, 4, 4), **kwargs)\n    return _create_edgenext('edgenext_xx_small', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef edgenext_x_small(pretrained=False, **kwargs) -> EdgeNeXt:\n    # 2.34M & 538.0M @ 256 resolution\n    # 75.00% Top-1 accuracy\n    # No AA, No Mixup & Cutmix, DropPath=0.0, BS=4096, lr=0.006, multi-scale-sampler\n    # Jetson FPS=31.61 versus 28.49 for MobileViT_XS\n    # For A100: FPS @ BS=1: 179.55 & @ BS=256: 4404.95 versus FPS @ BS=1: 94.55 & @ BS=256: 2361.53 for MobileViT_XS\n    model_kwargs = dict(depths=(3, 3, 9, 3), dims=(32, 64, 100, 192), heads=(4, 4, 4, 4), **kwargs)\n    return _create_edgenext('edgenext_x_small', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef edgenext_small(pretrained=False, **kwargs) -> EdgeNeXt:\n    # 5.59M & 1260.59M @ 256 resolution\n    # 79.43% Top-1 accuracy\n    # AA=True, No Mixup & Cutmix, DropPath=0.1, BS=4096, lr=0.006, multi-scale-sampler\n    # Jetson FPS=20.47 versus 18.86 for MobileViT_S\n    # For A100: FPS @ BS=1: 172.33 & @ BS=256: 3010.25 versus FPS @ BS=1: 93.84 & @ BS=256: 1785.92 for MobileViT_S\n    model_kwargs = dict(depths=(3, 3, 9, 3), dims=(48, 96, 160, 304),  **kwargs)\n    return _create_edgenext('edgenext_small', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef edgenext_base(pretrained=False, **kwargs) -> EdgeNeXt:\n    # 18.51M & 3840.93M @ 256 resolution\n    # 82.5% (normal) 83.7% (USI) Top-1 accuracy\n    # AA=True, Mixup & Cutmix, DropPath=0.1, BS=4096, lr=0.006, multi-scale-sampler\n    # Jetson FPS=xx.xx versus xx.xx for MobileViT_S\n    # For A100: FPS @ BS=1: xxx.xx & @ BS=256: xxxx.xx\n    model_kwargs = dict(depths=[3, 3, 9, 3], dims=[80, 160, 288, 584], **kwargs)\n    return _create_edgenext('edgenext_base', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef edgenext_small_rw(pretrained=False, **kwargs) -> EdgeNeXt:\n    model_kwargs = dict(\n        depths=(3, 3, 9, 3), dims=(48, 96, 192, 384),\n        downsample_block=True, conv_bias=False, stem_type='overlap', **kwargs)\n    return _create_edgenext('edgenext_small_rw', pretrained=pretrained, **model_kwargs)\n\n",
  "\"\"\" Selective Kernel Networks (ResNet base)\n\nPaper: Selective Kernel Networks (https://arxiv.org/abs/1903.06586)\n\nThis was inspired by reading 'Compounding the Performance Improvements...' (https://arxiv.org/abs/2001.06268)\nand a streamlined impl at https://github.com/clovaai/assembled-cnn but I ended up building something closer\nto the original paper with some modifications of my own to better balance param count vs accuracy.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\n\nfrom torch import nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import SelectiveKernel, ConvNormAct, create_attn\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\nfrom .resnet import ResNet\n\n\nclass SelectiveKernelBasic(nn.Module):\n    expansion = 1\n\n    def __init__(\n            self,\n            inplanes,\n            planes,\n            stride=1,\n            downsample=None,\n            cardinality=1,\n            base_width=64,\n            sk_kwargs=None,\n            reduce_first=1,\n            dilation=1,\n            first_dilation=None,\n            act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d,\n            attn_layer=None,\n            aa_layer=None,\n            drop_block=None,\n            drop_path=None,\n    ):\n        super(SelectiveKernelBasic, self).__init__()\n\n        sk_kwargs = sk_kwargs or {}\n        conv_kwargs = dict(act_layer=act_layer, norm_layer=norm_layer)\n        assert cardinality == 1, 'BasicBlock only supports cardinality of 1'\n        assert base_width == 64, 'BasicBlock doest not support changing base width'\n        first_planes = planes // reduce_first\n        outplanes = planes * self.expansion\n        first_dilation = first_dilation or dilation\n\n        self.conv1 = SelectiveKernel(\n            inplanes, first_planes, stride=stride, dilation=first_dilation,\n            aa_layer=aa_layer, drop_layer=drop_block, **conv_kwargs, **sk_kwargs)\n        self.conv2 = ConvNormAct(\n            first_planes, outplanes, kernel_size=3, dilation=dilation, apply_act=False, **conv_kwargs)\n        self.se = create_attn(attn_layer, outplanes)\n        self.act = act_layer(inplace=True)\n        self.downsample = downsample\n        self.drop_path = drop_path\n\n    def zero_init_last(self):\n        if getattr(self.conv2.bn, 'weight', None) is not None:\n            nn.init.zeros_(self.conv2.bn.weight)\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv1(x)\n        x = self.conv2(x)\n        if self.se is not None:\n            x = self.se(x)\n        if self.drop_path is not None:\n            x = self.drop_path(x)\n        if self.downsample is not None:\n            shortcut = self.downsample(shortcut)\n        x += shortcut\n        x = self.act(x)\n        return x\n\n\nclass SelectiveKernelBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(\n            self,\n            inplanes,\n            planes,\n            stride=1,\n            downsample=None,\n            cardinality=1,\n            base_width=64,\n            sk_kwargs=None,\n            reduce_first=1,\n            dilation=1,\n            first_dilation=None,\n            act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d,\n            attn_layer=None,\n            aa_layer=None,\n            drop_block=None,\n            drop_path=None,\n    ):\n        super(SelectiveKernelBottleneck, self).__init__()\n\n        sk_kwargs = sk_kwargs or {}\n        conv_kwargs = dict(act_layer=act_layer, norm_layer=norm_layer)\n        width = int(math.floor(planes * (base_width / 64)) * cardinality)\n        first_planes = width // reduce_first\n        outplanes = planes * self.expansion\n        first_dilation = first_dilation or dilation\n\n        self.conv1 = ConvNormAct(inplanes, first_planes, kernel_size=1, **conv_kwargs)\n        self.conv2 = SelectiveKernel(\n            first_planes, width, stride=stride, dilation=first_dilation, groups=cardinality,\n            aa_layer=aa_layer, drop_layer=drop_block, **conv_kwargs, **sk_kwargs)\n        self.conv3 = ConvNormAct(width, outplanes, kernel_size=1, apply_act=False, **conv_kwargs)\n        self.se = create_attn(attn_layer, outplanes)\n        self.act = act_layer(inplace=True)\n        self.downsample = downsample\n        self.drop_path = drop_path\n\n    def zero_init_last(self):\n        if getattr(self.conv3.bn, 'weight', None) is not None:\n            nn.init.zeros_(self.conv3.bn.weight)\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        if self.se is not None:\n            x = self.se(x)\n        if self.drop_path is not None:\n            x = self.drop_path(x)\n        if self.downsample is not None:\n            shortcut = self.downsample(shortcut)\n        x += shortcut\n        x = self.act(x)\n        return x\n\n\ndef _create_skresnet(variant, pretrained=False, **kwargs):\n    return build_model_with_cfg(\n        ResNet,\n        variant,\n        pretrained,\n        **kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'conv1', 'classifier': 'fc',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'skresnet18.ra_in1k': _cfg(hf_hub_id='timm/'),\n    'skresnet34.ra_in1k': _cfg(hf_hub_id='timm/'),\n    'skresnet50.untrained': _cfg(),\n    'skresnet50d.untrained': _cfg(\n        first_conv='conv1.0'),\n    'skresnext50_32x4d.ra_in1k': _cfg(hf_hub_id='timm/'),\n})\n\n\n@register_model\ndef skresnet18(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a Selective Kernel ResNet-18 model.\n\n    Different from configs in Select Kernel paper or \"Compounding the Performance Improvements...\" this\n    variation splits the input channels to the selective convolutions to keep param count down.\n    \"\"\"\n    sk_kwargs = dict(rd_ratio=1 / 8, rd_divisor=16, split_input=True)\n    model_args = dict(\n        block=SelectiveKernelBasic, layers=[2, 2, 2, 2], block_args=dict(sk_kwargs=sk_kwargs),\n        zero_init_last=False, **kwargs)\n    return _create_skresnet('skresnet18', pretrained, **model_args)\n\n\n@register_model\ndef skresnet34(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a Selective Kernel ResNet-34 model.\n\n    Different from configs in Select Kernel paper or \"Compounding the Performance Improvements...\" this\n    variation splits the input channels to the selective convolutions to keep param count down.\n    \"\"\"\n    sk_kwargs = dict(rd_ratio=1 / 8, rd_divisor=16, split_input=True)\n    model_args = dict(\n        block=SelectiveKernelBasic, layers=[3, 4, 6, 3], block_args=dict(sk_kwargs=sk_kwargs),\n        zero_init_last=False, **kwargs)\n    return _create_skresnet('skresnet34', pretrained, **model_args)\n\n\n@register_model\ndef skresnet50(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a Select Kernel ResNet-50 model.\n\n    Different from configs in Select Kernel paper or \"Compounding the Performance Improvements...\" this\n    variation splits the input channels to the selective convolutions to keep param count down.\n    \"\"\"\n    sk_kwargs = dict(split_input=True)\n    model_args = dict(\n        block=SelectiveKernelBottleneck, layers=[3, 4, 6, 3], block_args=dict(sk_kwargs=sk_kwargs),\n        zero_init_last=False, **kwargs)\n    return _create_skresnet('skresnet50', pretrained, **model_args)\n\n\n@register_model\ndef skresnet50d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a Select Kernel ResNet-50-D model.\n\n    Different from configs in Select Kernel paper or \"Compounding the Performance Improvements...\" this\n    variation splits the input channels to the selective convolutions to keep param count down.\n    \"\"\"\n    sk_kwargs = dict(split_input=True)\n    model_args = dict(\n        block=SelectiveKernelBottleneck, layers=[3, 4, 6, 3], stem_width=32, stem_type='deep', avg_down=True,\n        block_args=dict(sk_kwargs=sk_kwargs), zero_init_last=False, **kwargs)\n    return _create_skresnet('skresnet50d', pretrained, **model_args)\n\n\n@register_model\ndef skresnext50_32x4d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"Constructs a Select Kernel ResNeXt50-32x4d model. This should be equivalent to\n    the SKNet-50 model in the Select Kernel Paper\n    \"\"\"\n    sk_kwargs = dict(rd_ratio=1/16, rd_divisor=32, split_input=False)\n    model_args = dict(\n        block=SelectiveKernelBottleneck, layers=[3, 4, 6, 3], cardinality=32, base_width=4,\n        block_args=dict(sk_kwargs=sk_kwargs), zero_init_last=False, **kwargs)\n    return _create_skresnet('skresnext50_32x4d', pretrained, **model_args)\n\n",
  "\"\"\" ReXNet\n\nA PyTorch impl of `ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network` -\nhttps://arxiv.org/abs/2007.00992\n\nAdapted from original impl at https://github.com/clovaai/rexnet\nCopyright (c) 2020-present NAVER Corp. MIT license\n\nChanges for timm, feature extraction, and rounded channel variant hacked together by Ross Wightman\nCopyright 2020 Ross Wightman\n\"\"\"\n\nfrom functools import partial\nfrom math import ceil\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import ClassifierHead, create_act_layer, ConvNormAct, DropPath, make_divisible, SEModule\nfrom ._builder import build_model_with_cfg\nfrom ._efficientnet_builder import efficientnet_init_weights\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import generate_default_cfgs, register_model\n\n__all__ = ['RexNet']  # model_registry will add each entrypoint fn to this\n\n\nSEWithNorm = partial(SEModule, norm_layer=nn.BatchNorm2d)\n\n\nclass LinearBottleneck(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            stride,\n            dilation=(1, 1),\n            exp_ratio=1.0,\n            se_ratio=0.,\n            ch_div=1,\n            act_layer='swish',\n            dw_act_layer='relu6',\n            drop_path=None,\n    ):\n        super(LinearBottleneck, self).__init__()\n        self.use_shortcut = stride == 1 and dilation[0] == dilation[1] and in_chs <= out_chs\n        self.in_channels = in_chs\n        self.out_channels = out_chs\n\n        if exp_ratio != 1.:\n            dw_chs = make_divisible(round(in_chs * exp_ratio), divisor=ch_div)\n            self.conv_exp = ConvNormAct(in_chs, dw_chs, act_layer=act_layer)\n        else:\n            dw_chs = in_chs\n            self.conv_exp = None\n\n        self.conv_dw = ConvNormAct(\n            dw_chs,\n            dw_chs,\n            kernel_size=3,\n            stride=stride,\n            dilation=dilation[0],\n            groups=dw_chs,\n            apply_act=False,\n        )\n        if se_ratio > 0:\n            self.se = SEWithNorm(dw_chs, rd_channels=make_divisible(int(dw_chs * se_ratio), ch_div))\n        else:\n            self.se = None\n        self.act_dw = create_act_layer(dw_act_layer)\n\n        self.conv_pwl = ConvNormAct(dw_chs, out_chs, 1, apply_act=False)\n        self.drop_path = drop_path\n\n    def feat_channels(self, exp=False):\n        return self.conv_dw.out_channels if exp else self.out_channels\n\n    def forward(self, x):\n        shortcut = x\n        if self.conv_exp is not None:\n            x = self.conv_exp(x)\n        x = self.conv_dw(x)\n        if self.se is not None:\n            x = self.se(x)\n        x = self.act_dw(x)\n        x = self.conv_pwl(x)\n        if self.use_shortcut:\n            if self.drop_path is not None:\n                x = self.drop_path(x)\n            x = torch.cat([x[:, 0:self.in_channels] + shortcut, x[:, self.in_channels:]], dim=1)\n        return x\n\n\ndef _block_cfg(\n        width_mult=1.0,\n        depth_mult=1.0,\n        initial_chs=16,\n        final_chs=180,\n        se_ratio=0.,\n        ch_div=1,\n):\n    layers = [1, 2, 2, 3, 3, 5]\n    strides = [1, 2, 2, 2, 1, 2]\n    layers = [ceil(element * depth_mult) for element in layers]\n    strides = sum([[element] + [1] * (layers[idx] - 1) for idx, element in enumerate(strides)], [])\n    exp_ratios = [1] * layers[0] + [6] * sum(layers[1:])\n    depth = sum(layers[:]) * 3\n    base_chs = initial_chs / width_mult if width_mult < 1.0 else initial_chs\n\n    # The following channel configuration is a simple instance to make each layer become an expand layer.\n    out_chs_list = []\n    for i in range(depth // 3):\n        out_chs_list.append(make_divisible(round(base_chs * width_mult), divisor=ch_div))\n        base_chs += final_chs / (depth // 3 * 1.0)\n\n    se_ratios = [0.] * (layers[0] + layers[1]) + [se_ratio] * sum(layers[2:])\n\n    return list(zip(out_chs_list, exp_ratios, strides, se_ratios))\n\n\ndef _build_blocks(\n        block_cfg,\n        prev_chs,\n        width_mult,\n        ch_div=1,\n        output_stride=32,\n        act_layer='swish',\n        dw_act_layer='relu6',\n        drop_path_rate=0.,\n):\n    feat_chs = [prev_chs]\n    feature_info = []\n    curr_stride = 2\n    dilation = 1\n    features = []\n    num_blocks = len(block_cfg)\n    for block_idx, (chs, exp_ratio, stride, se_ratio) in enumerate(block_cfg):\n        next_dilation = dilation\n        if stride > 1:\n            fname = 'stem' if block_idx == 0 else f'features.{block_idx - 1}'\n            feature_info += [dict(num_chs=feat_chs[-1], reduction=curr_stride, module=fname)]\n            if curr_stride >= output_stride:\n                next_dilation = dilation * stride\n                stride = 1\n        block_dpr = drop_path_rate * block_idx / (num_blocks - 1)  # stochastic depth linear decay rule\n        drop_path = DropPath(block_dpr) if block_dpr > 0. else None\n        features.append(LinearBottleneck(\n            in_chs=prev_chs,\n            out_chs=chs,\n            exp_ratio=exp_ratio,\n            stride=stride,\n            dilation=(dilation, next_dilation),\n            se_ratio=se_ratio,\n            ch_div=ch_div,\n            act_layer=act_layer,\n            dw_act_layer=dw_act_layer,\n            drop_path=drop_path,\n        ))\n        curr_stride *= stride\n        dilation = next_dilation\n        prev_chs = chs\n        feat_chs += [features[-1].feat_channels()]\n    pen_chs = make_divisible(1280 * width_mult, divisor=ch_div)\n    feature_info += [dict(num_chs=feat_chs[-1], reduction=curr_stride, module=f'features.{len(features) - 1}')]\n    features.append(ConvNormAct(prev_chs, pen_chs, act_layer=act_layer))\n    return features, feature_info\n\n\nclass RexNet(nn.Module):\n    def __init__(\n            self,\n            in_chans=3,\n            num_classes=1000,\n            global_pool='avg',\n            output_stride=32,\n            initial_chs=16,\n            final_chs=180,\n            width_mult=1.0,\n            depth_mult=1.0,\n            se_ratio=1/12.,\n            ch_div=1,\n            act_layer='swish',\n            dw_act_layer='relu6',\n            drop_rate=0.2,\n            drop_path_rate=0.,\n    ):\n        super(RexNet, self).__init__()\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        self.grad_checkpointing = False\n\n        assert output_stride in (32, 16, 8)\n        stem_base_chs = 32 / width_mult if width_mult < 1.0 else 32\n        stem_chs = make_divisible(round(stem_base_chs * width_mult), divisor=ch_div)\n        self.stem = ConvNormAct(in_chans, stem_chs, 3, stride=2, act_layer=act_layer)\n\n        block_cfg = _block_cfg(width_mult, depth_mult, initial_chs, final_chs, se_ratio, ch_div)\n        features, self.feature_info = _build_blocks(\n            block_cfg,\n            stem_chs,\n            width_mult,\n            ch_div,\n            output_stride,\n            act_layer,\n            dw_act_layer,\n            drop_path_rate,\n        )\n        self.num_features = features[-1].out_channels\n        self.features = nn.Sequential(*features)\n\n        self.head = ClassifierHead(self.num_features, num_classes, global_pool, drop_rate)\n\n        efficientnet_init_weights(self)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^stem',\n            blocks=r'^features\\.(\\d+)',\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate)\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.features, x, flatten=True)\n        else:\n            x = self.features(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_rexnet(variant, pretrained, **kwargs):\n    feature_cfg = dict(flatten_sequential=True)\n    return build_model_with_cfg(\n        RexNet,\n        variant,\n        pretrained,\n        feature_cfg=feature_cfg,\n        **kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.conv', 'classifier': 'head.fc',\n        'license': 'mit', **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'rexnet_100.nav_in1k': _cfg(hf_hub_id='timm/'),\n    'rexnet_130.nav_in1k': _cfg(hf_hub_id='timm/'),\n    'rexnet_150.nav_in1k': _cfg(hf_hub_id='timm/'),\n    'rexnet_200.nav_in1k': _cfg(hf_hub_id='timm/'),\n    'rexnet_300.nav_in1k': _cfg(hf_hub_id='timm/'),\n    'rexnetr_100.untrained': _cfg(),\n    'rexnetr_130.untrained': _cfg(),\n    'rexnetr_150.untrained': _cfg(),\n    'rexnetr_200.sw_in12k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.95, test_crop_pct=1.0, test_input_size=(3, 288, 288), license='apache-2.0'),\n    'rexnetr_300.sw_in12k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.95, test_crop_pct=1.0, test_input_size=(3, 288, 288), license='apache-2.0'),\n    'rexnetr_200.sw_in12k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=11821,\n        crop_pct=0.95, test_crop_pct=1.0, test_input_size=(3, 288, 288), license='apache-2.0'),\n    'rexnetr_300.sw_in12k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=11821,\n        crop_pct=0.95, test_crop_pct=1.0, test_input_size=(3, 288, 288), license='apache-2.0'),\n})\n\n\n@register_model\ndef rexnet_100(pretrained=False, **kwargs) -> RexNet:\n    \"\"\"ReXNet V1 1.0x\"\"\"\n    return _create_rexnet('rexnet_100', pretrained, **kwargs)\n\n\n@register_model\ndef rexnet_130(pretrained=False, **kwargs) -> RexNet:\n    \"\"\"ReXNet V1 1.3x\"\"\"\n    return _create_rexnet('rexnet_130', pretrained, width_mult=1.3, **kwargs)\n\n\n@register_model\ndef rexnet_150(pretrained=False, **kwargs) -> RexNet:\n    \"\"\"ReXNet V1 1.5x\"\"\"\n    return _create_rexnet('rexnet_150', pretrained, width_mult=1.5, **kwargs)\n\n\n@register_model\ndef rexnet_200(pretrained=False, **kwargs) -> RexNet:\n    \"\"\"ReXNet V1 2.0x\"\"\"\n    return _create_rexnet('rexnet_200', pretrained, width_mult=2.0, **kwargs)\n\n\n@register_model\ndef rexnet_300(pretrained=False, **kwargs) -> RexNet:\n    \"\"\"ReXNet V1 3.0x\"\"\"\n    return _create_rexnet('rexnet_300', pretrained, width_mult=3.0, **kwargs)\n\n\n@register_model\ndef rexnetr_100(pretrained=False, **kwargs) -> RexNet:\n    \"\"\"ReXNet V1 1.0x w/ rounded (mod 8) channels\"\"\"\n    return _create_rexnet('rexnetr_100', pretrained, ch_div=8, **kwargs)\n\n\n@register_model\ndef rexnetr_130(pretrained=False, **kwargs) -> RexNet:\n    \"\"\"ReXNet V1 1.3x w/ rounded (mod 8) channels\"\"\"\n    return _create_rexnet('rexnetr_130', pretrained, width_mult=1.3, ch_div=8, **kwargs)\n\n\n@register_model\ndef rexnetr_150(pretrained=False, **kwargs) -> RexNet:\n    \"\"\"ReXNet V1 1.5x w/ rounded (mod 8) channels\"\"\"\n    return _create_rexnet('rexnetr_150', pretrained, width_mult=1.5, ch_div=8, **kwargs)\n\n\n@register_model\ndef rexnetr_200(pretrained=False, **kwargs) -> RexNet:\n    \"\"\"ReXNet V1 2.0x w/ rounded (mod 8) channels\"\"\"\n    return _create_rexnet('rexnetr_200', pretrained, width_mult=2.0, ch_div=8, **kwargs)\n\n\n@register_model\ndef rexnetr_300(pretrained=False, **kwargs) -> RexNet:\n    \"\"\"ReXNet V1 3.0x w/ rounded (mod 16) channels\"\"\"\n    return _create_rexnet('rexnetr_300', pretrained, width_mult=3.0, ch_div=16, **kwargs)\n",
  "\"\"\" Vision Transformer (ViT) in PyTorch\n\nA PyTorch implement of Vision Transformers as described in:\n\n'Exploring Plain Vision Transformer Backbones for Object Detection'\n    - https://arxiv.org/abs/2203.16527\n\n'Segment Anything Model (SAM)'\n    - https://github.com/facebookresearch/segment-anything/\n\n\"\"\"\nimport logging\nfrom functools import partial\nfrom typing import Callable, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import PatchEmbed, Mlp, DropPath, PatchDropout, LayerNorm2d, ClassifierHead, NormMlpClassifierHead,\\\n    Format, resample_abs_pos_embed_nhwc\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import generate_default_cfgs, register_model\n\n# model_registry will add each entrypoint fn to this\n__all__ = ['VisionTransformerSAM']\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass Attention(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads=8,\n            qkv_bias=True,\n            qk_norm=False,\n            attn_drop=0.,\n            proj_drop=0.,\n            norm_layer=nn.LayerNorm,\n            use_rel_pos: bool = False,\n            rel_pos_zero_init: bool = True,\n            input_size: Optional[Tuple[int, int]] = None,\n    ):\n        super().__init__()\n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.use_rel_pos = use_rel_pos\n        if self.use_rel_pos:\n            assert (\n                input_size is not None\n            ), \"Input size must be provided if using relative positional encoding.\"\n            # initialize relative positional embeddings\n            self.rel_pos_h = nn.Parameter(torch.zeros(\n                2 * input_size[0] - 1, self.head_dim))\n            self.rel_pos_w = nn.Parameter(torch.zeros(\n                2 * input_size[1] - 1, self.head_dim))\n\n    def forward(self, x):\n        B, H, W, _ = x.shape\n        qkv = self.qkv(x).reshape(\n            B, H * W, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        # qkv with shape (3, B, nHead, H * W, C)\n        q, k, v = qkv.reshape(3, B * self.num_heads, H * W, -1).unbind(0)\n        # q, k, v with shape (B * nHead, H * W, C)\n        q, k = self.q_norm(q), self.k_norm(k)\n        q = q * self.scale\n        attn = q @ k.transpose(-2, -1)\n\n        if self.use_rel_pos:\n            attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))\n\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        x = (attn @ v).view(B, self.num_heads, H, W, -1).permute(0, 2, 3, 1, 4).reshape(B, H, W, -1)\n        x = self.proj(x)\n\n        return x\n\n\nclass LayerScale(nn.Module):\n    def __init__(self, dim, init_values=1e-5, inplace=False):\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n\n\nclass Block(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=True,\n            qk_norm=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            init_values=None,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            mlp_layer=Mlp,\n            use_rel_pos=False,\n            window_size=0,\n            input_size=None,\n    ):\n        super().__init__()\n        self.window_size = window_size\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_norm=qk_norm,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n            norm_layer=norm_layer,\n            use_rel_pos=use_rel_pos,\n            input_size=input_size if window_size == 0 else (window_size, window_size),\n        )\n        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        self.mlp = mlp_layer(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        shortcut = x\n        x = self.norm1(x)\n        # Window partition\n        if self.window_size > 0:\n            H, W = x.shape[1], x.shape[2]\n            x, pad_hw = window_partition(x, self.window_size)\n\n        x = self.drop_path1(self.ls1(self.attn(x)))\n        # Reverse window partition\n        if self.window_size > 0:\n            x = window_unpartition(x, self.window_size, pad_hw, (H, W))\n\n        x = shortcut + x\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n\n        return x\n\n\ndef window_partition(x: torch.Tensor, window_size: int) -> Tuple[torch.Tensor, Tuple[int, int]]:\n    \"\"\"\n    Partition into non-overlapping windows with padding if needed.\n    Args:\n        x (tensor): input tokens with [B, H, W, C].\n        window_size (int): window size.\n\n    Returns:\n        windows: windows after partition with [B * num_windows, window_size, window_size, C].\n        (Hp, Wp): padded height and width before partition\n    \"\"\"\n    B, H, W, C = x.shape\n\n    pad_h = (window_size - H % window_size) % window_size\n    pad_w = (window_size - W % window_size) % window_size\n    if pad_h > 0 or pad_w > 0:\n        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n    Hp, Wp = H + pad_h, W + pad_w\n\n    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows, (Hp, Wp)\n\n\ndef window_unpartition(\n    windows: torch.Tensor, window_size: int, pad_hw: Tuple[int, int], hw: Tuple[int, int]\n) -> torch.Tensor:\n    \"\"\"\n    Window unpartition into original sequences and removing padding.\n    Args:\n        windows (tensor): input tokens with [B * num_windows, window_size, window_size, C].\n        window_size (int): window size.\n        pad_hw (Tuple): padded height and width (Hp, Wp).\n        hw (Tuple): original height and width (H, W) before padding.\n\n    Returns:\n        x: unpartitioned sequences with [B, H, W, C].\n    \"\"\"\n    Hp, Wp = pad_hw\n    H, W = hw\n    B = windows.shape[0] // (Hp * Wp // window_size // window_size)\n    x = windows.view(B, Hp // window_size, Wp // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, -1)\n\n    if Hp > H or Wp > W:\n        x = x[:, :H, :W, :].contiguous()\n    return x\n\n\ndef get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Get relative positional embeddings according to the relative positions of\n        query and key sizes.\n    Args:\n        q_size (int): size of query q.\n        k_size (int): size of key k.\n        rel_pos (Tensor): relative position embeddings (L, C).\n\n    Returns:\n        Extracted positional embeddings according to relative positions.\n    \"\"\"\n    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n    # Interpolate rel pos if needed.\n    if rel_pos.shape[0] != max_rel_dist:\n        # Interpolate rel pos.\n        rel_pos_resized = F.interpolate(\n            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),\n            size=max_rel_dist,\n            mode=\"linear\",\n        )\n        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n    else:\n        rel_pos_resized = rel_pos\n\n    # Scale the coords with short length if shapes for q and k are different.\n    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n\n    return rel_pos_resized[relative_coords.long()]\n\n\ndef add_decomposed_rel_pos(\n    attn: torch.Tensor,\n    q: torch.Tensor,\n    rel_pos_h: torch.Tensor,\n    rel_pos_w: torch.Tensor,\n    q_size: Tuple[int, int],\n    k_size: Tuple[int, int],\n) -> torch.Tensor:\n    \"\"\"\n    Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.\n    https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py\n    Args:\n        attn (Tensor): attention map.\n        q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n\n    Returns:\n        attn (Tensor): attention map with added relative positional embeddings.\n    \"\"\"\n    q_h, q_w = q_size\n    k_h, k_w = k_size\n    Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n    Rw = get_rel_pos(q_w, k_w, rel_pos_w)\n\n    B, _, dim = q.shape\n    r_q = q.reshape(B, q_h, q_w, dim)\n    rel_h = torch.einsum(\"bhwc,hkc->bhwk\", r_q, Rh)\n    rel_w = torch.einsum(\"bhwc,wkc->bhwk\", r_q, Rw)\n\n    attn = (\n        attn.view(B, q_h, q_w, k_h, k_w) +\n        rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n    ).view(B, q_h * q_w, k_h * k_w)\n\n    return attn\n\n\nclass VisionTransformerSAM(nn.Module):\n    \"\"\" Vision Transformer for Segment-Anything Model(SAM)\n\n    A PyTorch impl of : `Exploring Plain Vision Transformer Backbones for Object Detection` or `Segment Anything Model (SAM)`\n        - https://arxiv.org/abs/2010.11929\n    \"\"\"\n\n    def __init__(\n            self,\n            img_size: int = 1024,\n            patch_size: int = 16,\n            in_chans: int = 3,\n            num_classes: int = 768,\n            embed_dim: int = 768,\n            depth: int = 12,\n            num_heads: int = 12,\n            mlp_ratio: float = 4.,\n            qkv_bias: bool = True,\n            qk_norm: bool = False,\n            init_values: Optional[float] = None,\n            pre_norm: bool = False,\n            drop_rate: float = 0.,\n            pos_drop_rate: float = 0.,\n            patch_drop_rate: float = 0.,\n            proj_drop_rate: float = 0.,\n            attn_drop_rate: float = 0.,\n            drop_path_rate: float = 0.,\n            weight_init: str = '',\n            embed_layer: Callable = partial(\n                PatchEmbed, output_fmt=Format.NHWC, strict_img_size=False),\n            norm_layer: Optional[Callable] = nn.LayerNorm,\n            act_layer: Optional[Callable] = nn.GELU,\n            block_fn: Callable = Block,\n            mlp_layer: Callable = Mlp,\n            use_abs_pos: bool = True,\n            use_rel_pos: bool = False,\n            window_size: int = 14,\n            global_attn_indexes: Tuple[int, ...] = (),\n            neck_chans: int = 256,\n            global_pool: str = 'avg',\n            head_hidden_size: Optional[int] = None\n    ):\n        \"\"\"\n        Args:\n            img_size: Input image size.\n            patch_size: Patch size.\n            in_chans: Number of image input channels.\n            num_classes: Mumber of classes for classification head.\n            global_pool: Type of global pooling for final sequence (default: 'token').\n            embed_dim: Transformer embedding dimension.\n            depth: Depth of transformer.\n            num_heads: Number of attention heads.\n            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n            qkv_bias: Enable bias for qkv projections if True.\n            init_values: Layer-scale init values (layer-scale enabled if not None).\n            drop_rate: Head dropout rate.\n            pos_drop_rate: Position embedding dropout rate.\n            attn_drop_rate: Attention dropout rate.\n            drop_path_rate: Stochastic depth rate.\n            weight_init: Weight initialization scheme.\n            embed_layer: Patch embedding layer.\n            norm_layer: Normalization layer.\n            act_layer: MLP activation layer.\n            block_fn: Transformer block layer.\n            use_abs_pos: If True, use absolute positional embeddings.\n            use_rel_pos: If True, add relative positional embeddings to the attention map.\n            window_size: Window size for window attention blocks. If 0, not use window attention.\n            global_attn_indexes: Indexes for blocks using global attention. Used when window_size > 0.\n            global_pool: Global pooling type.\n            head_hidden_size: If set, use NormMlpHead\n        \"\"\"\n        super().__init__()\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        act_layer = act_layer or nn.GELU\n\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        # num_features for consistency with other models\n        self.num_features = self.embed_dim = embed_dim\n        self.grad_checkpointing = False\n\n        self.patch_embed = embed_layer(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            bias=not pre_norm,  # disable bias if pre-norm is used\n        )\n        grid_size = self.patch_embed.grid_size\n        if use_abs_pos:\n            # Initialize absolute positional embedding with pretrain image size.\n            self.pos_embed = nn.Parameter(torch.zeros(1, grid_size[0], grid_size[1], embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n        if patch_drop_rate > 0:\n            self.patch_drop = PatchDropout(\n                patch_drop_rate,\n                num_prefix_tokens=0,\n            )\n        else:\n            self.patch_drop = nn.Identity()\n        self.norm_pre = norm_layer(embed_dim) if pre_norm else nn.Identity()\n\n        # stochastic depth decay rule\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n        self.blocks = nn.Sequential(*[\n            block_fn(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_norm=qk_norm,\n                init_values=init_values,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                mlp_layer=mlp_layer,\n                use_rel_pos=use_rel_pos,\n                window_size=window_size if i not in global_attn_indexes else 0,\n                input_size=grid_size,\n            )\n            for i in range(depth)])\n\n        if neck_chans:\n            self.neck = nn.Sequential(\n                nn.Conv2d(\n                    embed_dim,\n                    neck_chans,\n                    kernel_size=1,\n                    bias=False,\n                ),\n                LayerNorm2d(neck_chans),\n                nn.Conv2d(\n                    neck_chans,\n                    neck_chans,\n                    kernel_size=3,\n                    padding=1,\n                    bias=False,\n                ),\n                LayerNorm2d(neck_chans),\n            )\n        else:\n            self.neck = nn.Identity()\n            neck_chans = embed_dim\n\n        # Classifier Head\n        if head_hidden_size:\n            self.head = NormMlpClassifierHead(\n                neck_chans,\n                num_classes,\n                hidden_size=head_hidden_size,\n                pool_type=global_pool,\n                drop_rate=drop_rate,\n            )\n        else:\n            self.head = ClassifierHead(\n                neck_chans,\n                num_classes,\n                pool_type=global_pool,\n                drop_rate=drop_rate,\n            )\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'dist_token'}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^pos_embed|patch_embed',  # stem and embed\n            blocks=[(r'^blocks\\.(\\d+)', None), (r'^norm', (99999,))]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes=0, global_pool=None):\n        self.head.reset(num_classes, global_pool)\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        if self.pos_embed is not None:\n            # dynamically resize abs pos embedding if needed\n            x = x + resample_abs_pos_embed_nhwc(self.pos_embed, x.shape[1:3])\n        x = self.pos_drop(x)\n        x = self.patch_drop(x)\n        x = self.norm_pre(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        x = self.neck(x.permute(0, 3, 1, 2))\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=True) if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef checkpoint_filter_fn(\n        state_dict,\n        model,\n):\n    \"\"\" Remap SAM checkpoints -> timm \"\"\"\n    sam_checkpoint = 'image_encoder.patch_embed.proj.weight' in state_dict\n    out_dict = {}\n    for k, v in state_dict.items():\n        if k.startswith('image_encoder.'):\n            k = k[14:]\n            k = k.replace('mlp.lin', 'mlp.fc')\n        else:\n            if sam_checkpoint:\n                continue\n        out_dict[k] = v\n    return out_dict\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 1024, 1024), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,\n        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n\n    # Segment-Anyhing Model (SAM) pretrained - https://github.com/facebookresearch/segment-anything (no classifier head, for fine-tune/features only)\n    'samvit_base_patch16.sa1b': _cfg(\n        url='https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth',\n        hf_hub_id='timm/',\n        license='apache-2.0',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0,\n        input_size=(3, 1024, 1024), crop_pct=1.0),\n    'samvit_large_patch16.sa1b': _cfg(\n        url='https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth',\n        hf_hub_id='timm/',\n        license='apache-2.0',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0,\n        input_size=(3, 1024, 1024), crop_pct=1.0),\n    'samvit_huge_patch16.sa1b': _cfg(\n        url='https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth',\n        hf_hub_id='timm/',\n        license='apache-2.0',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0,\n        input_size=(3, 1024, 1024), crop_pct=1.0),\n})\n\n\ndef _create_vision_transformer(variant, pretrained=False, **kwargs):\n    if kwargs.get('features_only', None):\n        raise RuntimeError(\n            'features_only not implemented for Vision Transformer models.')\n\n    return build_model_with_cfg(\n        VisionTransformerSAM,\n        variant,\n        pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        **kwargs,\n    )\n\n\n@register_model\ndef samvit_base_patch16(pretrained=False, **kwargs) -> VisionTransformerSAM:\n    \"\"\" ViT-B/16 for Segment-Anything\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=768, depth=12, num_heads=12, global_attn_indexes=[2, 5, 8, 11],\n        window_size=14, use_rel_pos=True, img_size=1024,\n    )\n    model = _create_vision_transformer(\n        'samvit_base_patch16', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef samvit_large_patch16(pretrained=False, **kwargs) -> VisionTransformerSAM:\n    \"\"\" ViT-L/16 for Segment-Anything\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=1024, depth=24, num_heads=16, global_attn_indexes=[5, 11, 17, 23],\n        window_size=14, use_rel_pos=True, img_size=1024,\n    )\n    model = _create_vision_transformer(\n        'samvit_large_patch16', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef samvit_huge_patch16(pretrained=False, **kwargs) -> VisionTransformerSAM:\n    \"\"\" ViT-H/16 for Segment-Anything\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=1280, depth=32, num_heads=16, global_attn_indexes=[7, 15, 23, 31],\n        window_size=14, use_rel_pos=True, img_size=1024,\n    )\n    model = _create_vision_transformer(\n        'samvit_huge_patch16', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n",
  "\"\"\"Pytorch impl of Aligned Xception 41, 65, 71\n\nThis is a correct, from scratch impl of Aligned Xception (Deeplab) models compatible with TF weights at\nhttps://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import ClassifierHead, ConvNormAct, create_conv2d, get_norm_act_layer\nfrom timm.layers.helpers import to_3tuple\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['XceptionAligned']\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            kernel_size=3,\n            stride=1,\n            dilation=1,\n            padding='',\n            act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d,\n    ):\n        super(SeparableConv2d, self).__init__()\n        self.kernel_size = kernel_size\n        self.dilation = dilation\n\n        # depthwise convolution\n        self.conv_dw = create_conv2d(\n            in_chs, in_chs, kernel_size, stride=stride,\n            padding=padding, dilation=dilation, depthwise=True)\n        self.bn_dw = norm_layer(in_chs)\n        self.act_dw = act_layer(inplace=True) if act_layer is not None else nn.Identity()\n\n        # pointwise convolution\n        self.conv_pw = create_conv2d(in_chs, out_chs, kernel_size=1)\n        self.bn_pw = norm_layer(out_chs)\n        self.act_pw = act_layer(inplace=True) if act_layer is not None else nn.Identity()\n\n    def forward(self, x):\n        x = self.conv_dw(x)\n        x = self.bn_dw(x)\n        x = self.act_dw(x)\n        x = self.conv_pw(x)\n        x = self.bn_pw(x)\n        x = self.act_pw(x)\n        return x\n\n\nclass PreSeparableConv2d(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            kernel_size=3,\n            stride=1,\n            dilation=1,\n            padding='',\n            act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d,\n            first_act=True,\n    ):\n        super(PreSeparableConv2d, self).__init__()\n        norm_act_layer = get_norm_act_layer(norm_layer, act_layer=act_layer)\n        self.kernel_size = kernel_size\n        self.dilation = dilation\n\n        self.norm = norm_act_layer(in_chs, inplace=True) if first_act else nn.Identity()\n        # depthwise convolution\n        self.conv_dw = create_conv2d(\n            in_chs, in_chs, kernel_size, stride=stride,\n            padding=padding, dilation=dilation, depthwise=True)\n\n        # pointwise convolution\n        self.conv_pw = create_conv2d(in_chs, out_chs, kernel_size=1)\n\n    def forward(self, x):\n        x = self.norm(x)\n        x = self.conv_dw(x)\n        x = self.conv_pw(x)\n        return x\n\n\nclass XceptionModule(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            stride=1,\n            dilation=1,\n            pad_type='',\n            start_with_relu=True,\n            no_skip=False,\n            act_layer=nn.ReLU,\n            norm_layer=None,\n    ):\n        super(XceptionModule, self).__init__()\n        out_chs = to_3tuple(out_chs)\n        self.in_channels = in_chs\n        self.out_channels = out_chs[-1]\n        self.no_skip = no_skip\n        if not no_skip and (self.out_channels != self.in_channels or stride != 1):\n            self.shortcut = ConvNormAct(\n                in_chs, self.out_channels, 1, stride=stride, norm_layer=norm_layer, apply_act=False)\n        else:\n            self.shortcut = None\n\n        separable_act_layer = None if start_with_relu else act_layer\n        self.stack = nn.Sequential()\n        for i in range(3):\n            if start_with_relu:\n                self.stack.add_module(f'act{i + 1}', act_layer(inplace=i > 0))\n            self.stack.add_module(f'conv{i + 1}', SeparableConv2d(\n                in_chs, out_chs[i], 3, stride=stride if i == 2 else 1, dilation=dilation, padding=pad_type,\n                act_layer=separable_act_layer, norm_layer=norm_layer))\n            in_chs = out_chs[i]\n\n    def forward(self, x):\n        skip = x\n        x = self.stack(x)\n        if self.shortcut is not None:\n            skip = self.shortcut(skip)\n        if not self.no_skip:\n            x = x + skip\n        return x\n\n\nclass PreXceptionModule(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            stride=1,\n            dilation=1,\n            pad_type='',\n            no_skip=False,\n            act_layer=nn.ReLU,\n            norm_layer=None,\n    ):\n        super(PreXceptionModule, self).__init__()\n        out_chs = to_3tuple(out_chs)\n        self.in_channels = in_chs\n        self.out_channels = out_chs[-1]\n        self.no_skip = no_skip\n        if not no_skip and (self.out_channels != self.in_channels or stride != 1):\n            self.shortcut = create_conv2d(in_chs, self.out_channels, 1, stride=stride)\n        else:\n            self.shortcut = nn.Identity()\n\n        self.norm = get_norm_act_layer(norm_layer, act_layer=act_layer)(in_chs, inplace=True)\n        self.stack = nn.Sequential()\n        for i in range(3):\n            self.stack.add_module(f'conv{i + 1}', PreSeparableConv2d(\n                in_chs,\n                out_chs[i],\n                3,\n                stride=stride if i == 2 else 1,\n                dilation=dilation,\n                padding=pad_type,\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n                first_act=i > 0,\n            ))\n            in_chs = out_chs[i]\n\n    def forward(self, x):\n        x = self.norm(x)\n        skip = x\n        x = self.stack(x)\n        if not self.no_skip:\n            x = x + self.shortcut(skip)\n        return x\n\n\nclass XceptionAligned(nn.Module):\n    \"\"\"Modified Aligned Xception\n    \"\"\"\n\n    def __init__(\n            self,\n            block_cfg,\n            num_classes=1000,\n            in_chans=3,\n            output_stride=32,\n            preact=False,\n            act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d,\n            drop_rate=0.,\n            global_pool='avg',\n    ):\n        super(XceptionAligned, self).__init__()\n        assert output_stride in (8, 16, 32)\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        self.grad_checkpointing = False\n\n        layer_args = dict(act_layer=act_layer, norm_layer=norm_layer)\n        self.stem = nn.Sequential(*[\n            ConvNormAct(in_chans, 32, kernel_size=3, stride=2, **layer_args),\n            create_conv2d(32, 64, kernel_size=3, stride=1) if preact else\n            ConvNormAct(32, 64, kernel_size=3, stride=1, **layer_args)\n        ])\n\n        curr_dilation = 1\n        curr_stride = 2\n        self.feature_info = []\n        self.blocks = nn.Sequential()\n        module_fn = PreXceptionModule if preact else XceptionModule\n        for i, b in enumerate(block_cfg):\n            b['dilation'] = curr_dilation\n            if b['stride'] > 1:\n                name = f'blocks.{i}.stack.conv2' if preact else f'blocks.{i}.stack.act3'\n                self.feature_info += [dict(num_chs=to_3tuple(b['out_chs'])[-2], reduction=curr_stride, module=name)]\n                next_stride = curr_stride * b['stride']\n                if next_stride > output_stride:\n                    curr_dilation *= b['stride']\n                    b['stride'] = 1\n                else:\n                    curr_stride = next_stride\n            self.blocks.add_module(str(i), module_fn(**b, **layer_args))\n            self.num_features = self.blocks[-1].out_channels\n\n        self.feature_info += [dict(\n            num_chs=self.num_features, reduction=curr_stride, module='blocks.' + str(len(self.blocks) - 1))]\n        self.act = act_layer(inplace=True) if preact else nn.Identity()\n        self.head = ClassifierHead(\n            in_features=self.num_features,\n            num_classes=num_classes,\n            pool_type=global_pool,\n            drop_rate=drop_rate,\n        )\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^stem',\n            blocks=r'^blocks\\.(\\d+)',\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.head.reset(num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        x = self.act(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=pre_logits)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _xception(variant, pretrained=False, **kwargs):\n    return build_model_with_cfg(\n        XceptionAligned,\n        variant,\n        pretrained,\n        feature_cfg=dict(flatten_sequential=True, feature_cls='hook'),\n        **kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (10, 10),\n        'crop_pct': 0.903, 'interpolation': 'bicubic',\n        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,\n        'first_conv': 'stem.0.conv', 'classifier': 'head.fc',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'xception65.ra3_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.94,\n    ),\n\n    'xception41.tf_in1k': _cfg(hf_hub_id='timm/'),\n    'xception65.tf_in1k': _cfg(hf_hub_id='timm/'),\n    'xception71.tf_in1k': _cfg(hf_hub_id='timm/'),\n\n    'xception41p.ra3_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.94,\n    ),\n    'xception65p.ra3_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.94,\n    ),\n})\n\n\n@register_model\ndef xception41(pretrained=False, **kwargs) -> XceptionAligned:\n    \"\"\" Modified Aligned Xception-41\n    \"\"\"\n    block_cfg = [\n        # entry flow\n        dict(in_chs=64, out_chs=128, stride=2),\n        dict(in_chs=128, out_chs=256, stride=2),\n        dict(in_chs=256, out_chs=728, stride=2),\n        # middle flow\n        *([dict(in_chs=728, out_chs=728, stride=1)] * 8),\n        # exit flow\n        dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2),\n        dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True, start_with_relu=False),\n    ]\n    model_args = dict(block_cfg=block_cfg, norm_layer=partial(nn.BatchNorm2d, eps=.001, momentum=.1), **kwargs)\n    return _xception('xception41', pretrained=pretrained, **model_args)\n\n\n@register_model\ndef xception65(pretrained=False, **kwargs) -> XceptionAligned:\n    \"\"\" Modified Aligned Xception-65\n    \"\"\"\n    block_cfg = [\n        # entry flow\n        dict(in_chs=64, out_chs=128, stride=2),\n        dict(in_chs=128, out_chs=256, stride=2),\n        dict(in_chs=256, out_chs=728, stride=2),\n        # middle flow\n        *([dict(in_chs=728, out_chs=728, stride=1)] * 16),\n        # exit flow\n        dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2),\n        dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True, start_with_relu=False),\n    ]\n    model_args = dict(block_cfg=block_cfg, norm_layer=partial(nn.BatchNorm2d, eps=.001, momentum=.1), **kwargs)\n    return _xception('xception65', pretrained=pretrained, **model_args)\n\n\n@register_model\ndef xception71(pretrained=False, **kwargs) -> XceptionAligned:\n    \"\"\" Modified Aligned Xception-71\n    \"\"\"\n    block_cfg = [\n        # entry flow\n        dict(in_chs=64, out_chs=128, stride=2),\n        dict(in_chs=128, out_chs=256, stride=1),\n        dict(in_chs=256, out_chs=256, stride=2),\n        dict(in_chs=256, out_chs=728, stride=1),\n        dict(in_chs=728, out_chs=728, stride=2),\n        # middle flow\n        *([dict(in_chs=728, out_chs=728, stride=1)] * 16),\n        # exit flow\n        dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2),\n        dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True, start_with_relu=False),\n    ]\n    model_args = dict(block_cfg=block_cfg, norm_layer=partial(nn.BatchNorm2d, eps=.001, momentum=.1), **kwargs)\n    return _xception('xception71', pretrained=pretrained, **model_args)\n\n\n@register_model\ndef xception41p(pretrained=False, **kwargs) -> XceptionAligned:\n    \"\"\" Modified Aligned Xception-41 w/ Pre-Act\n    \"\"\"\n    block_cfg = [\n        # entry flow\n        dict(in_chs=64, out_chs=128, stride=2),\n        dict(in_chs=128, out_chs=256, stride=2),\n        dict(in_chs=256, out_chs=728, stride=2),\n        # middle flow\n        *([dict(in_chs=728, out_chs=728, stride=1)] * 8),\n        # exit flow\n        dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2),\n        dict(in_chs=1024, out_chs=(1536, 1536, 2048), no_skip=True, stride=1),\n    ]\n    model_args = dict(block_cfg=block_cfg, preact=True, norm_layer=nn.BatchNorm2d, **kwargs)\n    return _xception('xception41p', pretrained=pretrained, **model_args)\n\n\n@register_model\ndef xception65p(pretrained=False, **kwargs) -> XceptionAligned:\n    \"\"\" Modified Aligned Xception-65 w/ Pre-Act\n    \"\"\"\n    block_cfg = [\n        # entry flow\n        dict(in_chs=64, out_chs=128, stride=2),\n        dict(in_chs=128, out_chs=256, stride=2),\n        dict(in_chs=256, out_chs=728, stride=2),\n        # middle flow\n        *([dict(in_chs=728, out_chs=728, stride=1)] * 16),\n        # exit flow\n        dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2),\n        dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True),\n    ]\n    model_args = dict(\n        block_cfg=block_cfg, preact=True, norm_layer=partial(nn.BatchNorm2d, eps=.001, momentum=.1), **kwargs)\n    return _xception('xception65p', pretrained=pretrained, **model_args)\n",
  "\"\"\" MobileViT\n\nPaper:\nV1: `MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer` - https://arxiv.org/abs/2110.02178\nV2: `Separable Self-attention for Mobile Vision Transformers` - https://arxiv.org/abs/2206.02680\n\nMobileVitBlock and checkpoints adapted from https://github.com/apple/ml-cvnets (original copyright below)\nLicense: https://github.com/apple/ml-cvnets/blob/main/LICENSE (Apple open source)\n\nRest of code, ByobNet, and Transformer block hacked together by / Copyright 2022, Ross Wightman\n\"\"\"\n#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2020 Apple Inc. All Rights Reserved.\n#\nimport math\nfrom typing import Callable, Tuple, Optional\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom timm.layers import to_2tuple, make_divisible, GroupNorm1, ConvMlp, DropPath, is_exportable\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_module\nfrom ._registry import register_model, generate_default_cfgs, register_model_deprecations\nfrom .byobnet import register_block, ByoBlockCfg, ByoModelCfg, ByobNet, LayerFn, num_groups\nfrom .vision_transformer import Block as TransformerBlock\n\n__all__ = []\n\n\ndef _inverted_residual_block(d, c, s, br=4.0):\n    # inverted residual is a bottleneck block with bottle_ratio > 1 applied to in_chs, linear output, gs=1 (depthwise)\n    return ByoBlockCfg(\n        type='bottle', d=d, c=c, s=s, gs=1, br=br,\n        block_kwargs=dict(bottle_in=True, linear_out=True))\n\n\ndef _mobilevit_block(d, c, s, transformer_dim, transformer_depth, patch_size=4, br=4.0):\n    # inverted residual + mobilevit blocks as per MobileViT network\n    return (\n        _inverted_residual_block(d=d, c=c, s=s, br=br),\n        ByoBlockCfg(\n            type='mobilevit', d=1, c=c, s=1,\n            block_kwargs=dict(\n                transformer_dim=transformer_dim,\n                transformer_depth=transformer_depth,\n                patch_size=patch_size)\n        )\n    )\n\n\ndef _mobilevitv2_block(d, c, s, transformer_depth, patch_size=2, br=2.0, transformer_br=0.5):\n    # inverted residual + mobilevit blocks as per MobileViT network\n    return (\n        _inverted_residual_block(d=d, c=c, s=s, br=br),\n        ByoBlockCfg(\n            type='mobilevit2', d=1, c=c, s=1, br=transformer_br, gs=1,\n            block_kwargs=dict(\n                transformer_depth=transformer_depth,\n                patch_size=patch_size)\n        )\n    )\n\n\ndef _mobilevitv2_cfg(multiplier=1.0):\n    chs = (64, 128, 256, 384, 512)\n    if multiplier != 1.0:\n        chs = tuple([int(c * multiplier) for c in chs])\n    cfg = ByoModelCfg(\n        blocks=(\n            _inverted_residual_block(d=1, c=chs[0], s=1, br=2.0),\n            _inverted_residual_block(d=2, c=chs[1], s=2, br=2.0),\n            _mobilevitv2_block(d=1, c=chs[2], s=2, transformer_depth=2),\n            _mobilevitv2_block(d=1, c=chs[3], s=2, transformer_depth=4),\n            _mobilevitv2_block(d=1, c=chs[4], s=2, transformer_depth=3),\n        ),\n        stem_chs=int(32 * multiplier),\n        stem_type='3x3',\n        stem_pool='',\n        downsample='',\n        act_layer='silu',\n    )\n    return cfg\n\n\nmodel_cfgs = dict(\n    mobilevit_xxs=ByoModelCfg(\n        blocks=(\n            _inverted_residual_block(d=1, c=16, s=1, br=2.0),\n            _inverted_residual_block(d=3, c=24, s=2, br=2.0),\n            _mobilevit_block(d=1, c=48, s=2, transformer_dim=64, transformer_depth=2, patch_size=2, br=2.0),\n            _mobilevit_block(d=1, c=64, s=2, transformer_dim=80, transformer_depth=4, patch_size=2, br=2.0),\n            _mobilevit_block(d=1, c=80, s=2, transformer_dim=96, transformer_depth=3, patch_size=2, br=2.0),\n        ),\n        stem_chs=16,\n        stem_type='3x3',\n        stem_pool='',\n        downsample='',\n        act_layer='silu',\n        num_features=320,\n    ),\n\n    mobilevit_xs=ByoModelCfg(\n        blocks=(\n            _inverted_residual_block(d=1, c=32, s=1),\n            _inverted_residual_block(d=3, c=48, s=2),\n            _mobilevit_block(d=1, c=64, s=2, transformer_dim=96, transformer_depth=2, patch_size=2),\n            _mobilevit_block(d=1, c=80, s=2, transformer_dim=120, transformer_depth=4, patch_size=2),\n            _mobilevit_block(d=1, c=96, s=2, transformer_dim=144, transformer_depth=3, patch_size=2),\n        ),\n        stem_chs=16,\n        stem_type='3x3',\n        stem_pool='',\n        downsample='',\n        act_layer='silu',\n        num_features=384,\n    ),\n\n    mobilevit_s=ByoModelCfg(\n        blocks=(\n            _inverted_residual_block(d=1, c=32, s=1),\n            _inverted_residual_block(d=3, c=64, s=2),\n            _mobilevit_block(d=1, c=96, s=2, transformer_dim=144, transformer_depth=2, patch_size=2),\n            _mobilevit_block(d=1, c=128, s=2, transformer_dim=192, transformer_depth=4, patch_size=2),\n            _mobilevit_block(d=1, c=160, s=2, transformer_dim=240, transformer_depth=3, patch_size=2),\n        ),\n        stem_chs=16,\n        stem_type='3x3',\n        stem_pool='',\n        downsample='',\n        act_layer='silu',\n        num_features=640,\n    ),\n\n    semobilevit_s=ByoModelCfg(\n        blocks=(\n            _inverted_residual_block(d=1, c=32, s=1),\n            _inverted_residual_block(d=3, c=64, s=2),\n            _mobilevit_block(d=1, c=96, s=2, transformer_dim=144, transformer_depth=2, patch_size=2),\n            _mobilevit_block(d=1, c=128, s=2, transformer_dim=192, transformer_depth=4, patch_size=2),\n            _mobilevit_block(d=1, c=160, s=2, transformer_dim=240, transformer_depth=3, patch_size=2),\n        ),\n        stem_chs=16,\n        stem_type='3x3',\n        stem_pool='',\n        downsample='',\n        attn_layer='se',\n        attn_kwargs=dict(rd_ratio=1/8),\n        num_features=640,\n    ),\n\n    mobilevitv2_050=_mobilevitv2_cfg(.50),\n    mobilevitv2_075=_mobilevitv2_cfg(.75),\n    mobilevitv2_125=_mobilevitv2_cfg(1.25),\n    mobilevitv2_100=_mobilevitv2_cfg(1.0),\n    mobilevitv2_150=_mobilevitv2_cfg(1.5),\n    mobilevitv2_175=_mobilevitv2_cfg(1.75),\n    mobilevitv2_200=_mobilevitv2_cfg(2.0),\n)\n\n\n@register_notrace_module\nclass MobileVitBlock(nn.Module):\n    \"\"\" MobileViT block\n        Paper: https://arxiv.org/abs/2110.02178?context=cs.LG\n    \"\"\"\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: Optional[int] = None,\n            kernel_size: int = 3,\n            stride: int = 1,\n            bottle_ratio: float = 1.0,\n            group_size: Optional[int] = None,\n            dilation: Tuple[int, int] = (1, 1),\n            mlp_ratio: float = 2.0,\n            transformer_dim: Optional[int] = None,\n            transformer_depth: int = 2,\n            patch_size: int = 8,\n            num_heads: int = 4,\n            attn_drop: float = 0.,\n            drop: int = 0.,\n            no_fusion: bool = False,\n            drop_path_rate: float = 0.,\n            layers: LayerFn = None,\n            transformer_norm_layer: Callable = nn.LayerNorm,\n            **kwargs,  # eat unused args\n    ):\n        super(MobileVitBlock, self).__init__()\n\n        layers = layers or LayerFn()\n        groups = num_groups(group_size, in_chs)\n        out_chs = out_chs or in_chs\n        transformer_dim = transformer_dim or make_divisible(bottle_ratio * in_chs)\n\n        self.conv_kxk = layers.conv_norm_act(\n            in_chs, in_chs, kernel_size=kernel_size,\n            stride=stride, groups=groups, dilation=dilation[0])\n        self.conv_1x1 = nn.Conv2d(in_chs, transformer_dim, kernel_size=1, bias=False)\n\n        self.transformer = nn.Sequential(*[\n            TransformerBlock(\n                transformer_dim,\n                mlp_ratio=mlp_ratio,\n                num_heads=num_heads,\n                qkv_bias=True,\n                attn_drop=attn_drop,\n                proj_drop=drop,\n                drop_path=drop_path_rate,\n                act_layer=layers.act,\n                norm_layer=transformer_norm_layer,\n            )\n            for _ in range(transformer_depth)\n        ])\n        self.norm = transformer_norm_layer(transformer_dim)\n\n        self.conv_proj = layers.conv_norm_act(transformer_dim, out_chs, kernel_size=1, stride=1)\n\n        if no_fusion:\n            self.conv_fusion = None\n        else:\n            self.conv_fusion = layers.conv_norm_act(in_chs + out_chs, out_chs, kernel_size=kernel_size, stride=1)\n\n        self.patch_size = to_2tuple(patch_size)\n        self.patch_area = self.patch_size[0] * self.patch_size[1]\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        shortcut = x\n\n        # Local representation\n        x = self.conv_kxk(x)\n        x = self.conv_1x1(x)\n\n        # Unfold (feature map -> patches)\n        patch_h, patch_w = self.patch_size\n        B, C, H, W = x.shape\n        new_h, new_w = math.ceil(H / patch_h) * patch_h, math.ceil(W / patch_w) * patch_w\n        num_patch_h, num_patch_w = new_h // patch_h, new_w // patch_w  # n_h, n_w\n        num_patches = num_patch_h * num_patch_w  # N\n        interpolate = False\n        if new_h != H or new_w != W:\n            # Note: Padding can be done, but then it needs to be handled in attention function.\n            x = F.interpolate(x, size=(new_h, new_w), mode=\"bilinear\", align_corners=False)\n            interpolate = True\n\n        # [B, C, H, W] --> [B * C * n_h, n_w, p_h, p_w]\n        x = x.reshape(B * C * num_patch_h, patch_h, num_patch_w, patch_w).transpose(1, 2)\n        # [B * C * n_h, n_w, p_h, p_w] --> [BP, N, C] where P = p_h * p_w and N = n_h * n_w\n        x = x.reshape(B, C, num_patches, self.patch_area).transpose(1, 3).reshape(B * self.patch_area, num_patches, -1)\n\n        # Global representations\n        x = self.transformer(x)\n        x = self.norm(x)\n\n        # Fold (patch -> feature map)\n        # [B, P, N, C] --> [B*C*n_h, n_w, p_h, p_w]\n        x = x.contiguous().view(B, self.patch_area, num_patches, -1)\n        x = x.transpose(1, 3).reshape(B * C * num_patch_h, num_patch_w, patch_h, patch_w)\n        # [B*C*n_h, n_w, p_h, p_w] --> [B*C*n_h, p_h, n_w, p_w] --> [B, C, H, W]\n        x = x.transpose(1, 2).reshape(B, C, num_patch_h * patch_h, num_patch_w * patch_w)\n        if interpolate:\n            x = F.interpolate(x, size=(H, W), mode=\"bilinear\", align_corners=False)\n\n        x = self.conv_proj(x)\n        if self.conv_fusion is not None:\n            x = self.conv_fusion(torch.cat((shortcut, x), dim=1))\n        return x\n\n\nclass LinearSelfAttention(nn.Module):\n    \"\"\"\n    This layer applies a self-attention with linear complexity, as described in `https://arxiv.org/abs/2206.02680`\n    This layer can be used for self- as well as cross-attention.\n    Args:\n        embed_dim (int): :math:`C` from an expected input of size :math:`(N, C, H, W)`\n        attn_drop (float): Dropout value for context scores. Default: 0.0\n        bias (bool): Use bias in learnable layers. Default: True\n    Shape:\n        - Input: :math:`(N, C, P, N)` where :math:`N` is the batch size, :math:`C` is the input channels,\n        :math:`P` is the number of pixels in the patch, and :math:`N` is the number of patches\n        - Output: same as the input\n    .. note::\n        For MobileViTv2, we unfold the feature map [B, C, H, W] into [B, C, P, N] where P is the number of pixels\n        in a patch and N is the number of patches. Because channel is the first dimension in this unfolded tensor,\n        we use point-wise convolution (instead of a linear layer). This avoids a transpose operation (which may be\n        expensive on resource-constrained devices) that may be required to convert the unfolded tensor from\n        channel-first to channel-last format in case of a linear layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim: int,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n        bias: bool = True,\n    ) -> None:\n        super().__init__()\n        self.embed_dim = embed_dim\n\n        self.qkv_proj = nn.Conv2d(\n            in_channels=embed_dim,\n            out_channels=1 + (2 * embed_dim),\n            bias=bias,\n            kernel_size=1,\n        )\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.out_proj = nn.Conv2d(\n            in_channels=embed_dim,\n            out_channels=embed_dim,\n            bias=bias,\n            kernel_size=1,\n        )\n        self.out_drop = nn.Dropout(proj_drop)\n\n    def _forward_self_attn(self, x: torch.Tensor) -> torch.Tensor:\n        # [B, C, P, N] --> [B, h + 2d, P, N]\n        qkv = self.qkv_proj(x)\n\n        # Project x into query, key and value\n        # Query --> [B, 1, P, N]\n        # value, key --> [B, d, P, N]\n        query, key, value = qkv.split([1, self.embed_dim, self.embed_dim], dim=1)\n\n        # apply softmax along N dimension\n        context_scores = F.softmax(query, dim=-1)\n        context_scores = self.attn_drop(context_scores)\n\n        # Compute context vector\n        # [B, d, P, N] x [B, 1, P, N] -> [B, d, P, N] --> [B, d, P, 1]\n        context_vector = (key * context_scores).sum(dim=-1, keepdim=True)\n\n        # combine context vector with values\n        # [B, d, P, N] * [B, d, P, 1] --> [B, d, P, N]\n        out = F.relu(value) * context_vector.expand_as(value)\n        out = self.out_proj(out)\n        out = self.out_drop(out)\n        return out\n\n    @torch.jit.ignore()\n    def _forward_cross_attn(self, x: torch.Tensor, x_prev: Optional[torch.Tensor] = None) -> torch.Tensor:\n        # x --> [B, C, P, N]\n        # x_prev = [B, C, P, M]\n        batch_size, in_dim, kv_patch_area, kv_num_patches = x.shape\n        q_patch_area, q_num_patches = x.shape[-2:]\n\n        assert (\n            kv_patch_area == q_patch_area\n        ), \"The number of pixels in a patch for query and key_value should be the same\"\n\n        # compute query, key, and value\n        # [B, C, P, M] --> [B, 1 + d, P, M]\n        qk = F.conv2d(\n            x_prev,\n            weight=self.qkv_proj.weight[:self.embed_dim + 1],\n            bias=self.qkv_proj.bias[:self.embed_dim + 1],\n        )\n\n        # [B, 1 + d, P, M] --> [B, 1, P, M], [B, d, P, M]\n        query, key = qk.split([1, self.embed_dim], dim=1)\n        # [B, C, P, N] --> [B, d, P, N]\n        value = F.conv2d(\n            x,\n            weight=self.qkv_proj.weight[self.embed_dim + 1],\n            bias=self.qkv_proj.bias[self.embed_dim + 1] if self.qkv_proj.bias is not None else None,\n        )\n\n        # apply softmax along M dimension\n        context_scores = F.softmax(query, dim=-1)\n        context_scores = self.attn_drop(context_scores)\n\n        # compute context vector\n        # [B, d, P, M] * [B, 1, P, M] -> [B, d, P, M] --> [B, d, P, 1]\n        context_vector = (key * context_scores).sum(dim=-1, keepdim=True)\n\n        # combine context vector with values\n        # [B, d, P, N] * [B, d, P, 1] --> [B, d, P, N]\n        out = F.relu(value) * context_vector.expand_as(value)\n        out = self.out_proj(out)\n        out = self.out_drop(out)\n        return out\n\n    def forward(self, x: torch.Tensor, x_prev: Optional[torch.Tensor] = None) -> torch.Tensor:\n        if x_prev is None:\n            return self._forward_self_attn(x)\n        else:\n            return self._forward_cross_attn(x, x_prev=x_prev)\n\n\nclass LinearTransformerBlock(nn.Module):\n    \"\"\"\n    This class defines the pre-norm transformer encoder with linear self-attention in `MobileViTv2 paper <>`_\n    Args:\n        embed_dim (int): :math:`C_{in}` from an expected input of size :math:`(B, C_{in}, P, N)`\n        mlp_ratio (float): Inner dimension ratio of the FFN relative to embed_dim\n        drop (float): Dropout rate. Default: 0.0\n        attn_drop (float): Dropout rate for attention in multi-head attention. Default: 0.0\n        drop_path (float): Stochastic depth rate Default: 0.0\n        norm_layer (Callable): Normalization layer. Default: layer_norm_2d\n    Shape:\n        - Input: :math:`(B, C_{in}, P, N)` where :math:`B` is batch size, :math:`C_{in}` is input embedding dim,\n            :math:`P` is number of pixels in a patch, and :math:`N` is number of patches,\n        - Output: same shape as the input\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim: int,\n        mlp_ratio: float = 2.0,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        drop_path: float = 0.0,\n        act_layer=None,\n        norm_layer=None,\n    ) -> None:\n        super().__init__()\n        act_layer = act_layer or nn.SiLU\n        norm_layer = norm_layer or GroupNorm1\n\n        self.norm1 = norm_layer(embed_dim)\n        self.attn = LinearSelfAttention(embed_dim=embed_dim, attn_drop=attn_drop, proj_drop=drop)\n        self.drop_path1 = DropPath(drop_path)\n\n        self.norm2 = norm_layer(embed_dim)\n        self.mlp = ConvMlp(\n            in_features=embed_dim,\n            hidden_features=int(embed_dim * mlp_ratio),\n            act_layer=act_layer,\n            drop=drop)\n        self.drop_path2 = DropPath(drop_path)\n\n    def forward(self, x: torch.Tensor, x_prev: Optional[torch.Tensor] = None) -> torch.Tensor:\n        if x_prev is None:\n            # self-attention\n            x = x + self.drop_path1(self.attn(self.norm1(x)))\n        else:\n            # cross-attention\n            res = x\n            x = self.norm1(x)  # norm\n            x = self.attn(x, x_prev)  # attn\n            x = self.drop_path1(x) + res  # residual\n\n        # Feed forward network\n        x = x + self.drop_path2(self.mlp(self.norm2(x)))\n        return x\n\n\n@register_notrace_module\nclass MobileVitV2Block(nn.Module):\n    \"\"\"\n    This class defines the `MobileViTv2 block <>`_\n    \"\"\"\n\n    def __init__(\n        self,\n        in_chs: int,\n        out_chs: Optional[int] = None,\n        kernel_size: int = 3,\n        bottle_ratio: float = 1.0,\n        group_size: Optional[int] = 1,\n        dilation: Tuple[int, int] = (1, 1),\n        mlp_ratio: float = 2.0,\n        transformer_dim: Optional[int] = None,\n        transformer_depth: int = 2,\n        patch_size: int = 8,\n        attn_drop: float = 0.,\n        drop: int = 0.,\n        drop_path_rate: float = 0.,\n        layers: LayerFn = None,\n        transformer_norm_layer: Callable = GroupNorm1,\n        **kwargs,  # eat unused args\n    ):\n        super(MobileVitV2Block, self).__init__()\n        layers = layers or LayerFn()\n        groups = num_groups(group_size, in_chs)\n        out_chs = out_chs or in_chs\n        transformer_dim = transformer_dim or make_divisible(bottle_ratio * in_chs)\n\n        self.conv_kxk = layers.conv_norm_act(\n            in_chs, in_chs, kernel_size=kernel_size,\n            stride=1, groups=groups, dilation=dilation[0])\n        self.conv_1x1 = nn.Conv2d(in_chs, transformer_dim, kernel_size=1, bias=False)\n\n        self.transformer = nn.Sequential(*[\n            LinearTransformerBlock(\n                transformer_dim,\n                mlp_ratio=mlp_ratio,\n                attn_drop=attn_drop,\n                drop=drop,\n                drop_path=drop_path_rate,\n                act_layer=layers.act,\n                norm_layer=transformer_norm_layer\n            )\n            for _ in range(transformer_depth)\n        ])\n        self.norm = transformer_norm_layer(transformer_dim)\n\n        self.conv_proj = layers.conv_norm_act(transformer_dim, out_chs, kernel_size=1, stride=1, apply_act=False)\n\n        self.patch_size = to_2tuple(patch_size)\n        self.patch_area = self.patch_size[0] * self.patch_size[1]\n        self.coreml_exportable = is_exportable()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, C, H, W = x.shape\n        patch_h, patch_w = self.patch_size\n        new_h, new_w = math.ceil(H / patch_h) * patch_h, math.ceil(W / patch_w) * patch_w\n        num_patch_h, num_patch_w = new_h // patch_h, new_w // patch_w  # n_h, n_w\n        num_patches = num_patch_h * num_patch_w  # N\n        if new_h != H or new_w != W:\n            x = F.interpolate(x, size=(new_h, new_w), mode=\"bilinear\", align_corners=True)\n\n        # Local representation\n        x = self.conv_kxk(x)\n        x = self.conv_1x1(x)\n\n        # Unfold (feature map -> patches), [B, C, H, W] -> [B, C, P, N]\n        C = x.shape[1]\n        if self.coreml_exportable:\n            x = F.unfold(x, kernel_size=(patch_h, patch_w), stride=(patch_h, patch_w))\n        else:\n            x = x.reshape(B, C, num_patch_h, patch_h, num_patch_w, patch_w).permute(0, 1, 3, 5, 2, 4)\n        x = x.reshape(B, C, -1, num_patches)\n\n        # Global representations\n        x = self.transformer(x)\n        x = self.norm(x)\n\n        # Fold (patches -> feature map), [B, C, P, N] --> [B, C, H, W]\n        if self.coreml_exportable:\n            # adopted from https://github.com/apple/ml-cvnets/blob/main/cvnets/modules/mobilevit_block.py#L609-L624\n            x = x.reshape(B, C * patch_h * patch_w, num_patch_h, num_patch_w)\n            x = F.pixel_shuffle(x, upscale_factor=patch_h)\n        else:\n            x = x.reshape(B, C, patch_h, patch_w, num_patch_h, num_patch_w).permute(0, 1, 4, 2, 5, 3)\n            x = x.reshape(B, C, num_patch_h * patch_h, num_patch_w * patch_w)\n\n        x = self.conv_proj(x)\n        return x\n\n\nregister_block('mobilevit', MobileVitBlock)\nregister_block('mobilevit2', MobileVitV2Block)\n\n\ndef _create_mobilevit(variant, cfg_variant=None, pretrained=False, **kwargs):\n    return build_model_with_cfg(\n        ByobNet, variant, pretrained,\n        model_cfg=model_cfgs[variant] if not cfg_variant else model_cfgs[cfg_variant],\n        feature_cfg=dict(flatten_sequential=True),\n        **kwargs)\n\n\ndef _create_mobilevit2(variant, cfg_variant=None, pretrained=False, **kwargs):\n    return build_model_with_cfg(\n        ByobNet, variant, pretrained,\n        model_cfg=model_cfgs[variant] if not cfg_variant else model_cfgs[cfg_variant],\n        feature_cfg=dict(flatten_sequential=True),\n        **kwargs)\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': (8, 8),\n        'crop_pct': 0.9, 'interpolation': 'bicubic',\n        'mean': (0., 0., 0.), 'std': (1., 1., 1.),\n        'first_conv': 'stem.conv', 'classifier': 'head.fc',\n        'fixed_input_size': False,\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'mobilevit_xxs.cvnets_in1k': _cfg(hf_hub_id='timm/'),\n    'mobilevit_xs.cvnets_in1k': _cfg(hf_hub_id='timm/'),\n    'mobilevit_s.cvnets_in1k': _cfg(hf_hub_id='timm/'),\n\n    'mobilevitv2_050.cvnets_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.888),\n    'mobilevitv2_075.cvnets_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.888),\n    'mobilevitv2_100.cvnets_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.888),\n    'mobilevitv2_125.cvnets_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.888),\n    'mobilevitv2_150.cvnets_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.888),\n    'mobilevitv2_175.cvnets_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.888),\n    'mobilevitv2_200.cvnets_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.888),\n\n    'mobilevitv2_150.cvnets_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.888),\n    'mobilevitv2_175.cvnets_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.888),\n    'mobilevitv2_200.cvnets_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.888),\n\n    'mobilevitv2_150.cvnets_in22k_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),\n    'mobilevitv2_175.cvnets_in22k_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),\n    'mobilevitv2_200.cvnets_in22k_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),\n})\n\n\n@register_model\ndef mobilevit_xxs(pretrained=False, **kwargs) -> ByobNet:\n    return _create_mobilevit('mobilevit_xxs', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef mobilevit_xs(pretrained=False, **kwargs) -> ByobNet:\n    return _create_mobilevit('mobilevit_xs', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef mobilevit_s(pretrained=False, **kwargs) -> ByobNet:\n    return _create_mobilevit('mobilevit_s', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef mobilevitv2_050(pretrained=False, **kwargs) -> ByobNet:\n    return _create_mobilevit('mobilevitv2_050', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef mobilevitv2_075(pretrained=False, **kwargs) -> ByobNet:\n    return _create_mobilevit('mobilevitv2_075', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef mobilevitv2_100(pretrained=False, **kwargs) -> ByobNet:\n    return _create_mobilevit('mobilevitv2_100', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef mobilevitv2_125(pretrained=False, **kwargs) -> ByobNet:\n    return _create_mobilevit('mobilevitv2_125', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef mobilevitv2_150(pretrained=False, **kwargs) -> ByobNet:\n    return _create_mobilevit('mobilevitv2_150', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef mobilevitv2_175(pretrained=False, **kwargs) -> ByobNet:\n    return _create_mobilevit('mobilevitv2_175', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef mobilevitv2_200(pretrained=False, **kwargs) -> ByobNet:\n    return _create_mobilevit('mobilevitv2_200', pretrained=pretrained, **kwargs)\n\n\nregister_model_deprecations(__name__, {\n    'mobilevitv2_150_in22ft1k': 'mobilevitv2_150.cvnets_in22k_ft_in1k',\n    'mobilevitv2_175_in22ft1k': 'mobilevitv2_175.cvnets_in22k_ft_in1k',\n    'mobilevitv2_200_in22ft1k': 'mobilevitv2_200.cvnets_in22k_ft_in1k',\n\n    'mobilevitv2_150_384_in22ft1k': 'mobilevitv2_150.cvnets_in22k_ft_in1k_384',\n    'mobilevitv2_175_384_in22ft1k': 'mobilevitv2_175.cvnets_in22k_ft_in1k_384',\n    'mobilevitv2_200_384_in22ft1k': 'mobilevitv2_200.cvnets_in22k_ft_in1k_384',\n})",
  "\"\"\" Pooling-based Vision Transformer (PiT) in PyTorch\n\nA PyTorch implement of Pooling-based Vision Transformers as described in\n'Rethinking Spatial Dimensions of Vision Transformers' - https://arxiv.org/abs/2103.16302\n\nThis code was adapted from the original version at https://github.com/naver-ai/pit, original copyright below.\n\nModifications for timm by / Copyright 2020 Ross Wightman\n\"\"\"\n# PiT\n# Copyright 2021-present NAVER Corp.\n# Apache License v2.0\n\nimport math\nimport re\nfrom functools import partial\nfrom typing import Sequence, Tuple\n\nimport torch\nfrom torch import nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import trunc_normal_, to_2tuple, LayerNorm\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\nfrom .vision_transformer import Block\n\n\n__all__ = ['PoolingVisionTransformer']  # model_registry will add each entrypoint fn to this\n\n\nclass SequentialTuple(nn.Sequential):\n    \"\"\" This module exists to work around torchscript typing issues list -> list\"\"\"\n    def __init__(self, *args):\n        super(SequentialTuple, self).__init__(*args)\n\n    def forward(self, x: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n        for module in self:\n            x = module(x)\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(\n            self,\n            base_dim,\n            depth,\n            heads,\n            mlp_ratio,\n            pool=None,\n            proj_drop=.0,\n            attn_drop=.0,\n            drop_path_prob=None,\n            norm_layer=None,\n    ):\n        super(Transformer, self).__init__()\n        embed_dim = base_dim * heads\n\n        self.pool = pool\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n        self.blocks = nn.Sequential(*[\n            Block(\n                dim=embed_dim,\n                num_heads=heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=True,\n                proj_drop=proj_drop,\n                attn_drop=attn_drop,\n                drop_path=drop_path_prob[i],\n                norm_layer=partial(nn.LayerNorm, eps=1e-6)\n            )\n            for i in range(depth)])\n\n    def forward(self, x: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n        x, cls_tokens = x\n        token_length = cls_tokens.shape[1]\n        if self.pool is not None:\n            x, cls_tokens = self.pool(x, cls_tokens)\n\n        B, C, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        x = self.norm(x)\n        x = self.blocks(x)\n\n        cls_tokens = x[:, :token_length]\n        x = x[:, token_length:]\n        x = x.transpose(1, 2).reshape(B, C, H, W)\n\n        return x, cls_tokens\n\n\nclass Pooling(nn.Module):\n    def __init__(self, in_feature, out_feature, stride, padding_mode='zeros'):\n        super(Pooling, self).__init__()\n\n        self.conv = nn.Conv2d(\n            in_feature,\n            out_feature,\n            kernel_size=stride + 1,\n            padding=stride // 2,\n            stride=stride,\n            padding_mode=padding_mode,\n            groups=in_feature,\n        )\n        self.fc = nn.Linear(in_feature, out_feature)\n\n    def forward(self, x, cls_token) -> Tuple[torch.Tensor, torch.Tensor]:\n        x = self.conv(x)\n        cls_token = self.fc(cls_token)\n        return x, cls_token\n\n\nclass ConvEmbedding(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            img_size: int = 224,\n            patch_size: int = 16,\n            stride: int = 8,\n            padding: int = 0,\n    ):\n        super(ConvEmbedding, self).__init__()\n        padding = padding\n        self.img_size = to_2tuple(img_size)\n        self.patch_size = to_2tuple(patch_size)\n        self.height = math.floor((self.img_size[0] + 2 * padding - self.patch_size[0]) / stride + 1)\n        self.width = math.floor((self.img_size[1] + 2 * padding - self.patch_size[1]) / stride + 1)\n        self.grid_size = (self.height, self.width)\n\n        self.conv = nn.Conv2d(\n            in_channels, out_channels, kernel_size=patch_size,\n            stride=stride, padding=padding, bias=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass PoolingVisionTransformer(nn.Module):\n    \"\"\" Pooling-based Vision Transformer\n\n    A PyTorch implement of 'Rethinking Spatial Dimensions of Vision Transformers'\n        - https://arxiv.org/abs/2103.16302\n    \"\"\"\n    def __init__(\n            self,\n            img_size: int = 224,\n            patch_size: int = 16,\n            stride: int = 8,\n            stem_type: str = 'overlap',\n            base_dims: Sequence[int] = (48, 48, 48),\n            depth: Sequence[int] = (2, 6, 4),\n            heads: Sequence[int] = (2, 4, 8),\n            mlp_ratio: float = 4,\n            num_classes=1000,\n            in_chans=3,\n            global_pool='token',\n            distilled=False,\n            drop_rate=0.,\n            pos_drop_drate=0.,\n            proj_drop_rate=0.,\n            attn_drop_rate=0.,\n            drop_path_rate=0.,\n    ):\n        super(PoolingVisionTransformer, self).__init__()\n        assert global_pool in ('token',)\n\n        self.base_dims = base_dims\n        self.heads = heads\n        embed_dim = base_dims[0] * heads[0]\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.num_tokens = 2 if distilled else 1\n        self.feature_info = []\n\n        self.patch_embed = ConvEmbedding(in_chans, embed_dim, img_size, patch_size, stride)\n        self.pos_embed = nn.Parameter(torch.randn(1, embed_dim, self.patch_embed.height, self.patch_embed.width))\n        self.cls_token = nn.Parameter(torch.randn(1, self.num_tokens, embed_dim))\n        self.pos_drop = nn.Dropout(p=pos_drop_drate)\n\n        transformers = []\n        # stochastic depth decay rule\n        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depth)).split(depth)]\n        prev_dim = embed_dim\n        for i in range(len(depth)):\n            pool = None\n            embed_dim = base_dims[i] * heads[i]\n            if i > 0:\n                pool = Pooling(\n                    prev_dim,\n                    embed_dim,\n                    stride=2,\n                )\n            transformers += [Transformer(\n                base_dims[i],\n                depth[i],\n                heads[i],\n                mlp_ratio,\n                pool=pool,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path_prob=dpr[i],\n            )]\n            prev_dim = embed_dim\n            self.feature_info += [dict(num_chs=prev_dim, reduction=(stride - 1) * 2**i, module=f'transformers.{i}')]\n\n        self.transformers = SequentialTuple(*transformers)\n        self.norm = nn.LayerNorm(base_dims[-1] * heads[-1], eps=1e-6)\n        self.num_features = self.embed_dim = embed_dim\n\n        # Classifier head\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n        self.head_dist = None\n        if distilled:\n            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n        self.distilled_training = False  # must set this True to train w/ distillation token\n\n        trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    @torch.jit.ignore\n    def set_distilled_training(self, enable=True):\n        self.distilled_training = enable\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, 'gradient checkpointing not supported'\n\n    def get_classifier(self):\n        if self.head_dist is not None:\n            return self.head, self.head_dist\n        else:\n            return self.head\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n        if self.head_dist is not None:\n            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = self.pos_drop(x + self.pos_embed)\n        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n        x, cls_tokens = self.transformers((x, cls_tokens))\n        cls_tokens = self.norm(cls_tokens)\n        return cls_tokens\n\n    def forward_head(self, x, pre_logits: bool = False) -> torch.Tensor:\n        if self.head_dist is not None:\n            assert self.global_pool == 'token'\n            x, x_dist = x[:, 0], x[:, 1]\n            x = self.head_drop(x)\n            x_dist = self.head_drop(x)\n            if not pre_logits:\n                x = self.head(x)\n                x_dist = self.head_dist(x_dist)\n            if self.distilled_training and self.training and not torch.jit.is_scripting():\n                # only return separate classification predictions when training in distilled mode\n                return x, x_dist\n            else:\n                # during standard train / finetune, inference average the classifier predictions\n                return (x + x_dist) / 2\n        else:\n            if self.global_pool == 'token':\n                x = x[:, 0]\n            x = self.head_drop(x)\n            if not pre_logits:\n                x = self.head(x)\n            return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    \"\"\" preprocess checkpoints \"\"\"\n    out_dict = {}\n    p_blocks = re.compile(r'pools\\.(\\d)\\.')\n    for k, v in state_dict.items():\n        # FIXME need to update resize for PiT impl\n        # if k == 'pos_embed' and v.shape != model.pos_embed.shape:\n        #     # To resize pos embedding when using model at different size from pretrained weights\n        #     v = resize_pos_embed(v, model.pos_embed)\n        k = p_blocks.sub(lambda exp: f'transformers.{int(exp.group(1)) + 1}.pool.', k)\n        out_dict[k] = v\n    return out_dict\n\n\ndef _create_pit(variant, pretrained=False, **kwargs):\n    default_out_indices = tuple(range(3))\n    out_indices = kwargs.pop('out_indices', default_out_indices)\n\n    model = build_model_with_cfg(\n        PoolingVisionTransformer,\n        variant,\n        pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        feature_cfg=dict(feature_cls='hook', no_rewrite=True, out_indices=out_indices),\n        **kwargs,\n    )\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'patch_embed.conv', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    # deit models (FB weights)\n    'pit_ti_224.in1k': _cfg(hf_hub_id='timm/'),\n    'pit_xs_224.in1k': _cfg(hf_hub_id='timm/'),\n    'pit_s_224.in1k': _cfg(hf_hub_id='timm/'),\n    'pit_b_224.in1k': _cfg(hf_hub_id='timm/'),\n    'pit_ti_distilled_224.in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier=('head', 'head_dist')),\n    'pit_xs_distilled_224.in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier=('head', 'head_dist')),\n    'pit_s_distilled_224.in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier=('head', 'head_dist')),\n    'pit_b_distilled_224.in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier=('head', 'head_dist')),\n})\n\n\n@register_model\ndef pit_b_224(pretrained=False, **kwargs) -> PoolingVisionTransformer:\n    model_args = dict(\n        patch_size=14,\n        stride=7,\n        base_dims=[64, 64, 64],\n        depth=[3, 6, 4],\n        heads=[4, 8, 16],\n        mlp_ratio=4,\n    )\n    return _create_pit('pit_b_224', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef pit_s_224(pretrained=False, **kwargs) -> PoolingVisionTransformer:\n    model_args = dict(\n        patch_size=16,\n        stride=8,\n        base_dims=[48, 48, 48],\n        depth=[2, 6, 4],\n        heads=[3, 6, 12],\n        mlp_ratio=4,\n    )\n    return _create_pit('pit_s_224', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef pit_xs_224(pretrained=False, **kwargs) -> PoolingVisionTransformer:\n    model_args = dict(\n        patch_size=16,\n        stride=8,\n        base_dims=[48, 48, 48],\n        depth=[2, 6, 4],\n        heads=[2, 4, 8],\n        mlp_ratio=4,\n    )\n    return _create_pit('pit_xs_224', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef pit_ti_224(pretrained=False, **kwargs) -> PoolingVisionTransformer:\n    model_args = dict(\n        patch_size=16,\n        stride=8,\n        base_dims=[32, 32, 32],\n        depth=[2, 6, 4],\n        heads=[2, 4, 8],\n        mlp_ratio=4,\n    )\n    return _create_pit('pit_ti_224', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef pit_b_distilled_224(pretrained=False, **kwargs) -> PoolingVisionTransformer:\n    model_args = dict(\n        patch_size=14,\n        stride=7,\n        base_dims=[64, 64, 64],\n        depth=[3, 6, 4],\n        heads=[4, 8, 16],\n        mlp_ratio=4,\n        distilled=True,\n    )\n    return _create_pit('pit_b_distilled_224', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef pit_s_distilled_224(pretrained=False, **kwargs) -> PoolingVisionTransformer:\n    model_args = dict(\n        patch_size=16,\n        stride=8,\n        base_dims=[48, 48, 48],\n        depth=[2, 6, 4],\n        heads=[3, 6, 12],\n        mlp_ratio=4,\n        distilled=True,\n    )\n    return _create_pit('pit_s_distilled_224', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef pit_xs_distilled_224(pretrained=False, **kwargs) -> PoolingVisionTransformer:\n    model_args = dict(\n        patch_size=16,\n        stride=8,\n        base_dims=[48, 48, 48],\n        depth=[2, 6, 4],\n        heads=[2, 4, 8],\n        mlp_ratio=4,\n        distilled=True,\n    )\n    return _create_pit('pit_xs_distilled_224', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef pit_ti_distilled_224(pretrained=False, **kwargs) -> PoolingVisionTransformer:\n    model_args = dict(\n        patch_size=16,\n        stride=8,\n        base_dims=[32, 32, 32],\n        depth=[2, 6, 4],\n        heads=[2, 4, 8],\n        mlp_ratio=4,\n        distilled=True,\n    )\n    return _create_pit('pit_ti_distilled_224', pretrained, **dict(model_args, **kwargs))\n",
  "\"\"\"PyTorch CspNet\n\nA PyTorch implementation of Cross Stage Partial Networks including:\n* CSPResNet50\n* CSPResNeXt50\n* CSPDarkNet53\n* and DarkNet53 for good measure\n\nBased on paper `CSPNet: A New Backbone that can Enhance Learning Capability of CNN` - https://arxiv.org/abs/1911.11929\n\nReference impl via darknet cfg files at https://github.com/WongKinYiu/CrossStagePartialNetworks\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom dataclasses import dataclass, asdict, replace\nfrom functools import partial\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import ClassifierHead, ConvNormAct, ConvNormActAa, DropPath, get_attn, create_act_layer, make_divisible\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import named_apply, MATCH_PREV_GROUP\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['CspNet']  # model_registry will add each entrypoint fn to this\n\n\n@dataclass\nclass CspStemCfg:\n    out_chs: Union[int, Tuple[int, ...]] = 32\n    stride: Union[int, Tuple[int, ...]] = 2\n    kernel_size: int = 3\n    padding: Union[int, str] = ''\n    pool: Optional[str] = ''\n\n\ndef _pad_arg(x, n):\n    # pads an argument tuple to specified n by padding with last value\n    if not isinstance(x, (tuple, list)):\n        x = (x,)\n    curr_n = len(x)\n    pad_n = n - curr_n\n    if pad_n <= 0:\n        return x[:n]\n    return tuple(x + (x[-1],) * pad_n)\n\n\n@dataclass\nclass CspStagesCfg:\n    depth: Tuple[int, ...] = (3, 3, 5, 2)  # block depth (number of block repeats in stages)\n    out_chs: Tuple[int, ...] = (128, 256, 512, 1024)  # number of output channels for blocks in stage\n    stride: Union[int, Tuple[int, ...]] = 2  # stride of stage\n    groups: Union[int, Tuple[int, ...]] = 1  # num kxk conv groups\n    block_ratio: Union[float, Tuple[float, ...]] = 1.0\n    bottle_ratio: Union[float, Tuple[float, ...]] = 1.  # bottleneck-ratio of blocks in stage\n    avg_down: Union[bool, Tuple[bool, ...]] = False\n    attn_layer: Optional[Union[str, Tuple[str, ...]]] = None\n    attn_kwargs: Optional[Union[Dict, Tuple[Dict]]] = None\n    stage_type: Union[str, Tuple[str]] = 'csp'  # stage type ('csp', 'cs2', 'dark')\n    block_type: Union[str, Tuple[str]] = 'bottle'  # blocks type for stages ('bottle', 'dark')\n\n    # cross-stage only\n    expand_ratio: Union[float, Tuple[float, ...]] = 1.0\n    cross_linear: Union[bool, Tuple[bool, ...]] = False\n    down_growth: Union[bool, Tuple[bool, ...]] = False\n\n    def __post_init__(self):\n        n = len(self.depth)\n        assert len(self.out_chs) == n\n        self.stride = _pad_arg(self.stride, n)\n        self.groups = _pad_arg(self.groups, n)\n        self.block_ratio = _pad_arg(self.block_ratio, n)\n        self.bottle_ratio = _pad_arg(self.bottle_ratio, n)\n        self.avg_down = _pad_arg(self.avg_down, n)\n        self.attn_layer = _pad_arg(self.attn_layer, n)\n        self.attn_kwargs = _pad_arg(self.attn_kwargs, n)\n        self.stage_type = _pad_arg(self.stage_type, n)\n        self.block_type = _pad_arg(self.block_type, n)\n\n        self.expand_ratio = _pad_arg(self.expand_ratio, n)\n        self.cross_linear = _pad_arg(self.cross_linear, n)\n        self.down_growth = _pad_arg(self.down_growth, n)\n\n\n@dataclass\nclass CspModelCfg:\n    stem: CspStemCfg\n    stages: CspStagesCfg\n    zero_init_last: bool = True  # zero init last weight (usually bn) in residual path\n    act_layer: str = 'leaky_relu'\n    norm_layer: str = 'batchnorm'\n    aa_layer: Optional[str] = None  # FIXME support string factory for this\n\n\ndef _cs3_cfg(\n        width_multiplier=1.0,\n        depth_multiplier=1.0,\n        avg_down=False,\n        act_layer='silu',\n        focus=False,\n        attn_layer=None,\n        attn_kwargs=None,\n        bottle_ratio=1.0,\n        block_type='dark',\n):\n    if focus:\n        stem_cfg = CspStemCfg(\n            out_chs=make_divisible(64 * width_multiplier),\n            kernel_size=6, stride=2, padding=2, pool='')\n    else:\n        stem_cfg = CspStemCfg(\n            out_chs=tuple([make_divisible(c * width_multiplier) for c in (32, 64)]),\n            kernel_size=3, stride=2, pool='')\n    return CspModelCfg(\n        stem=stem_cfg,\n        stages=CspStagesCfg(\n            out_chs=tuple([make_divisible(c * width_multiplier) for c in (128, 256, 512, 1024)]),\n            depth=tuple([int(d * depth_multiplier) for d in (3, 6, 9, 3)]),\n            stride=2,\n            bottle_ratio=bottle_ratio,\n            block_ratio=0.5,\n            avg_down=avg_down,\n            attn_layer=attn_layer,\n            attn_kwargs=attn_kwargs,\n            stage_type='cs3',\n            block_type=block_type,\n        ),\n        act_layer=act_layer,\n    )\n\n\nclass BottleneckBlock(nn.Module):\n    \"\"\" ResNe(X)t Bottleneck Block\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            dilation=1,\n            bottle_ratio=0.25,\n            groups=1,\n            act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d,\n            attn_last=False,\n            attn_layer=None,\n            drop_block=None,\n            drop_path=0.\n    ):\n        super(BottleneckBlock, self).__init__()\n        mid_chs = int(round(out_chs * bottle_ratio))\n        ckwargs = dict(act_layer=act_layer, norm_layer=norm_layer)\n        attn_last = attn_layer is not None and attn_last\n        attn_first = attn_layer is not None and not attn_last\n\n        self.conv1 = ConvNormAct(in_chs, mid_chs, kernel_size=1, **ckwargs)\n        self.conv2 = ConvNormAct(\n            mid_chs, mid_chs, kernel_size=3, dilation=dilation, groups=groups,\n            drop_layer=drop_block, **ckwargs)\n        self.attn2 = attn_layer(mid_chs, act_layer=act_layer) if attn_first else nn.Identity()\n        self.conv3 = ConvNormAct(mid_chs, out_chs, kernel_size=1, apply_act=False, **ckwargs)\n        self.attn3 = attn_layer(out_chs, act_layer=act_layer) if attn_last else nn.Identity()\n        self.drop_path = DropPath(drop_path) if drop_path else nn.Identity()\n        self.act3 = create_act_layer(act_layer)\n\n    def zero_init_last(self):\n        nn.init.zeros_(self.conv3.bn.weight)\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.attn2(x)\n        x = self.conv3(x)\n        x = self.attn3(x)\n        x = self.drop_path(x) + shortcut\n        # FIXME partial shortcut needed if first block handled as per original, not used for my current impl\n        #x[:, :shortcut.size(1)] += shortcut\n        x = self.act3(x)\n        return x\n\n\nclass DarkBlock(nn.Module):\n    \"\"\" DarkNet Block\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            dilation=1,\n            bottle_ratio=0.5,\n            groups=1,\n            act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d,\n            attn_layer=None,\n            drop_block=None,\n            drop_path=0.\n    ):\n        super(DarkBlock, self).__init__()\n        mid_chs = int(round(out_chs * bottle_ratio))\n        ckwargs = dict(act_layer=act_layer, norm_layer=norm_layer)\n\n        self.conv1 = ConvNormAct(in_chs, mid_chs, kernel_size=1, **ckwargs)\n        self.attn = attn_layer(mid_chs, act_layer=act_layer) if attn_layer is not None else nn.Identity()\n        self.conv2 = ConvNormAct(\n            mid_chs, out_chs, kernel_size=3, dilation=dilation, groups=groups,\n            drop_layer=drop_block, **ckwargs)\n        self.drop_path = DropPath(drop_path) if drop_path else nn.Identity()\n\n    def zero_init_last(self):\n        nn.init.zeros_(self.conv2.bn.weight)\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv1(x)\n        x = self.attn(x)\n        x = self.conv2(x)\n        x = self.drop_path(x) + shortcut\n        return x\n\n\nclass EdgeBlock(nn.Module):\n    \"\"\" EdgeResidual / Fused-MBConv / MobileNetV1-like 3x3 + 1x1 block (w/ activated output)\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            dilation=1,\n            bottle_ratio=0.5,\n            groups=1,\n            act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d,\n            attn_layer=None,\n            drop_block=None,\n            drop_path=0.\n    ):\n        super(EdgeBlock, self).__init__()\n        mid_chs = int(round(out_chs * bottle_ratio))\n        ckwargs = dict(act_layer=act_layer, norm_layer=norm_layer)\n\n        self.conv1 = ConvNormAct(\n            in_chs, mid_chs, kernel_size=3, dilation=dilation, groups=groups,\n            drop_layer=drop_block, **ckwargs)\n        self.attn = attn_layer(mid_chs, act_layer=act_layer) if attn_layer is not None else nn.Identity()\n        self.conv2 = ConvNormAct(mid_chs, out_chs, kernel_size=1, **ckwargs)\n        self.drop_path = DropPath(drop_path) if drop_path else nn.Identity()\n\n    def zero_init_last(self):\n        nn.init.zeros_(self.conv2.bn.weight)\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv1(x)\n        x = self.attn(x)\n        x = self.conv2(x)\n        x = self.drop_path(x) + shortcut\n        return x\n\n\nclass CrossStage(nn.Module):\n    \"\"\"Cross Stage.\"\"\"\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            stride,\n            dilation,\n            depth,\n            block_ratio=1.,\n            bottle_ratio=1.,\n            expand_ratio=1.,\n            groups=1,\n            first_dilation=None,\n            avg_down=False,\n            down_growth=False,\n            cross_linear=False,\n            block_dpr=None,\n            block_fn=BottleneckBlock,\n            **block_kwargs,\n    ):\n        super(CrossStage, self).__init__()\n        first_dilation = first_dilation or dilation\n        down_chs = out_chs if down_growth else in_chs  # grow downsample channels to output channels\n        self.expand_chs = exp_chs = int(round(out_chs * expand_ratio))\n        block_out_chs = int(round(out_chs * block_ratio))\n        conv_kwargs = dict(act_layer=block_kwargs.get('act_layer'), norm_layer=block_kwargs.get('norm_layer'))\n        aa_layer = block_kwargs.pop('aa_layer', None)\n\n        if stride != 1 or first_dilation != dilation:\n            if avg_down:\n                self.conv_down = nn.Sequential(\n                    nn.AvgPool2d(2) if stride == 2 else nn.Identity(),  # FIXME dilation handling\n                    ConvNormActAa(in_chs, out_chs, kernel_size=1, stride=1, groups=groups, **conv_kwargs)\n                )\n            else:\n                self.conv_down = ConvNormActAa(\n                    in_chs, down_chs, kernel_size=3, stride=stride, dilation=first_dilation, groups=groups,\n                    aa_layer=aa_layer, **conv_kwargs)\n            prev_chs = down_chs\n        else:\n            self.conv_down = nn.Identity()\n            prev_chs = in_chs\n\n        # FIXME this 1x1 expansion is pushed down into the cross and block paths in the darknet cfgs. Also,\n        # there is also special case for the first stage for some of the model that results in uneven split\n        # across the two paths. I did it this way for simplicity for now.\n        self.conv_exp = ConvNormAct(prev_chs, exp_chs, kernel_size=1, apply_act=not cross_linear, **conv_kwargs)\n        prev_chs = exp_chs // 2  # output of conv_exp is always split in two\n\n        self.blocks = nn.Sequential()\n        for i in range(depth):\n            self.blocks.add_module(str(i), block_fn(\n                in_chs=prev_chs,\n                out_chs=block_out_chs,\n                dilation=dilation,\n                bottle_ratio=bottle_ratio,\n                groups=groups,\n                drop_path=block_dpr[i] if block_dpr is not None else 0.,\n                **block_kwargs,\n            ))\n            prev_chs = block_out_chs\n\n        # transition convs\n        self.conv_transition_b = ConvNormAct(prev_chs, exp_chs // 2, kernel_size=1, **conv_kwargs)\n        self.conv_transition = ConvNormAct(exp_chs, out_chs, kernel_size=1, **conv_kwargs)\n\n    def forward(self, x):\n        x = self.conv_down(x)\n        x = self.conv_exp(x)\n        xs, xb = x.split(self.expand_chs // 2, dim=1)\n        xb = self.blocks(xb)\n        xb = self.conv_transition_b(xb).contiguous()\n        out = self.conv_transition(torch.cat([xs, xb], dim=1))\n        return out\n\n\nclass CrossStage3(nn.Module):\n    \"\"\"Cross Stage 3.\n    Similar to CrossStage, but with only one transition conv for the output.\n    \"\"\"\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            stride,\n            dilation,\n            depth,\n            block_ratio=1.,\n            bottle_ratio=1.,\n            expand_ratio=1.,\n            groups=1,\n            first_dilation=None,\n            avg_down=False,\n            down_growth=False,\n            cross_linear=False,\n            block_dpr=None,\n            block_fn=BottleneckBlock,\n            **block_kwargs,\n    ):\n        super(CrossStage3, self).__init__()\n        first_dilation = first_dilation or dilation\n        down_chs = out_chs if down_growth else in_chs  # grow downsample channels to output channels\n        self.expand_chs = exp_chs = int(round(out_chs * expand_ratio))\n        block_out_chs = int(round(out_chs * block_ratio))\n        conv_kwargs = dict(act_layer=block_kwargs.get('act_layer'), norm_layer=block_kwargs.get('norm_layer'))\n        aa_layer = block_kwargs.pop('aa_layer', None)\n\n        if stride != 1 or first_dilation != dilation:\n            if avg_down:\n                self.conv_down = nn.Sequential(\n                    nn.AvgPool2d(2) if stride == 2 else nn.Identity(),  # FIXME dilation handling\n                    ConvNormActAa(in_chs, out_chs, kernel_size=1, stride=1, groups=groups, **conv_kwargs)\n                )\n            else:\n                self.conv_down = ConvNormActAa(\n                    in_chs, down_chs, kernel_size=3, stride=stride, dilation=first_dilation, groups=groups,\n                    aa_layer=aa_layer, **conv_kwargs)\n            prev_chs = down_chs\n        else:\n            self.conv_down = None\n            prev_chs = in_chs\n\n        # expansion conv\n        self.conv_exp = ConvNormAct(prev_chs, exp_chs, kernel_size=1, apply_act=not cross_linear, **conv_kwargs)\n        prev_chs = exp_chs // 2  # expanded output is split in 2 for blocks and cross stage\n\n        self.blocks = nn.Sequential()\n        for i in range(depth):\n            self.blocks.add_module(str(i), block_fn(\n                in_chs=prev_chs,\n                out_chs=block_out_chs,\n                dilation=dilation,\n                bottle_ratio=bottle_ratio,\n                groups=groups,\n                drop_path=block_dpr[i] if block_dpr is not None else 0.,\n                **block_kwargs,\n            ))\n            prev_chs = block_out_chs\n\n        # transition convs\n        self.conv_transition = ConvNormAct(exp_chs, out_chs, kernel_size=1, **conv_kwargs)\n\n    def forward(self, x):\n        x = self.conv_down(x)\n        x = self.conv_exp(x)\n        x1, x2 = x.split(self.expand_chs // 2, dim=1)\n        x1 = self.blocks(x1)\n        out = self.conv_transition(torch.cat([x1, x2], dim=1))\n        return out\n\n\nclass DarkStage(nn.Module):\n    \"\"\"DarkNet stage.\"\"\"\n\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            stride,\n            dilation,\n            depth,\n            block_ratio=1.,\n            bottle_ratio=1.,\n            groups=1,\n            first_dilation=None,\n            avg_down=False,\n            block_fn=BottleneckBlock,\n            block_dpr=None,\n            **block_kwargs,\n    ):\n        super(DarkStage, self).__init__()\n        first_dilation = first_dilation or dilation\n        conv_kwargs = dict(act_layer=block_kwargs.get('act_layer'), norm_layer=block_kwargs.get('norm_layer'))\n        aa_layer = block_kwargs.pop('aa_layer', None)\n\n        if avg_down:\n            self.conv_down = nn.Sequential(\n                nn.AvgPool2d(2) if stride == 2 else nn.Identity(),   # FIXME dilation handling\n                ConvNormActAa(in_chs, out_chs, kernel_size=1, stride=1, groups=groups, **conv_kwargs)\n            )\n        else:\n            self.conv_down = ConvNormActAa(\n                in_chs, out_chs, kernel_size=3, stride=stride, dilation=first_dilation, groups=groups,\n                aa_layer=aa_layer, **conv_kwargs)\n\n        prev_chs = out_chs\n        block_out_chs = int(round(out_chs * block_ratio))\n        self.blocks = nn.Sequential()\n        for i in range(depth):\n            self.blocks.add_module(str(i), block_fn(\n                in_chs=prev_chs,\n                out_chs=block_out_chs,\n                dilation=dilation,\n                bottle_ratio=bottle_ratio,\n                groups=groups,\n                drop_path=block_dpr[i] if block_dpr is not None else 0.,\n                **block_kwargs\n            ))\n            prev_chs = block_out_chs\n\n    def forward(self, x):\n        x = self.conv_down(x)\n        x = self.blocks(x)\n        return x\n\n\ndef create_csp_stem(\n        in_chans=3,\n        out_chs=32,\n        kernel_size=3,\n        stride=2,\n        pool='',\n        padding='',\n        act_layer=nn.ReLU,\n        norm_layer=nn.BatchNorm2d,\n        aa_layer=None,\n):\n    stem = nn.Sequential()\n    feature_info = []\n    if not isinstance(out_chs, (tuple, list)):\n        out_chs = [out_chs]\n    stem_depth = len(out_chs)\n    assert stem_depth\n    assert stride in (1, 2, 4)\n    prev_feat = None\n    prev_chs = in_chans\n    last_idx = stem_depth - 1\n    stem_stride = 1\n    for i, chs in enumerate(out_chs):\n        conv_name = f'conv{i + 1}'\n        conv_stride = 2 if (i == 0 and stride > 1) or (i == last_idx and stride > 2 and not pool) else 1\n        if conv_stride > 1 and prev_feat is not None:\n            feature_info.append(prev_feat)\n        stem.add_module(conv_name, ConvNormAct(\n            prev_chs, chs, kernel_size,\n            stride=conv_stride,\n            padding=padding if i == 0 else '',\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n        ))\n        stem_stride *= conv_stride\n        prev_chs = chs\n        prev_feat = dict(num_chs=prev_chs, reduction=stem_stride, module='.'.join(['stem', conv_name]))\n    if pool:\n        assert stride > 2\n        if prev_feat is not None:\n            feature_info.append(prev_feat)\n        if aa_layer is not None:\n            stem.add_module('pool', nn.MaxPool2d(kernel_size=3, stride=1, padding=1))\n            stem.add_module('aa', aa_layer(channels=prev_chs, stride=2))\n            pool_name = 'aa'\n        else:\n            stem.add_module('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n            pool_name = 'pool'\n        stem_stride *= 2\n        prev_feat = dict(num_chs=prev_chs, reduction=stem_stride, module='.'.join(['stem', pool_name]))\n    feature_info.append(prev_feat)\n    return stem, feature_info\n\n\ndef _get_stage_fn(stage_args):\n    stage_type = stage_args.pop('stage_type')\n    assert stage_type in ('dark', 'csp', 'cs3')\n    if stage_type == 'dark':\n        stage_args.pop('expand_ratio', None)\n        stage_args.pop('cross_linear', None)\n        stage_args.pop('down_growth', None)\n        stage_fn = DarkStage\n    elif stage_type == 'csp':\n        stage_fn = CrossStage\n    else:\n        stage_fn = CrossStage3\n    return stage_fn, stage_args\n\n\ndef _get_block_fn(stage_args):\n    block_type = stage_args.pop('block_type')\n    assert block_type in ('dark', 'edge', 'bottle')\n    if block_type == 'dark':\n        return DarkBlock, stage_args\n    elif block_type == 'edge':\n        return EdgeBlock, stage_args\n    else:\n        return BottleneckBlock, stage_args\n\n\ndef _get_attn_fn(stage_args):\n    attn_layer = stage_args.pop('attn_layer')\n    attn_kwargs = stage_args.pop('attn_kwargs', None) or {}\n    if attn_layer is not None:\n        attn_layer = get_attn(attn_layer)\n        if attn_kwargs:\n            attn_layer = partial(attn_layer, **attn_kwargs)\n    return attn_layer, stage_args\n\n\ndef create_csp_stages(\n        cfg: CspModelCfg,\n        drop_path_rate: float,\n        output_stride: int,\n        stem_feat: Dict[str, Any],\n):\n    cfg_dict = asdict(cfg.stages)\n    num_stages = len(cfg.stages.depth)\n    cfg_dict['block_dpr'] = [None] * num_stages if not drop_path_rate else \\\n        [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(cfg.stages.depth)).split(cfg.stages.depth)]\n    stage_args = [dict(zip(cfg_dict.keys(), values)) for values in zip(*cfg_dict.values())]\n    block_kwargs = dict(\n        act_layer=cfg.act_layer,\n        norm_layer=cfg.norm_layer,\n    )\n\n    dilation = 1\n    net_stride = stem_feat['reduction']\n    prev_chs = stem_feat['num_chs']\n    prev_feat = stem_feat\n    feature_info = []\n    stages = []\n    for stage_idx, stage_args in enumerate(stage_args):\n        stage_fn, stage_args = _get_stage_fn(stage_args)\n        block_fn, stage_args = _get_block_fn(stage_args)\n        attn_fn, stage_args = _get_attn_fn(stage_args)\n        stride = stage_args.pop('stride')\n        if stride != 1 and prev_feat:\n            feature_info.append(prev_feat)\n        if net_stride >= output_stride and stride > 1:\n            dilation *= stride\n            stride = 1\n        net_stride *= stride\n        first_dilation = 1 if dilation in (1, 2) else 2\n\n        stages += [stage_fn(\n            prev_chs,\n            **stage_args,\n            stride=stride,\n            first_dilation=first_dilation,\n            dilation=dilation,\n            block_fn=block_fn,\n            aa_layer=cfg.aa_layer,\n            attn_layer=attn_fn,  # will be passed through stage as block_kwargs\n            **block_kwargs,\n        )]\n        prev_chs = stage_args['out_chs']\n        prev_feat = dict(num_chs=prev_chs, reduction=net_stride, module=f'stages.{stage_idx}')\n\n    feature_info.append(prev_feat)\n    return nn.Sequential(*stages), feature_info\n\n\nclass CspNet(nn.Module):\n    \"\"\"Cross Stage Partial base model.\n\n    Paper: `CSPNet: A New Backbone that can Enhance Learning Capability of CNN` - https://arxiv.org/abs/1911.11929\n    Ref Impl: https://github.com/WongKinYiu/CrossStagePartialNetworks\n\n    NOTE: There are differences in the way I handle the 1x1 'expansion' conv in this impl vs the\n    darknet impl. I did it this way for simplicity and less special cases.\n    \"\"\"\n\n    def __init__(\n            self,\n            cfg: CspModelCfg,\n            in_chans=3,\n            num_classes=1000,\n            output_stride=32,\n            global_pool='avg',\n            drop_rate=0.,\n            drop_path_rate=0.,\n            zero_init_last=True,\n            **kwargs,\n    ):\n        \"\"\"\n        Args:\n            cfg (CspModelCfg): Model architecture configuration\n            in_chans (int): Number of input channels (default: 3)\n            num_classes (int): Number of classifier classes (default: 1000)\n            output_stride (int): Output stride of network, one of (8, 16, 32) (default: 32)\n            global_pool (str): Global pooling type (default: 'avg')\n            drop_rate (float): Dropout rate (default: 0.)\n            drop_path_rate (float): Stochastic depth drop-path rate (default: 0.)\n            zero_init_last (bool): Zero-init last weight of residual path\n            kwargs (dict): Extra kwargs overlayed onto cfg\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        assert output_stride in (8, 16, 32)\n\n        cfg = replace(cfg, **kwargs)  # overlay kwargs onto cfg\n        layer_args = dict(\n            act_layer=cfg.act_layer,\n            norm_layer=cfg.norm_layer,\n            aa_layer=cfg.aa_layer\n        )\n        self.feature_info = []\n\n        # Construct the stem\n        self.stem, stem_feat_info = create_csp_stem(in_chans, **asdict(cfg.stem), **layer_args)\n        self.feature_info.extend(stem_feat_info[:-1])\n\n        # Construct the stages\n        self.stages, stage_feat_info = create_csp_stages(\n            cfg,\n            drop_path_rate=drop_path_rate,\n            output_stride=output_stride,\n            stem_feat=stem_feat_info[-1],\n        )\n        prev_chs = stage_feat_info[-1]['num_chs']\n        self.feature_info.extend(stage_feat_info)\n\n        # Construct the head\n        self.num_features = prev_chs\n        self.head = ClassifierHead(\n            in_features=prev_chs, num_classes=num_classes, pool_type=global_pool, drop_rate=drop_rate)\n\n        named_apply(partial(_init_weights, zero_init_last=zero_init_last), self)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^stem',\n            blocks=r'^stages\\.(\\d+)' if coarse else [\n                (r'^stages\\.(\\d+)\\.blocks\\.(\\d+)', None),\n                (r'^stages\\.(\\d+)\\..*transition', MATCH_PREV_GROUP),  # map to last block in stage\n                (r'^stages\\.(\\d+)', (0,)),\n            ]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, 'gradient checkpointing not supported'\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate)\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        x = self.stages(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=pre_logits)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _init_weights(module, name, zero_init_last=False):\n    if isinstance(module, nn.Conv2d):\n        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Linear):\n        nn.init.normal_(module.weight, mean=0.0, std=0.01)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif zero_init_last and hasattr(module, 'zero_init_last'):\n        module.zero_init_last()\n\n\nmodel_cfgs = dict(\n    cspresnet50=CspModelCfg(\n        stem=CspStemCfg(out_chs=64, kernel_size=7, stride=4, pool='max'),\n        stages=CspStagesCfg(\n            depth=(3, 3, 5, 2),\n            out_chs=(128, 256, 512, 1024),\n            stride=(1, 2),\n            expand_ratio=2.,\n            bottle_ratio=0.5,\n            cross_linear=True,\n        ),\n    ),\n    cspresnet50d=CspModelCfg(\n        stem=CspStemCfg(out_chs=(32, 32, 64), kernel_size=3, stride=4, pool='max'),\n        stages=CspStagesCfg(\n            depth=(3, 3, 5, 2),\n            out_chs=(128, 256, 512, 1024),\n            stride=(1,) + (2,),\n            expand_ratio=2.,\n            bottle_ratio=0.5,\n            block_ratio=1.,\n            cross_linear=True,\n        ),\n    ),\n    cspresnet50w=CspModelCfg(\n        stem=CspStemCfg(out_chs=(32, 32, 64), kernel_size=3, stride=4, pool='max'),\n        stages=CspStagesCfg(\n            depth=(3, 3, 5, 2),\n            out_chs=(256, 512, 1024, 2048),\n            stride=(1,) + (2,),\n            expand_ratio=1.,\n            bottle_ratio=0.25,\n            block_ratio=0.5,\n            cross_linear=True,\n        ),\n    ),\n    cspresnext50=CspModelCfg(\n        stem=CspStemCfg(out_chs=64, kernel_size=7, stride=4, pool='max'),\n        stages=CspStagesCfg(\n            depth=(3, 3, 5, 2),\n            out_chs=(256, 512, 1024, 2048),\n            stride=(1,) + (2,),\n            groups=32,\n            expand_ratio=1.,\n            bottle_ratio=1.,\n            block_ratio=0.5,\n            cross_linear=True,\n        ),\n    ),\n    cspdarknet53=CspModelCfg(\n        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),\n        stages=CspStagesCfg(\n            depth=(1, 2, 8, 8, 4),\n            out_chs=(64, 128, 256, 512, 1024),\n            stride=2,\n            expand_ratio=(2.,) + (1.,),\n            bottle_ratio=(0.5,) + (1.,),\n            block_ratio=(1.,) + (0.5,),\n            down_growth=True,\n            block_type='dark',\n        ),\n    ),\n    darknet17=CspModelCfg(\n        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),\n        stages=CspStagesCfg(\n            depth=(1,) * 5,\n            out_chs=(64, 128, 256, 512, 1024),\n            stride=(2,),\n            bottle_ratio=(0.5,),\n            block_ratio=(1.,),\n            stage_type='dark',\n            block_type='dark',\n        ),\n    ),\n    darknet21=CspModelCfg(\n        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),\n        stages=CspStagesCfg(\n            depth=(1, 1, 1, 2, 2),\n            out_chs=(64, 128, 256, 512, 1024),\n            stride=(2,),\n            bottle_ratio=(0.5,),\n            block_ratio=(1.,),\n            stage_type='dark',\n            block_type='dark',\n\n        ),\n    ),\n    sedarknet21=CspModelCfg(\n        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),\n        stages=CspStagesCfg(\n            depth=(1, 1, 1, 2, 2),\n            out_chs=(64, 128, 256, 512, 1024),\n            stride=2,\n            bottle_ratio=0.5,\n            block_ratio=1.,\n            attn_layer='se',\n            stage_type='dark',\n            block_type='dark',\n\n        ),\n    ),\n    darknet53=CspModelCfg(\n        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),\n        stages=CspStagesCfg(\n            depth=(1, 2, 8, 8, 4),\n            out_chs=(64, 128, 256, 512, 1024),\n            stride=2,\n            bottle_ratio=0.5,\n            block_ratio=1.,\n            stage_type='dark',\n            block_type='dark',\n        ),\n    ),\n    darknetaa53=CspModelCfg(\n        stem=CspStemCfg(out_chs=32, kernel_size=3, stride=1, pool=''),\n        stages=CspStagesCfg(\n            depth=(1, 2, 8, 8, 4),\n            out_chs=(64, 128, 256, 512, 1024),\n            stride=2,\n            bottle_ratio=0.5,\n            block_ratio=1.,\n            avg_down=True,\n            stage_type='dark',\n            block_type='dark',\n        ),\n    ),\n\n    cs3darknet_s=_cs3_cfg(width_multiplier=0.5, depth_multiplier=0.5),\n    cs3darknet_m=_cs3_cfg(width_multiplier=0.75, depth_multiplier=0.67),\n    cs3darknet_l=_cs3_cfg(),\n    cs3darknet_x=_cs3_cfg(width_multiplier=1.25, depth_multiplier=1.33),\n\n    cs3darknet_focus_s=_cs3_cfg(width_multiplier=0.5, depth_multiplier=0.5, focus=True),\n    cs3darknet_focus_m=_cs3_cfg(width_multiplier=0.75, depth_multiplier=0.67, focus=True),\n    cs3darknet_focus_l=_cs3_cfg(focus=True),\n    cs3darknet_focus_x=_cs3_cfg(width_multiplier=1.25, depth_multiplier=1.33, focus=True),\n\n    cs3sedarknet_l=_cs3_cfg(attn_layer='se', attn_kwargs=dict(rd_ratio=.25)),\n    cs3sedarknet_x=_cs3_cfg(attn_layer='se', width_multiplier=1.25, depth_multiplier=1.33),\n\n    cs3sedarknet_xdw=CspModelCfg(\n        stem=CspStemCfg(out_chs=(32, 64), kernel_size=3, stride=2, pool=''),\n        stages=CspStagesCfg(\n            depth=(3, 6, 12, 4),\n            out_chs=(256, 512, 1024, 2048),\n            stride=2,\n            groups=(1, 1, 256, 512),\n            bottle_ratio=0.5,\n            block_ratio=0.5,\n            attn_layer='se',\n        ),\n        act_layer='silu',\n    ),\n\n    cs3edgenet_x=_cs3_cfg(width_multiplier=1.25, depth_multiplier=1.33, bottle_ratio=1.5, block_type='edge'),\n    cs3se_edgenet_x=_cs3_cfg(\n        width_multiplier=1.25, depth_multiplier=1.33, bottle_ratio=1.5, block_type='edge',\n        attn_layer='se', attn_kwargs=dict(rd_ratio=.25)),\n)\n\n\ndef _create_cspnet(variant, pretrained=False, **kwargs):\n    if variant.startswith('darknet') or variant.startswith('cspdarknet'):\n        # NOTE: DarkNet is one of few models with stride==1 features w/ 6 out_indices [0..5]\n        default_out_indices = (0, 1, 2, 3, 4, 5)\n    else:\n        default_out_indices = (0, 1, 2, 3, 4)\n    out_indices = kwargs.pop('out_indices', default_out_indices)\n    return build_model_with_cfg(\n        CspNet, variant, pretrained,\n        model_cfg=model_cfgs[variant],\n        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),\n        **kwargs)\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': (8, 8),\n        'crop_pct': 0.887, 'interpolation': 'bilinear',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.conv1.conv', 'classifier': 'head.fc',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'cspresnet50.ra_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/cspresnet50_ra-d3e8d487.pth'),\n    'cspresnet50d.untrained': _cfg(),\n    'cspresnet50w.untrained': _cfg(),\n    'cspresnext50.ra_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/cspresnext50_ra_224-648b4713.pth',\n    ),\n    'cspdarknet53.ra_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/cspdarknet53_ra_256-d05c7c21.pth'),\n\n    'darknet17.untrained': _cfg(),\n    'darknet21.untrained': _cfg(),\n    'sedarknet21.untrained': _cfg(),\n    'darknet53.c2ns_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/darknet53_256_c2ns-3aeff817.pth',\n        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'darknetaa53.c2ns_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/darknetaa53_c2ns-5c28ec8a.pth',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n\n    'cs3darknet_s.untrained': _cfg(interpolation='bicubic'),\n    'cs3darknet_m.c2ns_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3darknet_m_c2ns-43f06604.pth',\n        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=0.95,\n    ),\n    'cs3darknet_l.c2ns_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3darknet_l_c2ns-16220c5d.pth',\n        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=0.95),\n    'cs3darknet_x.c2ns_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3darknet_x_c2ns-4e4490aa.pth',\n        interpolation='bicubic', crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),\n\n    'cs3darknet_focus_s.untrained': _cfg(interpolation='bicubic'),\n    'cs3darknet_focus_m.c2ns_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3darknet_focus_m_c2ns-e23bed41.pth',\n        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=0.95),\n    'cs3darknet_focus_l.c2ns_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3darknet_focus_l_c2ns-65ef8888.pth',\n        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=0.95),\n    'cs3darknet_focus_x.untrained': _cfg(interpolation='bicubic'),\n\n    'cs3sedarknet_l.c2ns_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3sedarknet_l_c2ns-e8d1dc13.pth',\n        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=0.95),\n    'cs3sedarknet_x.c2ns_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3sedarknet_x_c2ns-b4d0abc0.pth',\n        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=1.0),\n\n    'cs3sedarknet_xdw.untrained': _cfg(interpolation='bicubic'),\n\n    'cs3edgenet_x.c2_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3edgenet_x_c2-2e1610a9.pth',\n        interpolation='bicubic', test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'cs3se_edgenet_x.c2ns_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/cs3se_edgenet_x_c2ns-76f8e3ac.pth',\n        interpolation='bicubic', crop_pct=0.95, test_input_size=(3, 320, 320), test_crop_pct=1.0),\n})\n\n\n@register_model\ndef cspresnet50(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cspresnet50', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cspresnet50d(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cspresnet50d', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cspresnet50w(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cspresnet50w', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cspresnext50(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cspresnext50', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cspdarknet53(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cspdarknet53', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef darknet17(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('darknet17', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef darknet21(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('darknet21', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef sedarknet21(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('sedarknet21', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef darknet53(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('darknet53', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef darknetaa53(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('darknetaa53', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cs3darknet_s(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cs3darknet_s', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cs3darknet_m(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cs3darknet_m', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cs3darknet_l(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cs3darknet_l', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cs3darknet_x(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cs3darknet_x', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cs3darknet_focus_s(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cs3darknet_focus_s', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cs3darknet_focus_m(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cs3darknet_focus_m', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cs3darknet_focus_l(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cs3darknet_focus_l', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cs3darknet_focus_x(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cs3darknet_focus_x', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cs3sedarknet_l(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cs3sedarknet_l', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cs3sedarknet_x(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cs3sedarknet_x', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cs3sedarknet_xdw(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cs3sedarknet_xdw', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cs3edgenet_x(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cs3edgenet_x', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef cs3se_edgenet_x(pretrained=False, **kwargs) -> CspNet:\n    return _create_cspnet('cs3se_edgenet_x', pretrained=pretrained, **kwargs)",
  "\"\"\" BEiT: BERT Pre-Training of Image Transformers (https://arxiv.org/abs/2106.08254)\n\nModel from official source: https://github.com/microsoft/unilm/tree/master/beit\n\n@inproceedings{beit,\ntitle={{BEiT}: {BERT} Pre-Training of Image Transformers},\nauthor={Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=p-BhZSz59o4}\n}\n\nBEiT-v2 from https://github.com/microsoft/unilm/tree/master/beit2\n\n@article{beitv2,\ntitle={{BEiT v2}: Masked Image Modeling with Vector-Quantized Visual Tokenizers},\nauthor={Zhiliang Peng and Li Dong and Hangbo Bao and Qixiang Ye and Furu Wei},\nyear={2022},\neprint={2208.06366},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n\nAt this point only the 1k fine-tuned classification weights and model configs have been added,\nsee original source above for pre-training models and procedure.\n\nModifications by / Copyright 2021 Ross Wightman, original copyrights below\n\"\"\"\n# --------------------------------------------------------\n# BEIT: BERT Pre-Training of Image Transformers (https://arxiv.org/abs/2106.08254)\n# Github source: https://github.com/microsoft/unilm/tree/master/beit\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# By Hangbo Bao\n# Based on timm and DeiT code bases\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/facebookresearch/deit/\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\n\nimport math\nfrom typing import Callable, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.checkpoint import checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import PatchEmbed, Mlp, SwiGLU, LayerNorm, DropPath, trunc_normal_, use_fused_attn\n\nfrom ._builder import build_model_with_cfg\nfrom ._registry import generate_default_cfgs, register_model\nfrom .vision_transformer import checkpoint_filter_fn\n\n__all__ = ['Beit']\n\n\ndef gen_relative_position_index(window_size: Tuple[int, int]) -> torch.Tensor:\n    num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n    # cls to token & token 2 cls & cls to cls\n    # get pair-wise relative position index for each token inside the window\n    window_area = window_size[0] * window_size[1]\n    coords = torch.stack(torch.meshgrid(\n        [torch.arange(window_size[0]),\n         torch.arange(window_size[1])]))  # 2, Wh, Ww\n    coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n    relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n    relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n    relative_coords[:, :, 1] += window_size[1] - 1\n    relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n    relative_position_index = torch.zeros(size=(window_area + 1,) * 2, dtype=relative_coords.dtype)\n    relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n    relative_position_index[0, 0:] = num_relative_distance - 3\n    relative_position_index[0:, 0] = num_relative_distance - 2\n    relative_position_index[0, 0] = num_relative_distance - 1\n    return relative_position_index\n\n\nclass Attention(nn.Module):\n    fused_attn: torch.jit.Final[bool]\n\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int = 8,\n            qkv_bias: bool = False,\n            attn_drop: float = 0.,\n            proj_drop: float = 0.,\n            window_size: Optional[Tuple[int, int]] = None,\n            attn_head_dim: Optional[int] = None,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = head_dim ** -0.5\n        self.fused_attn = use_fused_attn()\n\n        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.register_buffer('k_bias', torch.zeros(all_head_dim), persistent=False)\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.k_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(\n                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n            self.register_buffer(\"relative_position_index\", gen_relative_position_index(window_size))\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def _get_rel_pos_bias(self):\n        relative_position_bias = self.relative_position_bias_table[\n            self.relative_position_index.view(-1)].view(\n            self.window_size[0] * self.window_size[1] + 1,\n            self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        return relative_position_bias.unsqueeze(0)\n\n    def forward(self, x, shared_rel_pos_bias: Optional[torch.Tensor] = None):\n        B, N, C = x.shape\n\n        qkv_bias = torch.cat((self.q_bias, self.k_bias, self.v_bias)) if self.q_bias is not None else None\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)  # B, num_heads, N, head_dim\n\n        if self.fused_attn:\n            rel_pos_bias = None\n            if self.relative_position_bias_table is not None:\n                rel_pos_bias = self._get_rel_pos_bias()\n                if shared_rel_pos_bias is not None:\n                    rel_pos_bias = rel_pos_bias + shared_rel_pos_bias\n            elif shared_rel_pos_bias is not None:\n                rel_pos_bias = shared_rel_pos_bias\n\n            x = F.scaled_dot_product_attention(\n                q, k, v,\n                attn_mask=rel_pos_bias,\n                dropout_p=self.attn_drop.p,\n            )\n        else:\n            q = q * self.scale\n            attn = (q @ k.transpose(-2, -1))\n\n            if self.relative_position_bias_table is not None:\n                attn = attn + self._get_rel_pos_bias()\n            if shared_rel_pos_bias is not None:\n                attn = attn + shared_rel_pos_bias\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            x = attn @ v\n\n        x = x.transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int,\n            qkv_bias: bool = False,\n            mlp_ratio: float = 4.,\n            scale_mlp: bool = False,\n            swiglu_mlp: bool = False,\n            proj_drop: float = 0.,\n            attn_drop: float = 0.,\n            drop_path: float = 0.,\n            init_values: Optional[float] = None,\n            act_layer: Callable = nn.GELU,\n            norm_layer: Callable = LayerNorm,\n            window_size: Optional[Tuple[int, int]] = None,\n            attn_head_dim: Optional[int] = None,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n            window_size=window_size,\n            attn_head_dim=attn_head_dim,\n        )\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        if swiglu_mlp:\n            self.mlp = SwiGLU(\n                in_features=dim,\n                hidden_features=int(dim * mlp_ratio),\n                norm_layer=norm_layer if scale_mlp else None,\n                drop=proj_drop,\n            )\n        else:\n            self.mlp = Mlp(\n                in_features=dim,\n                hidden_features=int(dim * mlp_ratio),\n                act_layer=act_layer,\n                norm_layer=norm_layer if scale_mlp else None,\n                drop=proj_drop,\n            )\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        if init_values:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones(dim))\n            self.gamma_2 = nn.Parameter(init_values * torch.ones(dim))\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n    def forward(self, x, shared_rel_pos_bias: Optional[torch.Tensor] = None):\n        if self.gamma_1 is None:\n            x = x + self.drop_path1(self.attn(self.norm1(x), shared_rel_pos_bias=shared_rel_pos_bias))\n            x = x + self.drop_path2(self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path1(self.gamma_1 * self.attn(self.norm1(x), shared_rel_pos_bias=shared_rel_pos_bias))\n            x = x + self.drop_path2(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.window_area = window_size[0] * window_size[1]\n        num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(torch.zeros(num_relative_distance, num_heads))\n        # trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.register_buffer(\"relative_position_index\", gen_relative_position_index(window_size))\n\n    def forward(self):\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_area + 1, self.window_area + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n\nclass Beit(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(\n            self,\n            img_size: Union[int, Tuple[int, int]] = 224,\n            patch_size: Union[int, Tuple[int, int]] = 16,\n            in_chans: int = 3,\n            num_classes: int = 1000,\n            global_pool: str = 'avg',\n            embed_dim: int = 768,\n            depth: int = 12,\n            num_heads: int = 12,\n            qkv_bias: bool = True,\n            mlp_ratio: float = 4.,\n            swiglu_mlp: bool = False,\n            scale_mlp: bool = False,\n            drop_rate: float = 0.,\n            pos_drop_rate: float = 0.,\n            proj_drop_rate: float = 0.,\n            attn_drop_rate: float = 0.,\n            drop_path_rate: float = 0.,\n            norm_layer: Callable = LayerNorm,\n            init_values: Optional[float] = None,\n            use_abs_pos_emb: bool = True,\n            use_rel_pos_bias: bool = False,\n            use_shared_rel_pos_bias: bool = False,\n            head_init_scale: float = 0.001,\n    ):\n        super().__init__()\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.num_prefix_tokens = 1\n        self.grad_checkpointing = False\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n        )\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim)) if use_abs_pos_emb else None\n        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(\n                window_size=self.patch_embed.grid_size,\n                num_heads=num_heads,\n            )\n        else:\n            self.rel_pos_bias = None\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                qkv_bias=qkv_bias,\n                mlp_ratio=mlp_ratio,\n                scale_mlp=scale_mlp,\n                swiglu_mlp=swiglu_mlp,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n                init_values=init_values,\n                window_size=self.patch_embed.grid_size if use_rel_pos_bias else None,\n            )\n            for i in range(depth)])\n\n        use_fc_norm = self.global_pool == 'avg'\n        self.norm = nn.Identity() if use_fc_norm else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_fc_norm else nn.Identity()\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        self.apply(self._init_weights)\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n\n        self.fix_init_weight()\n        if isinstance(self.head, nn.Linear):\n            trunc_normal_(self.head.weight, std=.02)\n            self.head.weight.data.mul_(head_init_scale)\n            self.head.bias.data.mul_(head_init_scale)\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        nwd = {'pos_embed', 'cls_token'}\n        for n, _ in self.named_parameters():\n            if 'relative_position_bias_table' in n:\n                nwd.add(n)\n        return nwd\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^cls_token|pos_embed|patch_embed|rel_pos_bias',  # stem and embed\n            blocks=[(r'^blocks\\.(\\d+)', None), (r'^norm', (99999,))],\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for blk in self.blocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint(blk, x, shared_rel_pos_bias=rel_pos_bias)\n            else:\n                x = blk(x, shared_rel_pos_bias=rel_pos_bias)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool:\n            x = x[:, self.num_prefix_tokens:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]\n        x = self.fc_norm(x)\n        x = self.head_drop(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'beit_base_patch16_224.in22k_ft_in22k_in1k': _cfg(\n        url='https://conversationhub.blob.core.windows.net/beit-share-public/beit/beit_base_patch16_224_pt22k_ft22kto1k.pth',\n        hf_hub_id='timm/'),\n    'beit_base_patch16_384.in22k_ft_in22k_in1k': _cfg(\n        url='https://conversationhub.blob.core.windows.net/beit-share-public/beit/beit_base_patch16_384_pt22k_ft22kto1k.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), crop_pct=1.0,\n    ),\n    'beit_base_patch16_224.in22k_ft_in22k': _cfg(\n        url='https://conversationhub.blob.core.windows.net/beit-share-public/beit/beit_base_patch16_224_pt22k_ft22k.pth',\n        hf_hub_id='timm/',\n        num_classes=21841,\n    ),\n    'beit_large_patch16_224.in22k_ft_in22k_in1k': _cfg(\n        url='https://conversationhub.blob.core.windows.net/beit-share-public/beit/beit_large_patch16_224_pt22k_ft22kto1k.pth',\n        hf_hub_id='timm/'),\n    'beit_large_patch16_384.in22k_ft_in22k_in1k': _cfg(\n        url='https://conversationhub.blob.core.windows.net/beit-share-public/beit/beit_large_patch16_384_pt22k_ft22kto1k.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), crop_pct=1.0,\n    ),\n    'beit_large_patch16_512.in22k_ft_in22k_in1k': _cfg(\n        url='https://conversationhub.blob.core.windows.net/beit-share-public/beit/beit_large_patch16_512_pt22k_ft22kto1k.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 512, 512), crop_pct=1.0,\n    ),\n    'beit_large_patch16_224.in22k_ft_in22k': _cfg(\n        url='https://conversationhub.blob.core.windows.net/beit-share-public/beit/beit_large_patch16_224_pt22k_ft22k.pth',\n        hf_hub_id='timm/',\n        num_classes=21841,\n    ),\n\n    'beitv2_base_patch16_224.in1k_ft_in22k_in1k': _cfg(\n        url='https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/beitv2_base_patch16_224_pt1k_ft21kto1k.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD\n    ),\n    'beitv2_base_patch16_224.in1k_ft_in1k': _cfg(\n        url='https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/beitv2_base_patch16_224_pt1k_ft1k.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD\n    ),\n    'beitv2_base_patch16_224.in1k_ft_in22k': _cfg(\n        url='https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/beitv2_base_patch16_224_pt1k_ft21k.pth',\n        hf_hub_id='timm/',\n        num_classes=21841, mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD\n    ),\n    'beitv2_large_patch16_224.in1k_ft_in22k_in1k': _cfg(\n        url='https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/beitv2_large_patch16_224_pt1k_ft21kto1k.pth',\n        hf_hub_id='timm/',\n        crop_pct=0.95, mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD\n    ),\n    'beitv2_large_patch16_224.in1k_ft_in1k': _cfg(\n        url='https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/beitv2_large_patch16_224_pt1k_ft1k.pth',\n        hf_hub_id='timm/',\n        crop_pct=0.95, mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD\n    ),\n    'beitv2_large_patch16_224.in1k_ft_in22k': _cfg(\n        url='https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/beitv2_large_patch16_224_pt1k_ft21k.pth',\n        hf_hub_id='timm/',\n        num_classes=21841, mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD\n    ),\n})\n\n\ndef _beit_checkpoint_filter_fn(state_dict, model):\n    if 'module' in state_dict:\n        # beit v2 didn't strip module\n        state_dict = state_dict['module']\n    return checkpoint_filter_fn(state_dict, model)\n\n\ndef _create_beit(variant, pretrained=False, **kwargs):\n    if kwargs.get('features_only', None):\n        raise RuntimeError('features_only not implemented for BEiT models.')\n\n    model = build_model_with_cfg(\n        Beit, variant, pretrained,\n        # FIXME an updated filter fn needed to interpolate rel pos emb if fine tuning to diff model sizes\n        pretrained_filter_fn=_beit_checkpoint_filter_fn,\n        **kwargs)\n    return model\n\n\n@register_model\ndef beit_base_patch16_224(pretrained=False, **kwargs) -> Beit:\n    model_args = dict(\n        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=0.1)\n    model = _create_beit('beit_base_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef beit_base_patch16_384(pretrained=False, **kwargs) -> Beit:\n    model_args = dict(\n        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12,\n        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=0.1)\n    model = _create_beit('beit_base_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef beit_large_patch16_224(pretrained=False, **kwargs) -> Beit:\n    model_args = dict(\n        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=1e-5)\n    model = _create_beit('beit_large_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef beit_large_patch16_384(pretrained=False, **kwargs) -> Beit:\n    model_args = dict(\n        img_size=384, patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=1e-5)\n    model = _create_beit('beit_large_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef beit_large_patch16_512(pretrained=False, **kwargs) -> Beit:\n    model_args = dict(\n        img_size=512, patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=1e-5)\n    model = _create_beit('beit_large_patch16_512', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef beitv2_base_patch16_224(pretrained=False, **kwargs) -> Beit:\n    model_args = dict(\n        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=1e-5)\n    model = _create_beit('beitv2_base_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef beitv2_large_patch16_224(pretrained=False, **kwargs) -> Beit:\n    model_args = dict(\n        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n        use_abs_pos_emb=False, use_rel_pos_bias=True, init_values=1e-5)\n    model = _create_beit('beitv2_large_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n",
  "\"\"\" Swin Transformer V2\nA PyTorch impl of : `Swin Transformer V2: Scaling Up Capacity and Resolution`\n    - https://arxiv.org/abs/2111.09883\n\nCode/weights from https://github.com/microsoft/Swin-Transformer, original copyright/license info below\n\nModifications and additions for timm hacked together by / Copyright 2022, Ross Wightman\n\"\"\"\n# --------------------------------------------------------\n# Swin Transformer V2\n# Copyright (c) 2022 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\nimport math\nfrom typing import Callable, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import PatchEmbed, Mlp, DropPath, to_2tuple, trunc_normal_, _assert, ClassifierHead\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_function\nfrom ._registry import generate_default_cfgs, register_model, register_model_deprecations\n\n__all__ = ['SwinTransformerV2']  # model_registry will add each entrypoint fn to this\n\n_int_or_tuple_2_t = Union[int, Tuple[int, int]]\n\n\ndef window_partition(x, window_size: Tuple[int, int]):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size[0], window_size[0], W // window_size[1], window_size[1], C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0], window_size[1], C)\n    return windows\n\n\n@register_notrace_function  # reason: int argument is a Proxy\ndef window_reverse(windows, window_size: Tuple[int, int], img_size: Tuple[int, int]):\n    \"\"\"\n    Args:\n        windows: (num_windows * B, window_size[0], window_size[1], C)\n        window_size (Tuple[int, int]): Window size\n        img_size (Tuple[int, int]): Image size\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    H, W = img_size\n    C = windows.shape[-1]\n    x = windows.view(-1, H // window_size[0], W // window_size[1], window_size[0], window_size[1], C)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, C)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n        pretrained_window_size (tuple[int]): The height and width of the window in pre-training.\n    \"\"\"\n\n    def __init__(\n            self,\n            dim,\n            window_size,\n            num_heads,\n            qkv_bias=True,\n            attn_drop=0.,\n            proj_drop=0.,\n            pretrained_window_size=[0, 0],\n    ):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.pretrained_window_size = pretrained_window_size\n        self.num_heads = num_heads\n\n        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))\n\n        # mlp to generate continuous relative position bias\n        self.cpb_mlp = nn.Sequential(\n            nn.Linear(2, 512, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, num_heads, bias=False)\n        )\n\n        # get relative_coords_table\n        relative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.float32)\n        relative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.float32)\n        relative_coords_table = torch.stack(torch.meshgrid([\n            relative_coords_h,\n            relative_coords_w])).permute(1, 2, 0).contiguous().unsqueeze(0)  # 1, 2*Wh-1, 2*Ww-1, 2\n        if pretrained_window_size[0] > 0:\n            relative_coords_table[:, :, :, 0] /= (pretrained_window_size[0] - 1)\n            relative_coords_table[:, :, :, 1] /= (pretrained_window_size[1] - 1)\n        else:\n            relative_coords_table[:, :, :, 0] /= (self.window_size[0] - 1)\n            relative_coords_table[:, :, :, 1] /= (self.window_size[1] - 1)\n        relative_coords_table *= 8  # normalize to -8, 8\n        relative_coords_table = torch.sign(relative_coords_table) * torch.log2(\n            torch.abs(relative_coords_table) + 1.0) / math.log2(8)\n\n        self.register_buffer(\"relative_coords_table\", relative_coords_table, persistent=False)\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index, persistent=False)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(dim))\n            self.register_buffer('k_bias', torch.zeros(dim), persistent=False)\n            self.v_bias = nn.Parameter(torch.zeros(dim))\n        else:\n            self.q_bias = None\n            self.k_bias = None\n            self.v_bias = None\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask: Optional[torch.Tensor] = None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            qkv_bias = torch.cat((self.q_bias, self.k_bias, self.v_bias))\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)\n\n        # cosine attention\n        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n        logit_scale = torch.clamp(self.logit_scale, max=math.log(1. / 0.01)).exp()\n        attn = attn * logit_scale\n\n        relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads)\n        relative_position_bias = relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        relative_position_bias = 16 * torch.sigmoid(relative_position_bias)\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        if mask is not None:\n            num_win = mask.shape[0]\n            attn = attn.view(-1, num_win, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass SwinTransformerV2Block(nn.Module):\n    \"\"\" Swin Transformer Block.\n    \"\"\"\n\n    def __init__(\n            self,\n            dim,\n            input_resolution,\n            num_heads,\n            window_size=7,\n            shift_size=0,\n            mlp_ratio=4.,\n            qkv_bias=True,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            pretrained_window_size=0,\n    ):\n        \"\"\"\n        Args:\n            dim: Number of input channels.\n            input_resolution: Input resolution.\n            num_heads: Number of attention heads.\n            window_size: Window size.\n            shift_size: Shift size for SW-MSA.\n            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n            qkv_bias: If True, add a learnable bias to query, key, value.\n            proj_drop: Dropout rate.\n            attn_drop: Attention dropout rate.\n            drop_path: Stochastic depth rate.\n            act_layer: Activation layer.\n            norm_layer: Normalization layer.\n            pretrained_window_size: Window size in pretraining.\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = to_2tuple(input_resolution)\n        self.num_heads = num_heads\n        ws, ss = self._calc_window_shift(window_size, shift_size)\n        self.window_size: Tuple[int, int] = ws\n        self.shift_size: Tuple[int, int] = ss\n        self.window_area = self.window_size[0] * self.window_size[1]\n        self.mlp_ratio = mlp_ratio\n\n        self.attn = WindowAttention(\n            dim,\n            window_size=to_2tuple(self.window_size),\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n            pretrained_window_size=to_2tuple(pretrained_window_size),\n        )\n        self.norm1 = norm_layer(dim)\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n        self.norm2 = norm_layer(dim)\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        if any(self.shift_size):\n            # calculate attention mask for SW-MSA\n            H, W = self.input_resolution\n            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n            cnt = 0\n            for h in (\n                    slice(0, -self.window_size[0]),\n                    slice(-self.window_size[0], -self.shift_size[0]),\n                    slice(-self.shift_size[0], None)):\n                for w in (\n                        slice(0, -self.window_size[1]),\n                        slice(-self.window_size[1], -self.shift_size[1]),\n                        slice(-self.shift_size[1], None)):\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n            mask_windows = mask_windows.view(-1, self.window_area)\n            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n        else:\n            attn_mask = None\n\n        self.register_buffer(\"attn_mask\", attn_mask, persistent=False)\n\n    def _calc_window_shift(self, target_window_size, target_shift_size) -> Tuple[Tuple[int, int], Tuple[int, int]]:\n        target_window_size = to_2tuple(target_window_size)\n        target_shift_size = to_2tuple(target_shift_size)\n        window_size = [r if r <= w else w for r, w in zip(self.input_resolution, target_window_size)]\n        shift_size = [0 if r <= w else s for r, w, s in zip(self.input_resolution, window_size, target_shift_size)]\n        return tuple(window_size), tuple(shift_size)\n\n    def _attn(self, x):\n        B, H, W, C = x.shape\n\n        # cyclic shift\n        has_shift = any(self.shift_size)\n        if has_shift:\n            shifted_x = torch.roll(x, shifts=(-self.shift_size[0], -self.shift_size[1]), dims=(1, 2))\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n        x_windows = x_windows.view(-1, self.window_area, C)  # nW*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size[0], self.window_size[1], C)\n        shifted_x = window_reverse(attn_windows, self.window_size, self.input_resolution)  # B H' W' C\n\n        # reverse cyclic shift\n        if has_shift:\n            x = torch.roll(shifted_x, shifts=self.shift_size, dims=(1, 2))\n        else:\n            x = shifted_x\n        return x\n\n    def forward(self, x):\n        B, H, W, C = x.shape\n        x = x + self.drop_path1(self.norm1(self._attn(x)))\n        x = x.reshape(B, -1, C)\n        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n        x = x.reshape(B, H, W, C)\n        return x\n\n\nclass PatchMerging(nn.Module):\n    \"\"\" Patch Merging Layer.\n    \"\"\"\n\n    def __init__(self, dim, out_dim=None, norm_layer=nn.LayerNorm):\n        \"\"\"\n        Args:\n            dim (int): Number of input channels.\n            out_dim (int): Number of output channels (or 2 * dim if None)\n            norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n        self.out_dim = out_dim or 2 * dim\n        self.reduction = nn.Linear(4 * dim, self.out_dim, bias=False)\n        self.norm = norm_layer(self.out_dim)\n\n    def forward(self, x):\n        B, H, W, C = x.shape\n        _assert(H % 2 == 0, f\"x height ({H}) is not even.\")\n        _assert(W % 2 == 0, f\"x width ({W}) is not even.\")\n        x = x.reshape(B, H // 2, 2, W // 2, 2, C).permute(0, 1, 3, 4, 2, 5).flatten(3)\n        x = self.reduction(x)\n        x = self.norm(x)\n        return x\n\n\nclass SwinTransformerV2Stage(nn.Module):\n    \"\"\" A Swin Transformer V2 Stage.\n    \"\"\"\n\n    def __init__(\n            self,\n            dim,\n            out_dim,\n            input_resolution,\n            depth,\n            num_heads,\n            window_size,\n            downsample=False,\n            mlp_ratio=4.,\n            qkv_bias=True,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            norm_layer=nn.LayerNorm,\n            pretrained_window_size=0,\n            output_nchw=False,\n    ):\n        \"\"\"\n        Args:\n            dim: Number of input channels.\n            input_resolution: Input resolution.\n            depth: Number of blocks.\n            num_heads: Number of attention heads.\n            window_size: Local window size.\n            downsample: Use downsample layer at start of the block.\n            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n            qkv_bias: If True, add a learnable bias to query, key, value.\n            proj_drop: Projection dropout rate\n            attn_drop: Attention dropout rate.\n            drop_path: Stochastic depth rate.\n            norm_layer: Normalization layer.\n            pretrained_window_size: Local window size in pretraining.\n            output_nchw: Output tensors on NCHW format instead of NHWC.\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.output_resolution = tuple(i // 2 for i in input_resolution) if downsample else input_resolution\n        self.depth = depth\n        self.output_nchw = output_nchw\n        self.grad_checkpointing = False\n\n        # patch merging / downsample layer\n        if downsample:\n            self.downsample = PatchMerging(dim=dim, out_dim=out_dim, norm_layer=norm_layer)\n        else:\n            assert dim == out_dim\n            self.downsample = nn.Identity()\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinTransformerV2Block(\n                dim=out_dim,\n                input_resolution=self.output_resolution,\n                num_heads=num_heads,\n                window_size=window_size,\n                shift_size=0 if (i % 2 == 0) else window_size // 2,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                proj_drop=proj_drop,\n                attn_drop=attn_drop,\n                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                norm_layer=norm_layer,\n                pretrained_window_size=pretrained_window_size,\n            )\n            for i in range(depth)])\n\n    def forward(self, x):\n        x = self.downsample(x)\n\n        for blk in self.blocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        return x\n\n    def _init_respostnorm(self):\n        for blk in self.blocks:\n            nn.init.constant_(blk.norm1.bias, 0)\n            nn.init.constant_(blk.norm1.weight, 0)\n            nn.init.constant_(blk.norm2.bias, 0)\n            nn.init.constant_(blk.norm2.weight, 0)\n\n\nclass SwinTransformerV2(nn.Module):\n    \"\"\" Swin Transformer V2\n\n    A PyTorch impl of : `Swin Transformer V2: Scaling Up Capacity and Resolution`\n        - https://arxiv.org/abs/2111.09883\n    \"\"\"\n\n    def __init__(\n            self,\n            img_size: _int_or_tuple_2_t = 224,\n            patch_size: int = 4,\n            in_chans: int = 3,\n            num_classes: int = 1000,\n            global_pool: str = 'avg',\n            embed_dim: int = 96,\n            depths: Tuple[int, ...] = (2, 2, 6, 2),\n            num_heads: Tuple[int, ...] = (3, 6, 12, 24),\n            window_size: _int_or_tuple_2_t = 7,\n            mlp_ratio: float = 4.,\n            qkv_bias: bool = True,\n            drop_rate: float = 0.,\n            proj_drop_rate: float = 0.,\n            attn_drop_rate: float = 0.,\n            drop_path_rate: float = 0.1,\n            norm_layer: Callable = nn.LayerNorm,\n            pretrained_window_sizes: Tuple[int, ...] = (0, 0, 0, 0),\n            **kwargs,\n    ):\n        \"\"\"\n        Args:\n            img_size: Input image size.\n            patch_size: Patch size.\n            in_chans: Number of input image channels.\n            num_classes: Number of classes for classification head.\n            embed_dim: Patch embedding dimension.\n            depths: Depth of each Swin Transformer stage (layer).\n            num_heads: Number of attention heads in different layers.\n            window_size: Window size.\n            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n            qkv_bias: If True, add a learnable bias to query, key, value.\n            drop_rate: Head dropout rate.\n            proj_drop_rate: Projection dropout rate.\n            attn_drop_rate: Attention dropout rate.\n            drop_path_rate: Stochastic depth rate.\n            norm_layer: Normalization layer.\n            patch_norm: If True, add normalization after patch embedding.\n            pretrained_window_sizes: Pretrained window sizes of each layer.\n            output_fmt: Output tensor format if not None, otherwise output 'NHWC' by default.\n        \"\"\"\n        super().__init__()\n\n        self.num_classes = num_classes\n        assert global_pool in ('', 'avg')\n        self.global_pool = global_pool\n        self.output_fmt = 'NHWC'\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n        self.feature_info = []\n\n        if not isinstance(embed_dim, (tuple, list)):\n            embed_dim = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim[0],\n            norm_layer=norm_layer,\n            output_fmt='NHWC',\n        )\n\n        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n        layers = []\n        in_dim = embed_dim[0]\n        scale = 1\n        for i in range(self.num_layers):\n            out_dim = embed_dim[i]\n            layers += [SwinTransformerV2Stage(\n                dim=in_dim,\n                out_dim=out_dim,\n                input_resolution=(\n                    self.patch_embed.grid_size[0] // scale,\n                    self.patch_embed.grid_size[1] // scale),\n                depth=depths[i],\n                downsample=i > 0,\n                num_heads=num_heads[i],\n                window_size=window_size,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n                pretrained_window_size=pretrained_window_sizes[i],\n            )]\n            in_dim = out_dim\n            if i > 0:\n                scale *= 2\n            self.feature_info += [dict(num_chs=out_dim, reduction=4 * scale, module=f'layers.{i}')]\n\n        self.layers = nn.Sequential(*layers)\n        self.norm = norm_layer(self.num_features)\n        self.head = ClassifierHead(\n            self.num_features,\n            num_classes,\n            pool_type=global_pool,\n            drop_rate=drop_rate,\n            input_fmt=self.output_fmt,\n        )\n\n        self.apply(self._init_weights)\n        for bly in self.layers:\n            bly._init_respostnorm()\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        nod = set()\n        for n, m in self.named_modules():\n            if any([kw in n for kw in (\"cpb_mlp\", \"logit_scale\", 'relative_position_bias_table')]):\n                nod.add(n)\n        return nod\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^absolute_pos_embed|patch_embed',  # stem and embed\n            blocks=r'^layers\\.(\\d+)' if coarse else [\n                (r'^layers\\.(\\d+).downsample', (0,)),\n                (r'^layers\\.(\\d+)\\.\\w+\\.(\\d+)', None),\n                (r'^norm', (99999,)),\n            ]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        for l in self.layers:\n            l.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        self.head.reset(num_classes, global_pool)\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = self.layers(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=True) if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    state_dict = state_dict.get('model', state_dict)\n    state_dict = state_dict.get('state_dict', state_dict)\n    native_checkpoint = 'head.fc.weight' in state_dict\n    out_dict = {}\n    import re\n    for k, v in state_dict.items():\n        if any([n in k for n in ('relative_position_index', 'relative_coords_table', 'attn_mask')]):\n            continue  # skip buffers that should not be persistent\n        if not native_checkpoint:\n            # skip layer remapping for updated checkpoints\n            k = re.sub(r'layers.(\\d+).downsample', lambda x: f'layers.{int(x.group(1)) + 1}.downsample', k)\n            k = k.replace('head.', 'head.fc.')\n        out_dict[k] = v\n      \n    return out_dict\n\n\ndef _create_swin_transformer_v2(variant, pretrained=False, **kwargs):\n    default_out_indices = tuple(i for i, _ in enumerate(kwargs.get('depths', (1, 1, 1, 1))))\n    out_indices = kwargs.pop('out_indices', default_out_indices)\n\n    model = build_model_with_cfg(\n        SwinTransformerV2, variant, pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),\n        **kwargs)\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': (8, 8),\n        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'patch_embed.proj', 'classifier': 'head.fc',\n        'license': 'mit', **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'swinv2_base_window12to16_192to256.ms_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12to16_192to256_22kto1k_ft.pth',\n    ),\n    'swinv2_base_window12to24_192to384.ms_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12to24_192to384_22kto1k_ft.pth',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0,\n    ),\n    'swinv2_large_window12to16_192to256.ms_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12to16_192to256_22kto1k_ft.pth',\n    ),\n    'swinv2_large_window12to24_192to384.ms_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12to24_192to384_22kto1k_ft.pth',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0,\n    ),\n\n    'swinv2_tiny_window8_256.ms_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_tiny_patch4_window8_256.pth',\n    ),\n    'swinv2_tiny_window16_256.ms_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_tiny_patch4_window16_256.pth',\n    ),\n    'swinv2_small_window8_256.ms_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_small_patch4_window8_256.pth',\n    ),\n    'swinv2_small_window16_256.ms_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_small_patch4_window16_256.pth',\n    ),\n    'swinv2_base_window8_256.ms_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window8_256.pth',\n    ),\n    'swinv2_base_window16_256.ms_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window16_256.pth',\n    ),\n\n    'swinv2_base_window12_192.ms_in22k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_base_patch4_window12_192_22k.pth',\n        num_classes=21841, input_size=(3, 192, 192), pool_size=(6, 6)\n    ),\n    'swinv2_large_window12_192.ms_in22k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/SwinTransformer/storage/releases/download/v2.0.0/swinv2_large_patch4_window12_192_22k.pth',\n        num_classes=21841, input_size=(3, 192, 192), pool_size=(6, 6)\n    ),\n})\n\n\n@register_model\ndef swinv2_tiny_window16_256(pretrained=False, **kwargs) -> SwinTransformerV2:\n    \"\"\"\n    \"\"\"\n    model_args = dict(window_size=16, embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24))\n    return _create_swin_transformer_v2(\n        'swinv2_tiny_window16_256', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_tiny_window8_256(pretrained=False, **kwargs) -> SwinTransformerV2:\n    \"\"\"\n    \"\"\"\n    model_args = dict(window_size=8, embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24))\n    return _create_swin_transformer_v2(\n        'swinv2_tiny_window8_256', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_small_window16_256(pretrained=False, **kwargs) -> SwinTransformerV2:\n    \"\"\"\n    \"\"\"\n    model_args = dict(window_size=16, embed_dim=96, depths=(2, 2, 18, 2), num_heads=(3, 6, 12, 24))\n    return _create_swin_transformer_v2(\n        'swinv2_small_window16_256', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_small_window8_256(pretrained=False, **kwargs) -> SwinTransformerV2:\n    \"\"\"\n    \"\"\"\n    model_args = dict(window_size=8, embed_dim=96, depths=(2, 2, 18, 2), num_heads=(3, 6, 12, 24))\n    return _create_swin_transformer_v2(\n        'swinv2_small_window8_256', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_base_window16_256(pretrained=False, **kwargs) -> SwinTransformerV2:\n    \"\"\"\n    \"\"\"\n    model_args = dict(window_size=16, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32))\n    return _create_swin_transformer_v2(\n        'swinv2_base_window16_256', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_base_window8_256(pretrained=False, **kwargs) -> SwinTransformerV2:\n    \"\"\"\n    \"\"\"\n    model_args = dict(window_size=8, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32))\n    return _create_swin_transformer_v2(\n        'swinv2_base_window8_256', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_base_window12_192(pretrained=False, **kwargs) -> SwinTransformerV2:\n    \"\"\"\n    \"\"\"\n    model_args = dict(window_size=12, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32))\n    return _create_swin_transformer_v2(\n        'swinv2_base_window12_192', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_base_window12to16_192to256(pretrained=False, **kwargs) -> SwinTransformerV2:\n    \"\"\"\n    \"\"\"\n    model_args = dict(\n        window_size=16, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32),\n        pretrained_window_sizes=(12, 12, 12, 6))\n    return _create_swin_transformer_v2(\n        'swinv2_base_window12to16_192to256', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_base_window12to24_192to384(pretrained=False, **kwargs) -> SwinTransformerV2:\n    \"\"\"\n    \"\"\"\n    model_args = dict(\n        window_size=24, embed_dim=128, depths=(2, 2, 18, 2), num_heads=(4, 8, 16, 32),\n        pretrained_window_sizes=(12, 12, 12, 6))\n    return _create_swin_transformer_v2(\n        'swinv2_base_window12to24_192to384', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_large_window12_192(pretrained=False, **kwargs) -> SwinTransformerV2:\n    \"\"\"\n    \"\"\"\n    model_args = dict(window_size=12, embed_dim=192, depths=(2, 2, 18, 2), num_heads=(6, 12, 24, 48))\n    return _create_swin_transformer_v2(\n        'swinv2_large_window12_192', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_large_window12to16_192to256(pretrained=False, **kwargs) -> SwinTransformerV2:\n    \"\"\"\n    \"\"\"\n    model_args = dict(\n        window_size=16, embed_dim=192, depths=(2, 2, 18, 2), num_heads=(6, 12, 24, 48),\n        pretrained_window_sizes=(12, 12, 12, 6))\n    return _create_swin_transformer_v2(\n        'swinv2_large_window12to16_192to256', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_large_window12to24_192to384(pretrained=False, **kwargs) -> SwinTransformerV2:\n    \"\"\"\n    \"\"\"\n    model_args = dict(\n        window_size=24, embed_dim=192, depths=(2, 2, 18, 2), num_heads=(6, 12, 24, 48),\n        pretrained_window_sizes=(12, 12, 12, 6))\n    return _create_swin_transformer_v2(\n        'swinv2_large_window12to24_192to384', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\nregister_model_deprecations(__name__, {\n    'swinv2_base_window12_192_22k': 'swinv2_base_window12_192.ms_in22k',\n    'swinv2_base_window12to16_192to256_22kft1k': 'swinv2_base_window12to16_192to256.ms_in22k_ft_in1k',\n    'swinv2_base_window12to24_192to384_22kft1k': 'swinv2_base_window12to24_192to384.ms_in22k_ft_in1k',\n    'swinv2_large_window12_192_22k': 'swinv2_large_window12_192.ms_in22k',\n    'swinv2_large_window12to16_192to256_22kft1k': 'swinv2_large_window12to16_192to256.ms_in22k_ft_in1k',\n    'swinv2_large_window12to24_192to384_22kft1k': 'swinv2_large_window12to24_192to384.ms_in22k_ft_in1k',\n})\n",
  "\"\"\" Bring-Your-Own-Blocks Network\n\nA flexible network w/ dataclass based config for stacking those NN blocks.\n\nThis model is currently used to implement the following networks:\n\nGPU Efficient (ResNets) - gernet_l/m/s (original versions called genet, but this was already used (by SENet author)).\nPaper: `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090\nCode and weights: https://github.com/idstcv/GPU-Efficient-Networks, licensed Apache 2.0\n\nRepVGG - repvgg_*\nPaper: `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\nCode and weights: https://github.com/DingXiaoH/RepVGG, licensed MIT\n\nIn all cases the models have been modified to fit within the design of ByobNet. I've remapped\nthe original weights and verified accuracies.\n\nFor GPU Efficient nets, I used the original names for the blocks since they were for the most part\nthe same as original residual blocks in ResNe(X)t, DarkNet, and other existing models. Note also some\nchanges introduced in RegNet were also present in the stem and bottleneck blocks for this model.\n\nA significant number of different network archs can be implemented here, including variants of the\nabove nets that include attention.\n\nHacked together by / copyright Ross Wightman, 2021.\n\"\"\"\nimport math\nfrom dataclasses import dataclass, field, replace\nfrom functools import partial\nfrom typing import Tuple, List, Dict, Optional, Union, Any, Callable, Sequence\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import ClassifierHead, ConvNormAct, BatchNormAct2d, DropPath, AvgPool2dSame, \\\n    create_conv2d, get_act_layer, get_norm_act_layer, get_attn, make_divisible, to_2tuple, EvoNorm2dS0a\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import named_apply, checkpoint_seq\nfrom ._registry import generate_default_cfgs, register_model\n\n__all__ = ['ByobNet', 'ByoModelCfg', 'ByoBlockCfg', 'create_byob_stem', 'create_block']\n\n\n@dataclass\nclass ByoBlockCfg:\n    type: Union[str, nn.Module]\n    d: int  # block depth (number of block repeats in stage)\n    c: int  # number of output channels for each block in stage\n    s: int = 2  # stride of stage (first block)\n    gs: Optional[Union[int, Callable]] = None  # group-size of blocks in stage, conv is depthwise if gs == 1\n    br: float = 1.  # bottleneck-ratio of blocks in stage\n\n    # NOTE: these config items override the model cfgs that are applied to all blocks by default\n    attn_layer: Optional[str] = None\n    attn_kwargs: Optional[Dict[str, Any]] = None\n    self_attn_layer: Optional[str] = None\n    self_attn_kwargs: Optional[Dict[str, Any]] = None\n    block_kwargs: Optional[Dict[str, Any]] = None\n\n\n@dataclass\nclass ByoModelCfg:\n    blocks: Tuple[Union[ByoBlockCfg, Tuple[ByoBlockCfg, ...]], ...]\n    downsample: str = 'conv1x1'\n    stem_type: str = '3x3'\n    stem_pool: Optional[str] = 'maxpool'\n    stem_chs: int = 32\n    width_factor: float = 1.0\n    num_features: int = 0  # num out_channels for final conv, no final 1x1 conv if 0\n    zero_init_last: bool = True  # zero init last weight (usually bn) in residual path\n    fixed_input_size: bool = False  # model constrained to a fixed-input size / img_size must be provided on creation\n\n    act_layer: str = 'relu'\n    norm_layer: str = 'batchnorm'\n\n    # NOTE: these config items will be overridden by the block cfg (per-block) if they are set there\n    attn_layer: Optional[str] = None\n    attn_kwargs: dict = field(default_factory=lambda: dict())\n    self_attn_layer: Optional[str] = None\n    self_attn_kwargs: dict = field(default_factory=lambda: dict())\n    block_kwargs: Dict[str, Any] = field(default_factory=lambda: dict())\n\n\ndef _rep_vgg_bcfg(d=(4, 6, 16, 1), wf=(1., 1., 1., 1.), groups=0):\n    c = (64, 128, 256, 512)\n    group_size = 0\n    if groups > 0:\n        group_size = lambda chs, idx: chs // groups if (idx + 1) % 2 == 0 else 0\n    bcfg = tuple([ByoBlockCfg(type='rep', d=d, c=c * wf, gs=group_size) for d, c, wf in zip(d, c, wf)])\n    return bcfg\n\n\ndef interleave_blocks(\n        types: Tuple[str, str], d,\n        every: Union[int, List[int]] = 1,\n        first: bool = False,\n        **kwargs,\n) -> Tuple[ByoBlockCfg]:\n    \"\"\" interleave 2 block types in stack\n    \"\"\"\n    assert len(types) == 2\n    if isinstance(every, int):\n        every = list(range(0 if first else every, d, every + 1))\n        if not every:\n            every = [d - 1]\n    set(every)\n    blocks = []\n    for i in range(d):\n        block_type = types[1] if i in every else types[0]\n        blocks += [ByoBlockCfg(type=block_type, d=1, **kwargs)]\n    return tuple(blocks)\n\n\ndef expand_blocks_cfg(stage_blocks_cfg: Union[ByoBlockCfg, Sequence[ByoBlockCfg]]) -> List[ByoBlockCfg]:\n    if not isinstance(stage_blocks_cfg, Sequence):\n        stage_blocks_cfg = (stage_blocks_cfg,)\n    block_cfgs = []\n    for i, cfg in enumerate(stage_blocks_cfg):\n        block_cfgs += [replace(cfg, d=1) for _ in range(cfg.d)]\n    return block_cfgs\n\n\ndef num_groups(group_size, channels):\n    if not group_size:  # 0 or None\n        return 1  # normal conv with 1 group\n    else:\n        # NOTE group_size == 1 -> depthwise conv\n        assert channels % group_size == 0\n        return channels // group_size\n\n\n@dataclass\nclass LayerFn:\n    conv_norm_act: Callable = ConvNormAct\n    norm_act: Callable = BatchNormAct2d\n    act: Callable = nn.ReLU\n    attn: Optional[Callable] = None\n    self_attn: Optional[Callable] = None\n\n\nclass DownsampleAvg(nn.Module):\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: int,\n            stride: int = 1,\n            dilation: int = 1,\n            apply_act: bool = False,\n            layers: LayerFn = None,\n    ):\n        \"\"\" AvgPool Downsampling as in 'D' ResNet variants.\"\"\"\n        super(DownsampleAvg, self).__init__()\n        layers = layers or LayerFn()\n        avg_stride = stride if dilation == 1 else 1\n        if stride > 1 or dilation > 1:\n            avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d\n            self.pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)\n        else:\n            self.pool = nn.Identity()\n        self.conv = layers.conv_norm_act(in_chs, out_chs, 1, apply_act=apply_act)\n\n    def forward(self, x):\n        return self.conv(self.pool(x))\n\n\ndef create_shortcut(\n        downsample_type: str,\n        in_chs: int,\n        out_chs: int,\n        stride: int,\n        dilation: Tuple[int, int],\n        layers: LayerFn,\n        **kwargs,\n):\n    assert downsample_type in ('avg', 'conv1x1', '')\n    if in_chs != out_chs or stride != 1 or dilation[0] != dilation[1]:\n        if not downsample_type:\n            return None  # no shortcut\n        elif downsample_type == 'avg':\n            return DownsampleAvg(in_chs, out_chs, stride=stride, dilation=dilation[0], **kwargs)\n        else:\n            return layers.conv_norm_act(in_chs, out_chs, kernel_size=1, stride=stride, dilation=dilation[0], **kwargs)\n    else:\n        return nn.Identity()  # identity shortcut\n\n\nclass BasicBlock(nn.Module):\n    \"\"\" ResNet Basic Block - kxk + kxk\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: int,\n            kernel_size: int = 3,\n            stride: int = 1,\n            dilation: Tuple[int, int] = (1, 1),\n            group_size: Optional[int] = None,\n            bottle_ratio: float = 1.0,\n            downsample: str = 'avg',\n            attn_last: bool = True,\n            linear_out: bool = False,\n            layers: LayerFn = None,\n            drop_block: Callable = None,\n            drop_path_rate: float = 0.,\n    ):\n        super(BasicBlock, self).__init__()\n        layers = layers or LayerFn()\n        mid_chs = make_divisible(out_chs * bottle_ratio)\n        groups = num_groups(group_size, mid_chs)\n\n        self.shortcut = create_shortcut(\n            downsample, in_chs, out_chs,\n            stride=stride, dilation=dilation, apply_act=False, layers=layers,\n        )\n\n        self.conv1_kxk = layers.conv_norm_act(in_chs, mid_chs, kernel_size, stride=stride, dilation=dilation[0])\n        self.attn = nn.Identity() if attn_last or layers.attn is None else layers.attn(mid_chs)\n        self.conv2_kxk = layers.conv_norm_act(\n            mid_chs, out_chs, kernel_size,\n            dilation=dilation[1], groups=groups, drop_layer=drop_block, apply_act=False,\n        )\n        self.attn_last = nn.Identity() if not attn_last or layers.attn is None else layers.attn(out_chs)\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n        self.act = nn.Identity() if linear_out else layers.act(inplace=True)\n\n    def init_weights(self, zero_init_last: bool = False):\n        if zero_init_last and self.shortcut is not None and getattr(self.conv2_kxk.bn, 'weight', None) is not None:\n            nn.init.zeros_(self.conv2_kxk.bn.weight)\n        for attn in (self.attn, self.attn_last):\n            if hasattr(attn, 'reset_parameters'):\n                attn.reset_parameters()\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv1_kxk(x)\n        x = self.conv2_kxk(x)\n        x = self.attn(x)\n        x = self.drop_path(x)\n        if self.shortcut is not None:\n            x = x + self.shortcut(shortcut)\n        return self.act(x)\n\n\nclass BottleneckBlock(nn.Module):\n    \"\"\" ResNet-like Bottleneck Block - 1x1 - kxk - 1x1\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: int,\n            kernel_size: int = 3,\n            stride: int = 1,\n            dilation: Tuple[int, int] = (1, 1),\n            bottle_ratio: float = 1.,\n            group_size: Optional[int] = None,\n            downsample: str = 'avg',\n            attn_last: bool = False,\n            linear_out: bool = False,\n            extra_conv: bool = False,\n            bottle_in: bool = False,\n            layers: LayerFn = None,\n            drop_block: Callable = None,\n            drop_path_rate: float = 0.,\n    ):\n        super(BottleneckBlock, self).__init__()\n        layers = layers or LayerFn()\n        mid_chs = make_divisible((in_chs if bottle_in else out_chs) * bottle_ratio)\n        groups = num_groups(group_size, mid_chs)\n\n        self.shortcut = create_shortcut(\n            downsample, in_chs, out_chs,\n            stride=stride, dilation=dilation, apply_act=False, layers=layers,\n        )\n\n        self.conv1_1x1 = layers.conv_norm_act(in_chs, mid_chs, 1)\n        self.conv2_kxk = layers.conv_norm_act(\n            mid_chs, mid_chs, kernel_size,\n            stride=stride, dilation=dilation[0], groups=groups, drop_layer=drop_block,\n        )\n        if extra_conv:\n            self.conv2b_kxk = layers.conv_norm_act(\n                mid_chs, mid_chs, kernel_size, dilation=dilation[1], groups=groups)\n        else:\n            self.conv2b_kxk = nn.Identity()\n        self.attn = nn.Identity() if attn_last or layers.attn is None else layers.attn(mid_chs)\n        self.conv3_1x1 = layers.conv_norm_act(mid_chs, out_chs, 1, apply_act=False)\n        self.attn_last = nn.Identity() if not attn_last or layers.attn is None else layers.attn(out_chs)\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n        self.act = nn.Identity() if linear_out else layers.act(inplace=True)\n\n    def init_weights(self, zero_init_last: bool = False):\n        if zero_init_last and self.shortcut is not None and getattr(self.conv3_1x1.bn, 'weight', None) is not None:\n            nn.init.zeros_(self.conv3_1x1.bn.weight)\n        for attn in (self.attn, self.attn_last):\n            if hasattr(attn, 'reset_parameters'):\n                attn.reset_parameters()\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv1_1x1(x)\n        x = self.conv2_kxk(x)\n        x = self.conv2b_kxk(x)\n        x = self.attn(x)\n        x = self.conv3_1x1(x)\n        x = self.attn_last(x)\n        x = self.drop_path(x)\n        if self.shortcut is not None:\n            x = x + self.shortcut(shortcut)\n        return self.act(x)\n\n\nclass DarkBlock(nn.Module):\n    \"\"\" DarkNet-like (1x1 + 3x3 w/ stride) block\n\n    The GE-Net impl included a 1x1 + 3x3 block in their search space. It was not used in the feature models.\n    This block is pretty much a DarkNet block (also DenseNet) hence the name. Neither DarkNet or DenseNet\n    uses strides within the block (external 3x3 or maxpool downsampling is done in front of the block repeats).\n\n    If one does want to use a lot of these blocks w/ stride, I'd recommend using the EdgeBlock (3x3 /w stride + 1x1)\n    for more optimal compute.\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: int,\n            kernel_size: int = 3,\n            stride: int = 1,\n            dilation: Tuple[int, int] = (1, 1),\n            bottle_ratio: float = 1.0,\n            group_size: Optional[int] = None,\n            downsample: str = 'avg',\n            attn_last: bool = True,\n            linear_out: bool = False,\n            layers: LayerFn = None,\n            drop_block: Callable = None,\n            drop_path_rate: float = 0.,\n    ):\n        super(DarkBlock, self).__init__()\n        layers = layers or LayerFn()\n        mid_chs = make_divisible(out_chs * bottle_ratio)\n        groups = num_groups(group_size, mid_chs)\n\n        self.shortcut = create_shortcut(\n            downsample, in_chs, out_chs,\n            stride=stride, dilation=dilation, apply_act=False, layers=layers,\n        )\n\n        self.conv1_1x1 = layers.conv_norm_act(in_chs, mid_chs, 1)\n        self.attn = nn.Identity() if attn_last or layers.attn is None else layers.attn(mid_chs)\n        self.conv2_kxk = layers.conv_norm_act(\n            mid_chs, out_chs, kernel_size,\n            stride=stride, dilation=dilation[0], groups=groups, drop_layer=drop_block, apply_act=False,\n        )\n        self.attn_last = nn.Identity() if not attn_last or layers.attn is None else layers.attn(out_chs)\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n        self.act = nn.Identity() if linear_out else layers.act(inplace=True)\n\n    def init_weights(self, zero_init_last: bool = False):\n        if zero_init_last and self.shortcut is not None and getattr(self.conv2_kxk.bn, 'weight', None) is not None:\n            nn.init.zeros_(self.conv2_kxk.bn.weight)\n        for attn in (self.attn, self.attn_last):\n            if hasattr(attn, 'reset_parameters'):\n                attn.reset_parameters()\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv1_1x1(x)\n        x = self.attn(x)\n        x = self.conv2_kxk(x)\n        x = self.attn_last(x)\n        x = self.drop_path(x)\n        if self.shortcut is not None:\n            x = x + self.shortcut(shortcut)\n        return self.act(x)\n\n\nclass EdgeBlock(nn.Module):\n    \"\"\" EdgeResidual-like (3x3 + 1x1) block\n\n    A two layer block like DarkBlock, but with the order of the 3x3 and 1x1 convs reversed.\n    Very similar to the EfficientNet Edge-Residual block but this block it ends with activations, is\n    intended to be used with either expansion or bottleneck contraction, and can use DW/group/non-grouped convs.\n\n    FIXME is there a more common 3x3 + 1x1 conv block to name this after?\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: int,\n            kernel_size: int = 3,\n            stride: int = 1,\n            dilation: Tuple[int, int] = (1, 1),\n            bottle_ratio: float = 1.0,\n            group_size: Optional[int] = None,\n            downsample: str = 'avg',\n            attn_last: bool = False,\n            linear_out: bool = False,\n            layers: LayerFn = None,\n            drop_block: Callable = None,\n            drop_path_rate: float = 0.,\n    ):\n        super(EdgeBlock, self).__init__()\n        layers = layers or LayerFn()\n        mid_chs = make_divisible(out_chs * bottle_ratio)\n        groups = num_groups(group_size, mid_chs)\n\n        self.shortcut = create_shortcut(\n            downsample, in_chs, out_chs,\n            stride=stride, dilation=dilation, apply_act=False, layers=layers,\n        )\n\n        self.conv1_kxk = layers.conv_norm_act(\n            in_chs, mid_chs, kernel_size,\n            stride=stride, dilation=dilation[0], groups=groups, drop_layer=drop_block,\n        )\n        self.attn = nn.Identity() if attn_last or layers.attn is None else layers.attn(mid_chs)\n        self.conv2_1x1 = layers.conv_norm_act(mid_chs, out_chs, 1, apply_act=False)\n        self.attn_last = nn.Identity() if not attn_last or layers.attn is None else layers.attn(out_chs)\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n        self.act = nn.Identity() if linear_out else layers.act(inplace=True)\n\n    def init_weights(self, zero_init_last: bool = False):\n        if zero_init_last and self.shortcut is not None and getattr(self.conv2_1x1.bn, 'weight', None) is not None:\n            nn.init.zeros_(self.conv2_1x1.bn.weight)\n        for attn in (self.attn, self.attn_last):\n            if hasattr(attn, 'reset_parameters'):\n                attn.reset_parameters()\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv1_kxk(x)\n        x = self.attn(x)\n        x = self.conv2_1x1(x)\n        x = self.attn_last(x)\n        x = self.drop_path(x)\n        if self.shortcut is not None:\n            x = x + self.shortcut(shortcut)\n        return self.act(x)\n\n\nclass RepVggBlock(nn.Module):\n    \"\"\" RepVGG Block.\n\n    Adapted from impl at https://github.com/DingXiaoH/RepVGG\n\n    This version does not currently support the deploy optimization. It is currently fixed in 'train' mode.\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: int,\n            kernel_size: int = 3,\n            stride: int = 1,\n            dilation: Tuple[int, int] = (1, 1),\n            bottle_ratio: float = 1.0,\n            group_size: Optional[int] = None,\n            downsample: str = '',\n            layers: LayerFn = None,\n            drop_block: Callable = None,\n            drop_path_rate: float = 0.,\n    ):\n        super(RepVggBlock, self).__init__()\n        layers = layers or LayerFn()\n        groups = num_groups(group_size, in_chs)\n\n        use_ident = in_chs == out_chs and stride == 1 and dilation[0] == dilation[1]\n        self.identity = layers.norm_act(out_chs, apply_act=False) if use_ident else None\n        self.conv_kxk = layers.conv_norm_act(\n            in_chs, out_chs, kernel_size,\n            stride=stride, dilation=dilation[0], groups=groups, drop_layer=drop_block, apply_act=False,\n        )\n        self.conv_1x1 = layers.conv_norm_act(in_chs, out_chs, 1, stride=stride, groups=groups, apply_act=False)\n        self.attn = nn.Identity() if layers.attn is None else layers.attn(out_chs)\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. and use_ident else nn.Identity()\n        self.act = layers.act(inplace=True)\n\n    def init_weights(self, zero_init_last: bool = False):\n        # NOTE this init overrides that base model init with specific changes for the block type\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                nn.init.normal_(m.weight, .1, .1)\n                nn.init.normal_(m.bias, 0, .1)\n        if hasattr(self.attn, 'reset_parameters'):\n            self.attn.reset_parameters()\n\n    def forward(self, x):\n        if self.identity is None:\n            x = self.conv_1x1(x) + self.conv_kxk(x)\n        else:\n            identity = self.identity(x)\n            x = self.conv_1x1(x) + self.conv_kxk(x)\n            x = self.drop_path(x)  # not in the paper / official impl, experimental\n            x = x + identity\n        x = self.attn(x)  # no attn in the paper / official impl, experimental\n        return self.act(x)\n\n\nclass SelfAttnBlock(nn.Module):\n    \"\"\" ResNet-like Bottleneck Block - 1x1 - optional kxk - self attn - 1x1\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: int,\n            kernel_size: int = 3,\n            stride: int = 1,\n            dilation: Tuple[int, int] = (1, 1),\n            bottle_ratio: float = 1.,\n            group_size: Optional[int] = None,\n            downsample: str = 'avg',\n            extra_conv: bool = False,\n            linear_out: bool = False,\n            bottle_in: bool = False,\n            post_attn_na: bool = True,\n            feat_size: Optional[Tuple[int, int]] = None,\n            layers: LayerFn = None,\n            drop_block: Callable = None,\n            drop_path_rate: float = 0.,\n    ):\n        super(SelfAttnBlock, self).__init__()\n        assert layers is not None\n        mid_chs = make_divisible((in_chs if bottle_in else out_chs) * bottle_ratio)\n        groups = num_groups(group_size, mid_chs)\n\n        self.shortcut = create_shortcut(\n            downsample, in_chs, out_chs,\n            stride=stride, dilation=dilation, apply_act=False, layers=layers,\n        )\n\n        self.conv1_1x1 = layers.conv_norm_act(in_chs, mid_chs, 1)\n        if extra_conv:\n            self.conv2_kxk = layers.conv_norm_act(\n                mid_chs, mid_chs, kernel_size,\n                stride=stride, dilation=dilation[0], groups=groups, drop_layer=drop_block,\n            )\n            stride = 1  # striding done via conv if enabled\n        else:\n            self.conv2_kxk = nn.Identity()\n        opt_kwargs = {} if feat_size is None else dict(feat_size=feat_size)\n        # FIXME need to dilate self attn to have dilated network support, moop moop\n        self.self_attn = layers.self_attn(mid_chs, stride=stride, **opt_kwargs)\n        self.post_attn = layers.norm_act(mid_chs) if post_attn_na else nn.Identity()\n        self.conv3_1x1 = layers.conv_norm_act(mid_chs, out_chs, 1, apply_act=False)\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n        self.act = nn.Identity() if linear_out else layers.act(inplace=True)\n\n    def init_weights(self, zero_init_last: bool = False):\n        if zero_init_last and self.shortcut is not None and getattr(self.conv3_1x1.bn, 'weight', None) is not None:\n            nn.init.zeros_(self.conv3_1x1.bn.weight)\n        if hasattr(self.self_attn, 'reset_parameters'):\n            self.self_attn.reset_parameters()\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv1_1x1(x)\n        x = self.conv2_kxk(x)\n        x = self.self_attn(x)\n        x = self.post_attn(x)\n        x = self.conv3_1x1(x)\n        x = self.drop_path(x)\n        if self.shortcut is not None:\n            x = x + self.shortcut(shortcut)\n        return self.act(x)\n\n\n_block_registry = dict(\n    basic=BasicBlock,\n    bottle=BottleneckBlock,\n    dark=DarkBlock,\n    edge=EdgeBlock,\n    rep=RepVggBlock,\n    self_attn=SelfAttnBlock,\n)\n\n\ndef register_block(block_type:str, block_fn: nn.Module):\n    _block_registry[block_type] = block_fn\n\n\ndef create_block(block: Union[str, nn.Module], **kwargs):\n    if isinstance(block, (nn.Module, partial)):\n        return block(**kwargs)\n    assert block in _block_registry, f'Unknown block type ({block}'\n    return _block_registry[block](**kwargs)\n\n\nclass Stem(nn.Sequential):\n\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: int,\n            kernel_size: int = 3,\n            stride: int = 4,\n            pool: str = 'maxpool',\n            num_rep: int = 3,\n            num_act: Optional[int] = None,\n            chs_decay: float = 0.5,\n            layers: LayerFn = None,\n    ):\n        super().__init__()\n        assert stride in (2, 4)\n        layers = layers or LayerFn()\n\n        if isinstance(out_chs, (list, tuple)):\n            num_rep = len(out_chs)\n            stem_chs = out_chs\n        else:\n            stem_chs = [round(out_chs * chs_decay ** i) for i in range(num_rep)][::-1]\n\n        self.stride = stride\n        self.feature_info = []  # track intermediate features\n        prev_feat = ''\n        stem_strides = [2] + [1] * (num_rep - 1)\n        if stride == 4 and not pool:\n            # set last conv in stack to be strided if stride == 4 and no pooling layer\n            stem_strides[-1] = 2\n\n        num_act = num_rep if num_act is None else num_act\n        # if num_act < num_rep, first convs in stack won't have bn + act\n        stem_norm_acts = [False] * (num_rep - num_act) + [True] * num_act\n        prev_chs = in_chs\n        curr_stride = 1\n        for i, (ch, s, na) in enumerate(zip(stem_chs, stem_strides, stem_norm_acts)):\n            layer_fn = layers.conv_norm_act if na else create_conv2d\n            conv_name = f'conv{i + 1}'\n            if i > 0 and s > 1:\n                self.feature_info.append(dict(num_chs=prev_chs, reduction=curr_stride, module=prev_feat))\n            self.add_module(conv_name, layer_fn(prev_chs, ch, kernel_size=kernel_size, stride=s))\n            prev_chs = ch\n            curr_stride *= s\n            prev_feat = conv_name\n\n        if pool and 'max' in pool.lower():\n            self.feature_info.append(dict(num_chs=prev_chs, reduction=curr_stride, module=prev_feat))\n            self.add_module('pool', nn.MaxPool2d(3, 2, 1))\n            curr_stride *= 2\n            prev_feat = 'pool'\n\n        self.feature_info.append(dict(num_chs=prev_chs, reduction=curr_stride, module=prev_feat))\n        assert curr_stride == stride\n\n\ndef create_byob_stem(\n        in_chs: int,\n        out_chs: int,\n        stem_type: str = '',\n        pool_type: str = '',\n        feat_prefix: str = 'stem',\n        layers: LayerFn = None,\n):\n    layers = layers or LayerFn()\n    assert stem_type in ('', 'quad', 'quad2', 'tiered', 'deep', 'rep', '7x7', '3x3')\n    if 'quad' in stem_type:\n        # based on NFNet stem, stack of 4 3x3 convs\n        num_act = 2 if 'quad2' in stem_type else None\n        stem = Stem(in_chs, out_chs, num_rep=4, num_act=num_act, pool=pool_type, layers=layers)\n    elif 'tiered' in stem_type:\n        # 3x3 stack of 3 convs as in my ResNet-T\n        stem = Stem(in_chs, (3 * out_chs // 8, out_chs // 2, out_chs), pool=pool_type, layers=layers)\n    elif 'deep' in stem_type:\n        # 3x3 stack of 3 convs as in ResNet-D\n        stem = Stem(in_chs, out_chs, num_rep=3, chs_decay=1.0, pool=pool_type, layers=layers)\n    elif 'rep' in stem_type:\n        stem = RepVggBlock(in_chs, out_chs, stride=2, layers=layers)\n    elif '7x7' in stem_type:\n        # 7x7 stem conv as in ResNet\n        if pool_type:\n            stem = Stem(in_chs, out_chs, 7, num_rep=1, pool=pool_type, layers=layers)\n        else:\n            stem = layers.conv_norm_act(in_chs, out_chs, 7, stride=2)\n    else:\n        # 3x3 stem conv as in RegNet is the default\n        if pool_type:\n            stem = Stem(in_chs, out_chs, 3, num_rep=1, pool=pool_type, layers=layers)\n        else:\n            stem = layers.conv_norm_act(in_chs, out_chs, 3, stride=2)\n\n    if isinstance(stem, Stem):\n        feature_info = [dict(f, module='.'.join([feat_prefix, f['module']])) for f in stem.feature_info]\n    else:\n        feature_info = [dict(num_chs=out_chs, reduction=2, module=feat_prefix)]\n    return stem, feature_info\n\n\ndef reduce_feat_size(feat_size, stride=2):\n    return None if feat_size is None else tuple([s // stride for s in feat_size])\n\n\ndef override_kwargs(block_kwargs, model_kwargs):\n    \"\"\" Override model level attn/self-attn/block kwargs w/ block level\n\n    NOTE: kwargs are NOT merged across levels, block_kwargs will fully replace model_kwargs\n    for the block if set to anything that isn't None.\n\n    i.e. an empty block_kwargs dict will remove kwargs set at model level for that block\n    \"\"\"\n    out_kwargs = block_kwargs if block_kwargs is not None else model_kwargs\n    return out_kwargs or {}  # make sure None isn't returned\n\n\ndef update_block_kwargs(block_kwargs: Dict[str, Any], block_cfg: ByoBlockCfg, model_cfg: ByoModelCfg, ):\n    layer_fns = block_kwargs['layers']\n\n    # override attn layer / args with block local config\n    attn_set = block_cfg.attn_layer is not None\n    if attn_set or block_cfg.attn_kwargs is not None:\n        # override attn layer config\n        if attn_set and not block_cfg.attn_layer:\n            # empty string for attn_layer type will disable attn for this block\n            attn_layer = None\n        else:\n            attn_kwargs = override_kwargs(block_cfg.attn_kwargs, model_cfg.attn_kwargs)\n            attn_layer = block_cfg.attn_layer or model_cfg.attn_layer\n            attn_layer = partial(get_attn(attn_layer), **attn_kwargs) if attn_layer is not None else None\n        layer_fns = replace(layer_fns, attn=attn_layer)\n\n    # override self-attn layer / args with block local cfg\n    self_attn_set = block_cfg.self_attn_layer is not None\n    if self_attn_set or block_cfg.self_attn_kwargs is not None:\n        # override attn layer config\n        if self_attn_set and not block_cfg.self_attn_layer:  # attn_layer == ''\n            # empty string for self_attn_layer type will disable attn for this block\n            self_attn_layer = None\n        else:\n            self_attn_kwargs = override_kwargs(block_cfg.self_attn_kwargs, model_cfg.self_attn_kwargs)\n            self_attn_layer = block_cfg.self_attn_layer or model_cfg.self_attn_layer\n            self_attn_layer = partial(get_attn(self_attn_layer), **self_attn_kwargs) \\\n                if self_attn_layer is not None else None\n        layer_fns = replace(layer_fns, self_attn=self_attn_layer)\n\n    block_kwargs['layers'] = layer_fns\n\n    # add additional block_kwargs specified in block_cfg or model_cfg, precedence to block if set\n    block_kwargs.update(override_kwargs(block_cfg.block_kwargs, model_cfg.block_kwargs))\n\n\ndef create_byob_stages(\n        cfg: ByoModelCfg,\n        drop_path_rate: float,\n        output_stride: int,\n        stem_feat: Dict[str, Any],\n        feat_size: Optional[int] = None,\n        layers: Optional[LayerFn] = None,\n        block_kwargs_fn: Optional[Callable] = update_block_kwargs,\n):\n\n    layers = layers or LayerFn()\n    feature_info = []\n    block_cfgs = [expand_blocks_cfg(s) for s in cfg.blocks]\n    depths = [sum([bc.d for bc in stage_bcs]) for stage_bcs in block_cfgs]\n    dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n    dilation = 1\n    net_stride = stem_feat['reduction']\n    prev_chs = stem_feat['num_chs']\n    prev_feat = stem_feat\n    stages = []\n    for stage_idx, stage_block_cfgs in enumerate(block_cfgs):\n        stride = stage_block_cfgs[0].s\n        if stride != 1 and prev_feat:\n            feature_info.append(prev_feat)\n        if net_stride >= output_stride and stride > 1:\n            dilation *= stride\n            stride = 1\n        net_stride *= stride\n        first_dilation = 1 if dilation in (1, 2) else 2\n\n        blocks = []\n        for block_idx, block_cfg in enumerate(stage_block_cfgs):\n            out_chs = make_divisible(block_cfg.c * cfg.width_factor)\n            group_size = block_cfg.gs\n            if isinstance(group_size, Callable):\n                group_size = group_size(out_chs, block_idx)\n            block_kwargs = dict(  # Blocks used in this model must accept these arguments\n                in_chs=prev_chs,\n                out_chs=out_chs,\n                stride=stride if block_idx == 0 else 1,\n                dilation=(first_dilation, dilation),\n                group_size=group_size,\n                bottle_ratio=block_cfg.br,\n                downsample=cfg.downsample,\n                drop_path_rate=dpr[stage_idx][block_idx],\n                layers=layers,\n            )\n            if block_cfg.type in ('self_attn',):\n                # add feat_size arg for blocks that support/need it\n                block_kwargs['feat_size'] = feat_size\n            block_kwargs_fn(block_kwargs, block_cfg=block_cfg, model_cfg=cfg)\n            blocks += [create_block(block_cfg.type, **block_kwargs)]\n            first_dilation = dilation\n            prev_chs = out_chs\n            if stride > 1 and block_idx == 0:\n                feat_size = reduce_feat_size(feat_size, stride)\n\n        stages += [nn.Sequential(*blocks)]\n        prev_feat = dict(num_chs=prev_chs, reduction=net_stride, module=f'stages.{stage_idx}')\n\n    feature_info.append(prev_feat)\n    return nn.Sequential(*stages), feature_info\n\n\ndef get_layer_fns(cfg: ByoModelCfg):\n    act = get_act_layer(cfg.act_layer)\n    norm_act = get_norm_act_layer(norm_layer=cfg.norm_layer, act_layer=act)\n    conv_norm_act = partial(ConvNormAct, norm_layer=cfg.norm_layer, act_layer=act)\n    attn = partial(get_attn(cfg.attn_layer), **cfg.attn_kwargs) if cfg.attn_layer else None\n    self_attn = partial(get_attn(cfg.self_attn_layer), **cfg.self_attn_kwargs) if cfg.self_attn_layer else None\n    layer_fn = LayerFn(conv_norm_act=conv_norm_act, norm_act=norm_act, act=act, attn=attn, self_attn=self_attn)\n    return layer_fn\n\n\nclass ByobNet(nn.Module):\n    \"\"\" 'Bring-your-own-blocks' Net\n\n    A flexible network backbone that allows building model stem + blocks via\n    dataclass cfg definition w/ factory functions for module instantiation.\n\n    Current assumption is that both stem and blocks are in conv-bn-act order (w/ block ending in act).\n    \"\"\"\n    def __init__(\n            self,\n            cfg: ByoModelCfg,\n            num_classes: int = 1000,\n            in_chans: int = 3,\n            global_pool: str = 'avg',\n            output_stride: int = 32,\n            img_size: Optional[Union[int, Tuple[int, int]]] = None,\n            drop_rate: float = 0.,\n            drop_path_rate: float =0.,\n            zero_init_last: bool = True,\n            **kwargs,\n    ):\n        \"\"\"\n        Args:\n            cfg: Model architecture configuration.\n            num_classes: Number of classifier classes.\n            in_chans: Number of input channels.\n            global_pool: Global pooling type.\n            output_stride: Output stride of network, one of (8, 16, 32).\n            img_size: Image size for fixed image size models (i.e. self-attn).\n            drop_rate: Classifier dropout rate.\n            drop_path_rate: Stochastic depth drop-path rate.\n            zero_init_last: Zero-init last weight of residual path.\n            **kwargs: Extra kwargs overlayed onto cfg.\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        self.grad_checkpointing = False\n\n        cfg = replace(cfg, **kwargs)  # overlay kwargs onto cfg\n        layers = get_layer_fns(cfg)\n        if cfg.fixed_input_size:\n            assert img_size is not None, 'img_size argument is required for fixed input size model'\n        feat_size = to_2tuple(img_size) if img_size is not None else None\n\n        self.feature_info = []\n        stem_chs = int(round((cfg.stem_chs or cfg.blocks[0].c) * cfg.width_factor))\n        self.stem, stem_feat = create_byob_stem(in_chans, stem_chs, cfg.stem_type, cfg.stem_pool, layers=layers)\n        self.feature_info.extend(stem_feat[:-1])\n        feat_size = reduce_feat_size(feat_size, stride=stem_feat[-1]['reduction'])\n\n        self.stages, stage_feat = create_byob_stages(\n            cfg,\n            drop_path_rate,\n            output_stride,\n            stem_feat[-1],\n            layers=layers,\n            feat_size=feat_size,\n        )\n        self.feature_info.extend(stage_feat[:-1])\n\n        prev_chs = stage_feat[-1]['num_chs']\n        if cfg.num_features:\n            self.num_features = int(round(cfg.width_factor * cfg.num_features))\n            self.final_conv = layers.conv_norm_act(prev_chs, self.num_features, 1)\n        else:\n            self.num_features = prev_chs\n            self.final_conv = nn.Identity()\n        self.feature_info += [\n            dict(num_chs=self.num_features, reduction=stage_feat[-1]['reduction'], module='final_conv')]\n\n        self.head = ClassifierHead(\n            self.num_features,\n            num_classes,\n            pool_type=global_pool,\n            drop_rate=self.drop_rate,\n        )\n\n        # init weights\n        named_apply(partial(_init_weights, zero_init_last=zero_init_last), self)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^stem',\n            blocks=[\n                (r'^stages\\.(\\d+)' if coarse else r'^stages\\.(\\d+)\\.(\\d+)', None),\n                (r'^final_conv', (99999,))\n            ]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.head.reset(num_classes, global_pool)\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.stages, x)\n        else:\n            x = self.stages(x)\n        x = self.final_conv(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=pre_logits)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _init_weights(module, name='', zero_init_last=False):\n    if isinstance(module, nn.Conv2d):\n        fan_out = module.kernel_size[0] * module.kernel_size[1] * module.out_channels\n        fan_out //= module.groups\n        module.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n        if module.bias is not None:\n            module.bias.data.zero_()\n    elif isinstance(module, nn.Linear):\n        nn.init.normal_(module.weight, mean=0.0, std=0.01)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.BatchNorm2d):\n        nn.init.ones_(module.weight)\n        nn.init.zeros_(module.bias)\n    elif hasattr(module, 'init_weights'):\n        module.init_weights(zero_init_last=zero_init_last)\n\n\nmodel_cfgs = dict(\n    gernet_l=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='basic', d=1, c=128, s=2, gs=0, br=1.),\n            ByoBlockCfg(type='basic', d=2, c=192, s=2, gs=0, br=1.),\n            ByoBlockCfg(type='bottle', d=6, c=640, s=2, gs=0, br=1 / 4),\n            ByoBlockCfg(type='bottle', d=5, c=640, s=2, gs=1, br=3.),\n            ByoBlockCfg(type='bottle', d=4, c=640, s=1, gs=1, br=3.),\n        ),\n        stem_chs=32,\n        stem_pool=None,\n        num_features=2560,\n    ),\n    gernet_m=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='basic', d=1, c=128, s=2, gs=0, br=1.),\n            ByoBlockCfg(type='basic', d=2, c=192, s=2, gs=0, br=1.),\n            ByoBlockCfg(type='bottle', d=6, c=640, s=2, gs=0, br=1 / 4),\n            ByoBlockCfg(type='bottle', d=4, c=640, s=2, gs=1, br=3.),\n            ByoBlockCfg(type='bottle', d=1, c=640, s=1, gs=1, br=3.),\n        ),\n        stem_chs=32,\n        stem_pool=None,\n        num_features=2560,\n    ),\n    gernet_s=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='basic', d=1, c=48, s=2, gs=0, br=1.),\n            ByoBlockCfg(type='basic', d=3, c=48, s=2, gs=0, br=1.),\n            ByoBlockCfg(type='bottle', d=7, c=384, s=2, gs=0, br=1 / 4),\n            ByoBlockCfg(type='bottle', d=2, c=560, s=2, gs=1, br=3.),\n            ByoBlockCfg(type='bottle', d=1, c=256, s=1, gs=1, br=3.),\n        ),\n        stem_chs=13,\n        stem_pool=None,\n        num_features=1920,\n    ),\n\n    repvgg_a2=ByoModelCfg(\n        blocks=_rep_vgg_bcfg(d=(2, 4, 14, 1), wf=(1.5, 1.5, 1.5, 2.75)),\n        stem_type='rep',\n        stem_chs=64,\n    ),\n    repvgg_b0=ByoModelCfg(\n        blocks=_rep_vgg_bcfg(wf=(1., 1., 1., 2.5)),\n        stem_type='rep',\n        stem_chs=64,\n    ),\n    repvgg_b1=ByoModelCfg(\n        blocks=_rep_vgg_bcfg(wf=(2., 2., 2., 4.)),\n        stem_type='rep',\n        stem_chs=64,\n    ),\n    repvgg_b1g4=ByoModelCfg(\n        blocks=_rep_vgg_bcfg(wf=(2., 2., 2., 4.), groups=4),\n        stem_type='rep',\n        stem_chs=64,\n    ),\n    repvgg_b2=ByoModelCfg(\n        blocks=_rep_vgg_bcfg(wf=(2.5, 2.5, 2.5, 5.)),\n        stem_type='rep',\n        stem_chs=64,\n    ),\n    repvgg_b2g4=ByoModelCfg(\n        blocks=_rep_vgg_bcfg(wf=(2.5, 2.5, 2.5, 5.), groups=4),\n        stem_type='rep',\n        stem_chs=64,\n    ),\n    repvgg_b3=ByoModelCfg(\n        blocks=_rep_vgg_bcfg(wf=(3., 3., 3., 5.)),\n        stem_type='rep',\n        stem_chs=64,\n    ),\n    repvgg_b3g4=ByoModelCfg(\n        blocks=_rep_vgg_bcfg(wf=(3., 3., 3., 5.), groups=4),\n        stem_type='rep',\n        stem_chs=64,\n    ),\n\n    # 4 x conv stem w/ 2 act, no maxpool, 2,4,6,4 repeats, group size 32 in first 3 blocks\n    # DW convs in last block, 2048 pre-FC, silu act\n    resnet51q=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=4, c=512, s=2, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=6, c=1536, s=2, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=4, c=1536, s=2, gs=1, br=1.0),\n        ),\n        stem_chs=128,\n        stem_type='quad2',\n        stem_pool=None,\n        num_features=2048,\n        act_layer='silu',\n    ),\n\n    # 4 x conv stem w/ 4 act, no maxpool, 1,4,6,4 repeats, edge block first, group size 32 in next 2 blocks\n    # DW convs in last block, 4 conv for each bottle block, 2048 pre-FC, silu act\n    resnet61q=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='edge', d=1, c=256, s=1, gs=0, br=1.0, block_kwargs=dict()),\n            ByoBlockCfg(type='bottle', d=4, c=512, s=2, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=6, c=1536, s=2, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=4, c=1536, s=2, gs=1, br=1.0),\n        ),\n        stem_chs=128,\n        stem_type='quad',\n        stem_pool=None,\n        num_features=2048,\n        act_layer='silu',\n        block_kwargs=dict(extra_conv=True),\n    ),\n\n    # A series of ResNeXt-26 models w/ one of none, GC, SE, ECA, BAT attn, group size 32, SiLU act,\n    # and a tiered stem w/ maxpool\n    resnext26ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=1024, s=2, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=2048, s=2, gs=32, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='maxpool',\n        act_layer='silu',\n    ),\n    gcresnext26ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=1024, s=2, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=2048, s=2, gs=32, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='maxpool',\n        act_layer='silu',\n        attn_layer='gca',\n    ),\n    seresnext26ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=1024, s=2, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=2048, s=2, gs=32, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='maxpool',\n        act_layer='silu',\n        attn_layer='se',\n    ),\n    eca_resnext26ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=1024, s=2, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=2048, s=2, gs=32, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='maxpool',\n        act_layer='silu',\n        attn_layer='eca',\n    ),\n    bat_resnext26ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=1024, s=2, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=2048, s=2, gs=32, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='maxpool',\n        act_layer='silu',\n        attn_layer='bat',\n        attn_kwargs=dict(block_size=8)\n    ),\n\n    # ResNet-32 (2, 3, 3, 2) models w/ no attn, no groups, SiLU act, no pre-fc feat layer, tiered stem w/o maxpool\n    resnet32ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=3, c=1536, s=2, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=1536, s=2, gs=0, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='',\n        num_features=0,\n        act_layer='silu',\n    ),\n\n    # ResNet-33 (2, 3, 3, 2) models w/ no attn, no groups, SiLU act, 1280 pre-FC feat, tiered stem w/o maxpool\n    resnet33ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=3, c=1536, s=2, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=1536, s=2, gs=0, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='',\n        num_features=1280,\n        act_layer='silu',\n    ),\n\n    # A series of ResNet-33 (2, 3, 3, 2) models w/ one of GC, SE, ECA attn, no groups, SiLU act, 1280 pre-FC feat\n    # and a tiered stem w/ no maxpool\n    gcresnet33ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=3, c=1536, s=2, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=1536, s=2, gs=0, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='',\n        num_features=1280,\n        act_layer='silu',\n        attn_layer='gca',\n    ),\n    seresnet33ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=3, c=1536, s=2, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=1536, s=2, gs=0, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='',\n        num_features=1280,\n        act_layer='silu',\n        attn_layer='se',\n    ),\n    eca_resnet33ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=3, c=1536, s=2, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=1536, s=2, gs=0, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='',\n        num_features=1280,\n        act_layer='silu',\n        attn_layer='eca',\n    ),\n\n    gcresnet50t=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=3, c=256, s=1, br=0.25),\n            ByoBlockCfg(type='bottle', d=4, c=512, s=2, br=0.25),\n            ByoBlockCfg(type='bottle', d=6, c=1024, s=2, br=0.25),\n            ByoBlockCfg(type='bottle', d=3, c=2048, s=2, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='',\n        attn_layer='gca',\n    ),\n\n    gcresnext50ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=3, c=256, s=1, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=4, c=512, s=2, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=6, c=1024, s=2, gs=32, br=0.25),\n            ByoBlockCfg(type='bottle', d=3, c=2048, s=2, gs=32, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='maxpool',\n        act_layer='silu',\n        attn_layer='gca',\n    ),\n\n    # experimental models, closer to a RegNetZ than a ResNet. Similar to EfficientNets but w/ groups instead of DW\n    regnetz_b16=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=48, s=2, gs=16, br=3),\n            ByoBlockCfg(type='bottle', d=6, c=96, s=2, gs=16, br=3),\n            ByoBlockCfg(type='bottle', d=12, c=192, s=2, gs=16, br=3),\n            ByoBlockCfg(type='bottle', d=2, c=288, s=2, gs=16, br=3),\n        ),\n        stem_chs=32,\n        stem_pool='',\n        downsample='',\n        num_features=1536,\n        act_layer='silu',\n        attn_layer='se',\n        attn_kwargs=dict(rd_ratio=0.25),\n        block_kwargs=dict(bottle_in=True, linear_out=True),\n    ),\n    regnetz_c16=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=48, s=2, gs=16, br=4),\n            ByoBlockCfg(type='bottle', d=6, c=96, s=2, gs=16, br=4),\n            ByoBlockCfg(type='bottle', d=12, c=192, s=2, gs=16, br=4),\n            ByoBlockCfg(type='bottle', d=2, c=288, s=2, gs=16, br=4),\n        ),\n        stem_chs=32,\n        stem_pool='',\n        downsample='',\n        num_features=1536,\n        act_layer='silu',\n        attn_layer='se',\n        attn_kwargs=dict(rd_ratio=0.25),\n        block_kwargs=dict(bottle_in=True, linear_out=True),\n    ),\n    regnetz_d32=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=3, c=64, s=1, gs=32, br=4),\n            ByoBlockCfg(type='bottle', d=6, c=128, s=2, gs=32, br=4),\n            ByoBlockCfg(type='bottle', d=12, c=256, s=2, gs=32, br=4),\n            ByoBlockCfg(type='bottle', d=3, c=384, s=2, gs=32, br=4),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='',\n        downsample='',\n        num_features=1792,\n        act_layer='silu',\n        attn_layer='se',\n        attn_kwargs=dict(rd_ratio=0.25),\n        block_kwargs=dict(bottle_in=True, linear_out=True),\n    ),\n    regnetz_d8=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=3, c=64, s=1, gs=8, br=4),\n            ByoBlockCfg(type='bottle', d=6, c=128, s=2, gs=8, br=4),\n            ByoBlockCfg(type='bottle', d=12, c=256, s=2, gs=8, br=4),\n            ByoBlockCfg(type='bottle', d=3, c=384, s=2, gs=8, br=4),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='',\n        downsample='',\n        num_features=1792,\n        act_layer='silu',\n        attn_layer='se',\n        attn_kwargs=dict(rd_ratio=0.25),\n        block_kwargs=dict(bottle_in=True, linear_out=True),\n    ),\n    regnetz_e8=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=3, c=96, s=1, gs=8, br=4),\n            ByoBlockCfg(type='bottle', d=8, c=192, s=2, gs=8, br=4),\n            ByoBlockCfg(type='bottle', d=16, c=384, s=2, gs=8, br=4),\n            ByoBlockCfg(type='bottle', d=3, c=512, s=2, gs=8, br=4),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='',\n        downsample='',\n        num_features=2048,\n        act_layer='silu',\n        attn_layer='se',\n        attn_kwargs=dict(rd_ratio=0.25),\n        block_kwargs=dict(bottle_in=True, linear_out=True),\n    ),\n\n    # experimental EvoNorm configs\n    regnetz_b16_evos=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=48, s=2, gs=16, br=3),\n            ByoBlockCfg(type='bottle', d=6, c=96, s=2, gs=16, br=3),\n            ByoBlockCfg(type='bottle', d=12, c=192, s=2, gs=16, br=3),\n            ByoBlockCfg(type='bottle', d=2, c=288, s=2, gs=16, br=3),\n        ),\n        stem_chs=32,\n        stem_pool='',\n        downsample='',\n        num_features=1536,\n        act_layer='silu',\n        norm_layer=partial(EvoNorm2dS0a, group_size=16),\n        attn_layer='se',\n        attn_kwargs=dict(rd_ratio=0.25),\n        block_kwargs=dict(bottle_in=True, linear_out=True),\n    ),\n    regnetz_c16_evos=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=48, s=2, gs=16, br=4),\n            ByoBlockCfg(type='bottle', d=6, c=96, s=2, gs=16, br=4),\n            ByoBlockCfg(type='bottle', d=12, c=192, s=2, gs=16, br=4),\n            ByoBlockCfg(type='bottle', d=2, c=288, s=2, gs=16, br=4),\n        ),\n        stem_chs=32,\n        stem_pool='',\n        downsample='',\n        num_features=1536,\n        act_layer='silu',\n        norm_layer=partial(EvoNorm2dS0a, group_size=16),\n        attn_layer='se',\n        attn_kwargs=dict(rd_ratio=0.25),\n        block_kwargs=dict(bottle_in=True, linear_out=True),\n    ),\n    regnetz_d8_evos=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=3, c=64, s=1, gs=8, br=4),\n            ByoBlockCfg(type='bottle', d=6, c=128, s=2, gs=8, br=4),\n            ByoBlockCfg(type='bottle', d=12, c=256, s=2, gs=8, br=4),\n            ByoBlockCfg(type='bottle', d=3, c=384, s=2, gs=8, br=4),\n        ),\n        stem_chs=64,\n        stem_type='deep',\n        stem_pool='',\n        downsample='',\n        num_features=1792,\n        act_layer='silu',\n        norm_layer=partial(EvoNorm2dS0a, group_size=16),\n        attn_layer='se',\n        attn_kwargs=dict(rd_ratio=0.25),\n        block_kwargs=dict(bottle_in=True, linear_out=True),\n    ),\n)\n\n\ndef _create_byobnet(variant, pretrained=False, **kwargs):\n    return build_model_with_cfg(\n        ByobNet, variant, pretrained,\n        model_cfg=model_cfgs[variant],\n        feature_cfg=dict(flatten_sequential=True),\n        **kwargs)\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bilinear',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.conv', 'classifier': 'head.fc',\n        **kwargs\n    }\n\n\ndef _cfgr(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': (8, 8),\n        'crop_pct': 0.9, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.conv1.conv', 'classifier': 'head.fc',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    # GPU-Efficient (ResNet) weights\n    'gernet_s.idstcv_in1k': _cfg(hf_hub_id='timm/'),\n    'gernet_m.idstcv_in1k': _cfg(hf_hub_id='timm/'),\n    'gernet_l.idstcv_in1k': _cfg(hf_hub_id='timm/', input_size=(3, 256, 256), pool_size=(8, 8)),\n\n    # RepVGG weights\n    'repvgg_a2.rvgg_in1k': _cfg(\n        hf_hub_id='timm/',\n        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv'), license='mit'),\n    'repvgg_b0.rvgg_in1k': _cfg(\n        hf_hub_id='timm/',\n        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv'), license='mit'),\n    'repvgg_b1.rvgg_in1k': _cfg(\n        hf_hub_id='timm/',\n        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv'), license='mit'),\n    'repvgg_b1g4.rvgg_in1k': _cfg(\n        hf_hub_id='timm/',\n        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv'), license='mit'),\n    'repvgg_b2.rvgg_in1k': _cfg(\n        hf_hub_id='timm/',\n        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv'), license='mit'),\n    'repvgg_b2g4.rvgg_in1k': _cfg(\n        hf_hub_id='timm/',\n        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv'), license='mit'),\n    'repvgg_b3.rvgg_in1k': _cfg(\n        hf_hub_id='timm/',\n        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv'), license='mit'),\n    'repvgg_b3g4.rvgg_in1k': _cfg(\n        hf_hub_id='timm/',\n        first_conv=('stem.conv_kxk.conv', 'stem.conv_1x1.conv'), license='mit'),\n\n    # experimental ResNet configs\n    'resnet51q.ra2_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet51q_ra2-d47dcc76.pth',\n        first_conv='stem.conv1', input_size=(3, 256, 256), pool_size=(8, 8),\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'resnet61q.ra2_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet61q_ra2-6afc536c.pth',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n\n    # ResNeXt-26 models with different attention in Bottleneck blocks\n    'resnext26ts.ra2_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/resnext26ts_256_ra2-8bbd9106.pth',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'seresnext26ts.ch_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/seresnext26ts_256-6f0d74a3.pth',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'gcresnext26ts.ch_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/gcresnext26ts_256-e414378b.pth',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'eca_resnext26ts.ch_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/eca_resnext26ts_256-5a1d030f.pth',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'bat_resnext26ts.ch_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/bat_resnext26ts_256-fa6fd595.pth',\n        min_input_size=(3, 256, 256)),\n\n    # ResNet-32 / 33 models with different attention in Bottleneck blocks\n    'resnet32ts.ra2_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/resnet32ts_256-aacf5250.pth',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'resnet33ts.ra2_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/resnet33ts_256-e91b09a4.pth',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'gcresnet33ts.ra2_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/gcresnet33ts_256-0e0cd345.pth',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'seresnet33ts.ra2_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/seresnet33ts_256-f8ad44d9.pth',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'eca_resnet33ts.ra2_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/eca_resnet33ts_256-8f98face.pth',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n\n    'gcresnet50t.ra2_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/gcresnet50t_256-96374d1c.pth',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n\n    'gcresnext50ts.ch_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/gcresnext50ts_256-3e0f515e.pth',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n\n    # custom `timm` specific RegNetZ inspired models w/ different sizing from paper\n    'regnetz_b16.ra3_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/regnetz_b_raa-677d9606.pth',\n        first_conv='stem.conv', mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 224, 224), pool_size=(7, 7), crop_pct=0.94, test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'regnetz_c16.ra3_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/regnetz_c_rab2_256-a54bf36a.pth',\n        first_conv='stem.conv', mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        crop_pct=0.94, test_input_size=(3, 320, 320), test_crop_pct=1.0),\n    'regnetz_d32.ra3_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/regnetz_d_rab_256-b8073a89.pth',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=0.95, test_input_size=(3, 320, 320)),\n    'regnetz_d8.ra3_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/regnetz_d8_bh-afc03c55.pth',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=0.94, test_input_size=(3, 320, 320), test_crop_pct=1.0),\n    'regnetz_e8.ra3_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/regnetz_e8_bh-aace8e6e.pth',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=0.94, test_input_size=(3, 320, 320), test_crop_pct=1.0),\n\n    'regnetz_b16_evos.untrained': _cfgr(\n        first_conv='stem.conv', mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 224, 224), pool_size=(7, 7), crop_pct=0.95, test_input_size=(3, 288, 288)),\n    'regnetz_c16_evos.ch_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetz_c16_evos_ch-d8311942.pth',\n        first_conv='stem.conv', mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        crop_pct=0.95, test_input_size=(3, 320, 320)),\n    'regnetz_d8_evos.ch_in1k': _cfgr(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/regnetz_d8_evos_ch-2bc12646.pth',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=0.95, test_input_size=(3, 320, 320), test_crop_pct=1.0),\n})\n\n\n@register_model\ndef gernet_l(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" GEResNet-Large (GENet-Large from official impl)\n    `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090\n    \"\"\"\n    return _create_byobnet('gernet_l', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef gernet_m(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" GEResNet-Medium (GENet-Normal from official impl)\n    `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090\n    \"\"\"\n    return _create_byobnet('gernet_m', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef gernet_s(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" EResNet-Small (GENet-Small from official impl)\n    `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090\n    \"\"\"\n    return _create_byobnet('gernet_s', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef repvgg_a2(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" RepVGG-A2\n    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\n    \"\"\"\n    return _create_byobnet('repvgg_a2', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef repvgg_b0(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" RepVGG-B0\n    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\n    \"\"\"\n    return _create_byobnet('repvgg_b0', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef repvgg_b1(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" RepVGG-B1\n    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\n    \"\"\"\n    return _create_byobnet('repvgg_b1', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef repvgg_b1g4(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" RepVGG-B1g4\n    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\n    \"\"\"\n    return _create_byobnet('repvgg_b1g4', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef repvgg_b2(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" RepVGG-B2\n    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\n    \"\"\"\n    return _create_byobnet('repvgg_b2', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef repvgg_b2g4(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" RepVGG-B2g4\n    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\n    \"\"\"\n    return _create_byobnet('repvgg_b2g4', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef repvgg_b3(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" RepVGG-B3\n    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\n    \"\"\"\n    return _create_byobnet('repvgg_b3', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef repvgg_b3g4(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" RepVGG-B3g4\n    `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697\n    \"\"\"\n    return _create_byobnet('repvgg_b3g4', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef resnet51q(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('resnet51q', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef resnet61q(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('resnet61q', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef resnext26ts(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('resnext26ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef gcresnext26ts(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('gcresnext26ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef seresnext26ts(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('seresnext26ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef eca_resnext26ts(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('eca_resnext26ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef bat_resnext26ts(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('bat_resnext26ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef resnet32ts(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('resnet32ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef resnet33ts(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('resnet33ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef gcresnet33ts(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('gcresnet33ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef seresnet33ts(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('seresnet33ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef eca_resnet33ts(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('eca_resnet33ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef gcresnet50t(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('gcresnet50t', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef gcresnext50ts(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('gcresnext50ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef regnetz_b16(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('regnetz_b16', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef regnetz_c16(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('regnetz_c16', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef regnetz_d32(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('regnetz_d32', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef regnetz_d8(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('regnetz_d8', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef regnetz_e8(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('regnetz_e8', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef regnetz_b16_evos(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('regnetz_b16_evos', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef regnetz_c16_evos(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('regnetz_c16_evos', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef regnetz_d8_evos(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\"\n    \"\"\"\n    return _create_byobnet('regnetz_d8_evos', pretrained=pretrained, **kwargs)\n",
  "\"\"\" Swin Transformer V2\n\nA PyTorch impl of : `Swin Transformer V2: Scaling Up Capacity and Resolution`\n    - https://arxiv.org/pdf/2111.09883\n\nCode adapted from https://github.com/ChristophReich1996/Swin-Transformer-V2, original copyright/license info below\n\nThis implementation is experimental and subject to change in manners that will break weight compat:\n* Size of the pos embed MLP are not spelled out in paper in terms of dim, fixed for all models? vary with num_heads?\n  * currently dim is fixed, I feel it may make sense to scale with num_heads (dim per head)\n* The specifics of the memory saving 'sequential attention' are not detailed, Christoph Reich has an impl at\n  GitHub link above. It needs further investigation as throughput vs mem tradeoff doesn't appear beneficial.\n* num_heads per stage is not detailed for Huge and Giant model variants\n* 'Giant' is 3B params in paper but ~2.6B here despite matching paper dim + block counts\n* experiments are ongoing wrt to 'main branch' norm layer use and weight init scheme\n\nNoteworthy additions over official Swin v1:\n* MLP relative position embedding is looking promising and adapts to different image/window sizes\n* This impl has been designed to allow easy change of image size with matching window size changes\n* Non-square image size and window size are supported\n\nModifications and additions for timm hacked together by / Copyright 2022, Ross Wightman\n\"\"\"\n# --------------------------------------------------------\n# Swin Transformer V2 reimplementation\n# Copyright (c) 2021 Christoph Reich\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Christoph Reich\n# --------------------------------------------------------\nimport logging\nimport math\nfrom typing import Tuple, Optional, List, Union, Any, Type\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import DropPath, Mlp, ClassifierHead, to_2tuple, _assert\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_function\nfrom ._manipulate import named_apply\nfrom ._registry import generate_default_cfgs, register_model\n\n__all__ = ['SwinTransformerV2Cr']  # model_registry will add each entrypoint fn to this\n\n_logger = logging.getLogger(__name__)\n\n\ndef bchw_to_bhwc(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Permutes a tensor from the shape (B, C, H, W) to (B, H, W, C). \"\"\"\n    return x.permute(0, 2, 3, 1)\n\n\ndef bhwc_to_bchw(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Permutes a tensor from the shape (B, H, W, C) to (B, C, H, W). \"\"\"\n    return x.permute(0, 3, 1, 2)\n\n\ndef window_partition(x, window_size: Tuple[int, int]):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size[0], window_size[0], W // window_size[1], window_size[1], C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0], window_size[1], C)\n    return windows\n\n\n@register_notrace_function  # reason: int argument is a Proxy\ndef window_reverse(windows, window_size: Tuple[int, int], img_size: Tuple[int, int]):\n    \"\"\"\n    Args:\n        windows: (num_windows * B, window_size[0], window_size[1], C)\n        window_size (Tuple[int, int]): Window size\n        img_size (Tuple[int, int]): Image size\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    H, W = img_size\n    C = windows.shape[-1]\n    x = windows.view(-1, H // window_size[0], W // window_size[1], window_size[0], window_size[1], C)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, C)\n    return x\n\n\nclass WindowMultiHeadAttention(nn.Module):\n    r\"\"\"This class implements window-based Multi-Head-Attention with log-spaced continuous position bias.\n\n    Args:\n        dim (int): Number of input features\n        window_size (int): Window size\n        num_heads (int): Number of attention heads\n        drop_attn (float): Dropout rate of attention map\n        drop_proj (float): Dropout rate after projection\n        meta_hidden_dim (int): Number of hidden features in the two layer MLP meta network\n        sequential_attn (bool): If true sequential self-attention is performed\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        window_size: Tuple[int, int],\n        drop_attn: float = 0.0,\n        drop_proj: float = 0.0,\n        meta_hidden_dim: int = 384,  # FIXME what's the optimal value?\n        sequential_attn: bool = False,\n    ) -> None:\n        super(WindowMultiHeadAttention, self).__init__()\n        assert dim % num_heads == 0, \\\n            \"The number of input features (in_features) are not divisible by the number of heads (num_heads).\"\n        self.in_features: int = dim\n        self.window_size: Tuple[int, int] = window_size\n        self.num_heads: int = num_heads\n        self.sequential_attn: bool = sequential_attn\n\n        self.qkv = nn.Linear(in_features=dim, out_features=dim * 3, bias=True)\n        self.attn_drop = nn.Dropout(drop_attn)\n        self.proj = nn.Linear(in_features=dim, out_features=dim, bias=True)\n        self.proj_drop = nn.Dropout(drop_proj)\n        # meta network for positional encodings\n        self.meta_mlp = Mlp(\n            2,  # x, y\n            hidden_features=meta_hidden_dim,\n            out_features=num_heads,\n            act_layer=nn.ReLU,\n            drop=(0.125, 0.)  # FIXME should there be stochasticity, appears to 'overfit' without?\n        )\n        # NOTE old checkpoints used inverse of logit_scale ('tau') following the paper, see conversion fn\n        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones(num_heads)))\n        self._make_pair_wise_relative_positions()\n\n    def _make_pair_wise_relative_positions(self) -> None:\n        \"\"\"Method initializes the pair-wise relative positions to compute the positional biases.\"\"\"\n        device = self.logit_scale.device\n        coordinates = torch.stack(torch.meshgrid([\n            torch.arange(self.window_size[0], device=device),\n            torch.arange(self.window_size[1], device=device)]), dim=0).flatten(1)\n        relative_coordinates = coordinates[:, :, None] - coordinates[:, None, :]\n        relative_coordinates = relative_coordinates.permute(1, 2, 0).reshape(-1, 2).float()\n        relative_coordinates_log = torch.sign(relative_coordinates) * torch.log(\n            1.0 + relative_coordinates.abs())\n        self.register_buffer(\"relative_coordinates_log\", relative_coordinates_log, persistent=False)\n\n    def update_input_size(self, new_window_size: int, **kwargs: Any) -> None:\n        \"\"\"Method updates the window size and so the pair-wise relative positions\n\n        Args:\n            new_window_size (int): New window size\n            kwargs (Any): Unused\n        \"\"\"\n        # Set new window size and new pair-wise relative positions\n        self.window_size: int = new_window_size\n        self._make_pair_wise_relative_positions()\n\n    def _relative_positional_encodings(self) -> torch.Tensor:\n        \"\"\"Method computes the relative positional encodings\n\n        Returns:\n            relative_position_bias (torch.Tensor): Relative positional encodings\n            (1, number of heads, window size ** 2, window size ** 2)\n        \"\"\"\n        window_area = self.window_size[0] * self.window_size[1]\n        relative_position_bias = self.meta_mlp(self.relative_coordinates_log)\n        relative_position_bias = relative_position_bias.transpose(1, 0).reshape(\n            self.num_heads, window_area, window_area\n        )\n        relative_position_bias = relative_position_bias.unsqueeze(0)\n        return relative_position_bias\n\n    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\" Forward pass.\n        Args:\n            x (torch.Tensor): Input tensor of the shape (B * windows, N, C)\n            mask (Optional[torch.Tensor]): Attention mask for the shift case\n\n        Returns:\n            Output tensor of the shape [B * windows, N, C]\n        \"\"\"\n        Bw, L, C = x.shape\n\n        qkv = self.qkv(x).view(Bw, L, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        query, key, value = qkv.unbind(0)\n\n        # compute attention map with scaled cosine attention\n        attn = (F.normalize(query, dim=-1) @ F.normalize(key, dim=-1).transpose(-2, -1))\n        logit_scale = torch.clamp(self.logit_scale.reshape(1, self.num_heads, 1, 1), max=math.log(1. / 0.01)).exp()\n        attn = attn * logit_scale\n        attn = attn + self._relative_positional_encodings()\n\n        if mask is not None:\n            # Apply mask if utilized\n            num_win: int = mask.shape[0]\n            attn = attn.view(Bw // num_win, num_win, self.num_heads, L, L)\n            attn = attn + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, L, L)\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ value).transpose(1, 2).reshape(Bw, L, -1)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass SwinTransformerV2CrBlock(nn.Module):\n    r\"\"\"This class implements the Swin transformer block.\n\n    Args:\n        dim (int): Number of input channels\n        num_heads (int): Number of attention heads to be utilized\n        feat_size (Tuple[int, int]): Input resolution\n        window_size (Tuple[int, int]): Window size to be utilized\n        shift_size (int): Shifting size to be used\n        mlp_ratio (int): Ratio of the hidden dimension in the FFN to the input channels\n        proj_drop (float): Dropout in input mapping\n        drop_attn (float): Dropout rate of attention map\n        drop_path (float): Dropout in main path\n        extra_norm (bool): Insert extra norm on 'main' branch if True\n        sequential_attn (bool): If true sequential self-attention is performed\n        norm_layer (Type[nn.Module]): Type of normalization layer to be utilized\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        feat_size: Tuple[int, int],\n        window_size: Tuple[int, int],\n        shift_size: Tuple[int, int] = (0, 0),\n        mlp_ratio: float = 4.0,\n        init_values: Optional[float] = 0,\n        proj_drop: float = 0.0,\n        drop_attn: float = 0.0,\n        drop_path: float = 0.0,\n        extra_norm: bool = False,\n        sequential_attn: bool = False,\n        norm_layer: Type[nn.Module] = nn.LayerNorm,\n    ) -> None:\n        super(SwinTransformerV2CrBlock, self).__init__()\n        self.dim: int = dim\n        self.feat_size: Tuple[int, int] = feat_size\n        self.target_shift_size: Tuple[int, int] = to_2tuple(shift_size)\n        self.window_size, self.shift_size = self._calc_window_shift(to_2tuple(window_size))\n        self.window_area = self.window_size[0] * self.window_size[1]\n        self.init_values: Optional[float] = init_values\n\n        # attn branch\n        self.attn = WindowMultiHeadAttention(\n            dim=dim,\n            num_heads=num_heads,\n            window_size=self.window_size,\n            drop_attn=drop_attn,\n            drop_proj=proj_drop,\n            sequential_attn=sequential_attn,\n        )\n        self.norm1 = norm_layer(dim)\n        self.drop_path1 = DropPath(drop_prob=drop_path) if drop_path > 0.0 else nn.Identity()\n\n        # mlp branch\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            drop=proj_drop,\n            out_features=dim,\n        )\n        self.norm2 = norm_layer(dim)\n        self.drop_path2 = DropPath(drop_prob=drop_path) if drop_path > 0.0 else nn.Identity()\n\n        # Extra main branch norm layer mentioned for Huge/Giant models in V2 paper.\n        # Also being used as final network norm and optional stage ending norm while still in a C-last format.\n        self.norm3 = norm_layer(dim) if extra_norm else nn.Identity()\n\n        self._make_attention_mask()\n        self.init_weights()\n\n    def _calc_window_shift(self, target_window_size):\n        window_size = [f if f <= w else w for f, w in zip(self.feat_size, target_window_size)]\n        shift_size = [0 if f <= w else s for f, w, s in zip(self.feat_size, window_size, self.target_shift_size)]\n        return tuple(window_size), tuple(shift_size)\n\n    def _make_attention_mask(self) -> None:\n        \"\"\"Method generates the attention mask used in shift case.\"\"\"\n        # Make masks for shift case\n        if any(self.shift_size):\n            # calculate attention mask for SW-MSA\n            H, W = self.feat_size\n            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n            cnt = 0\n            for h in (\n                    slice(0, -self.window_size[0]),\n                    slice(-self.window_size[0], -self.shift_size[0]),\n                    slice(-self.shift_size[0], None)):\n                for w in (\n                        slice(0, -self.window_size[1]),\n                        slice(-self.window_size[1], -self.shift_size[1]),\n                        slice(-self.shift_size[1], None)):\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n            mask_windows = window_partition(img_mask, self.window_size)  # num_windows, window_size, window_size, 1\n            mask_windows = mask_windows.view(-1, self.window_area)\n            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n        else:\n            attn_mask = None\n        self.register_buffer(\"attn_mask\", attn_mask, persistent=False)\n\n    def init_weights(self):\n        # extra, module specific weight init\n        if self.init_values is not None:\n            nn.init.constant_(self.norm1.weight, self.init_values)\n            nn.init.constant_(self.norm2.weight, self.init_values)\n\n    def update_input_size(self, new_window_size: Tuple[int, int], new_feat_size: Tuple[int, int]) -> None:\n        \"\"\"Method updates the image resolution to be processed and window size and so the pair-wise relative positions.\n\n        Args:\n            new_window_size (int): New window size\n            new_feat_size (Tuple[int, int]): New input resolution\n        \"\"\"\n        # Update input resolution\n        self.feat_size: Tuple[int, int] = new_feat_size\n        self.window_size, self.shift_size = self._calc_window_shift(to_2tuple(new_window_size))\n        self.window_area = self.window_size[0] * self.window_size[1]\n        self.attn.update_input_size(new_window_size=self.window_size)\n        self._make_attention_mask()\n\n    def _shifted_window_attn(self, x):\n        B, H, W, C = x.shape\n\n        # cyclic shift\n        sh, sw = self.shift_size\n        do_shift: bool = any(self.shift_size)\n        if do_shift:\n            # FIXME PyTorch XLA needs cat impl, roll not lowered\n            # x = torch.cat([x[:, sh:], x[:, :sh]], dim=1)\n            # x = torch.cat([x[:, :, sw:], x[:, :, :sw]], dim=2)\n            x = torch.roll(x, shifts=(-sh, -sw), dims=(1, 2))\n\n        # partition windows\n        x_windows = window_partition(x, self.window_size)  # num_windows * B, window_size, window_size, C\n        x_windows = x_windows.view(-1, self.window_size[0] * self.window_size[1], C)\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # num_windows * B, window_size * window_size, C\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size[0], self.window_size[1], C)\n        x = window_reverse(attn_windows, self.window_size, self.feat_size)  # B H' W' C\n\n        # reverse cyclic shift\n        if do_shift:\n            # FIXME PyTorch XLA needs cat impl, roll not lowered\n            # x = torch.cat([x[:, -sh:], x[:, :-sh]], dim=1)\n            # x = torch.cat([x[:, :, -sw:], x[:, :, :-sw]], dim=2)\n            x = torch.roll(x, shifts=(sh, sw), dims=(1, 2))\n\n        return x\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward pass.\n\n        Args:\n            x (torch.Tensor): Input tensor of the shape [B, C, H, W]\n\n        Returns:\n            output (torch.Tensor): Output tensor of the shape [B, C, H, W]\n        \"\"\"\n        # post-norm branches (op -> norm -> drop)\n        x = x + self.drop_path1(self.norm1(self._shifted_window_attn(x)))\n\n        B, H, W, C = x.shape\n        x = x.reshape(B, -1, C)\n        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n        x = self.norm3(x)  # main-branch norm enabled for some blocks / stages (every 6 for Huge/Giant)\n        x = x.reshape(B, H, W, C)\n        return x\n\n\nclass PatchMerging(nn.Module):\n    \"\"\" This class implements the patch merging as a strided convolution with a normalization before.\n    Args:\n        dim (int): Number of input channels\n        norm_layer (Type[nn.Module]): Type of normalization layer to be utilized.\n    \"\"\"\n\n    def __init__(self, dim: int, norm_layer: Type[nn.Module] = nn.LayerNorm) -> None:\n        super(PatchMerging, self).__init__()\n        self.norm = norm_layer(4 * dim)\n        self.reduction = nn.Linear(in_features=4 * dim, out_features=2 * dim, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\" Forward pass.\n        Args:\n            x (torch.Tensor): Input tensor of the shape [B, C, H, W]\n        Returns:\n            output (torch.Tensor): Output tensor of the shape [B, 2 * C, H // 2, W // 2]\n        \"\"\"\n        B, H, W, C = x.shape\n        x = x.reshape(B, H // 2, 2, W // 2, 2, C).permute(0, 1, 3, 4, 2, 5).flatten(3)\n        x = self.norm(x)\n        x = self.reduction(x)\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" 2D Image to Patch Embedding \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.num_patches = self.grid_size[0] * self.grid_size[1]\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        _assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\")\n        _assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\")\n        x = self.proj(x)\n        x = self.norm(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n        return x\n\n\nclass SwinTransformerV2CrStage(nn.Module):\n    r\"\"\"This class implements a stage of the Swin transformer including multiple layers.\n\n    Args:\n        embed_dim (int): Number of input channels\n        depth (int): Depth of the stage (number of layers)\n        downscale (bool): If true input is downsampled (see Fig. 3 or V1 paper)\n        feat_size (Tuple[int, int]): input feature map size (H, W)\n        num_heads (int): Number of attention heads to be utilized\n        window_size (int): Window size to be utilized\n        mlp_ratio (int): Ratio of the hidden dimension in the FFN to the input channels\n        proj_drop (float): Dropout in input mapping\n        drop_attn (float): Dropout rate of attention map\n        drop_path (float): Dropout in main path\n        norm_layer (Type[nn.Module]): Type of normalization layer to be utilized. Default: nn.LayerNorm\n        extra_norm_period (int): Insert extra norm layer on main branch every N (period) blocks\n        extra_norm_stage (bool): End each stage with an extra norm layer in main branch\n        sequential_attn (bool): If true sequential self-attention is performed\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim: int,\n        depth: int,\n        downscale: bool,\n        num_heads: int,\n        feat_size: Tuple[int, int],\n        window_size: Tuple[int, int],\n        mlp_ratio: float = 4.0,\n        init_values: Optional[float] = 0.0,\n        proj_drop: float = 0.0,\n        drop_attn: float = 0.0,\n        drop_path: Union[List[float], float] = 0.0,\n        norm_layer: Type[nn.Module] = nn.LayerNorm,\n        extra_norm_period: int = 0,\n        extra_norm_stage: bool = False,\n        sequential_attn: bool = False,\n    ) -> None:\n        super(SwinTransformerV2CrStage, self).__init__()\n        self.downscale: bool = downscale\n        self.grad_checkpointing: bool = False\n        self.feat_size: Tuple[int, int] = (feat_size[0] // 2, feat_size[1] // 2) if downscale else feat_size\n\n        if downscale:\n            self.downsample = PatchMerging(embed_dim, norm_layer=norm_layer)\n            embed_dim = embed_dim * 2\n        else:\n            self.downsample = nn.Identity()\n\n        def _extra_norm(index):\n            i = index + 1\n            if extra_norm_period and i % extra_norm_period == 0:\n                return True\n            return i == depth if extra_norm_stage else False\n\n        self.blocks = nn.Sequential(*[\n            SwinTransformerV2CrBlock(\n                dim=embed_dim,\n                num_heads=num_heads,\n                feat_size=self.feat_size,\n                window_size=window_size,\n                shift_size=tuple([0 if ((index % 2) == 0) else w // 2 for w in window_size]),\n                mlp_ratio=mlp_ratio,\n                init_values=init_values,\n                proj_drop=proj_drop,\n                drop_attn=drop_attn,\n                drop_path=drop_path[index] if isinstance(drop_path, list) else drop_path,\n                extra_norm=_extra_norm(index),\n                sequential_attn=sequential_attn,\n                norm_layer=norm_layer,\n            )\n            for index in range(depth)]\n        )\n\n    def update_input_size(self, new_window_size: int, new_feat_size: Tuple[int, int]) -> None:\n        \"\"\"Method updates the resolution to utilize and the window size and so the pair-wise relative positions.\n\n        Args:\n            new_window_size (int): New window size\n            new_feat_size (Tuple[int, int]): New input resolution\n        \"\"\"\n        self.feat_size: Tuple[int, int] = (\n            (new_feat_size[0] // 2, new_feat_size[1] // 2) if self.downscale else new_feat_size\n        )\n        for block in self.blocks:\n            block.update_input_size(new_window_size=new_window_size, new_feat_size=self.feat_size)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward pass.\n        Args:\n            x (torch.Tensor): Input tensor of the shape [B, C, H, W] or [B, L, C]\n        Returns:\n            output (torch.Tensor): Output tensor of the shape [B, 2 * C, H // 2, W // 2]\n        \"\"\"\n        x = bchw_to_bhwc(x)\n        x = self.downsample(x)\n        for block in self.blocks:\n            # Perform checkpointing if utilized\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint.checkpoint(block, x)\n            else:\n                x = block(x)\n        x = bhwc_to_bchw(x)\n        return x\n\n\nclass SwinTransformerV2Cr(nn.Module):\n    r\"\"\" Swin Transformer V2\n        A PyTorch impl of : `Swin Transformer V2: Scaling Up Capacity and Resolution`  -\n          https://arxiv.org/pdf/2111.09883\n\n    Args:\n        img_size: Input resolution.\n        window_size: Window size. If None, img_size // window_div\n        img_window_ratio: Window size to image size ratio.\n        patch_size: Patch size.\n        in_chans: Number of input channels.\n        depths: Depth of the stage (number of layers).\n        num_heads: Number of attention heads to be utilized.\n        embed_dim: Patch embedding dimension.\n        num_classes: Number of output classes.\n        mlp_ratio:  Ratio of the hidden dimension in the FFN to the input channels.\n        drop_rate: Dropout rate.\n        proj_drop_rate: Projection dropout rate.\n        attn_drop_rate: Dropout rate of attention map.\n        drop_path_rate: Stochastic depth rate.\n        norm_layer: Type of normalization layer to be utilized.\n        extra_norm_period: Insert extra norm layer on main branch every N (period) blocks in stage\n        extra_norm_stage: End each stage with an extra norm layer in main branch\n        sequential_attn: If true sequential self-attention is performed.\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size: Tuple[int, int] = (224, 224),\n        patch_size: int = 4,\n        window_size: Optional[int] = None,\n        img_window_ratio: int = 32,\n        in_chans: int = 3,\n        num_classes: int = 1000,\n        embed_dim: int = 96,\n        depths: Tuple[int, ...] = (2, 2, 6, 2),\n        num_heads: Tuple[int, ...] = (3, 6, 12, 24),\n        mlp_ratio: float = 4.0,\n        init_values: Optional[float] = 0.,\n        drop_rate: float = 0.0,\n        proj_drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_rate: float = 0.0,\n        norm_layer: Type[nn.Module] = nn.LayerNorm,\n        extra_norm_period: int = 0,\n        extra_norm_stage: bool = False,\n        sequential_attn: bool = False,\n        global_pool: str = 'avg',\n        weight_init='skip',\n        **kwargs: Any\n    ) -> None:\n        super(SwinTransformerV2Cr, self).__init__()\n        img_size = to_2tuple(img_size)\n        window_size = tuple([\n            s // img_window_ratio for s in img_size]) if window_size is None else to_2tuple(window_size)\n\n        self.num_classes: int = num_classes\n        self.patch_size: int = patch_size\n        self.img_size: Tuple[int, int] = img_size\n        self.window_size: int = window_size\n        self.num_features: int = int(embed_dim * 2 ** (len(depths) - 1))\n        self.feature_info = []\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer,\n        )\n        patch_grid_size: Tuple[int, int] = self.patch_embed.grid_size\n\n        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n        stages = []\n        in_dim = embed_dim\n        in_scale = 1\n        for stage_idx, (depth, num_heads) in enumerate(zip(depths, num_heads)):\n            stages += [SwinTransformerV2CrStage(\n                embed_dim=in_dim,\n                depth=depth,\n                downscale=stage_idx != 0,\n                feat_size=(\n                    patch_grid_size[0] // in_scale,\n                    patch_grid_size[1] // in_scale\n                ),\n                num_heads=num_heads,\n                window_size=window_size,\n                mlp_ratio=mlp_ratio,\n                init_values=init_values,\n                proj_drop=proj_drop_rate,\n                drop_attn=attn_drop_rate,\n                drop_path=dpr[stage_idx],\n                extra_norm_period=extra_norm_period,\n                extra_norm_stage=extra_norm_stage or (stage_idx + 1) == len(depths),  # last stage ends w/ norm\n                sequential_attn=sequential_attn,\n                norm_layer=norm_layer,\n            )]\n            if stage_idx != 0:\n                in_dim *= 2\n                in_scale *= 2\n            self.feature_info += [dict(num_chs=in_dim, reduction=4 * in_scale, module=f'stages.{stage_idx}')]\n        self.stages = nn.Sequential(*stages)\n\n        self.head = ClassifierHead(\n            self.num_features,\n            num_classes,\n            pool_type=global_pool,\n            drop_rate=drop_rate,\n        )\n\n        # current weight init skips custom init and uses pytorch layer defaults, seems to work well\n        # FIXME more experiments needed\n        if weight_init != 'skip':\n            named_apply(init_weights, self)\n\n    def update_input_size(\n            self,\n            new_img_size: Optional[Tuple[int, int]] = None,\n            new_window_size: Optional[int] = None,\n            img_window_ratio: int = 32,\n    ) -> None:\n        \"\"\"Method updates the image resolution to be processed and window size and so the pair-wise relative positions.\n\n        Args:\n            new_window_size (Optional[int]): New window size, if None based on new_img_size // window_div\n            new_img_size (Optional[Tuple[int, int]]): New input resolution, if None current resolution is used\n            img_window_ratio (int): divisor for calculating window size from image size\n        \"\"\"\n        # Check parameters\n        if new_img_size is None:\n            new_img_size = self.img_size\n        else:\n            new_img_size = to_2tuple(new_img_size)\n        if new_window_size is None:\n            new_window_size = tuple([s // img_window_ratio for s in new_img_size])\n        # Compute new patch resolution & update resolution of each stage\n        new_patch_grid_size = (new_img_size[0] // self.patch_size, new_img_size[1] // self.patch_size)\n        for index, stage in enumerate(self.stages):\n            stage_scale = 2 ** max(index - 1, 0)\n            stage.update_input_size(\n                new_window_size=new_window_size,\n                new_img_size=(new_patch_grid_size[0] // stage_scale, new_patch_grid_size[1] // stage_scale),\n            )\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^patch_embed',  # stem and embed\n            blocks=r'^stages\\.(\\d+)' if coarse else [\n                (r'^stages\\.(\\d+).downsample', (0,)),\n                (r'^stages\\.(\\d+)\\.\\w+\\.(\\d+)', None),\n            ]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        for s in self.stages:\n            s.grad_checkpointing = enable\n\n    @torch.jit.ignore()\n    def get_classifier(self) -> nn.Module:\n        \"\"\"Method returns the classification head of the model.\n        Returns:\n            head (nn.Module): Current classification head\n        \"\"\"\n        return self.head.fc\n\n    def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None) -> None:\n        \"\"\"Method results the classification head\n\n        Args:\n            num_classes (int): Number of classes to be predicted\n            global_pool (str): Unused\n        \"\"\"\n        self.num_classes = num_classes\n        self.head.reset(num_classes, global_pool)\n\n    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.patch_embed(x)\n        x = self.stages(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=True) if pre_logits else self.head(x)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef init_weights(module: nn.Module, name: str = ''):\n    # FIXME WIP determining if there's a better weight init\n    if isinstance(module, nn.Linear):\n        if 'qkv' in name:\n            # treat the weights of Q, K, V separately\n            val = math.sqrt(6. / float(module.weight.shape[0] // 3 + module.weight.shape[1]))\n            nn.init.uniform_(module.weight, -val, val)\n        elif 'head' in name:\n            nn.init.zeros_(module.weight)\n        else:\n            nn.init.xavier_uniform_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif hasattr(module, 'init_weights'):\n        module.init_weights()\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n    state_dict = state_dict.get('model', state_dict)\n    state_dict = state_dict.get('state_dict', state_dict)\n    if 'head.fc.weight' in state_dict:\n        return state_dict\n    out_dict = {}\n    for k, v in state_dict.items():\n        if 'tau' in k:\n            # convert old tau based checkpoints -> logit_scale (inverse)\n            v = torch.log(1 / v)\n            k = k.replace('tau', 'logit_scale')\n        k = k.replace('head.', 'head.fc.')\n        out_dict[k] = v\n    return out_dict\n\n\ndef _create_swin_transformer_v2_cr(variant, pretrained=False, **kwargs):\n    default_out_indices = tuple(i for i, _ in enumerate(kwargs.get('depths', (1, 1, 1, 1))))\n    out_indices = kwargs.pop('out_indices', default_out_indices)\n\n    model = build_model_with_cfg(\n        SwinTransformerV2Cr, variant, pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),\n        **kwargs\n    )\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000,\n        'input_size': (3, 224, 224),\n        'pool_size': (7, 7),\n        'crop_pct': 0.9,\n        'interpolation': 'bicubic',\n        'fixed_input_size': True,\n        'mean': IMAGENET_DEFAULT_MEAN,\n        'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'patch_embed.proj',\n        'classifier': 'head.fc',\n        **kwargs,\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'swinv2_cr_tiny_384.untrained': _cfg(\n        url=\"\", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),\n    'swinv2_cr_tiny_224.untrained': _cfg(\n        url=\"\", input_size=(3, 224, 224), crop_pct=0.9),\n    'swinv2_cr_tiny_ns_224.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url=\"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-swinv2/swin_v2_cr_tiny_ns_224-ba8166c6.pth\",\n        input_size=(3, 224, 224), crop_pct=0.9),\n    'swinv2_cr_small_384.untrained': _cfg(\n        url=\"\", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),\n    'swinv2_cr_small_224.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url=\"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-swinv2/swin_v2_cr_small_224-0813c165.pth\",\n        input_size=(3, 224, 224), crop_pct=0.9),\n    'swinv2_cr_small_ns_224.sw_in1k': _cfg(\n        hf_hub_id='timm/',\n        url=\"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-swinv2/swin_v2_cr_small_ns_224_iv-2ce90f8e.pth\",\n        input_size=(3, 224, 224), crop_pct=0.9),\n    'swinv2_cr_small_ns_256.untrained': _cfg(\n        url=\"\", input_size=(3, 256, 256), crop_pct=1.0, pool_size=(8, 8)),\n    'swinv2_cr_base_384.untrained': _cfg(\n        url=\"\", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),\n    'swinv2_cr_base_224.untrained': _cfg(\n        url=\"\", input_size=(3, 224, 224), crop_pct=0.9),\n    'swinv2_cr_base_ns_224.untrained': _cfg(\n        url=\"\", input_size=(3, 224, 224), crop_pct=0.9),\n    'swinv2_cr_large_384.untrained': _cfg(\n        url=\"\", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),\n    'swinv2_cr_large_224.untrained': _cfg(\n        url=\"\", input_size=(3, 224, 224), crop_pct=0.9),\n    'swinv2_cr_huge_384.untrained': _cfg(\n        url=\"\", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),\n    'swinv2_cr_huge_224.untrained': _cfg(\n        url=\"\", input_size=(3, 224, 224), crop_pct=0.9),\n    'swinv2_cr_giant_384.untrained': _cfg(\n        url=\"\", input_size=(3, 384, 384), crop_pct=1.0, pool_size=(12, 12)),\n    'swinv2_cr_giant_224.untrained': _cfg(\n        url=\"\", input_size=(3, 224, 224), crop_pct=0.9),\n})\n\n\n@register_model\ndef swinv2_cr_tiny_384(pretrained=False, **kwargs) -> SwinTransformerV2Cr:\n    \"\"\"Swin-T V2 CR @ 384x384, trained ImageNet-1k\"\"\"\n    model_args = dict(\n        embed_dim=96,\n        depths=(2, 2, 6, 2),\n        num_heads=(3, 6, 12, 24),\n    )\n    return _create_swin_transformer_v2_cr('swinv2_cr_tiny_384', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_cr_tiny_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:\n    \"\"\"Swin-T V2 CR @ 224x224, trained ImageNet-1k\"\"\"\n    model_args = dict(\n        embed_dim=96,\n        depths=(2, 2, 6, 2),\n        num_heads=(3, 6, 12, 24),\n    )\n    return _create_swin_transformer_v2_cr('swinv2_cr_tiny_224', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_cr_tiny_ns_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:\n    \"\"\"Swin-T V2 CR @ 224x224, trained ImageNet-1k w/ extra stage norms.\n    ** Experimental, may make default if results are improved. **\n    \"\"\"\n    model_args = dict(\n        embed_dim=96,\n        depths=(2, 2, 6, 2),\n        num_heads=(3, 6, 12, 24),\n        extra_norm_stage=True,\n    )\n    return _create_swin_transformer_v2_cr('swinv2_cr_tiny_ns_224', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_cr_small_384(pretrained=False, **kwargs) -> SwinTransformerV2Cr:\n    \"\"\"Swin-S V2 CR @ 384x384, trained ImageNet-1k\"\"\"\n    model_args = dict(\n        embed_dim=96,\n        depths=(2, 2, 18, 2),\n        num_heads=(3, 6, 12, 24),\n    )\n    return _create_swin_transformer_v2_cr('swinv2_cr_small_384', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_cr_small_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:\n    \"\"\"Swin-S V2 CR @ 224x224, trained ImageNet-1k\"\"\"\n    model_args = dict(\n        embed_dim=96,\n        depths=(2, 2, 18, 2),\n        num_heads=(3, 6, 12, 24),\n    )\n    return _create_swin_transformer_v2_cr('swinv2_cr_small_224', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_cr_small_ns_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:\n    \"\"\"Swin-S V2 CR @ 224x224, trained ImageNet-1k\"\"\"\n    model_args = dict(\n        embed_dim=96,\n        depths=(2, 2, 18, 2),\n        num_heads=(3, 6, 12, 24),\n        extra_norm_stage=True,\n    )\n    return _create_swin_transformer_v2_cr('swinv2_cr_small_ns_224', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_cr_small_ns_256(pretrained=False, **kwargs) -> SwinTransformerV2Cr:\n    \"\"\"Swin-S V2 CR @ 256x256, trained ImageNet-1k\"\"\"\n    model_args = dict(\n        embed_dim=96,\n        depths=(2, 2, 18, 2),\n        num_heads=(3, 6, 12, 24),\n        extra_norm_stage=True,\n    )\n    return _create_swin_transformer_v2_cr('swinv2_cr_small_ns_256', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_cr_base_384(pretrained=False, **kwargs) -> SwinTransformerV2Cr:\n    \"\"\"Swin-B V2 CR @ 384x384, trained ImageNet-1k\"\"\"\n    model_args = dict(\n        embed_dim=128,\n        depths=(2, 2, 18, 2),\n        num_heads=(4, 8, 16, 32),\n    )\n    return _create_swin_transformer_v2_cr('swinv2_cr_base_384', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_cr_base_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:\n    \"\"\"Swin-B V2 CR @ 224x224, trained ImageNet-1k\"\"\"\n    model_args = dict(\n        embed_dim=128,\n        depths=(2, 2, 18, 2),\n        num_heads=(4, 8, 16, 32),\n    )\n    return _create_swin_transformer_v2_cr('swinv2_cr_base_224', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_cr_base_ns_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:\n    \"\"\"Swin-B V2 CR @ 224x224, trained ImageNet-1k\"\"\"\n    model_args = dict(\n        embed_dim=128,\n        depths=(2, 2, 18, 2),\n        num_heads=(4, 8, 16, 32),\n        extra_norm_stage=True,\n    )\n    return _create_swin_transformer_v2_cr('swinv2_cr_base_ns_224', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_cr_large_384(pretrained=False, **kwargs) -> SwinTransformerV2Cr:\n    \"\"\"Swin-L V2 CR @ 384x384, trained ImageNet-1k\"\"\"\n    model_args = dict(\n        embed_dim=192,\n        depths=(2, 2, 18, 2),\n        num_heads=(6, 12, 24, 48),\n    )\n    return _create_swin_transformer_v2_cr('swinv2_cr_large_384', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_cr_large_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:\n    \"\"\"Swin-L V2 CR @ 224x224, trained ImageNet-1k\"\"\"\n    model_args = dict(\n        embed_dim=192,\n        depths=(2, 2, 18, 2),\n        num_heads=(6, 12, 24, 48),\n    )\n    return _create_swin_transformer_v2_cr('swinv2_cr_large_224', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_cr_huge_384(pretrained=False, **kwargs) -> SwinTransformerV2Cr:\n    \"\"\"Swin-H V2 CR @ 384x384, trained ImageNet-1k\"\"\"\n    model_args = dict(\n        embed_dim=352,\n        depths=(2, 2, 18, 2),\n        num_heads=(11, 22, 44, 88),  # head count not certain for Huge, 384 & 224 trying diff values\n        extra_norm_period=6,\n    )\n    return _create_swin_transformer_v2_cr('swinv2_cr_huge_384', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_cr_huge_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:\n    \"\"\"Swin-H V2 CR @ 224x224, trained ImageNet-1k\"\"\"\n    model_args = dict(\n        embed_dim=352,\n        depths=(2, 2, 18, 2),\n        num_heads=(8, 16, 32, 64),  # head count not certain for Huge, 384 & 224 trying diff values\n        extra_norm_period=6,\n    )\n    return _create_swin_transformer_v2_cr('swinv2_cr_huge_224', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_cr_giant_384(pretrained=False, **kwargs) -> SwinTransformerV2Cr:\n    \"\"\"Swin-G V2 CR @ 384x384, trained ImageNet-1k\"\"\"\n    model_args = dict(\n        embed_dim=512,\n        depths=(2, 2, 42, 2),\n        num_heads=(16, 32, 64, 128),\n        extra_norm_period=6,\n    )\n    return _create_swin_transformer_v2_cr('swinv2_cr_giant_384', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef swinv2_cr_giant_224(pretrained=False, **kwargs) -> SwinTransformerV2Cr:\n    \"\"\"Swin-G V2 CR @ 224x224, trained ImageNet-1k\"\"\"\n    model_args = dict(\n        embed_dim=512,\n        depths=(2, 2, 42, 2),\n        num_heads=(16, 32, 64, 128),\n        extra_norm_period=6,\n    )\n    return _create_swin_transformer_v2_cr('swinv2_cr_giant_224', pretrained=pretrained, **dict(model_args, **kwargs))\n",
  "\"\"\"\nAn implementation of GhostNet Model as defined in:\nGhostNet: More Features from Cheap Operations. https://arxiv.org/abs/1911.11907\nThe train script of the model is similar to that of MobileNetV3\nOriginal model: https://github.com/huawei-noah/CV-backbones/tree/master/ghostnet_pytorch\n\"\"\"\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import SelectAdaptivePool2d, Linear, make_divisible\nfrom ._builder import build_model_with_cfg\nfrom ._efficientnet_blocks import SqueezeExcite, ConvBnAct\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['GhostNet']\n\n\n_SE_LAYER = partial(SqueezeExcite, gate_layer='hard_sigmoid', rd_round_fn=partial(make_divisible, divisor=4))\n\n\nclass GhostModule(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            kernel_size=1,\n            ratio=2,\n            dw_size=3,\n            stride=1,\n            relu=True,\n    ):\n        super(GhostModule, self).__init__()\n        self.out_chs = out_chs\n        init_chs = math.ceil(out_chs / ratio)\n        new_chs = init_chs * (ratio - 1)\n\n        self.primary_conv = nn.Sequential(\n            nn.Conv2d(in_chs, init_chs, kernel_size, stride, kernel_size // 2, bias=False),\n            nn.BatchNorm2d(init_chs),\n            nn.ReLU(inplace=True) if relu else nn.Identity(),\n        )\n\n        self.cheap_operation = nn.Sequential(\n            nn.Conv2d(init_chs, new_chs, dw_size, 1, dw_size//2, groups=init_chs, bias=False),\n            nn.BatchNorm2d(new_chs),\n            nn.ReLU(inplace=True) if relu else nn.Identity(),\n        )\n\n    def forward(self, x):\n        x1 = self.primary_conv(x)\n        x2 = self.cheap_operation(x1)\n        out = torch.cat([x1, x2], dim=1)\n        return out[:, :self.out_chs, :, :]\n\n\nclass GhostBottleneck(nn.Module):\n    \"\"\" Ghost bottleneck w/ optional SE\"\"\"\n\n    def __init__(\n            self,\n            in_chs,\n            mid_chs,\n            out_chs,\n            dw_kernel_size=3,\n            stride=1,\n            act_layer=nn.ReLU,\n            se_ratio=0.,\n    ):\n        super(GhostBottleneck, self).__init__()\n        has_se = se_ratio is not None and se_ratio > 0.\n        self.stride = stride\n\n        # Point-wise expansion\n        self.ghost1 = GhostModule(in_chs, mid_chs, relu=True)\n\n        # Depth-wise convolution\n        if self.stride > 1:\n            self.conv_dw = nn.Conv2d(\n                mid_chs, mid_chs, dw_kernel_size, stride=stride,\n                padding=(dw_kernel_size-1)//2, groups=mid_chs, bias=False)\n            self.bn_dw = nn.BatchNorm2d(mid_chs)\n        else:\n            self.conv_dw = None\n            self.bn_dw = None\n\n        # Squeeze-and-excitation\n        self.se = _SE_LAYER(mid_chs, rd_ratio=se_ratio) if has_se else None\n\n        # Point-wise linear projection\n        self.ghost2 = GhostModule(mid_chs, out_chs, relu=False)\n        \n        # shortcut\n        if in_chs == out_chs and self.stride == 1:\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(\n                    in_chs, in_chs, dw_kernel_size, stride=stride,\n                    padding=(dw_kernel_size-1)//2, groups=in_chs, bias=False),\n                nn.BatchNorm2d(in_chs),\n                nn.Conv2d(in_chs, out_chs, 1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_chs),\n            )\n\n    def forward(self, x):\n        shortcut = x\n\n        # 1st ghost bottleneck\n        x = self.ghost1(x)\n\n        # Depth-wise convolution\n        if self.conv_dw is not None:\n            x = self.conv_dw(x)\n            x = self.bn_dw(x)\n\n        # Squeeze-and-excitation\n        if self.se is not None:\n            x = self.se(x)\n\n        # 2nd ghost bottleneck\n        x = self.ghost2(x)\n        \n        x += self.shortcut(shortcut)\n        return x\n\n\nclass GhostNet(nn.Module):\n    def __init__(\n            self,\n            cfgs,\n            num_classes=1000,\n            width=1.0,\n            in_chans=3,\n            output_stride=32,\n            global_pool='avg',\n            drop_rate=0.2,\n    ):\n        super(GhostNet, self).__init__()\n        # setting of inverted residual blocks\n        assert output_stride == 32, 'only output_stride==32 is valid, dilation not supported'\n        self.cfgs = cfgs\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        self.grad_checkpointing = False\n        self.feature_info = []\n\n        # building first layer\n        stem_chs = make_divisible(16 * width, 4)\n        self.conv_stem = nn.Conv2d(in_chans, stem_chs, 3, 2, 1, bias=False)\n        self.feature_info.append(dict(num_chs=stem_chs, reduction=2, module=f'conv_stem'))\n        self.bn1 = nn.BatchNorm2d(stem_chs)\n        self.act1 = nn.ReLU(inplace=True)\n        prev_chs = stem_chs\n\n        # building inverted residual blocks\n        stages = nn.ModuleList([])\n        block = GhostBottleneck\n        stage_idx = 0\n        net_stride = 2\n        for cfg in self.cfgs:\n            layers = []\n            s = 1\n            for k, exp_size, c, se_ratio, s in cfg:\n                out_chs = make_divisible(c * width, 4)\n                mid_chs = make_divisible(exp_size * width, 4)\n                layers.append(block(prev_chs, mid_chs, out_chs, k, s, se_ratio=se_ratio))\n                prev_chs = out_chs\n            if s > 1:\n                net_stride *= 2\n                self.feature_info.append(dict(\n                    num_chs=prev_chs, reduction=net_stride, module=f'blocks.{stage_idx}'))\n            stages.append(nn.Sequential(*layers))\n            stage_idx += 1\n\n        out_chs = make_divisible(exp_size * width, 4)\n        stages.append(nn.Sequential(ConvBnAct(prev_chs, out_chs, 1)))\n        self.pool_dim = prev_chs = out_chs\n        \n        self.blocks = nn.Sequential(*stages)        \n\n        # building last several layers\n        self.num_features = out_chs = 1280\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.conv_head = nn.Conv2d(prev_chs, out_chs, 1, 1, 0, bias=True)\n        self.act2 = nn.ReLU(inplace=True)\n        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()  # don't flatten if pooling disabled\n        self.classifier = Linear(out_chs, num_classes) if num_classes > 0 else nn.Identity()\n\n        # FIXME init\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^conv_stem|bn1',\n            blocks=[\n                (r'^blocks\\.(\\d+)' if coarse else r'^blocks\\.(\\d+)\\.(\\d+)', None),\n                (r'conv_head', (99999,))\n            ]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.classifier\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        # cannot meaningfully change pooling of efficient head after creation\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()  # don't flatten if pooling disabled\n        self.classifier = Linear(self.pool_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x, flatten=True)\n        else:\n            x = self.blocks(x)\n        return x\n\n    def forward_head(self, x):\n        x = self.global_pool(x)\n        x = self.conv_head(x)\n        x = self.act2(x)\n        x = self.flatten(x)\n        if self.drop_rate > 0.:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        x = self.classifier(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_ghostnet(variant, width=1.0, pretrained=False, **kwargs):\n    \"\"\"\n    Constructs a GhostNet model\n    \"\"\"\n    cfgs = [\n        # k, t, c, SE, s \n        # stage1\n        [[3,  16,  16, 0, 1]],\n        # stage2\n        [[3,  48,  24, 0, 2]],\n        [[3,  72,  24, 0, 1]],\n        # stage3\n        [[5,  72,  40, 0.25, 2]],\n        [[5, 120,  40, 0.25, 1]],\n        # stage4\n        [[3, 240,  80, 0, 2]],\n        [[3, 200,  80, 0, 1],\n         [3, 184,  80, 0, 1],\n         [3, 184,  80, 0, 1],\n         [3, 480, 112, 0.25, 1],\n         [3, 672, 112, 0.25, 1]\n        ],\n        # stage5\n        [[5, 672, 160, 0.25, 2]],\n        [[5, 960, 160, 0, 1],\n         [5, 960, 160, 0.25, 1],\n         [5, 960, 160, 0, 1],\n         [5, 960, 160, 0.25, 1]\n        ]\n    ]\n    model_kwargs = dict(\n        cfgs=cfgs,\n        width=width,\n        **kwargs,\n    )\n    return build_model_with_cfg(\n        GhostNet,\n        variant,\n        pretrained,\n        feature_cfg=dict(flatten_sequential=True),\n        **model_kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bilinear',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'conv_stem', 'classifier': 'classifier',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'ghostnet_050.untrained': _cfg(),\n    'ghostnet_100.in1k': _cfg(\n        url='https://github.com/huawei-noah/CV-backbones/releases/download/ghostnet_pth/ghostnet_1x.pth'),\n    'ghostnet_130.untrained': _cfg(),\n})\n\n\n@register_model\ndef ghostnet_050(pretrained=False, **kwargs) -> GhostNet:\n    \"\"\" GhostNet-0.5x \"\"\"\n    model = _create_ghostnet('ghostnet_050', width=0.5, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef ghostnet_100(pretrained=False, **kwargs) -> GhostNet:\n    \"\"\" GhostNet-1.0x \"\"\"\n    model = _create_ghostnet('ghostnet_100', width=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef ghostnet_130(pretrained=False, **kwargs) -> GhostNet:\n    \"\"\" GhostNet-1.3x \"\"\"\n    model = _create_ghostnet('ghostnet_130', width=1.3, pretrained=pretrained, **kwargs)\n    return model\n",
  "\"\"\" Vision OutLOoker (VOLO) implementation\n\nPaper: `VOLO: Vision Outlooker for Visual Recognition` - https://arxiv.org/abs/2106.13112\n\nCode adapted from official impl at https://github.com/sail-sg/volo, original copyright in comment below\n\nModifications and additions for timm by / Copyright 2022, Ross Wightman\n\"\"\"\n# Copyright 2021 Sea Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport math\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.checkpoint import checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import DropPath, Mlp, to_2tuple, to_ntuple, trunc_normal_\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['VOLO']  # model_registry will add each entrypoint fn to this\n\n\nclass OutlookAttention(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            kernel_size=3,\n            padding=1,\n            stride=1,\n            qkv_bias=False,\n            attn_drop=0.,\n            proj_drop=0.,\n    ):\n        super().__init__()\n        head_dim = dim // num_heads\n        self.num_heads = num_heads\n        self.kernel_size = kernel_size\n        self.padding = padding\n        self.stride = stride\n        self.scale = head_dim ** -0.5\n\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n        self.attn = nn.Linear(dim, kernel_size ** 4 * num_heads)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        self.unfold = nn.Unfold(kernel_size=kernel_size, padding=padding, stride=stride)\n        self.pool = nn.AvgPool2d(kernel_size=stride, stride=stride, ceil_mode=True)\n\n    def forward(self, x):\n        B, H, W, C = x.shape\n\n        v = self.v(x).permute(0, 3, 1, 2)  # B, C, H, W\n\n        h, w = math.ceil(H / self.stride), math.ceil(W / self.stride)\n        v = self.unfold(v).reshape(\n            B, self.num_heads, C // self.num_heads,\n            self.kernel_size * self.kernel_size, h * w).permute(0, 1, 4, 3, 2)  # B,H,N,kxk,C/H\n\n        attn = self.pool(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n        attn = self.attn(attn).reshape(\n            B, h * w, self.num_heads, self.kernel_size * self.kernel_size,\n            self.kernel_size * self.kernel_size).permute(0, 2, 1, 3, 4)  # B,H,N,kxk,kxk\n        attn = attn * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).permute(0, 1, 4, 3, 2).reshape(B, C * self.kernel_size * self.kernel_size, h * w)\n        x = F.fold(x, output_size=(H, W), kernel_size=self.kernel_size, padding=self.padding, stride=self.stride)\n\n        x = self.proj(x.permute(0, 2, 3, 1))\n        x = self.proj_drop(x)\n\n        return x\n\n\nclass Outlooker(nn.Module):\n    def __init__(\n            self,\n            dim,\n            kernel_size,\n            padding,\n            stride=1,\n            num_heads=1,\n            mlp_ratio=3.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            qkv_bias=False,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = OutlookAttention(\n            dim,\n            num_heads,\n            kernel_size=kernel_size,\n            padding=padding,\n            stride=stride,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n        )\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n        )\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass Attention(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads=8,\n            qkv_bias=False,\n            attn_drop=0.,\n            proj_drop=0.,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, H, W, C = x.shape\n\n        qkv = self.qkv(x).reshape(B, H * W, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, H, W, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n\n\nclass Transformer(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop)\n\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass ClassAttention(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads=8,\n            head_dim=None,\n            qkv_bias=False,\n            attn_drop=0.,\n            proj_drop=0.,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        if head_dim is not None:\n            self.head_dim = head_dim\n        else:\n            head_dim = dim // num_heads\n            self.head_dim = head_dim\n        self.scale = head_dim ** -0.5\n\n        self.kv = nn.Linear(dim, self.head_dim * self.num_heads * 2, bias=qkv_bias)\n        self.q = nn.Linear(dim, self.head_dim * self.num_heads, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(self.head_dim * self.num_heads, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n\n        kv = self.kv(x).reshape(B, N, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        k, v = kv.unbind(0)\n        q = self.q(x[:, :1, :]).reshape(B, self.num_heads, 1, self.head_dim)\n        attn = ((q * self.scale) @ k.transpose(-2, -1))\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        cls_embed = (attn @ v).transpose(1, 2).reshape(B, 1, self.head_dim * self.num_heads)\n        cls_embed = self.proj(cls_embed)\n        cls_embed = self.proj_drop(cls_embed)\n        return cls_embed\n\n\nclass ClassBlock(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            head_dim=None,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = ClassAttention(\n            dim,\n            num_heads=num_heads,\n            head_dim=head_dim,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n        # NOTE: drop path for stochastic depth\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n\n    def forward(self, x):\n        cls_embed = x[:, :1]\n        cls_embed = cls_embed + self.drop_path(self.attn(self.norm1(x)))\n        cls_embed = cls_embed + self.drop_path(self.mlp(self.norm2(cls_embed)))\n        return torch.cat([cls_embed, x[:, 1:]], dim=1)\n\n\ndef get_block(block_type, **kargs):\n    if block_type == 'ca':\n        return ClassBlock(**kargs)\n\n\ndef rand_bbox(size, lam, scale=1):\n    \"\"\"\n    get bounding box as token labeling (https://github.com/zihangJiang/TokenLabeling)\n    return: bounding box\n    \"\"\"\n    W = size[1] // scale\n    H = size[2] // scale\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = (W * cut_rat).astype(int)\n    cut_h = (H * cut_rat).astype(int)\n\n    # uniform\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding.\n    Different with ViT use 1 conv layer, we use 4 conv layers to do patch embedding\n    \"\"\"\n\n    def __init__(\n            self,\n            img_size=224,\n            stem_conv=False,\n            stem_stride=1,\n            patch_size=8,\n            in_chans=3,\n            hidden_dim=64,\n            embed_dim=384,\n    ):\n        super().__init__()\n        assert patch_size in [4, 8, 16]\n        if stem_conv:\n            self.conv = nn.Sequential(\n                nn.Conv2d(in_chans, hidden_dim, kernel_size=7, stride=stem_stride, padding=3, bias=False),  # 112x112\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1, bias=False),  # 112x112\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1, bias=False),  # 112x112\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            self.conv = None\n\n        self.proj = nn.Conv2d(\n            hidden_dim, embed_dim, kernel_size=patch_size // stem_stride, stride=patch_size // stem_stride)\n        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n\n    def forward(self, x):\n        if self.conv is not None:\n            x = self.conv(x)\n        x = self.proj(x)  # B, C, H, W\n        return x\n\n\nclass Downsample(nn.Module):\n    \"\"\" Image to Patch Embedding, downsampling between stage1 and stage2\n    \"\"\"\n\n    def __init__(self, in_embed_dim, out_embed_dim, patch_size=2):\n        super().__init__()\n        self.proj = nn.Conv2d(in_embed_dim, out_embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        x = x.permute(0, 3, 1, 2)\n        x = self.proj(x)  # B, C, H, W\n        x = x.permute(0, 2, 3, 1)\n        return x\n\n\ndef outlooker_blocks(\n        block_fn,\n        index,\n        dim,\n        layers,\n        num_heads=1,\n        kernel_size=3,\n        padding=1,\n        stride=2,\n        mlp_ratio=3.,\n        qkv_bias=False,\n        attn_drop=0,\n        drop_path_rate=0.,\n        **kwargs,\n):\n    \"\"\"\n    generate outlooker layer in stage1\n    return: outlooker layers\n    \"\"\"\n    blocks = []\n    for block_idx in range(layers[index]):\n        block_dpr = drop_path_rate * (block_idx + sum(layers[:index])) / (sum(layers) - 1)\n        blocks.append(block_fn(\n            dim,\n            kernel_size=kernel_size,\n            padding=padding,\n            stride=stride,\n            num_heads=num_heads,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            drop_path=block_dpr,\n        ))\n    blocks = nn.Sequential(*blocks)\n    return blocks\n\n\ndef transformer_blocks(\n        block_fn,\n        index,\n        dim,\n        layers,\n        num_heads,\n        mlp_ratio=3.,\n        qkv_bias=False,\n        attn_drop=0,\n        drop_path_rate=0.,\n        **kwargs,\n):\n    \"\"\"\n    generate transformer layers in stage2\n    return: transformer layers\n    \"\"\"\n    blocks = []\n    for block_idx in range(layers[index]):\n        block_dpr = drop_path_rate * (block_idx + sum(layers[:index])) / (sum(layers) - 1)\n        blocks.append(block_fn(\n            dim,\n            num_heads,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            drop_path=block_dpr,\n        ))\n    blocks = nn.Sequential(*blocks)\n    return blocks\n\n\nclass VOLO(nn.Module):\n    \"\"\"\n    Vision Outlooker, the main class of our model\n    \"\"\"\n\n    def __init__(\n            self,\n            layers,\n            img_size=224,\n            in_chans=3,\n            num_classes=1000,\n            global_pool='token',\n            patch_size=8,\n            stem_hidden_dim=64,\n            embed_dims=None,\n            num_heads=None,\n            downsamples=(True, False, False, False),\n            outlook_attention=(True, False, False, False),\n            mlp_ratio=3.0,\n            qkv_bias=False,\n            drop_rate=0.,\n            pos_drop_rate=0.,\n            attn_drop_rate=0.,\n            drop_path_rate=0.,\n            norm_layer=nn.LayerNorm,\n            post_layers=('ca', 'ca'),\n            use_aux_head=True,\n            use_mix_token=False,\n            pooling_scale=2,\n    ):\n        super().__init__()\n        num_layers = len(layers)\n        mlp_ratio = to_ntuple(num_layers)(mlp_ratio)\n        img_size = to_2tuple(img_size)\n\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.mix_token = use_mix_token\n        self.pooling_scale = pooling_scale\n        self.num_features = embed_dims[-1]\n        if use_mix_token:  # enable token mixing, see token labeling for details.\n            self.beta = 1.0\n            assert global_pool == 'token', \"return all tokens if mix_token is enabled\"\n        self.grad_checkpointing = False\n\n        self.patch_embed = PatchEmbed(\n            stem_conv=True,\n            stem_stride=2,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            hidden_dim=stem_hidden_dim,\n            embed_dim=embed_dims[0],\n        )\n\n        # inital positional encoding, we add positional encoding after outlooker blocks\n        patch_grid = (img_size[0] // patch_size // pooling_scale, img_size[1] // patch_size // pooling_scale)\n        self.pos_embed = nn.Parameter(torch.zeros(1, patch_grid[0], patch_grid[1], embed_dims[-1]))\n        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n\n        # set the main block in network\n        network = []\n        for i in range(len(layers)):\n            if outlook_attention[i]:\n                # stage 1\n                stage = outlooker_blocks(\n                    Outlooker,\n                    i,\n                    embed_dims[i],\n                    layers,\n                    num_heads[i],\n                    mlp_ratio=mlp_ratio[i],\n                    qkv_bias=qkv_bias,\n                    attn_drop=attn_drop_rate,\n                    norm_layer=norm_layer,\n                )\n                network.append(stage)\n            else:\n                # stage 2\n                stage = transformer_blocks(\n                    Transformer,\n                    i,\n                    embed_dims[i],\n                    layers,\n                    num_heads[i],\n                    mlp_ratio=mlp_ratio[i],\n                    qkv_bias=qkv_bias,\n                    drop_path_rate=drop_path_rate,\n                    attn_drop=attn_drop_rate,\n                    norm_layer=norm_layer,\n                )\n                network.append(stage)\n\n            if downsamples[i]:\n                # downsampling between two stages\n                network.append(Downsample(embed_dims[i], embed_dims[i + 1], 2))\n\n        self.network = nn.ModuleList(network)\n\n        # set post block, for example, class attention layers\n        self.post_network = None\n        if post_layers is not None:\n            self.post_network = nn.ModuleList([\n                get_block(\n                    post_layers[i],\n                    dim=embed_dims[-1],\n                    num_heads=num_heads[-1],\n                    mlp_ratio=mlp_ratio[-1],\n                    qkv_bias=qkv_bias,\n                    attn_drop=attn_drop_rate,\n                    drop_path=0.,\n                    norm_layer=norm_layer)\n                for i in range(len(post_layers))\n            ])\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dims[-1]))\n            trunc_normal_(self.cls_token, std=.02)\n\n        # set output type\n        if use_aux_head:\n            self.aux_head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n        else:\n            self.aux_head = None\n        self.norm = norm_layer(self.num_features)\n\n        # Classifier head\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n        trunc_normal_(self.pos_embed, std=.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^cls_token|pos_embed|patch_embed',  # stem and embed\n            blocks=[\n                (r'^network\\.(\\d+)\\.(\\d+)', None),\n                (r'^network\\.(\\d+)', (0,)),\n            ],\n            blocks2=[\n                (r'^cls_token', (0,)),\n                (r'^post_network\\.(\\d+)', None),\n                (r'^norm', (99999,))\n            ],\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n        if self.aux_head is not None:\n            self.aux_head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_tokens(self, x):\n        for idx, block in enumerate(self.network):\n            if idx == 2:\n                # add positional encoding after outlooker blocks\n                x = x + self.pos_embed\n                x = self.pos_drop(x)\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint(block, x)\n            else:\n                x = block(x)\n\n        B, H, W, C = x.shape\n        x = x.reshape(B, -1, C)\n        return x\n\n    def forward_cls(self, x):\n        B, N, C = x.shape\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat([cls_tokens, x], dim=1)\n        for block in self.post_network:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint(block, x)\n            else:\n                x = block(x)\n        return x\n\n    def forward_train(self, x):\n        \"\"\" A separate forward fn for training with mix_token (if a train script supports).\n        Combining multiple modes in as single forward with different return types is torchscript hell.\n        \"\"\"\n        x = self.patch_embed(x)\n        x = x.permute(0, 2, 3, 1)  # B,C,H,W-> B,H,W,C\n\n        # mix token, see token labeling for details.\n        if self.mix_token and self.training:\n            lam = np.random.beta(self.beta, self.beta)\n            patch_h, patch_w = x.shape[1] // self.pooling_scale, x.shape[2] // self.pooling_scale\n            bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam, scale=self.pooling_scale)\n            temp_x = x.clone()\n            sbbx1, sbby1 = self.pooling_scale * bbx1, self.pooling_scale * bby1\n            sbbx2, sbby2 = self.pooling_scale * bbx2, self.pooling_scale * bby2\n            temp_x[:, sbbx1:sbbx2, sbby1:sbby2, :] = x.flip(0)[:, sbbx1:sbbx2, sbby1:sbby2, :]\n            x = temp_x\n        else:\n            bbx1, bby1, bbx2, bby2 = 0, 0, 0, 0\n\n        # step2: tokens learning in the two stages\n        x = self.forward_tokens(x)\n\n        # step3: post network, apply class attention or not\n        if self.post_network is not None:\n            x = self.forward_cls(x)\n        x = self.norm(x)\n\n        if self.global_pool == 'avg':\n            x_cls = x.mean(dim=1)\n        elif self.global_pool == 'token':\n            x_cls = x[:, 0]\n        else:\n            x_cls = x\n\n        if self.aux_head is None:\n            return x_cls\n\n        x_aux = self.aux_head(x[:, 1:])  # generate classes in all feature tokens, see token labeling\n        if not self.training:\n            return x_cls + 0.5 * x_aux.max(1)[0]\n\n        if self.mix_token and self.training:  # reverse \"mix token\", see token labeling for details.\n            x_aux = x_aux.reshape(x_aux.shape[0], patch_h, patch_w, x_aux.shape[-1])\n            temp_x = x_aux.clone()\n            temp_x[:, bbx1:bbx2, bby1:bby2, :] = x_aux.flip(0)[:, bbx1:bbx2, bby1:bby2, :]\n            x_aux = temp_x\n            x_aux = x_aux.reshape(x_aux.shape[0], patch_h * patch_w, x_aux.shape[-1])\n\n        # return these: 1. class token, 2. classes from all feature tokens, 3. bounding box\n        return x_cls, x_aux, (bbx1, bby1, bbx2, bby2)\n\n    def forward_features(self, x):\n        x = self.patch_embed(x).permute(0, 2, 3, 1)  # B,C,H,W-> B,H,W,C\n\n        # step2: tokens learning in the two stages\n        x = self.forward_tokens(x)\n\n        # step3: post network, apply class attention or not\n        if self.post_network is not None:\n            x = self.forward_cls(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool == 'avg':\n            out = x.mean(dim=1)\n        elif self.global_pool == 'token':\n            out = x[:, 0]\n        else:\n            out = x\n        x = self.head_drop(x)\n        if pre_logits:\n            return out\n        out = self.head(out)\n        if self.aux_head is not None:\n            # generate classes in all feature tokens, see token labeling\n            aux = self.aux_head(x[:, 1:])\n            out = out + 0.5 * aux.max(1)[0]\n        return out\n\n    def forward(self, x):\n        \"\"\" simplified forward (without mix token training) \"\"\"\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_volo(variant, pretrained=False, **kwargs):\n    if kwargs.get('features_only', None):\n        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n    return build_model_with_cfg(\n        VOLO,\n        variant,\n        pretrained,\n        **kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .96, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'patch_embed.conv.0', 'classifier': ('head', 'aux_head'),\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'volo_d1_224.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/sail-sg/volo/releases/download/volo_1/d1_224_84.2.pth.tar',\n        crop_pct=0.96),\n    'volo_d1_384.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/sail-sg/volo/releases/download/volo_1/d1_384_85.2.pth.tar',\n        crop_pct=1.0, input_size=(3, 384, 384)),\n    'volo_d2_224.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/sail-sg/volo/releases/download/volo_1/d2_224_85.2.pth.tar',\n        crop_pct=0.96),\n    'volo_d2_384.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/sail-sg/volo/releases/download/volo_1/d2_384_86.0.pth.tar',\n        crop_pct=1.0, input_size=(3, 384, 384)),\n    'volo_d3_224.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/sail-sg/volo/releases/download/volo_1/d3_224_85.4.pth.tar',\n        crop_pct=0.96),\n    'volo_d3_448.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/sail-sg/volo/releases/download/volo_1/d3_448_86.3.pth.tar',\n        crop_pct=1.0, input_size=(3, 448, 448)),\n    'volo_d4_224.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/sail-sg/volo/releases/download/volo_1/d4_224_85.7.pth.tar',\n        crop_pct=0.96),\n    'volo_d4_448.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/sail-sg/volo/releases/download/volo_1/d4_448_86.79.pth.tar',\n        crop_pct=1.15, input_size=(3, 448, 448)),\n    'volo_d5_224.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/sail-sg/volo/releases/download/volo_1/d5_224_86.10.pth.tar',\n        crop_pct=0.96),\n    'volo_d5_448.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/sail-sg/volo/releases/download/volo_1/d5_448_87.0.pth.tar',\n        crop_pct=1.15, input_size=(3, 448, 448)),\n    'volo_d5_512.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/sail-sg/volo/releases/download/volo_1/d5_512_87.07.pth.tar',\n        crop_pct=1.15, input_size=(3, 512, 512)),\n})\n\n\n@register_model\ndef volo_d1_224(pretrained=False, **kwargs) -> VOLO:\n    \"\"\" VOLO-D1 model, Params: 27M \"\"\"\n    model_args = dict(layers=(4, 4, 8, 2), embed_dims=(192, 384, 384, 384), num_heads=(6, 12, 12, 12), **kwargs)\n    model = _create_volo('volo_d1_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef volo_d1_384(pretrained=False, **kwargs) -> VOLO:\n    \"\"\" VOLO-D1 model, Params: 27M \"\"\"\n    model_args = dict(layers=(4, 4, 8, 2), embed_dims=(192, 384, 384, 384), num_heads=(6, 12, 12, 12), **kwargs)\n    model = _create_volo('volo_d1_384', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef volo_d2_224(pretrained=False, **kwargs) -> VOLO:\n    \"\"\" VOLO-D2 model, Params: 59M \"\"\"\n    model_args = dict(layers=(6, 4, 10, 4), embed_dims=(256, 512, 512, 512), num_heads=(8, 16, 16, 16), **kwargs)\n    model = _create_volo('volo_d2_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef volo_d2_384(pretrained=False, **kwargs) -> VOLO:\n    \"\"\" VOLO-D2 model, Params: 59M \"\"\"\n    model_args = dict(layers=(6, 4, 10, 4), embed_dims=(256, 512, 512, 512), num_heads=(8, 16, 16, 16), **kwargs)\n    model = _create_volo('volo_d2_384', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef volo_d3_224(pretrained=False, **kwargs) -> VOLO:\n    \"\"\" VOLO-D3 model, Params: 86M \"\"\"\n    model_args = dict(layers=(8, 8, 16, 4), embed_dims=(256, 512, 512, 512), num_heads=(8, 16, 16, 16), **kwargs)\n    model = _create_volo('volo_d3_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef volo_d3_448(pretrained=False, **kwargs) -> VOLO:\n    \"\"\" VOLO-D3 model, Params: 86M \"\"\"\n    model_args = dict(layers=(8, 8, 16, 4), embed_dims=(256, 512, 512, 512), num_heads=(8, 16, 16, 16), **kwargs)\n    model = _create_volo('volo_d3_448', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef volo_d4_224(pretrained=False, **kwargs) -> VOLO:\n    \"\"\" VOLO-D4 model, Params: 193M \"\"\"\n    model_args = dict(layers=(8, 8, 16, 4), embed_dims=(384, 768, 768, 768), num_heads=(12, 16, 16, 16), **kwargs)\n    model = _create_volo('volo_d4_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef volo_d4_448(pretrained=False, **kwargs) -> VOLO:\n    \"\"\" VOLO-D4 model, Params: 193M \"\"\"\n    model_args = dict(layers=(8, 8, 16, 4), embed_dims=(384, 768, 768, 768), num_heads=(12, 16, 16, 16), **kwargs)\n    model = _create_volo('volo_d4_448', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef volo_d5_224(pretrained=False, **kwargs) -> VOLO:\n    \"\"\" VOLO-D5 model, Params: 296M\n    stem_hidden_dim=128, the dim in patch embedding is 128 for VOLO-D5\n    \"\"\"\n    model_args = dict(\n        layers=(12, 12, 20, 4), embed_dims=(384, 768, 768, 768), num_heads=(12, 16, 16, 16),\n        mlp_ratio=4, stem_hidden_dim=128, **kwargs)\n    model = _create_volo('volo_d5_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef volo_d5_448(pretrained=False, **kwargs) -> VOLO:\n    \"\"\" VOLO-D5 model, Params: 296M\n    stem_hidden_dim=128, the dim in patch embedding is 128 for VOLO-D5\n    \"\"\"\n    model_args = dict(\n        layers=(12, 12, 20, 4), embed_dims=(384, 768, 768, 768), num_heads=(12, 16, 16, 16),\n        mlp_ratio=4, stem_hidden_dim=128, **kwargs)\n    model = _create_volo('volo_d5_448', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef volo_d5_512(pretrained=False, **kwargs) -> VOLO:\n    \"\"\" VOLO-D5 model, Params: 296M\n    stem_hidden_dim=128, the dim in patch embedding is 128 for VOLO-D5\n    \"\"\"\n    model_args = dict(\n        layers=(12, 12, 20, 4), embed_dims=(384, 768, 768, 768), num_heads=(12, 16, 16, 16),\n        mlp_ratio=4, stem_hidden_dim=128, **kwargs)\n    model = _create_volo('volo_d5_512', pretrained=pretrained, **model_args)\n    return model\n",
  "\"\"\" EfficientNet, MobileNetV3, etc Blocks\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nfrom timm.layers import create_conv2d, DropPath, make_divisible, create_act_layer, get_norm_act_layer\n\n__all__ = [\n    'SqueezeExcite', 'ConvBnAct', 'DepthwiseSeparableConv', 'InvertedResidual', 'CondConvResidual', 'EdgeResidual']\n\n\ndef num_groups(group_size, channels):\n    if not group_size:  # 0 or None\n        return 1  # normal conv with 1 group\n    else:\n        # NOTE group_size == 1 -> depthwise conv\n        assert channels % group_size == 0\n        return channels // group_size\n\n\nclass SqueezeExcite(nn.Module):\n    \"\"\" Squeeze-and-Excitation w/ specific features for EfficientNet/MobileNet family\n\n    Args:\n        in_chs (int): input channels to layer\n        rd_ratio (float): ratio of squeeze reduction\n        act_layer (nn.Module): activation layer of containing block\n        gate_layer (Callable): attention gate function\n        force_act_layer (nn.Module): override block's activation fn if this is set/bound\n        rd_round_fn (Callable): specify a fn to calculate rounding of reduced chs\n    \"\"\"\n\n    def __init__(\n            self, in_chs, rd_ratio=0.25, rd_channels=None, act_layer=nn.ReLU,\n            gate_layer=nn.Sigmoid, force_act_layer=None, rd_round_fn=None):\n        super(SqueezeExcite, self).__init__()\n        if rd_channels is None:\n            rd_round_fn = rd_round_fn or round\n            rd_channels = rd_round_fn(in_chs * rd_ratio)\n        act_layer = force_act_layer or act_layer\n        self.conv_reduce = nn.Conv2d(in_chs, rd_channels, 1, bias=True)\n        self.act1 = create_act_layer(act_layer, inplace=True)\n        self.conv_expand = nn.Conv2d(rd_channels, in_chs, 1, bias=True)\n        self.gate = create_act_layer(gate_layer)\n\n    def forward(self, x):\n        x_se = x.mean((2, 3), keepdim=True)\n        x_se = self.conv_reduce(x_se)\n        x_se = self.act1(x_se)\n        x_se = self.conv_expand(x_se)\n        return x * self.gate(x_se)\n\n\nclass ConvBnAct(nn.Module):\n    \"\"\" Conv + Norm Layer + Activation w/ optional skip connection\n    \"\"\"\n    def __init__(\n            self, in_chs, out_chs, kernel_size, stride=1, dilation=1, group_size=0, pad_type='',\n            skip=False, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d, drop_path_rate=0.):\n        super(ConvBnAct, self).__init__()\n        norm_act_layer = get_norm_act_layer(norm_layer, act_layer)\n        groups = num_groups(group_size, in_chs)\n        self.has_skip = skip and stride == 1 and in_chs == out_chs\n\n        self.conv = create_conv2d(\n            in_chs, out_chs, kernel_size, stride=stride, dilation=dilation, groups=groups, padding=pad_type)\n        self.bn1 = norm_act_layer(out_chs, inplace=True)\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate else nn.Identity()\n\n    def feature_info(self, location):\n        if location == 'expansion':  # output of conv after act, same as block coutput\n            return dict(module='bn1', hook_type='forward', num_chs=self.conv.out_channels)\n        else:  # location == 'bottleneck', block output\n            return dict(module='', num_chs=self.conv.out_channels)\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv(x)\n        x = self.bn1(x)\n        if self.has_skip:\n            x = self.drop_path(x) + shortcut\n        return x\n\n\nclass DepthwiseSeparableConv(nn.Module):\n    \"\"\" DepthwiseSeparable block\n    Used for DS convs in MobileNet-V1 and in the place of IR blocks that have no expansion\n    (factor of 1.0). This is an alternative to having a IR with an optional first pw conv.\n    \"\"\"\n    def __init__(\n            self, in_chs, out_chs, dw_kernel_size=3, stride=1, dilation=1, group_size=1, pad_type='',\n            noskip=False, pw_kernel_size=1, pw_act=False, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d,\n            se_layer=None, drop_path_rate=0.):\n        super(DepthwiseSeparableConv, self).__init__()\n        norm_act_layer = get_norm_act_layer(norm_layer, act_layer)\n        groups = num_groups(group_size, in_chs)\n        self.has_skip = (stride == 1 and in_chs == out_chs) and not noskip\n        self.has_pw_act = pw_act  # activation after point-wise conv\n\n        self.conv_dw = create_conv2d(\n            in_chs, in_chs, dw_kernel_size, stride=stride, dilation=dilation, padding=pad_type, groups=groups)\n        self.bn1 = norm_act_layer(in_chs, inplace=True)\n\n        # Squeeze-and-excitation\n        self.se = se_layer(in_chs, act_layer=act_layer) if se_layer else nn.Identity()\n\n        self.conv_pw = create_conv2d(in_chs, out_chs, pw_kernel_size, padding=pad_type)\n        self.bn2 = norm_act_layer(out_chs, inplace=True, apply_act=self.has_pw_act)\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate else nn.Identity()\n\n    def feature_info(self, location):\n        if location == 'expansion':  # after SE, input to PW\n            return dict(module='conv_pw', hook_type='forward_pre', num_chs=self.conv_pw.in_channels)\n        else:  # location == 'bottleneck', block output\n            return dict(module='', num_chs=self.conv_pw.out_channels)\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv_dw(x)\n        x = self.bn1(x)\n        x = self.se(x)\n        x = self.conv_pw(x)\n        x = self.bn2(x)\n        if self.has_skip:\n            x = self.drop_path(x) + shortcut\n        return x\n\n\nclass InvertedResidual(nn.Module):\n    \"\"\" Inverted residual block w/ optional SE\n\n    Originally used in MobileNet-V2 - https://arxiv.org/abs/1801.04381v4, this layer is often\n    referred to as 'MBConv' for (Mobile inverted bottleneck conv) and is also used in\n      * MNasNet - https://arxiv.org/abs/1807.11626\n      * EfficientNet - https://arxiv.org/abs/1905.11946\n      * MobileNet-V3 - https://arxiv.org/abs/1905.02244\n    \"\"\"\n\n    def __init__(\n            self, in_chs, out_chs, dw_kernel_size=3, stride=1, dilation=1, group_size=1, pad_type='',\n            noskip=False, exp_ratio=1.0, exp_kernel_size=1, pw_kernel_size=1, act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d, se_layer=None, conv_kwargs=None, drop_path_rate=0.):\n        super(InvertedResidual, self).__init__()\n        norm_act_layer = get_norm_act_layer(norm_layer, act_layer)\n        conv_kwargs = conv_kwargs or {}\n        mid_chs = make_divisible(in_chs * exp_ratio)\n        groups = num_groups(group_size, mid_chs)\n        self.has_skip = (in_chs == out_chs and stride == 1) and not noskip\n\n        # Point-wise expansion\n        self.conv_pw = create_conv2d(in_chs, mid_chs, exp_kernel_size, padding=pad_type, **conv_kwargs)\n        self.bn1 = norm_act_layer(mid_chs, inplace=True)\n\n        # Depth-wise convolution\n        self.conv_dw = create_conv2d(\n            mid_chs, mid_chs, dw_kernel_size, stride=stride, dilation=dilation,\n            groups=groups, padding=pad_type, **conv_kwargs)\n        self.bn2 = norm_act_layer(mid_chs, inplace=True)\n\n        # Squeeze-and-excitation\n        self.se = se_layer(mid_chs, act_layer=act_layer) if se_layer else nn.Identity()\n\n        # Point-wise linear projection\n        self.conv_pwl = create_conv2d(mid_chs, out_chs, pw_kernel_size, padding=pad_type, **conv_kwargs)\n        self.bn3 = norm_act_layer(out_chs, apply_act=False)\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate else nn.Identity()\n\n    def feature_info(self, location):\n        if location == 'expansion':  # after SE, input to PWL\n            return dict(module='conv_pwl', hook_type='forward_pre', num_chs=self.conv_pwl.in_channels)\n        else:  # location == 'bottleneck', block output\n            return dict(module='', num_chs=self.conv_pwl.out_channels)\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv_pw(x)\n        x = self.bn1(x)\n        x = self.conv_dw(x)\n        x = self.bn2(x)\n        x = self.se(x)\n        x = self.conv_pwl(x)\n        x = self.bn3(x)\n        if self.has_skip:\n            x = self.drop_path(x) + shortcut\n        return x\n\n\nclass CondConvResidual(InvertedResidual):\n    \"\"\" Inverted residual block w/ CondConv routing\"\"\"\n\n    def __init__(\n            self, in_chs, out_chs, dw_kernel_size=3, stride=1, dilation=1, group_size=1, pad_type='',\n            noskip=False, exp_ratio=1.0, exp_kernel_size=1, pw_kernel_size=1, act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d, se_layer=None, num_experts=0, drop_path_rate=0.):\n\n        self.num_experts = num_experts\n        conv_kwargs = dict(num_experts=self.num_experts)\n\n        super(CondConvResidual, self).__init__(\n            in_chs, out_chs, dw_kernel_size=dw_kernel_size, stride=stride, dilation=dilation, group_size=group_size,\n            pad_type=pad_type, act_layer=act_layer, noskip=noskip, exp_ratio=exp_ratio, exp_kernel_size=exp_kernel_size,\n            pw_kernel_size=pw_kernel_size, se_layer=se_layer, norm_layer=norm_layer, conv_kwargs=conv_kwargs,\n            drop_path_rate=drop_path_rate)\n\n        self.routing_fn = nn.Linear(in_chs, self.num_experts)\n\n    def forward(self, x):\n        shortcut = x\n        pooled_inputs = F.adaptive_avg_pool2d(x, 1).flatten(1)  # CondConv routing\n        routing_weights = torch.sigmoid(self.routing_fn(pooled_inputs))\n        x = self.conv_pw(x, routing_weights)\n        x = self.bn1(x)\n        x = self.conv_dw(x, routing_weights)\n        x = self.bn2(x)\n        x = self.se(x)\n        x = self.conv_pwl(x, routing_weights)\n        x = self.bn3(x)\n        if self.has_skip:\n            x = self.drop_path(x) + shortcut\n        return x\n\n\nclass EdgeResidual(nn.Module):\n    \"\"\" Residual block with expansion convolution followed by pointwise-linear w/ stride\n\n    Originally introduced in `EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML`\n        - https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html\n\n    This layer is also called FusedMBConv in the MobileDet, EfficientNet-X, and EfficientNet-V2 papers\n      * MobileDet - https://arxiv.org/abs/2004.14525\n      * EfficientNet-X - https://arxiv.org/abs/2102.05610\n      * EfficientNet-V2 - https://arxiv.org/abs/2104.00298\n    \"\"\"\n\n    def __init__(\n            self, in_chs, out_chs, exp_kernel_size=3, stride=1, dilation=1, group_size=0, pad_type='',\n            force_in_chs=0, noskip=False, exp_ratio=1.0, pw_kernel_size=1, act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d, se_layer=None, drop_path_rate=0.):\n        super(EdgeResidual, self).__init__()\n        norm_act_layer = get_norm_act_layer(norm_layer, act_layer)\n        if force_in_chs > 0:\n            mid_chs = make_divisible(force_in_chs * exp_ratio)\n        else:\n            mid_chs = make_divisible(in_chs * exp_ratio)\n        groups = num_groups(group_size, in_chs)\n        self.has_skip = (in_chs == out_chs and stride == 1) and not noskip\n\n        # Expansion convolution\n        self.conv_exp = create_conv2d(\n            in_chs, mid_chs, exp_kernel_size, stride=stride, dilation=dilation, groups=groups, padding=pad_type)\n        self.bn1 = norm_act_layer(mid_chs, inplace=True)\n\n        # Squeeze-and-excitation\n        self.se = se_layer(mid_chs, act_layer=act_layer) if se_layer else nn.Identity()\n\n        # Point-wise linear projection\n        self.conv_pwl = create_conv2d(mid_chs, out_chs, pw_kernel_size, padding=pad_type)\n        self.bn2 = norm_act_layer(out_chs, apply_act=False)\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate else nn.Identity()\n\n    def feature_info(self, location):\n        if location == 'expansion':  # after SE, before PWL\n            return dict(module='conv_pwl', hook_type='forward_pre', num_chs=self.conv_pwl.in_channels)\n        else:  # location == 'bottleneck', block output\n            return dict(module='', num_chs=self.conv_pwl.out_channels)\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv_exp(x)\n        x = self.bn1(x)\n        x = self.se(x)\n        x = self.conv_pwl(x)\n        x = self.bn2(x)\n        if self.has_skip:\n            x = self.drop_path(x) + shortcut\n        return x\n",
  "\"\"\" NasNet-A (Large)\n nasnetalarge implementation grabbed from Cadene's pretrained models\n https://github.com/Cadene/pretrained-models.pytorch\n\"\"\"\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.layers import ConvNormAct, create_conv2d, create_pool2d, create_classifier\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['NASNetALarge']\n\n\n\nclass ActConvBn(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=''):\n        super(ActConvBn, self).__init__()\n        self.act = nn.ReLU()\n        self.conv = create_conv2d(\n            in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1)\n\n    def forward(self, x):\n        x = self.act(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass SeparableConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=''):\n        super(SeparableConv2d, self).__init__()\n        self.depthwise_conv2d = create_conv2d(\n            in_channels, in_channels, kernel_size=kernel_size,\n            stride=stride, padding=padding, groups=in_channels)\n        self.pointwise_conv2d = create_conv2d(\n            in_channels, out_channels, kernel_size=1, padding=0)\n\n    def forward(self, x):\n        x = self.depthwise_conv2d(x)\n        x = self.pointwise_conv2d(x)\n        return x\n\n\nclass BranchSeparables(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, pad_type='', stem_cell=False):\n        super(BranchSeparables, self).__init__()\n        middle_channels = out_channels if stem_cell else in_channels\n        self.act_1 = nn.ReLU()\n        self.separable_1 = SeparableConv2d(\n            in_channels, middle_channels, kernel_size, stride=stride, padding=pad_type)\n        self.bn_sep_1 = nn.BatchNorm2d(middle_channels, eps=0.001, momentum=0.1)\n        self.act_2 = nn.ReLU(inplace=True)\n        self.separable_2 = SeparableConv2d(\n            middle_channels, out_channels, kernel_size, stride=1, padding=pad_type)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1)\n\n    def forward(self, x):\n        x = self.act_1(x)\n        x = self.separable_1(x)\n        x = self.bn_sep_1(x)\n        x = self.act_2(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass CellStem0(nn.Module):\n    def __init__(self, stem_size, num_channels=42, pad_type=''):\n        super(CellStem0, self).__init__()\n        self.num_channels = num_channels\n        self.stem_size = stem_size\n        self.conv_1x1 = ActConvBn(self.stem_size, self.num_channels, 1, stride=1)\n\n        self.comb_iter_0_left = BranchSeparables(self.num_channels, self.num_channels, 5, 2, pad_type)\n        self.comb_iter_0_right = BranchSeparables(self.stem_size, self.num_channels, 7, 2, pad_type, stem_cell=True)\n\n        self.comb_iter_1_left = create_pool2d('max', 3, 2, padding=pad_type)\n        self.comb_iter_1_right = BranchSeparables(self.stem_size, self.num_channels, 7, 2, pad_type, stem_cell=True)\n\n        self.comb_iter_2_left = create_pool2d('avg', 3, 2, count_include_pad=False, padding=pad_type)\n        self.comb_iter_2_right = BranchSeparables(self.stem_size, self.num_channels, 5, 2, pad_type, stem_cell=True)\n\n        self.comb_iter_3_right = create_pool2d('avg', 3, 1, count_include_pad=False, padding=pad_type)\n\n        self.comb_iter_4_left = BranchSeparables(self.num_channels, self.num_channels, 3, 1, pad_type)\n        self.comb_iter_4_right = create_pool2d('max', 3, 2, padding=pad_type)\n\n    def forward(self, x):\n        x1 = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x1)\n        x_comb_iter_0_right = self.comb_iter_0_right(x)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x1)\n        x_comb_iter_1_right = self.comb_iter_1_right(x)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x1)\n        x_comb_iter_2_right = self.comb_iter_2_right(x)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x1)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass CellStem1(nn.Module):\n\n    def __init__(self, stem_size, num_channels, pad_type=''):\n        super(CellStem1, self).__init__()\n        self.num_channels = num_channels\n        self.stem_size = stem_size\n        self.conv_1x1 = ActConvBn(2 * self.num_channels, self.num_channels, 1, stride=1)\n\n        self.act = nn.ReLU()\n        self.path_1 = nn.Sequential()\n        self.path_1.add_module('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_1.add_module('conv', nn.Conv2d(self.stem_size, self.num_channels // 2, 1, stride=1, bias=False))\n       \n        self.path_2 = nn.Sequential()\n        self.path_2.add_module('pad', nn.ZeroPad2d((-1, 1, -1, 1)))\n        self.path_2.add_module('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_2.add_module('conv', nn.Conv2d(self.stem_size, self.num_channels // 2, 1, stride=1, bias=False))\n\n        self.final_path_bn = nn.BatchNorm2d(self.num_channels, eps=0.001, momentum=0.1)\n\n        self.comb_iter_0_left = BranchSeparables(self.num_channels, self.num_channels, 5, 2, pad_type)\n        self.comb_iter_0_right = BranchSeparables(self.num_channels, self.num_channels, 7, 2, pad_type)\n\n        self.comb_iter_1_left = create_pool2d('max', 3, 2, padding=pad_type)\n        self.comb_iter_1_right = BranchSeparables(self.num_channels, self.num_channels, 7, 2, pad_type)\n\n        self.comb_iter_2_left = create_pool2d('avg', 3, 2, count_include_pad=False, padding=pad_type)\n        self.comb_iter_2_right = BranchSeparables(self.num_channels, self.num_channels, 5, 2, pad_type)\n\n        self.comb_iter_3_right = create_pool2d('avg', 3, 1, count_include_pad=False, padding=pad_type)\n\n        self.comb_iter_4_left = BranchSeparables(self.num_channels, self.num_channels, 3, 1, pad_type)\n        self.comb_iter_4_right = create_pool2d('max', 3, 2, padding=pad_type)\n\n    def forward(self, x_conv0, x_stem_0):\n        x_left = self.conv_1x1(x_stem_0)\n\n        x_relu = self.act(x_conv0)\n        # path 1\n        x_path1 = self.path_1(x_relu)\n        # path 2\n        x_path2 = self.path_2(x_relu)\n        # final path\n        x_right = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_left)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_right)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_right)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_left)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_left)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass FirstCell(nn.Module):\n\n    def __init__(self, in_chs_left, out_chs_left, in_chs_right, out_chs_right, pad_type=''):\n        super(FirstCell, self).__init__()\n        self.conv_1x1 = ActConvBn(in_chs_right, out_chs_right, 1, stride=1)\n\n        self.act = nn.ReLU()\n        self.path_1 = nn.Sequential()\n        self.path_1.add_module('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_1.add_module('conv', nn.Conv2d(in_chs_left, out_chs_left, 1, stride=1, bias=False))\n\n        self.path_2 = nn.Sequential()\n        self.path_2.add_module('pad', nn.ZeroPad2d((-1, 1, -1, 1)))\n        self.path_2.add_module('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_2.add_module('conv', nn.Conv2d(in_chs_left, out_chs_left, 1, stride=1, bias=False))\n\n        self.final_path_bn = nn.BatchNorm2d(out_chs_left * 2, eps=0.001, momentum=0.1)\n\n        self.comb_iter_0_left = BranchSeparables(out_chs_right, out_chs_right, 5, 1, pad_type)\n        self.comb_iter_0_right = BranchSeparables(out_chs_right, out_chs_right, 3, 1, pad_type)\n\n        self.comb_iter_1_left = BranchSeparables(out_chs_right, out_chs_right, 5, 1, pad_type)\n        self.comb_iter_1_right = BranchSeparables(out_chs_right, out_chs_right, 3, 1, pad_type)\n\n        self.comb_iter_2_left = create_pool2d('avg', 3, 1, count_include_pad=False, padding=pad_type)\n\n        self.comb_iter_3_left = create_pool2d('avg', 3, 1, count_include_pad=False, padding=pad_type)\n        self.comb_iter_3_right = create_pool2d('avg', 3, 1, count_include_pad=False, padding=pad_type)\n\n        self.comb_iter_4_left = BranchSeparables(out_chs_right, out_chs_right, 3, 1, pad_type)\n\n    def forward(self, x, x_prev):\n        x_relu = self.act(x_prev)\n        x_path1 = self.path_1(x_relu)\n        x_path2 = self.path_2(x_relu)\n        x_left = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass NormalCell(nn.Module):\n\n    def __init__(self, in_chs_left, out_chs_left, in_chs_right, out_chs_right, pad_type=''):\n        super(NormalCell, self).__init__()\n        self.conv_prev_1x1 = ActConvBn(in_chs_left, out_chs_left, 1, stride=1, padding=pad_type)\n        self.conv_1x1 = ActConvBn(in_chs_right, out_chs_right, 1, stride=1, padding=pad_type)\n\n        self.comb_iter_0_left = BranchSeparables(out_chs_right, out_chs_right, 5, 1, pad_type)\n        self.comb_iter_0_right = BranchSeparables(out_chs_left, out_chs_left, 3, 1, pad_type)\n\n        self.comb_iter_1_left = BranchSeparables(out_chs_left, out_chs_left, 5, 1, pad_type)\n        self.comb_iter_1_right = BranchSeparables(out_chs_left, out_chs_left, 3, 1, pad_type)\n\n        self.comb_iter_2_left = create_pool2d('avg', 3, 1, count_include_pad=False, padding=pad_type)\n\n        self.comb_iter_3_left = create_pool2d('avg', 3, 1, count_include_pad=False, padding=pad_type)\n        self.comb_iter_3_right = create_pool2d('avg', 3, 1, count_include_pad=False, padding=pad_type)\n\n        self.comb_iter_4_left = BranchSeparables(out_chs_right, out_chs_right, 3, 1, pad_type)\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass ReductionCell0(nn.Module):\n\n    def __init__(self, in_chs_left, out_chs_left, in_chs_right, out_chs_right, pad_type=''):\n        super(ReductionCell0, self).__init__()\n        self.conv_prev_1x1 = ActConvBn(in_chs_left, out_chs_left, 1, stride=1, padding=pad_type)\n        self.conv_1x1 = ActConvBn(in_chs_right, out_chs_right, 1, stride=1, padding=pad_type)\n\n        self.comb_iter_0_left = BranchSeparables(out_chs_right, out_chs_right, 5, 2, pad_type)\n        self.comb_iter_0_right = BranchSeparables(out_chs_right, out_chs_right, 7, 2, pad_type)\n\n        self.comb_iter_1_left = create_pool2d('max', 3, 2, padding=pad_type)\n        self.comb_iter_1_right = BranchSeparables(out_chs_right, out_chs_right, 7, 2, pad_type)\n\n        self.comb_iter_2_left = create_pool2d('avg', 3, 2, count_include_pad=False, padding=pad_type)\n        self.comb_iter_2_right = BranchSeparables(out_chs_right, out_chs_right, 5, 2, pad_type)\n\n        self.comb_iter_3_right = create_pool2d('avg', 3, 1, count_include_pad=False, padding=pad_type)\n\n        self.comb_iter_4_left = BranchSeparables(out_chs_right, out_chs_right, 3, 1, pad_type)\n        self.comb_iter_4_right = create_pool2d('max', 3, 2, padding=pad_type)\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass ReductionCell1(nn.Module):\n\n    def __init__(self, in_chs_left, out_chs_left, in_chs_right, out_chs_right, pad_type=''):\n        super(ReductionCell1, self).__init__()\n        self.conv_prev_1x1 = ActConvBn(in_chs_left, out_chs_left, 1, stride=1, padding=pad_type)\n        self.conv_1x1 = ActConvBn(in_chs_right, out_chs_right, 1, stride=1, padding=pad_type)\n\n        self.comb_iter_0_left = BranchSeparables(out_chs_right, out_chs_right, 5, 2, pad_type)\n        self.comb_iter_0_right = BranchSeparables(out_chs_right, out_chs_right, 7, 2, pad_type)\n\n        self.comb_iter_1_left = create_pool2d('max', 3, 2, padding=pad_type)\n        self.comb_iter_1_right = BranchSeparables(out_chs_right, out_chs_right, 7, 2, pad_type)\n\n        self.comb_iter_2_left = create_pool2d('avg', 3, 2, count_include_pad=False, padding=pad_type)\n        self.comb_iter_2_right = BranchSeparables(out_chs_right, out_chs_right, 5, 2, pad_type)\n\n        self.comb_iter_3_right = create_pool2d('avg', 3, 1, count_include_pad=False, padding=pad_type)\n\n        self.comb_iter_4_left = BranchSeparables(out_chs_right, out_chs_right, 3, 1, pad_type)\n        self.comb_iter_4_right = create_pool2d('max', 3, 2, padding=pad_type)\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass NASNetALarge(nn.Module):\n    \"\"\"NASNetALarge (6 @ 4032) \"\"\"\n\n    def __init__(\n            self,\n            num_classes=1000,\n            in_chans=3,\n            stem_size=96,\n            channel_multiplier=2,\n            num_features=4032,\n            output_stride=32,\n            drop_rate=0.,\n            global_pool='avg',\n            pad_type='same',\n    ):\n        super(NASNetALarge, self).__init__()\n        self.num_classes = num_classes\n        self.stem_size = stem_size\n        self.num_features = num_features\n        self.channel_multiplier = channel_multiplier\n        assert output_stride == 32\n\n        channels = self.num_features // 24\n        # 24 is default value for the architecture\n\n        self.conv0 = ConvNormAct(\n            in_channels=in_chans, out_channels=self.stem_size, kernel_size=3, padding=0, stride=2,\n            norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.1), apply_act=False)\n\n        self.cell_stem_0 = CellStem0(\n            self.stem_size, num_channels=channels // (channel_multiplier ** 2), pad_type=pad_type)\n        self.cell_stem_1 = CellStem1(\n            self.stem_size, num_channels=channels // channel_multiplier, pad_type=pad_type)\n\n        self.cell_0 = FirstCell(\n            in_chs_left=channels, out_chs_left=channels // 2,\n            in_chs_right=2 * channels, out_chs_right=channels, pad_type=pad_type)\n        self.cell_1 = NormalCell(\n            in_chs_left=2 * channels, out_chs_left=channels,\n            in_chs_right=6 * channels, out_chs_right=channels, pad_type=pad_type)\n        self.cell_2 = NormalCell(\n            in_chs_left=6 * channels, out_chs_left=channels,\n            in_chs_right=6 * channels, out_chs_right=channels, pad_type=pad_type)\n        self.cell_3 = NormalCell(\n            in_chs_left=6 * channels, out_chs_left=channels,\n            in_chs_right=6 * channels, out_chs_right=channels, pad_type=pad_type)\n        self.cell_4 = NormalCell(\n            in_chs_left=6 * channels, out_chs_left=channels,\n            in_chs_right=6 * channels, out_chs_right=channels, pad_type=pad_type)\n        self.cell_5 = NormalCell(\n            in_chs_left=6 * channels, out_chs_left=channels,\n            in_chs_right=6 * channels, out_chs_right=channels, pad_type=pad_type)\n\n        self.reduction_cell_0 = ReductionCell0(\n            in_chs_left=6 * channels, out_chs_left=2 * channels,\n            in_chs_right=6 * channels, out_chs_right=2 * channels, pad_type=pad_type)\n        self.cell_6 = FirstCell(\n            in_chs_left=6 * channels, out_chs_left=channels,\n            in_chs_right=8 * channels, out_chs_right=2 * channels, pad_type=pad_type)\n        self.cell_7 = NormalCell(\n            in_chs_left=8 * channels, out_chs_left=2 * channels,\n            in_chs_right=12 * channels, out_chs_right=2 * channels, pad_type=pad_type)\n        self.cell_8 = NormalCell(\n            in_chs_left=12 * channels, out_chs_left=2 * channels,\n            in_chs_right=12 * channels, out_chs_right=2 * channels, pad_type=pad_type)\n        self.cell_9 = NormalCell(\n            in_chs_left=12 * channels, out_chs_left=2 * channels,\n            in_chs_right=12 * channels, out_chs_right=2 * channels, pad_type=pad_type)\n        self.cell_10 = NormalCell(\n            in_chs_left=12 * channels, out_chs_left=2 * channels,\n            in_chs_right=12 * channels, out_chs_right=2 * channels, pad_type=pad_type)\n        self.cell_11 = NormalCell(\n            in_chs_left=12 * channels, out_chs_left=2 * channels,\n            in_chs_right=12 * channels, out_chs_right=2 * channels, pad_type=pad_type)\n\n        self.reduction_cell_1 = ReductionCell1(\n            in_chs_left=12 * channels, out_chs_left=4 * channels,\n            in_chs_right=12 * channels, out_chs_right=4 * channels, pad_type=pad_type)\n        self.cell_12 = FirstCell(\n            in_chs_left=12 * channels, out_chs_left=2 * channels,\n            in_chs_right=16 * channels, out_chs_right=4 * channels, pad_type=pad_type)\n        self.cell_13 = NormalCell(\n            in_chs_left=16 * channels, out_chs_left=4 * channels,\n            in_chs_right=24 * channels, out_chs_right=4 * channels, pad_type=pad_type)\n        self.cell_14 = NormalCell(\n            in_chs_left=24 * channels, out_chs_left=4 * channels,\n            in_chs_right=24 * channels, out_chs_right=4 * channels, pad_type=pad_type)\n        self.cell_15 = NormalCell(\n            in_chs_left=24 * channels, out_chs_left=4 * channels,\n            in_chs_right=24 * channels, out_chs_right=4 * channels, pad_type=pad_type)\n        self.cell_16 = NormalCell(\n            in_chs_left=24 * channels, out_chs_left=4 * channels,\n            in_chs_right=24 * channels, out_chs_right=4 * channels, pad_type=pad_type)\n        self.cell_17 = NormalCell(\n            in_chs_left=24 * channels, out_chs_left=4 * channels,\n            in_chs_right=24 * channels, out_chs_right=4 * channels, pad_type=pad_type)\n        self.act = nn.ReLU(inplace=True)\n        self.feature_info = [\n            dict(num_chs=96, reduction=2, module='conv0'),\n            dict(num_chs=168, reduction=4, module='cell_stem_1.conv_1x1.act'),\n            dict(num_chs=1008, reduction=8, module='reduction_cell_0.conv_1x1.act'),\n            dict(num_chs=2016, reduction=16, module='reduction_cell_1.conv_1x1.act'),\n            dict(num_chs=4032, reduction=32, module='act'),\n        ]\n\n        self.global_pool, self.head_drop, self.last_linear = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool, drop_rate=drop_rate)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^conv0|cell_stem_[01]',\n            blocks=[\n                (r'^cell_(\\d+)', None),\n                (r'^reduction_cell_0', (6,)),\n                (r'^reduction_cell_1', (12,)),\n            ]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, 'gradient checkpointing not supported'\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.last_linear\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.global_pool, self.last_linear = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        x_conv0 = self.conv0(x)\n\n        x_stem_0 = self.cell_stem_0(x_conv0)\n        x_stem_1 = self.cell_stem_1(x_conv0, x_stem_0)\n\n        x_cell_0 = self.cell_0(x_stem_1, x_stem_0)\n        x_cell_1 = self.cell_1(x_cell_0, x_stem_1)\n        x_cell_2 = self.cell_2(x_cell_1, x_cell_0)\n        x_cell_3 = self.cell_3(x_cell_2, x_cell_1)\n        x_cell_4 = self.cell_4(x_cell_3, x_cell_2)\n        x_cell_5 = self.cell_5(x_cell_4, x_cell_3)\n\n        x_reduction_cell_0 = self.reduction_cell_0(x_cell_5, x_cell_4)\n        x_cell_6 = self.cell_6(x_reduction_cell_0, x_cell_4)\n        x_cell_7 = self.cell_7(x_cell_6, x_reduction_cell_0)\n        x_cell_8 = self.cell_8(x_cell_7, x_cell_6)\n        x_cell_9 = self.cell_9(x_cell_8, x_cell_7)\n        x_cell_10 = self.cell_10(x_cell_9, x_cell_8)\n        x_cell_11 = self.cell_11(x_cell_10, x_cell_9)\n\n        x_reduction_cell_1 = self.reduction_cell_1(x_cell_11, x_cell_10)\n        x_cell_12 = self.cell_12(x_reduction_cell_1, x_cell_10)\n        x_cell_13 = self.cell_13(x_cell_12, x_reduction_cell_1)\n        x_cell_14 = self.cell_14(x_cell_13, x_cell_12)\n        x_cell_15 = self.cell_15(x_cell_14, x_cell_13)\n        x_cell_16 = self.cell_16(x_cell_15, x_cell_14)\n        x_cell_17 = self.cell_17(x_cell_16, x_cell_15)\n        x = self.act(x_cell_17)\n        return x\n\n    def forward_head(self, x):\n        x = self.global_pool(x)\n        x = self.head_drop(x)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_nasnet(variant, pretrained=False, **kwargs):\n    return build_model_with_cfg(\n        NASNetALarge,\n        variant,\n        pretrained,\n        feature_cfg=dict(feature_cls='hook', no_rewrite=True),  # not possible to re-write this model\n        **kwargs,\n    )\n\n\ndefault_cfgs = generate_default_cfgs({\n    'nasnetalarge.tf_in1k': {\n        'hf_hub_id': 'timm/',\n        'url': 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/nasnetalarge-dc4a7b8b.pth',\n        'input_size': (3, 331, 331),\n        'pool_size': (11, 11),\n        'crop_pct': 0.911,\n        'interpolation': 'bicubic',\n        'mean': (0.5, 0.5, 0.5),\n        'std': (0.5, 0.5, 0.5),\n        'num_classes': 1000,\n        'first_conv': 'conv0.conv',\n        'classifier': 'last_linear',\n    },\n})\n\n\n@register_model\ndef nasnetalarge(pretrained=False, **kwargs) -> NASNetALarge:\n    \"\"\"NASNet-A large model architecture.\n    \"\"\"\n    model_kwargs = dict(pad_type='same', **kwargs)\n    return _create_nasnet('nasnetalarge', pretrained, **model_kwargs)\n",
  "\"\"\" ConvNeXt\n\nPapers:\n* `A ConvNet for the 2020s` - https://arxiv.org/pdf/2201.03545.pdf\n@Article{liu2022convnet,\n  author  = {Zhuang Liu and Hanzi Mao and Chao-Yuan Wu and Christoph Feichtenhofer and Trevor Darrell and Saining Xie},\n  title   = {A ConvNet for the 2020s},\n  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year    = {2022},\n}\n\n* `ConvNeXt-V2 - Co-designing and Scaling ConvNets with Masked Autoencoders` - https://arxiv.org/abs/2301.00808\n@article{Woo2023ConvNeXtV2,\n  title={ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders},\n  author={Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon and Saining Xie},\n  year={2023},\n  journal={arXiv preprint arXiv:2301.00808},\n}\n\nOriginal code and weights from:\n* https://github.com/facebookresearch/ConvNeXt, original copyright below\n* https://github.com/facebookresearch/ConvNeXt-V2, original copyright below\n\nModel defs atto, femto, pico, nano and _ols / _hnf variants are timm originals.\n\nModifications and additions for timm hacked together by / Copyright 2022, Ross Wightman\n\"\"\"\n# ConvNeXt\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# This source code is licensed under the MIT license\n\n# ConvNeXt-V2\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree (Attribution-NonCommercial 4.0 International (CC BY-NC 4.0))\n# No code was used directly from ConvNeXt-V2, however the weights are CC BY-NC 4.0 so beware if using commercially.\n\nfrom collections import OrderedDict\nfrom functools import partial\nfrom typing import Callable, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\nfrom timm.layers import trunc_normal_, AvgPool2dSame, DropPath, Mlp, GlobalResponseNormMlp, \\\n    LayerNorm2d, LayerNorm, create_conv2d, get_act_layer, make_divisible, to_ntuple\nfrom timm.layers import NormMlpClassifierHead, ClassifierHead\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import named_apply, checkpoint_seq\nfrom ._registry import generate_default_cfgs, register_model, register_model_deprecations\n\n__all__ = ['ConvNeXt']  # model_registry will add each entrypoint fn to this\n\n\nclass Downsample(nn.Module):\n\n    def __init__(self, in_chs, out_chs, stride=1, dilation=1):\n        super().__init__()\n        avg_stride = stride if dilation == 1 else 1\n        if stride > 1 or dilation > 1:\n            avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d\n            self.pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)\n        else:\n            self.pool = nn.Identity()\n\n        if in_chs != out_chs:\n            self.conv = create_conv2d(in_chs, out_chs, 1, stride=1)\n        else:\n            self.conv = nn.Identity()\n\n    def forward(self, x):\n        x = self.pool(x)\n        x = self.conv(x)\n        return x\n\n\nclass ConvNeXtBlock(nn.Module):\n    \"\"\" ConvNeXt Block\n    There are two equivalent implementations:\n      (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n      (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n\n    Unlike the official impl, this one allows choice of 1 or 2, 1x1 conv can be faster with appropriate\n    choice of LayerNorm impl, however as model size increases the tradeoffs appear to change and nn.Linear\n    is a better choice. This was observed with PyTorch 1.10 on 3090 GPU, it could change over time & w/ different HW.\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: Optional[int] = None,\n            kernel_size: int = 7,\n            stride: int = 1,\n            dilation: Union[int, Tuple[int, int]] = (1, 1),\n            mlp_ratio: float = 4,\n            conv_mlp: bool = False,\n            conv_bias: bool = True,\n            use_grn: bool = False,\n            ls_init_value: Optional[float] = 1e-6,\n            act_layer: Union[str, Callable] = 'gelu',\n            norm_layer: Optional[Callable] = None,\n            drop_path: float = 0.,\n    ):\n        \"\"\"\n\n        Args:\n            in_chs: Block input channels.\n            out_chs: Block output channels (same as in_chs if None).\n            kernel_size: Depthwise convolution kernel size.\n            stride: Stride of depthwise convolution.\n            dilation: Tuple specifying input and output dilation of block.\n            mlp_ratio: MLP expansion ratio.\n            conv_mlp: Use 1x1 convolutions for MLP and a NCHW compatible norm layer if True.\n            conv_bias: Apply bias for all convolution (linear) layers.\n            use_grn: Use GlobalResponseNorm in MLP (from ConvNeXt-V2)\n            ls_init_value: Layer-scale init values, layer-scale applied if not None.\n            act_layer: Activation layer.\n            norm_layer: Normalization layer (defaults to LN if not specified).\n            drop_path: Stochastic depth probability.\n        \"\"\"\n        super().__init__()\n        out_chs = out_chs or in_chs\n        dilation = to_ntuple(2)(dilation)\n        act_layer = get_act_layer(act_layer)\n        if not norm_layer:\n            norm_layer = LayerNorm2d if conv_mlp else LayerNorm\n        mlp_layer = partial(GlobalResponseNormMlp if use_grn else Mlp, use_conv=conv_mlp)\n        self.use_conv_mlp = conv_mlp\n        self.conv_dw = create_conv2d(\n            in_chs,\n            out_chs,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation[0],\n            depthwise=True,\n            bias=conv_bias,\n        )\n        self.norm = norm_layer(out_chs)\n        self.mlp = mlp_layer(out_chs, int(mlp_ratio * out_chs), act_layer=act_layer)\n        self.gamma = nn.Parameter(ls_init_value * torch.ones(out_chs)) if ls_init_value is not None else None\n        if in_chs != out_chs or stride != 1 or dilation[0] != dilation[1]:\n            self.shortcut = Downsample(in_chs, out_chs, stride=stride, dilation=dilation[0])\n        else:\n            self.shortcut = nn.Identity()\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv_dw(x)\n        if self.use_conv_mlp:\n            x = self.norm(x)\n            x = self.mlp(x)\n        else:\n            x = x.permute(0, 2, 3, 1)\n            x = self.norm(x)\n            x = self.mlp(x)\n            x = x.permute(0, 3, 1, 2)\n        if self.gamma is not None:\n            x = x.mul(self.gamma.reshape(1, -1, 1, 1))\n\n        x = self.drop_path(x) + self.shortcut(shortcut)\n        return x\n\n\nclass ConvNeXtStage(nn.Module):\n\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            kernel_size=7,\n            stride=2,\n            depth=2,\n            dilation=(1, 1),\n            drop_path_rates=None,\n            ls_init_value=1.0,\n            conv_mlp=False,\n            conv_bias=True,\n            use_grn=False,\n            act_layer='gelu',\n            norm_layer=None,\n            norm_layer_cl=None\n    ):\n        super().__init__()\n        self.grad_checkpointing = False\n\n        if in_chs != out_chs or stride > 1 or dilation[0] != dilation[1]:\n            ds_ks = 2 if stride > 1 or dilation[0] != dilation[1] else 1\n            pad = 'same' if dilation[1] > 1 else 0  # same padding needed if dilation used\n            self.downsample = nn.Sequential(\n                norm_layer(in_chs),\n                create_conv2d(\n                    in_chs,\n                    out_chs,\n                    kernel_size=ds_ks,\n                    stride=stride,\n                    dilation=dilation[0],\n                    padding=pad,\n                    bias=conv_bias,\n                ),\n            )\n            in_chs = out_chs\n        else:\n            self.downsample = nn.Identity()\n\n        drop_path_rates = drop_path_rates or [0.] * depth\n        stage_blocks = []\n        for i in range(depth):\n            stage_blocks.append(ConvNeXtBlock(\n                in_chs=in_chs,\n                out_chs=out_chs,\n                kernel_size=kernel_size,\n                dilation=dilation[1],\n                drop_path=drop_path_rates[i],\n                ls_init_value=ls_init_value,\n                conv_mlp=conv_mlp,\n                conv_bias=conv_bias,\n                use_grn=use_grn,\n                act_layer=act_layer,\n                norm_layer=norm_layer if conv_mlp else norm_layer_cl,\n            ))\n            in_chs = out_chs\n        self.blocks = nn.Sequential(*stage_blocks)\n\n    def forward(self, x):\n        x = self.downsample(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        return x\n\n\nclass ConvNeXt(nn.Module):\n    r\"\"\" ConvNeXt\n        A PyTorch impl of : `A ConvNet for the 2020s`  - https://arxiv.org/pdf/2201.03545.pdf\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chans: int = 3,\n            num_classes: int = 1000,\n            global_pool: str = 'avg',\n            output_stride: int = 32,\n            depths: Tuple[int, ...] = (3, 3, 9, 3),\n            dims: Tuple[int, ...] = (96, 192, 384, 768),\n            kernel_sizes: Union[int, Tuple[int, ...]] = 7,\n            ls_init_value: Optional[float] = 1e-6,\n            stem_type: str = 'patch',\n            patch_size: int = 4,\n            head_init_scale: float = 1.,\n            head_norm_first: bool = False,\n            head_hidden_size: Optional[int] = None,\n            conv_mlp: bool = False,\n            conv_bias: bool = True,\n            use_grn: bool = False,\n            act_layer: Union[str, Callable] = 'gelu',\n            norm_layer: Optional[Union[str, Callable]] = None,\n            norm_eps: Optional[float] = None,\n            drop_rate: float = 0.,\n            drop_path_rate: float = 0.,\n    ):\n        \"\"\"\n        Args:\n            in_chans: Number of input image channels.\n            num_classes: Number of classes for classification head.\n            global_pool: Global pooling type.\n            output_stride: Output stride of network, one of (8, 16, 32).\n            depths: Number of blocks at each stage.\n            dims: Feature dimension at each stage.\n            kernel_sizes: Depthwise convolution kernel-sizes for each stage.\n            ls_init_value: Init value for Layer Scale, disabled if None.\n            stem_type: Type of stem.\n            patch_size: Stem patch size for patch stem.\n            head_init_scale: Init scaling value for classifier weights and biases.\n            head_norm_first: Apply normalization before global pool + head.\n            head_hidden_size: Size of MLP hidden layer in head if not None and head_norm_first == False.\n            conv_mlp: Use 1x1 conv in MLP, improves speed for small networks w/ chan last.\n            conv_bias: Use bias layers w/ all convolutions.\n            use_grn: Use Global Response Norm (ConvNeXt-V2) in MLP.\n            act_layer: Activation layer type.\n            norm_layer: Normalization layer type.\n            drop_rate: Head pre-classifier dropout rate.\n            drop_path_rate: Stochastic depth drop rate.\n        \"\"\"\n        super().__init__()\n        assert output_stride in (8, 16, 32)\n        kernel_sizes = to_ntuple(4)(kernel_sizes)\n        if norm_layer is None:\n            norm_layer = LayerNorm2d\n            norm_layer_cl = norm_layer if conv_mlp else LayerNorm\n            if norm_eps is not None:\n                norm_layer = partial(norm_layer, eps=norm_eps)\n                norm_layer_cl = partial(norm_layer_cl, eps=norm_eps)\n        else:\n            assert conv_mlp,\\\n                'If a norm_layer is specified, conv MLP must be used so all norm expect rank-4, channels-first input'\n            norm_layer_cl = norm_layer\n            if norm_eps is not None:\n                norm_layer_cl = partial(norm_layer_cl, eps=norm_eps)\n\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        self.feature_info = []\n\n        assert stem_type in ('patch', 'overlap', 'overlap_tiered')\n        if stem_type == 'patch':\n            # NOTE: this stem is a minimal form of ViT PatchEmbed, as used in SwinTransformer w/ patch_size = 4\n            self.stem = nn.Sequential(\n                nn.Conv2d(in_chans, dims[0], kernel_size=patch_size, stride=patch_size, bias=conv_bias),\n                norm_layer(dims[0]),\n            )\n            stem_stride = patch_size\n        else:\n            mid_chs = make_divisible(dims[0] // 2) if 'tiered' in stem_type else dims[0]\n            self.stem = nn.Sequential(\n                nn.Conv2d(in_chans, mid_chs, kernel_size=3, stride=2, padding=1, bias=conv_bias),\n                nn.Conv2d(mid_chs, dims[0], kernel_size=3, stride=2, padding=1, bias=conv_bias),\n                norm_layer(dims[0]),\n            )\n            stem_stride = 4\n\n        self.stages = nn.Sequential()\n        dp_rates = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n        stages = []\n        prev_chs = dims[0]\n        curr_stride = stem_stride\n        dilation = 1\n        # 4 feature resolution stages, each consisting of multiple residual blocks\n        for i in range(4):\n            stride = 2 if curr_stride == 2 or i > 0 else 1\n            if curr_stride >= output_stride and stride > 1:\n                dilation *= stride\n                stride = 1\n            curr_stride *= stride\n            first_dilation = 1 if dilation in (1, 2) else 2\n            out_chs = dims[i]\n            stages.append(ConvNeXtStage(\n                prev_chs,\n                out_chs,\n                kernel_size=kernel_sizes[i],\n                stride=stride,\n                dilation=(first_dilation, dilation),\n                depth=depths[i],\n                drop_path_rates=dp_rates[i],\n                ls_init_value=ls_init_value,\n                conv_mlp=conv_mlp,\n                conv_bias=conv_bias,\n                use_grn=use_grn,\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n                norm_layer_cl=norm_layer_cl,\n            ))\n            prev_chs = out_chs\n            # NOTE feature_info use currently assumes stage 0 == stride 1, rest are stride 2\n            self.feature_info += [dict(num_chs=prev_chs, reduction=curr_stride, module=f'stages.{i}')]\n        self.stages = nn.Sequential(*stages)\n        self.num_features = prev_chs\n\n        # if head_norm_first == true, norm -> global pool -> fc ordering, like most other nets\n        # otherwise pool -> norm -> fc, the default ConvNeXt ordering (pretrained FB weights)\n        if head_norm_first:\n            assert not head_hidden_size\n            self.norm_pre = norm_layer(self.num_features)\n            self.head = ClassifierHead(\n                self.num_features,\n                num_classes,\n                pool_type=global_pool,\n                drop_rate=self.drop_rate,\n            )\n        else:\n            self.norm_pre = nn.Identity()\n            self.head = NormMlpClassifierHead(\n                self.num_features,\n                num_classes,\n                hidden_size=head_hidden_size,\n                pool_type=global_pool,\n                drop_rate=self.drop_rate,\n                norm_layer=norm_layer,\n                act_layer='gelu',\n            )\n        named_apply(partial(_init_weights, head_init_scale=head_init_scale), self)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^stem',\n            blocks=r'^stages\\.(\\d+)' if coarse else [\n                (r'^stages\\.(\\d+)\\.downsample', (0,)),  # blocks\n                (r'^stages\\.(\\d+)\\.blocks\\.(\\d+)', None),\n                (r'^norm_pre', (99999,))\n            ]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        for s in self.stages:\n            s.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes=0, global_pool=None):\n        self.head.reset(num_classes, global_pool)\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        x = self.stages(x)\n        x = self.norm_pre(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=True) if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _init_weights(module, name=None, head_init_scale=1.0):\n    if isinstance(module, nn.Conv2d):\n        trunc_normal_(module.weight, std=.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Linear):\n        trunc_normal_(module.weight, std=.02)\n        nn.init.zeros_(module.bias)\n        if name and 'head.' in name:\n            module.weight.data.mul_(head_init_scale)\n            module.bias.data.mul_(head_init_scale)\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    \"\"\" Remap FB checkpoints -> timm \"\"\"\n    if 'head.norm.weight' in state_dict or 'norm_pre.weight' in state_dict:\n        return state_dict  # non-FB checkpoint\n    if 'model' in state_dict:\n        state_dict = state_dict['model']\n\n    out_dict = {}\n    if 'visual.trunk.stem.0.weight' in state_dict:\n        out_dict = {k.replace('visual.trunk.', ''): v for k, v in state_dict.items() if k.startswith('visual.trunk.')}\n        if 'visual.head.proj.weight' in state_dict:\n            out_dict['head.fc.weight'] = state_dict['visual.head.proj.weight']\n            out_dict['head.fc.bias'] = torch.zeros(state_dict['visual.head.proj.weight'].shape[0])\n        elif 'visual.head.mlp.fc1.weight' in state_dict:\n            out_dict['head.pre_logits.fc.weight'] = state_dict['visual.head.mlp.fc1.weight']\n            out_dict['head.pre_logits.fc.bias'] = state_dict['visual.head.mlp.fc1.bias']\n            out_dict['head.fc.weight'] = state_dict['visual.head.mlp.fc2.weight']\n            out_dict['head.fc.bias'] = torch.zeros(state_dict['visual.head.mlp.fc2.weight'].shape[0])\n        return out_dict\n\n    import re\n    for k, v in state_dict.items():\n        k = k.replace('downsample_layers.0.', 'stem.')\n        k = re.sub(r'stages.([0-9]+).([0-9]+)', r'stages.\\1.blocks.\\2', k)\n        k = re.sub(r'downsample_layers.([0-9]+).([0-9]+)', r'stages.\\1.downsample.\\2', k)\n        k = k.replace('dwconv', 'conv_dw')\n        k = k.replace('pwconv', 'mlp.fc')\n        if 'grn' in k:\n            k = k.replace('grn.beta', 'mlp.grn.bias')\n            k = k.replace('grn.gamma', 'mlp.grn.weight')\n            v = v.reshape(v.shape[-1])\n        k = k.replace('head.', 'head.fc.')\n        if k.startswith('norm.'):\n            k = k.replace('norm', 'head.norm')\n        if v.ndim == 2 and 'head' not in k:\n            model_shape = model.state_dict()[k].shape\n            v = v.reshape(model_shape)\n        out_dict[k] = v\n\n    return out_dict\n\n\ndef _create_convnext(variant, pretrained=False, **kwargs):\n    if kwargs.get('pretrained_cfg', '') == 'fcmae':\n        # NOTE fcmae pretrained weights have no classifier or final norm-layer (`head.norm`)\n        # This is workaround loading with num_classes=0 w/o removing norm-layer.\n        kwargs.setdefault('pretrained_strict', False)\n\n    model = build_model_with_cfg(\n        ConvNeXt, variant, pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        feature_cfg=dict(out_indices=(0, 1, 2, 3), flatten_sequential=True),\n        **kwargs)\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.0', 'classifier': 'head.fc',\n        **kwargs\n    }\n\n\ndef _cfgv2(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.0', 'classifier': 'head.fc',\n        'license': 'cc-by-nc-4.0', 'paper_ids': 'arXiv:2301.00808',\n        'paper_name': 'ConvNeXt-V2: Co-designing and Scaling ConvNets with Masked Autoencoders',\n        'origin_url': 'https://github.com/facebookresearch/ConvNeXt-V2',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    # timm specific variants\n    'convnext_tiny.in12k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnext_small.in12k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),\n\n    'convnext_atto.d2_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/convnext_atto_d2-01bb0f51.pth',\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=0.95),\n    'convnext_atto_ols.a2_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/convnext_atto_ols_a2-78d1c8f3.pth',\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=0.95),\n    'convnext_femto.d1_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/convnext_femto_d1-d71d5b4c.pth',\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=0.95),\n    'convnext_femto_ols.d1_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/convnext_femto_ols_d1-246bf2ed.pth',\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=0.95),\n    'convnext_pico.d1_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/convnext_pico_d1-10ad7f0d.pth',\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=0.95),\n    'convnext_pico_ols.d1_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/convnext_pico_ols_d1-611f0ca7.pth',\n        hf_hub_id='timm/',\n        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnext_nano.in12k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnext_nano.d1h_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/convnext_nano_d1h-7eb4bdea.pth',\n        hf_hub_id='timm/',\n        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnext_nano_ols.d1h_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/convnext_nano_ols_d1h-ae424a9a.pth',\n        hf_hub_id='timm/',\n        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnext_tiny_hnf.a2h_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/convnext_tiny_hnf_a2h-ab7e9df2.pth',\n        hf_hub_id='timm/',\n        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),\n\n    'convnext_tiny.in12k_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n       input_size=(3, 384, 384), pool_size=(12, 12),  crop_pct=1.0, crop_mode='squash'),\n    'convnext_small.in12k_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0,  crop_mode='squash'),\n\n    'convnext_nano.in12k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.95, num_classes=11821),\n    'convnext_tiny.in12k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.95, num_classes=11821),\n    'convnext_small.in12k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.95, num_classes=11821),\n\n    'convnext_tiny.fb_in22k_ft_in1k': _cfg(\n        url='https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_1k_224.pth',\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnext_small.fb_in22k_ft_in1k': _cfg(\n        url='https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_1k_224.pth',\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnext_base.fb_in22k_ft_in1k': _cfg(\n        url='https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_1k_224.pth',\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnext_large.fb_in22k_ft_in1k': _cfg(\n        url='https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_1k_224.pth',\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnext_xlarge.fb_in22k_ft_in1k': _cfg(\n        url='https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_1k_224_ema.pth',\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n\n    'convnext_tiny.fb_in1k': _cfg(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnext_small.fb_in1k': _cfg(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\",\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnext_base.fb_in1k': _cfg(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\",\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnext_large.fb_in1k': _cfg(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\",\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n\n    'convnext_tiny.fb_in22k_ft_in1k_384': _cfg(\n        url='https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_1k_384.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'convnext_small.fb_in22k_ft_in1k_384': _cfg(\n        url='https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_1k_384.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'convnext_base.fb_in22k_ft_in1k_384': _cfg(\n        url='https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_1k_384.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'convnext_large.fb_in22k_ft_in1k_384': _cfg(\n        url='https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_1k_384.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'convnext_xlarge.fb_in22k_ft_in1k_384': _cfg(\n        url='https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_1k_384_ema.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n\n    'convnext_tiny.fb_in22k': _cfg(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth\",\n        hf_hub_id='timm/',\n        num_classes=21841),\n    'convnext_small.fb_in22k': _cfg(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\",\n        hf_hub_id='timm/',\n        num_classes=21841),\n    'convnext_base.fb_in22k': _cfg(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\",\n        hf_hub_id='timm/',\n        num_classes=21841),\n    'convnext_large.fb_in22k': _cfg(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\",\n        hf_hub_id='timm/',\n        num_classes=21841),\n    'convnext_xlarge.fb_in22k': _cfg(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\",\n        hf_hub_id='timm/',\n        num_classes=21841),\n\n    'convnextv2_nano.fcmae_ft_in22k_in1k': _cfgv2(\n        url='https://dl.fbaipublicfiles.com/convnext/convnextv2/im22k/convnextv2_nano_22k_224_ema.pt',\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnextv2_nano.fcmae_ft_in22k_in1k_384': _cfgv2(\n        url='https://dl.fbaipublicfiles.com/convnext/convnextv2/im22k/convnextv2_nano_22k_384_ema.pt',\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'convnextv2_tiny.fcmae_ft_in22k_in1k': _cfgv2(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnextv2/im22k/convnextv2_tiny_22k_224_ema.pt\",\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnextv2_tiny.fcmae_ft_in22k_in1k_384': _cfgv2(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnextv2/im22k/convnextv2_tiny_22k_384_ema.pt\",\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'convnextv2_base.fcmae_ft_in22k_in1k': _cfgv2(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnextv2/im22k/convnextv2_base_22k_224_ema.pt\",\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnextv2_base.fcmae_ft_in22k_in1k_384': _cfgv2(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnextv2/im22k/convnextv2_base_22k_384_ema.pt\",\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'convnextv2_large.fcmae_ft_in22k_in1k': _cfgv2(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnextv2/im22k/convnextv2_large_22k_224_ema.pt\",\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnextv2_large.fcmae_ft_in22k_in1k_384': _cfgv2(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnextv2/im22k/convnextv2_large_22k_384_ema.pt\",\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'convnextv2_huge.fcmae_ft_in22k_in1k_384': _cfgv2(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnextv2/im22k/convnextv2_huge_22k_384_ema.pt\",\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'convnextv2_huge.fcmae_ft_in22k_in1k_512': _cfgv2(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnextv2/im22k/convnextv2_huge_22k_512_ema.pt\",\n        hf_hub_id='timm/',\n        input_size=(3, 512, 512), pool_size=(15, 15), crop_pct=1.0, crop_mode='squash'),\n\n    'convnextv2_atto.fcmae_ft_in1k': _cfgv2(\n        url='https://dl.fbaipublicfiles.com/convnext/convnextv2/im1k/convnextv2_atto_1k_224_ema.pt',\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=0.95),\n    'convnextv2_femto.fcmae_ft_in1k': _cfgv2(\n        url='https://dl.fbaipublicfiles.com/convnext/convnextv2/im1k/convnextv2_femto_1k_224_ema.pt',\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=0.95),\n    'convnextv2_pico.fcmae_ft_in1k': _cfgv2(\n        url='https://dl.fbaipublicfiles.com/convnext/convnextv2/im1k/convnextv2_pico_1k_224_ema.pt',\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=0.95),\n    'convnextv2_nano.fcmae_ft_in1k': _cfgv2(\n        url='https://dl.fbaipublicfiles.com/convnext/convnextv2/im1k/convnextv2_nano_1k_224_ema.pt',\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnextv2_tiny.fcmae_ft_in1k': _cfgv2(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnextv2/im1k/convnextv2_tiny_1k_224_ema.pt\",\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnextv2_base.fcmae_ft_in1k': _cfgv2(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnextv2/im1k/convnextv2_base_1k_224_ema.pt\",\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnextv2_large.fcmae_ft_in1k': _cfgv2(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnextv2/im1k/convnextv2_large_1k_224_ema.pt\",\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'convnextv2_huge.fcmae_ft_in1k': _cfgv2(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnextv2/im1k/convnextv2_huge_1k_224_ema.pt\",\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=1.0),\n\n    'convnextv2_atto.fcmae': _cfgv2(\n        url='https://dl.fbaipublicfiles.com/convnext/convnextv2/pt_only/convnextv2_atto_1k_224_fcmae.pt',\n        hf_hub_id='timm/',\n        num_classes=0),\n    'convnextv2_femto.fcmae': _cfgv2(\n        url='https://dl.fbaipublicfiles.com/convnext/convnextv2/pt_only/convnextv2_femto_1k_224_fcmae.pt',\n        hf_hub_id='timm/',\n        num_classes=0),\n    'convnextv2_pico.fcmae': _cfgv2(\n        url='https://dl.fbaipublicfiles.com/convnext/convnextv2/pt_only/convnextv2_pico_1k_224_fcmae.pt',\n        hf_hub_id='timm/',\n        num_classes=0),\n    'convnextv2_nano.fcmae': _cfgv2(\n        url='https://dl.fbaipublicfiles.com/convnext/convnextv2/pt_only/convnextv2_nano_1k_224_fcmae.pt',\n        hf_hub_id='timm/',\n        num_classes=0),\n    'convnextv2_tiny.fcmae': _cfgv2(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnextv2/pt_only/convnextv2_tiny_1k_224_fcmae.pt\",\n        hf_hub_id='timm/',\n        num_classes=0),\n    'convnextv2_base.fcmae': _cfgv2(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnextv2/pt_only/convnextv2_base_1k_224_fcmae.pt\",\n        hf_hub_id='timm/',\n        num_classes=0),\n    'convnextv2_large.fcmae': _cfgv2(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnextv2/pt_only/convnextv2_large_1k_224_fcmae.pt\",\n        hf_hub_id='timm/',\n        num_classes=0),\n    'convnextv2_huge.fcmae': _cfgv2(\n        url=\"https://dl.fbaipublicfiles.com/convnext/convnextv2/pt_only/convnextv2_huge_1k_224_fcmae.pt\",\n        hf_hub_id='timm/',\n        num_classes=0),\n\n    'convnextv2_small.untrained': _cfg(),\n\n    # CLIP weights, fine-tuned on in1k or in12k + in1k\n    'convnext_base.clip_laion2b_augreg_ft_in12k_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0),\n    'convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=1.0),\n    'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n\n    'convnext_base.clip_laion2b_augreg_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0),\n    'convnext_base.clip_laiona_augreg_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0),\n    'convnext_large_mlp.clip_laion2b_augreg_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0\n    ),\n    'convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'\n    ),\n    'convnext_xxlarge.clip_laion2b_soup_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0),\n\n    'convnext_base.clip_laion2b_augreg_ft_in12k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=11821,\n        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0),\n    'convnext_large_mlp.clip_laion2b_soup_ft_in12k_320': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=11821,\n        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=1.0),\n    'convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=11821,\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'convnext_large_mlp.clip_laion2b_soup_ft_in12k_384': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=11821,\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n\n    # CLIP original image tower weights\n    'convnext_base.clip_laion2b': _cfg(\n        hf_hub_id='laion/CLIP-convnext_base_w-laion2B-s13B-b82K',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, num_classes=640),\n    'convnext_base.clip_laion2b_augreg': _cfg(\n        hf_hub_id='laion/CLIP-convnext_base_w-laion2B-s13B-b82K-augreg',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, num_classes=640),\n    'convnext_base.clip_laiona': _cfg(\n        hf_hub_id='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, num_classes=640),\n    'convnext_base.clip_laiona_320': _cfg(\n        hf_hub_id='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=1.0, num_classes=640),\n    'convnext_base.clip_laiona_augreg_320': _cfg(\n        hf_hub_id='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=1.0, num_classes=640),\n    'convnext_large_mlp.clip_laion2b_augreg': _cfg(\n        hf_hub_id='laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, num_classes=768),\n    'convnext_large_mlp.clip_laion2b_ft_320': _cfg(\n        hf_hub_id='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=1.0, num_classes=768),\n    'convnext_large_mlp.clip_laion2b_ft_soup_320': _cfg(\n        hf_hub_id='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=1.0, num_classes=768),\n    'convnext_xxlarge.clip_laion2b_soup': _cfg(\n        hf_hub_id='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, num_classes=1024),\n    'convnext_xxlarge.clip_laion2b_rewind': _cfg(\n        hf_hub_id='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=1.0, num_classes=1024),\n})\n\n\n@register_model\ndef convnext_atto(pretrained=False, **kwargs) -> ConvNeXt:\n    # timm femto variant (NOTE: still tweaking depths, will vary between 3-4M param, current is 3.7M\n    model_args = dict(depths=(2, 2, 6, 2), dims=(40, 80, 160, 320), conv_mlp=True)\n    model = _create_convnext('convnext_atto', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnext_atto_ols(pretrained=False, **kwargs) -> ConvNeXt:\n    # timm femto variant with overlapping 3x3 conv stem, wider than non-ols femto above, current param count 3.7M\n    model_args = dict(depths=(2, 2, 6, 2), dims=(40, 80, 160, 320), conv_mlp=True, stem_type='overlap_tiered')\n    model = _create_convnext('convnext_atto_ols', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnext_femto(pretrained=False, **kwargs) -> ConvNeXt:\n    # timm femto variant\n    model_args = dict(depths=(2, 2, 6, 2), dims=(48, 96, 192, 384), conv_mlp=True)\n    model = _create_convnext('convnext_femto', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnext_femto_ols(pretrained=False, **kwargs) -> ConvNeXt:\n    # timm femto variant\n    model_args = dict(depths=(2, 2, 6, 2), dims=(48, 96, 192, 384), conv_mlp=True, stem_type='overlap_tiered')\n    model = _create_convnext('convnext_femto_ols', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnext_pico(pretrained=False, **kwargs) -> ConvNeXt:\n    # timm pico variant\n    model_args = dict(depths=(2, 2, 6, 2), dims=(64, 128, 256, 512), conv_mlp=True)\n    model = _create_convnext('convnext_pico', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnext_pico_ols(pretrained=False, **kwargs) -> ConvNeXt:\n    # timm nano variant with overlapping 3x3 conv stem\n    model_args = dict(depths=(2, 2, 6, 2), dims=(64, 128, 256, 512), conv_mlp=True,  stem_type='overlap_tiered')\n    model = _create_convnext('convnext_pico_ols', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnext_nano(pretrained=False, **kwargs) -> ConvNeXt:\n    # timm nano variant with standard stem and head\n    model_args = dict(depths=(2, 2, 8, 2), dims=(80, 160, 320, 640), conv_mlp=True)\n    model = _create_convnext('convnext_nano', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnext_nano_ols(pretrained=False, **kwargs) -> ConvNeXt:\n    # experimental nano variant with overlapping conv stem\n    model_args = dict(depths=(2, 2, 8, 2), dims=(80, 160, 320, 640), conv_mlp=True, stem_type='overlap')\n    model = _create_convnext('convnext_nano_ols', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnext_tiny_hnf(pretrained=False, **kwargs) -> ConvNeXt:\n    # experimental tiny variant with norm before pooling in head (head norm first)\n    model_args = dict(depths=(3, 3, 9, 3), dims=(96, 192, 384, 768), head_norm_first=True, conv_mlp=True)\n    model = _create_convnext('convnext_tiny_hnf', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnext_tiny(pretrained=False, **kwargs) -> ConvNeXt:\n    model_args = dict(depths=(3, 3, 9, 3), dims=(96, 192, 384, 768))\n    model = _create_convnext('convnext_tiny', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnext_small(pretrained=False, **kwargs) -> ConvNeXt:\n    model_args = dict(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768])\n    model = _create_convnext('convnext_small', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnext_base(pretrained=False, **kwargs) -> ConvNeXt:\n    model_args = dict(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024])\n    model = _create_convnext('convnext_base', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnext_large(pretrained=False, **kwargs) -> ConvNeXt:\n    model_args = dict(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536])\n    model = _create_convnext('convnext_large', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnext_large_mlp(pretrained=False, **kwargs) -> ConvNeXt:\n    model_args = dict(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], head_hidden_size=1536)\n    model = _create_convnext('convnext_large_mlp', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnext_xlarge(pretrained=False, **kwargs) -> ConvNeXt:\n    model_args = dict(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048])\n    model = _create_convnext('convnext_xlarge', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnext_xxlarge(pretrained=False, **kwargs) -> ConvNeXt:\n    model_args = dict(depths=[3, 4, 30, 3], dims=[384, 768, 1536, 3072], norm_eps=kwargs.pop('norm_eps', 1e-5))\n    model = _create_convnext('convnext_xxlarge', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnextv2_atto(pretrained=False, **kwargs) -> ConvNeXt:\n    # timm femto variant (NOTE: still tweaking depths, will vary between 3-4M param, current is 3.7M\n    model_args = dict(\n        depths=(2, 2, 6, 2), dims=(40, 80, 160, 320), use_grn=True, ls_init_value=None, conv_mlp=True)\n    model = _create_convnext('convnextv2_atto', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnextv2_femto(pretrained=False, **kwargs) -> ConvNeXt:\n    # timm femto variant\n    model_args = dict(\n        depths=(2, 2, 6, 2), dims=(48, 96, 192, 384), use_grn=True, ls_init_value=None, conv_mlp=True)\n    model = _create_convnext('convnextv2_femto', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnextv2_pico(pretrained=False, **kwargs) -> ConvNeXt:\n    # timm pico variant\n    model_args = dict(\n        depths=(2, 2, 6, 2), dims=(64, 128, 256, 512), use_grn=True, ls_init_value=None, conv_mlp=True)\n    model = _create_convnext('convnextv2_pico', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnextv2_nano(pretrained=False, **kwargs) -> ConvNeXt:\n    # timm nano variant with standard stem and head\n    model_args = dict(\n        depths=(2, 2, 8, 2), dims=(80, 160, 320, 640), use_grn=True, ls_init_value=None, conv_mlp=True)\n    model = _create_convnext('convnextv2_nano', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnextv2_tiny(pretrained=False, **kwargs) -> ConvNeXt:\n    model_args = dict(depths=(3, 3, 9, 3), dims=(96, 192, 384, 768), use_grn=True, ls_init_value=None)\n    model = _create_convnext('convnextv2_tiny', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnextv2_small(pretrained=False, **kwargs) -> ConvNeXt:\n    model_args = dict(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], use_grn=True, ls_init_value=None)\n    model = _create_convnext('convnextv2_small', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnextv2_base(pretrained=False, **kwargs) -> ConvNeXt:\n    model_args = dict(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], use_grn=True, ls_init_value=None)\n    model = _create_convnext('convnextv2_base', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnextv2_large(pretrained=False, **kwargs) -> ConvNeXt:\n    model_args = dict(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], use_grn=True, ls_init_value=None)\n    model = _create_convnext('convnextv2_large', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convnextv2_huge(pretrained=False, **kwargs) -> ConvNeXt:\n    model_args = dict(depths=[3, 3, 27, 3], dims=[352, 704, 1408, 2816], use_grn=True, ls_init_value=None)\n    model = _create_convnext('convnextv2_huge', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\nregister_model_deprecations(__name__, {\n    'convnext_tiny_in22ft1k': 'convnext_tiny.fb_in22k_ft_in1k',\n    'convnext_small_in22ft1k': 'convnext_small.fb_in22k_ft_in1k',\n    'convnext_base_in22ft1k': 'convnext_base.fb_in22k_ft_in1k',\n    'convnext_large_in22ft1k': 'convnext_large.fb_in22k_ft_in1k',\n    'convnext_xlarge_in22ft1k': 'convnext_xlarge.fb_in22k_ft_in1k',\n    'convnext_tiny_384_in22ft1k': 'convnext_tiny.fb_in22k_ft_in1k_384',\n    'convnext_small_384_in22ft1k': 'convnext_small.fb_in22k_ft_in1k_384',\n    'convnext_base_384_in22ft1k': 'convnext_base.fb_in22k_ft_in1k_384',\n    'convnext_large_384_in22ft1k': 'convnext_large.fb_in22k_ft_in1k_384',\n    'convnext_xlarge_384_in22ft1k': 'convnext_xlarge.fb_in22k_ft_in1k_384',\n    'convnext_tiny_in22k': 'convnext_tiny.fb_in22k',\n    'convnext_small_in22k': 'convnext_small.fb_in22k',\n    'convnext_base_in22k': 'convnext_base.fb_in22k',\n    'convnext_large_in22k': 'convnext_large.fb_in22k',\n    'convnext_xlarge_in22k': 'convnext_xlarge.fb_in22k',\n})\n",
  "\"\"\"Pre-Activation ResNet v2 with GroupNorm and Weight Standardization.\n\nA PyTorch implementation of ResNetV2 adapted from the Google Big-Transfoer (BiT) source code\nat https://github.com/google-research/big_transfer to match timm interfaces. The BiT weights have\nbeen included here as pretrained models from their original .NPZ checkpoints.\n\nAdditionally, supports non pre-activation bottleneck for use as a backbone for Vision Transfomers (ViT) and\nextra padding support to allow porting of official Hybrid ResNet pretrained weights from\nhttps://github.com/google-research/vision_transformer\n\nThanks to the Google team for the above two repositories and associated papers:\n* Big Transfer (BiT): General Visual Representation Learning - https://arxiv.org/abs/1912.11370\n* An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - https://arxiv.org/abs/2010.11929\n* Knowledge distillation: A good teacher is patient and consistent - https://arxiv.org/abs/2106.05237\n\nOriginal copyright of Google code below, modifications by Ross Wightman, Copyright 2020.\n\"\"\"\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import OrderedDict  # pylint: disable=g-importing-member\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import GroupNormAct, BatchNormAct2d, EvoNorm2dS0, FilterResponseNormTlu2d, ClassifierHead, \\\n    DropPath, AvgPool2dSame, create_pool2d, StdConv2d, create_conv2d, get_act_layer, get_norm_act_layer, make_divisible\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq, named_apply, adapt_input_conv\nfrom ._registry import generate_default_cfgs, register_model, register_model_deprecations\n\n__all__ = ['ResNetV2']  # model_registry will add each entrypoint fn to this\n\n\n\nclass PreActBottleneck(nn.Module):\n    \"\"\"Pre-activation (v2) bottleneck block.\n\n    Follows the implementation of \"Identity Mappings in Deep Residual Networks\":\n    https://github.com/KaimingHe/resnet-1k-layers/blob/master/resnet-pre-act.lua\n\n    Except it puts the stride on 3x3 conv when available.\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chs,\n            out_chs=None,\n            bottle_ratio=0.25,\n            stride=1,\n            dilation=1,\n            first_dilation=None,\n            groups=1,\n            act_layer=None,\n            conv_layer=None,\n            norm_layer=None,\n            proj_layer=None,\n            drop_path_rate=0.,\n    ):\n        super().__init__()\n        first_dilation = first_dilation or dilation\n        conv_layer = conv_layer or StdConv2d\n        norm_layer = norm_layer or partial(GroupNormAct, num_groups=32)\n        out_chs = out_chs or in_chs\n        mid_chs = make_divisible(out_chs * bottle_ratio)\n\n        if proj_layer is not None:\n            self.downsample = proj_layer(\n                in_chs, out_chs, stride=stride, dilation=dilation, first_dilation=first_dilation, preact=True,\n                conv_layer=conv_layer, norm_layer=norm_layer)\n        else:\n            self.downsample = None\n\n        self.norm1 = norm_layer(in_chs)\n        self.conv1 = conv_layer(in_chs, mid_chs, 1)\n        self.norm2 = norm_layer(mid_chs)\n        self.conv2 = conv_layer(mid_chs, mid_chs, 3, stride=stride, dilation=first_dilation, groups=groups)\n        self.norm3 = norm_layer(mid_chs)\n        self.conv3 = conv_layer(mid_chs, out_chs, 1)\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()\n\n    def zero_init_last(self):\n        nn.init.zeros_(self.conv3.weight)\n\n    def forward(self, x):\n        x_preact = self.norm1(x)\n\n        # shortcut branch\n        shortcut = x\n        if self.downsample is not None:\n            shortcut = self.downsample(x_preact)\n\n        # residual branch\n        x = self.conv1(x_preact)\n        x = self.conv2(self.norm2(x))\n        x = self.conv3(self.norm3(x))\n        x = self.drop_path(x)\n        return x + shortcut\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"Non Pre-activation bottleneck block, equiv to V1.5/V1b Bottleneck. Used for ViT.\n    \"\"\"\n    def __init__(\n            self,\n            in_chs,\n            out_chs=None,\n            bottle_ratio=0.25,\n            stride=1,\n            dilation=1,\n            first_dilation=None,\n            groups=1,\n            act_layer=None,\n            conv_layer=None,\n            norm_layer=None,\n            proj_layer=None,\n            drop_path_rate=0.,\n    ):\n        super().__init__()\n        first_dilation = first_dilation or dilation\n        act_layer = act_layer or nn.ReLU\n        conv_layer = conv_layer or StdConv2d\n        norm_layer = norm_layer or partial(GroupNormAct, num_groups=32)\n        out_chs = out_chs or in_chs\n        mid_chs = make_divisible(out_chs * bottle_ratio)\n\n        if proj_layer is not None:\n            self.downsample = proj_layer(\n                in_chs, out_chs, stride=stride, dilation=dilation, preact=False,\n                conv_layer=conv_layer, norm_layer=norm_layer)\n        else:\n            self.downsample = None\n\n        self.conv1 = conv_layer(in_chs, mid_chs, 1)\n        self.norm1 = norm_layer(mid_chs)\n        self.conv2 = conv_layer(mid_chs, mid_chs, 3, stride=stride, dilation=first_dilation, groups=groups)\n        self.norm2 = norm_layer(mid_chs)\n        self.conv3 = conv_layer(mid_chs, out_chs, 1)\n        self.norm3 = norm_layer(out_chs, apply_act=False)\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()\n        self.act3 = act_layer(inplace=True)\n\n    def zero_init_last(self):\n        if getattr(self.norm3, 'weight', None) is not None:\n            nn.init.zeros_(self.norm3.weight)\n\n    def forward(self, x):\n        # shortcut branch\n        shortcut = x\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        # residual\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.conv2(x)\n        x = self.norm2(x)\n        x = self.conv3(x)\n        x = self.norm3(x)\n        x = self.drop_path(x)\n        x = self.act3(x + shortcut)\n        return x\n\n\nclass DownsampleConv(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            stride=1,\n            dilation=1,\n            first_dilation=None,\n            preact=True,\n            conv_layer=None,\n            norm_layer=None,\n    ):\n        super(DownsampleConv, self).__init__()\n        self.conv = conv_layer(in_chs, out_chs, 1, stride=stride)\n        self.norm = nn.Identity() if preact else norm_layer(out_chs, apply_act=False)\n\n    def forward(self, x):\n        return self.norm(self.conv(x))\n\n\nclass DownsampleAvg(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            stride=1,\n            dilation=1,\n            first_dilation=None,\n            preact=True,\n            conv_layer=None,\n            norm_layer=None,\n    ):\n        \"\"\" AvgPool Downsampling as in 'D' ResNet variants. This is not in RegNet space but I might experiment.\"\"\"\n        super(DownsampleAvg, self).__init__()\n        avg_stride = stride if dilation == 1 else 1\n        if stride > 1 or dilation > 1:\n            avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d\n            self.pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)\n        else:\n            self.pool = nn.Identity()\n        self.conv = conv_layer(in_chs, out_chs, 1, stride=1)\n        self.norm = nn.Identity() if preact else norm_layer(out_chs, apply_act=False)\n\n    def forward(self, x):\n        return self.norm(self.conv(self.pool(x)))\n\n\nclass ResNetStage(nn.Module):\n    \"\"\"ResNet Stage.\"\"\"\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            stride,\n            dilation,\n            depth,\n            bottle_ratio=0.25,\n            groups=1,\n            avg_down=False,\n            block_dpr=None,\n            block_fn=PreActBottleneck,\n            act_layer=None,\n            conv_layer=None,\n            norm_layer=None,\n            **block_kwargs,\n    ):\n        super(ResNetStage, self).__init__()\n        first_dilation = 1 if dilation in (1, 2) else 2\n        layer_kwargs = dict(act_layer=act_layer, conv_layer=conv_layer, norm_layer=norm_layer)\n        proj_layer = DownsampleAvg if avg_down else DownsampleConv\n        prev_chs = in_chs\n        self.blocks = nn.Sequential()\n        for block_idx in range(depth):\n            drop_path_rate = block_dpr[block_idx] if block_dpr else 0.\n            stride = stride if block_idx == 0 else 1\n            self.blocks.add_module(str(block_idx), block_fn(\n                prev_chs,\n                out_chs,\n                stride=stride,\n                dilation=dilation,\n                bottle_ratio=bottle_ratio,\n                groups=groups,\n                first_dilation=first_dilation,\n                proj_layer=proj_layer,\n                drop_path_rate=drop_path_rate,\n                **layer_kwargs,\n                **block_kwargs,\n            ))\n            prev_chs = out_chs\n            first_dilation = dilation\n            proj_layer = None\n\n    def forward(self, x):\n        x = self.blocks(x)\n        return x\n\n\ndef is_stem_deep(stem_type):\n    return any([s in stem_type for s in ('deep', 'tiered')])\n\n\ndef create_resnetv2_stem(\n        in_chs,\n        out_chs=64,\n        stem_type='',\n        preact=True,\n        conv_layer=StdConv2d,\n        norm_layer=partial(GroupNormAct, num_groups=32),\n):\n    stem = OrderedDict()\n    assert stem_type in ('', 'fixed', 'same', 'deep', 'deep_fixed', 'deep_same', 'tiered')\n\n    # NOTE conv padding mode can be changed by overriding the conv_layer def\n    if is_stem_deep(stem_type):\n        # A 3 deep 3x3  conv stack as in ResNet V1D models\n        if 'tiered' in stem_type:\n            stem_chs = (3 * out_chs // 8, out_chs // 2)  # 'T' resnets in resnet.py\n        else:\n            stem_chs = (out_chs // 2, out_chs // 2)  # 'D' ResNets\n        stem['conv1'] = conv_layer(in_chs, stem_chs[0], kernel_size=3, stride=2)\n        stem['norm1'] = norm_layer(stem_chs[0])\n        stem['conv2'] = conv_layer(stem_chs[0], stem_chs[1], kernel_size=3, stride=1)\n        stem['norm2'] = norm_layer(stem_chs[1])\n        stem['conv3'] = conv_layer(stem_chs[1], out_chs, kernel_size=3, stride=1)\n        if not preact:\n            stem['norm3'] = norm_layer(out_chs)\n    else:\n        # The usual 7x7 stem conv\n        stem['conv'] = conv_layer(in_chs, out_chs, kernel_size=7, stride=2)\n        if not preact:\n            stem['norm'] = norm_layer(out_chs)\n\n    if 'fixed' in stem_type:\n        # 'fixed' SAME padding approximation that is used in BiT models\n        stem['pad'] = nn.ConstantPad2d(1, 0.)\n        stem['pool'] = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n    elif 'same' in stem_type:\n        # full, input size based 'SAME' padding, used in ViT Hybrid model\n        stem['pool'] = create_pool2d('max', kernel_size=3, stride=2, padding='same')\n    else:\n        # the usual PyTorch symmetric padding\n        stem['pool'] = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    return nn.Sequential(stem)\n\n\nclass ResNetV2(nn.Module):\n    \"\"\"Implementation of Pre-activation (v2) ResNet mode.\n    \"\"\"\n\n    def __init__(\n            self,\n            layers,\n            channels=(256, 512, 1024, 2048),\n            num_classes=1000,\n            in_chans=3,\n            global_pool='avg',\n            output_stride=32,\n            width_factor=1,\n            stem_chs=64,\n            stem_type='',\n            avg_down=False,\n            preact=True,\n            act_layer=nn.ReLU,\n            norm_layer=partial(GroupNormAct, num_groups=32),\n            conv_layer=StdConv2d,\n            drop_rate=0.,\n            drop_path_rate=0.,\n            zero_init_last=False,\n    ):\n        \"\"\"\n        Args:\n            layers (List[int]) : number of layers in each block\n            channels (List[int]) : number of channels in each block:\n            num_classes (int): number of classification classes (default 1000)\n            in_chans (int): number of input (color) channels. (default 3)\n            global_pool (str): Global pooling type. One of 'avg', 'max', 'avgmax', 'catavgmax' (default 'avg')\n            output_stride (int): output stride of the network, 32, 16, or 8. (default 32)\n            width_factor (int): channel (width) multiplication factor\n            stem_chs (int): stem width (default: 64)\n            stem_type (str): stem type (default: '' == 7x7)\n            avg_down (bool): average pooling in residual downsampling (default: False)\n            preact (bool): pre-activiation (default: True)\n            act_layer (Union[str, nn.Module]): activation layer\n            norm_layer (Union[str, nn.Module]): normalization layer\n            conv_layer (nn.Module): convolution module\n            drop_rate: classifier dropout rate (default: 0.)\n            drop_path_rate: stochastic depth rate (default: 0.)\n            zero_init_last: zero-init last weight in residual path (default: False)\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        wf = width_factor\n        norm_layer = get_norm_act_layer(norm_layer, act_layer=act_layer)\n        act_layer = get_act_layer(act_layer)\n\n        self.feature_info = []\n        stem_chs = make_divisible(stem_chs * wf)\n        self.stem = create_resnetv2_stem(\n            in_chans,\n            stem_chs,\n            stem_type,\n            preact,\n            conv_layer=conv_layer,\n            norm_layer=norm_layer,\n        )\n        stem_feat = ('stem.conv3' if is_stem_deep(stem_type) else 'stem.conv') if preact else 'stem.norm'\n        self.feature_info.append(dict(num_chs=stem_chs, reduction=2, module=stem_feat))\n\n        prev_chs = stem_chs\n        curr_stride = 4\n        dilation = 1\n        block_dprs = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(layers)).split(layers)]\n        block_fn = PreActBottleneck if preact else Bottleneck\n        self.stages = nn.Sequential()\n        for stage_idx, (d, c, bdpr) in enumerate(zip(layers, channels, block_dprs)):\n            out_chs = make_divisible(c * wf)\n            stride = 1 if stage_idx == 0 else 2\n            if curr_stride >= output_stride:\n                dilation *= stride\n                stride = 1\n            stage = ResNetStage(\n                prev_chs,\n                out_chs,\n                stride=stride,\n                dilation=dilation,\n                depth=d,\n                avg_down=avg_down,\n                act_layer=act_layer,\n                conv_layer=conv_layer,\n                norm_layer=norm_layer,\n                block_dpr=bdpr,\n                block_fn=block_fn,\n            )\n            prev_chs = out_chs\n            curr_stride *= stride\n            self.feature_info += [dict(num_chs=prev_chs, reduction=curr_stride, module=f'stages.{stage_idx}')]\n            self.stages.add_module(str(stage_idx), stage)\n\n        self.num_features = prev_chs\n        self.norm = norm_layer(self.num_features) if preact else nn.Identity()\n        self.head = ClassifierHead(\n            self.num_features,\n            num_classes,\n            pool_type=global_pool,\n            drop_rate=self.drop_rate,\n            use_conv=True,\n        )\n\n        self.init_weights(zero_init_last=zero_init_last)\n        self.grad_checkpointing = False\n\n    @torch.jit.ignore\n    def init_weights(self, zero_init_last=True):\n        named_apply(partial(_init_weights, zero_init_last=zero_init_last), self)\n\n    @torch.jit.ignore()\n    def load_pretrained(self, checkpoint_path, prefix='resnet/'):\n        _load_weights(self, checkpoint_path, prefix)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^stem',\n            blocks=r'^stages\\.(\\d+)' if coarse else [\n                (r'^stages\\.(\\d+)\\.blocks\\.(\\d+)', None),\n                (r'^norm', (99999,))\n            ]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.head.reset(num_classes, global_pool)\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.stages, x, flatten=True)\n        else:\n            x = self.stages(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=pre_logits)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _init_weights(module: nn.Module, name: str = '', zero_init_last=True):\n    if isinstance(module, nn.Linear) or ('head.fc' in name and isinstance(module, nn.Conv2d)):\n        nn.init.normal_(module.weight, mean=0.0, std=0.01)\n        nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Conv2d):\n        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, (nn.BatchNorm2d, nn.LayerNorm, nn.GroupNorm)):\n        nn.init.ones_(module.weight)\n        nn.init.zeros_(module.bias)\n    elif zero_init_last and hasattr(module, 'zero_init_last'):\n        module.zero_init_last()\n\n\n@torch.no_grad()\ndef _load_weights(model: nn.Module, checkpoint_path: str, prefix: str = 'resnet/'):\n    import numpy as np\n\n    def t2p(conv_weights):\n        \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n        if conv_weights.ndim == 4:\n            conv_weights = conv_weights.transpose([3, 2, 0, 1])\n        return torch.from_numpy(conv_weights)\n\n    weights = np.load(checkpoint_path)\n    stem_conv_w = adapt_input_conv(\n        model.stem.conv.weight.shape[1], t2p(weights[f'{prefix}root_block/standardized_conv2d/kernel']))\n    model.stem.conv.weight.copy_(stem_conv_w)\n    model.norm.weight.copy_(t2p(weights[f'{prefix}group_norm/gamma']))\n    model.norm.bias.copy_(t2p(weights[f'{prefix}group_norm/beta']))\n    if isinstance(getattr(model.head, 'fc', None), nn.Conv2d) and \\\n            model.head.fc.weight.shape[0] == weights[f'{prefix}head/conv2d/kernel'].shape[-1]:\n        model.head.fc.weight.copy_(t2p(weights[f'{prefix}head/conv2d/kernel']))\n        model.head.fc.bias.copy_(t2p(weights[f'{prefix}head/conv2d/bias']))\n    for i, (sname, stage) in enumerate(model.stages.named_children()):\n        for j, (bname, block) in enumerate(stage.blocks.named_children()):\n            cname = 'standardized_conv2d'\n            block_prefix = f'{prefix}block{i + 1}/unit{j + 1:02d}/'\n            block.conv1.weight.copy_(t2p(weights[f'{block_prefix}a/{cname}/kernel']))\n            block.conv2.weight.copy_(t2p(weights[f'{block_prefix}b/{cname}/kernel']))\n            block.conv3.weight.copy_(t2p(weights[f'{block_prefix}c/{cname}/kernel']))\n            block.norm1.weight.copy_(t2p(weights[f'{block_prefix}a/group_norm/gamma']))\n            block.norm2.weight.copy_(t2p(weights[f'{block_prefix}b/group_norm/gamma']))\n            block.norm3.weight.copy_(t2p(weights[f'{block_prefix}c/group_norm/gamma']))\n            block.norm1.bias.copy_(t2p(weights[f'{block_prefix}a/group_norm/beta']))\n            block.norm2.bias.copy_(t2p(weights[f'{block_prefix}b/group_norm/beta']))\n            block.norm3.bias.copy_(t2p(weights[f'{block_prefix}c/group_norm/beta']))\n            if block.downsample is not None:\n                w = weights[f'{block_prefix}a/proj/{cname}/kernel']\n                block.downsample.conv.weight.copy_(t2p(w))\n\n\ndef _create_resnetv2(variant, pretrained=False, **kwargs):\n    feature_cfg = dict(flatten_sequential=True)\n    return build_model_with_cfg(\n        ResNetV2, variant, pretrained,\n        feature_cfg=feature_cfg,\n        **kwargs,\n    )\n\n\ndef _create_resnetv2_bit(variant, pretrained=False, **kwargs):\n    return _create_resnetv2(\n        variant,\n        pretrained=pretrained,\n        stem_type='fixed',\n        conv_layer=partial(StdConv2d, eps=1e-8),\n        **kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bilinear',\n        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,\n        'first_conv': 'stem.conv', 'classifier': 'head.fc',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    #  Paper: Knowledge distillation: A good teacher is patient and consistent - https://arxiv.org/abs/2106.05237\n    'resnetv2_50x1_bit.goog_distilled_in1k': _cfg(\n        hf_hub_id='timm/',\n        interpolation='bicubic', custom_load=True),\n    'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        interpolation='bicubic', custom_load=True),\n    'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, interpolation='bicubic', custom_load=True),\n\n    # pretrained on imagenet21k, finetuned on imagenet1k\n    'resnetv2_50x1_bit.goog_in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, custom_load=True),\n    'resnetv2_50x3_bit.goog_in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, custom_load=True),\n    'resnetv2_101x1_bit.goog_in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, custom_load=True),\n    'resnetv2_101x3_bit.goog_in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, custom_load=True),\n    'resnetv2_152x2_bit.goog_in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, custom_load=True),\n    'resnetv2_152x4_bit.goog_in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 480, 480), pool_size=(15, 15), crop_pct=1.0, custom_load=True),  # only one at 480x480?\n\n    # trained on imagenet-21k\n    'resnetv2_50x1_bit.goog_in21k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=21843, custom_load=True),\n    'resnetv2_50x3_bit.goog_in21k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=21843, custom_load=True),\n    'resnetv2_101x1_bit.goog_in21k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=21843, custom_load=True),\n    'resnetv2_101x3_bit.goog_in21k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=21843, custom_load=True),\n    'resnetv2_152x2_bit.goog_in21k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=21843, custom_load=True),\n    'resnetv2_152x4_bit.goog_in21k': _cfg(\n        hf_hub_id='timm/',\n        num_classes=21843, custom_load=True),\n\n    'resnetv2_50.a1h_in1k': _cfg(\n        hf_hub_id='timm/',\n        interpolation='bicubic', crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'resnetv2_50d.untrained': _cfg(\n        interpolation='bicubic', first_conv='stem.conv1'),\n    'resnetv2_50t.untrained': _cfg(\n        interpolation='bicubic', first_conv='stem.conv1'),\n    'resnetv2_101.a1h_in1k': _cfg(\n        hf_hub_id='timm/',\n        interpolation='bicubic', crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'resnetv2_101d.untrained': _cfg(\n        interpolation='bicubic', first_conv='stem.conv1'),\n    'resnetv2_152.untrained': _cfg(\n        interpolation='bicubic'),\n    'resnetv2_152d.untrained': _cfg(\n        interpolation='bicubic', first_conv='stem.conv1'),\n\n    'resnetv2_50d_gn.ah_in1k': _cfg(\n        hf_hub_id='timm/',\n        interpolation='bicubic', first_conv='stem.conv1',\n        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'resnetv2_50d_evos.ah_in1k': _cfg(\n        hf_hub_id='timm/',\n        interpolation='bicubic', first_conv='stem.conv1',\n        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'resnetv2_50d_frn.untrained': _cfg(\n        interpolation='bicubic', first_conv='stem.conv1'),\n})\n\n\n@register_model\ndef resnetv2_50x1_bit(pretrained=False, **kwargs) -> ResNetV2:\n    return _create_resnetv2_bit(\n        'resnetv2_50x1_bit', pretrained=pretrained, layers=[3, 4, 6, 3], width_factor=1, **kwargs)\n\n\n@register_model\ndef resnetv2_50x3_bit(pretrained=False, **kwargs) -> ResNetV2:\n    return _create_resnetv2_bit(\n        'resnetv2_50x3_bit', pretrained=pretrained, layers=[3, 4, 6, 3], width_factor=3, **kwargs)\n\n\n@register_model\ndef resnetv2_101x1_bit(pretrained=False, **kwargs) -> ResNetV2:\n    return _create_resnetv2_bit(\n        'resnetv2_101x1_bit', pretrained=pretrained, layers=[3, 4, 23, 3], width_factor=1, **kwargs)\n\n\n@register_model\ndef resnetv2_101x3_bit(pretrained=False, **kwargs) -> ResNetV2:\n    return _create_resnetv2_bit(\n        'resnetv2_101x3_bit', pretrained=pretrained, layers=[3, 4, 23, 3], width_factor=3, **kwargs)\n\n\n@register_model\ndef resnetv2_152x2_bit(pretrained=False, **kwargs) -> ResNetV2:\n    return _create_resnetv2_bit(\n        'resnetv2_152x2_bit', pretrained=pretrained, layers=[3, 8, 36, 3], width_factor=2, **kwargs)\n\n\n@register_model\ndef resnetv2_152x4_bit(pretrained=False, **kwargs) -> ResNetV2:\n    return _create_resnetv2_bit(\n        'resnetv2_152x4_bit', pretrained=pretrained, layers=[3, 8, 36, 3], width_factor=4, **kwargs)\n\n\n@register_model\ndef resnetv2_50(pretrained=False, **kwargs) -> ResNetV2:\n    model_args = dict(layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d)\n    return _create_resnetv2('resnetv2_50', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetv2_50d(pretrained=False, **kwargs) -> ResNetV2:\n    model_args = dict(\n        layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d,\n        stem_type='deep', avg_down=True)\n    return _create_resnetv2('resnetv2_50d', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetv2_50t(pretrained=False, **kwargs) -> ResNetV2:\n    model_args = dict(\n        layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d,\n        stem_type='tiered', avg_down=True)\n    return _create_resnetv2('resnetv2_50t', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetv2_101(pretrained=False, **kwargs) -> ResNetV2:\n    model_args = dict(layers=[3, 4, 23, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d)\n    return _create_resnetv2('resnetv2_101', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetv2_101d(pretrained=False, **kwargs) -> ResNetV2:\n    model_args = dict(\n        layers=[3, 4, 23, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d,\n        stem_type='deep', avg_down=True)\n    return _create_resnetv2('resnetv2_101d', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetv2_152(pretrained=False, **kwargs) -> ResNetV2:\n    model_args = dict(layers=[3, 8, 36, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d)\n    return _create_resnetv2('resnetv2_152', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetv2_152d(pretrained=False, **kwargs) -> ResNetV2:\n    model_args = dict(\n        layers=[3, 8, 36, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d,\n        stem_type='deep', avg_down=True)\n    return _create_resnetv2('resnetv2_152d', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n# Experimental configs (may change / be removed)\n\n@register_model\ndef resnetv2_50d_gn(pretrained=False, **kwargs) -> ResNetV2:\n    model_args = dict(\n        layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=GroupNormAct,\n        stem_type='deep', avg_down=True)\n    return _create_resnetv2('resnetv2_50d_gn', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetv2_50d_evos(pretrained=False, **kwargs) -> ResNetV2:\n    model_args = dict(\n        layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=EvoNorm2dS0,\n        stem_type='deep', avg_down=True)\n    return _create_resnetv2('resnetv2_50d_evos', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef resnetv2_50d_frn(pretrained=False, **kwargs) -> ResNetV2:\n    model_args = dict(\n        layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=FilterResponseNormTlu2d,\n        stem_type='deep', avg_down=True)\n    return _create_resnetv2('resnetv2_50d_frn', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\nregister_model_deprecations(__name__, {\n    'resnetv2_50x1_bitm': 'resnetv2_50x1_bit.goog_in21k_ft_in1k',\n    'resnetv2_50x3_bitm': 'resnetv2_50x3_bit.goog_in21k_ft_in1k',\n    'resnetv2_101x1_bitm': 'resnetv2_101x1_bit.goog_in21k_ft_in1k',\n    'resnetv2_101x3_bitm': 'resnetv2_101x3_bit.goog_in21k_ft_in1k',\n    'resnetv2_152x2_bitm': 'resnetv2_152x2_bit.goog_in21k_ft_in1k',\n    'resnetv2_152x4_bitm': 'resnetv2_152x4_bit.goog_in21k_ft_in1k',\n    'resnetv2_50x1_bitm_in21k': 'resnetv2_50x1_bit.goog_in21k',\n    'resnetv2_50x3_bitm_in21k': 'resnetv2_50x3_bit.goog_in21k',\n    'resnetv2_101x1_bitm_in21k': 'resnetv2_101x1_bit.goog_in21k',\n    'resnetv2_101x3_bitm_in21k': 'resnetv2_101x3_bit.goog_in21k',\n    'resnetv2_152x2_bitm_in21k': 'resnetv2_152x2_bit.goog_in21k',\n    'resnetv2_152x4_bitm_in21k': 'resnetv2_152x4_bit.goog_in21k',\n    'resnetv2_50x1_bit_distilled': 'resnetv2_50x1_bit.goog_distilled_in1k',\n    'resnetv2_152x2_bit_teacher': 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k',\n    'resnetv2_152x2_bit_teacher_384': 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384',\n})\n",
  "\"\"\" Pyramid Vision Transformer v2\n\n@misc{wang2021pvtv2,\n      title={PVTv2: Improved Baselines with Pyramid Vision Transformer},\n      author={Wenhai Wang and Enze Xie and Xiang Li and Deng-Ping Fan and Kaitao Song and Ding Liang and\n        Tong Lu and Ping Luo and Ling Shao},\n      year={2021},\n      eprint={2106.13797},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\nBased on Apache 2.0 licensed code at https://github.com/whai362/PVT\n\nModifications and timm support by / Copyright 2022, Ross Wightman\n\"\"\"\n\nimport math\nfrom typing import Tuple, List, Callable, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import DropPath, to_2tuple, to_ntuple, trunc_normal_, LayerNorm, use_fused_attn\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['PyramidVisionTransformerV2']\n\n\nclass MlpWithDepthwiseConv(nn.Module):\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            drop=0.,\n            extra_relu=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.relu = nn.ReLU() if extra_relu else nn.Identity()\n        self.dwconv = nn.Conv2d(hidden_features, hidden_features, 3, 1, 1, bias=True, groups=hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x, feat_size: List[int]):\n        x = self.fc1(x)\n        B, N, C = x.shape\n        x = x.transpose(1, 2).view(B, C, feat_size[0], feat_size[1])\n        x = self.relu(x)\n        x = self.dwconv(x)\n        x = x.flatten(2).transpose(1, 2)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    fused_attn: torch.jit.Final[bool]\n\n    def __init__(\n            self,\n            dim,\n            num_heads=8,\n            sr_ratio=1,\n            linear_attn=False,\n            qkv_bias=True,\n            attn_drop=0.,\n            proj_drop=0.\n    ):\n        super().__init__()\n        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n\n        self.dim = dim\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.fused_attn = use_fused_attn()\n\n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        if not linear_attn:\n            self.pool = None\n            if sr_ratio > 1:\n                self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n                self.norm = nn.LayerNorm(dim)\n            else:\n                self.sr = None\n                self.norm = None\n            self.act = None\n        else:\n            self.pool = nn.AdaptiveAvgPool2d(7)\n            self.sr = nn.Conv2d(dim, dim, kernel_size=1, stride=1)\n            self.norm = nn.LayerNorm(dim)\n            self.act = nn.GELU()\n\n    def forward(self, x, feat_size: List[int]):\n        B, N, C = x.shape\n        H, W = feat_size\n        q = self.q(x).reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n\n        if self.pool is not None:\n            x = x.permute(0, 2, 1).reshape(B, C, H, W)\n            x = self.sr(self.pool(x)).reshape(B, C, -1).permute(0, 2, 1)\n            x = self.norm(x)\n            x = self.act(x)\n            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        else:\n            if self.sr is not None:\n                x = x.permute(0, 2, 1).reshape(B, C, H, W)\n                x = self.sr(x).reshape(B, C, -1).permute(0, 2, 1)\n                x = self.norm(x)\n                kv = self.kv(x).reshape(B, -1, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n            else:\n                kv = self.kv(x).reshape(B, -1, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        k, v = kv.unbind(0)\n\n        if self.fused_attn:\n            x = F.scaled_dot_product_attention(q, k, v, dropout_p=self.attn_drop.p)\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            x = attn @ v\n\n        x = x.transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            sr_ratio=1,\n            linear_attn=False,\n            qkv_bias=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=LayerNorm,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            sr_ratio=sr_ratio,\n            linear_attn=linear_attn,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n        )\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        self.mlp = MlpWithDepthwiseConv(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            drop=proj_drop,\n            extra_relu=linear_attn,\n        )\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x, feat_size: List[int]):\n        x = x + self.drop_path1(self.attn(self.norm1(x), feat_size))\n        x = x + self.drop_path2(self.mlp(self.norm2(x), feat_size))\n\n        return x\n\n\nclass OverlapPatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n        super().__init__()\n        patch_size = to_2tuple(patch_size)\n        assert max(patch_size) > stride, \"Set larger patch_size than stride\"\n        self.patch_size = patch_size\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, patch_size,\n            stride=stride, padding=(patch_size[0] // 2, patch_size[1] // 2))\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        x = self.proj(x)\n        x = x.permute(0, 2, 3, 1)\n        x = self.norm(x)\n        return x\n\n\nclass PyramidVisionTransformerStage(nn.Module):\n    def __init__(\n            self,\n            dim: int,\n            dim_out: int,\n            depth: int,\n            downsample: bool = True,\n            num_heads: int = 8,\n            sr_ratio: int = 1,\n            linear_attn: bool = False,\n            mlp_ratio: float = 4.0,\n            qkv_bias: bool = True,\n            proj_drop: float = 0.,\n            attn_drop: float = 0.,\n            drop_path: Union[List[float], float] = 0.0,\n            norm_layer: Callable = LayerNorm,\n    ):\n        super().__init__()\n        self.grad_checkpointing = False\n\n        if downsample:\n            self.downsample = OverlapPatchEmbed(\n                patch_size=3,\n                stride=2,\n                in_chans=dim,\n                embed_dim=dim_out,\n            )\n        else:\n            assert dim == dim_out\n            self.downsample = None\n\n        self.blocks = nn.ModuleList([Block(\n            dim=dim_out,\n            num_heads=num_heads,\n            sr_ratio=sr_ratio,\n            linear_attn=linear_attn,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            proj_drop=proj_drop,\n            attn_drop=attn_drop,\n            drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n            norm_layer=norm_layer,\n        ) for i in range(depth)])\n\n        self.norm = norm_layer(dim_out)\n\n    def forward(self, x):\n        # x is either B, C, H, W (if downsample) or B, H, W, C if not\n        if self.downsample is not None:\n            # input to downsample is B, C, H, W\n            x = self.downsample(x)  # output B, H, W, C\n        B, H, W, C = x.shape\n        feat_size = (H, W)\n        x = x.reshape(B, -1, C)\n        for blk in self.blocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint.checkpoint(blk, x, feat_size)\n            else:\n                x = blk(x, feat_size)\n        x = self.norm(x)\n        x = x.reshape(B, feat_size[0], feat_size[1], -1).permute(0, 3, 1, 2).contiguous()\n        return x\n\n\nclass PyramidVisionTransformerV2(nn.Module):\n    def __init__(\n            self,\n            in_chans=3,\n            num_classes=1000,\n            global_pool='avg',\n            depths=(3, 4, 6, 3),\n            embed_dims=(64, 128, 256, 512),\n            num_heads=(1, 2, 4, 8),\n            sr_ratios=(8, 4, 2, 1),\n            mlp_ratios=(8., 8., 4., 4.),\n            qkv_bias=True,\n            linear=False,\n            drop_rate=0.,\n            proj_drop_rate=0.,\n            attn_drop_rate=0.,\n            drop_path_rate=0.,\n            norm_layer=LayerNorm,\n    ):\n        super().__init__()\n        self.num_classes = num_classes\n        assert global_pool in ('avg', '')\n        self.global_pool = global_pool\n        self.depths = depths\n        num_stages = len(depths)\n        mlp_ratios = to_ntuple(num_stages)(mlp_ratios)\n        num_heads = to_ntuple(num_stages)(num_heads)\n        sr_ratios = to_ntuple(num_stages)(sr_ratios)\n        assert(len(embed_dims)) == num_stages\n        self.feature_info = []\n\n        self.patch_embed = OverlapPatchEmbed(\n            patch_size=7,\n            stride=4,\n            in_chans=in_chans,\n            embed_dim=embed_dims[0],\n        )\n\n        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n        cur = 0\n        prev_dim = embed_dims[0]\n        stages = []\n        for i in range(num_stages):\n            stages += [PyramidVisionTransformerStage(\n                dim=prev_dim,\n                dim_out=embed_dims[i],\n                depth=depths[i],\n                downsample=i > 0,\n                num_heads=num_heads[i],\n                sr_ratio=sr_ratios[i],\n                mlp_ratio=mlp_ratios[i],\n                linear_attn=linear,\n                qkv_bias=qkv_bias,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n            )]\n            prev_dim = embed_dims[i]\n            cur += depths[i]\n            self.feature_info += [dict(num_chs=prev_dim, reduction=4 * 2**i, module=f'stages.{i}')]\n        self.stages = nn.Sequential(*stages)\n\n        # classification head\n        self.num_features = embed_dims[-1]\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def freeze_patch_emb(self):\n        self.patch_embed.requires_grad = False\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^patch_embed',  # stem and embed\n            blocks=r'^stages\\.(\\d+)'\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        for s in self.stages:\n            s.grad_checkpointing = enable\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('avg', '')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = self.stages(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool:\n            x = x.mean(dim=(-1, -2))\n        x = self.head_drop(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _checkpoint_filter_fn(state_dict, model):\n    \"\"\" Remap original checkpoints -> timm \"\"\"\n    if 'patch_embed.proj.weight' in state_dict:\n        return state_dict  # non-original checkpoint, no remapping needed\n\n    out_dict = {}\n    import re\n    for k, v in state_dict.items():\n        if k.startswith('patch_embed'):\n            k = k.replace('patch_embed1', 'patch_embed')\n            k = k.replace('patch_embed2', 'stages.1.downsample')\n            k = k.replace('patch_embed3', 'stages.2.downsample')\n            k = k.replace('patch_embed4', 'stages.3.downsample')\n        k = k.replace('dwconv.dwconv', 'dwconv')\n        k = re.sub(r'block(\\d+).(\\d+)', lambda x: f'stages.{int(x.group(1)) - 1}.blocks.{x.group(2)}', k)\n        k = re.sub(r'^norm(\\d+)', lambda x: f'stages.{int(x.group(1)) - 1}.norm', k)\n        out_dict[k] = v\n    return out_dict\n\n\ndef _create_pvt2(variant, pretrained=False, **kwargs):\n    default_out_indices = tuple(range(4))\n    out_indices = kwargs.pop('out_indices', default_out_indices)\n    model = build_model_with_cfg(\n        PyramidVisionTransformerV2,\n        variant,\n        pretrained,\n        pretrained_filter_fn=_checkpoint_filter_fn,\n        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),\n        **kwargs,\n    )\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.9, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'patch_embed.proj', 'classifier': 'head', 'fixed_input_size': False,\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'pvt_v2_b0.in1k': _cfg(hf_hub_id='timm/'),\n    'pvt_v2_b1.in1k': _cfg(hf_hub_id='timm/'),\n    'pvt_v2_b2.in1k': _cfg(hf_hub_id='timm/'),\n    'pvt_v2_b3.in1k': _cfg(hf_hub_id='timm/'),\n    'pvt_v2_b4.in1k': _cfg(hf_hub_id='timm/'),\n    'pvt_v2_b5.in1k': _cfg(hf_hub_id='timm/'),\n    'pvt_v2_b2_li.in1k': _cfg(hf_hub_id='timm/'),\n})\n\n\n@register_model\ndef pvt_v2_b0(pretrained=False, **kwargs) -> PyramidVisionTransformerV2:\n    model_args = dict(depths=(2, 2, 2, 2), embed_dims=(32, 64, 160, 256), num_heads=(1, 2, 5, 8))\n    return _create_pvt2('pvt_v2_b0', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef pvt_v2_b1(pretrained=False, **kwargs) -> PyramidVisionTransformerV2:\n    model_args = dict(depths=(2, 2, 2, 2), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8))\n    return _create_pvt2('pvt_v2_b1', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef pvt_v2_b2(pretrained=False, **kwargs) -> PyramidVisionTransformerV2:\n    model_args = dict(depths=(3, 4, 6, 3), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8))\n    return _create_pvt2('pvt_v2_b2', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef pvt_v2_b3(pretrained=False, **kwargs) -> PyramidVisionTransformerV2:\n    model_args = dict(depths=(3, 4, 18, 3), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8))\n    return _create_pvt2('pvt_v2_b3', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef pvt_v2_b4(pretrained=False, **kwargs) -> PyramidVisionTransformerV2:\n    model_args = dict(depths=(3, 8, 27, 3), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8))\n    return _create_pvt2('pvt_v2_b4', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef pvt_v2_b5(pretrained=False, **kwargs) -> PyramidVisionTransformerV2:\n    model_args = dict(\n        depths=(3, 6, 40, 3), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8), mlp_ratios=(4, 4, 4, 4))\n    return _create_pvt2('pvt_v2_b5', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef pvt_v2_b2_li(pretrained=False, **kwargs) -> PyramidVisionTransformerV2:\n    model_args = dict(\n        depths=(3, 4, 6, 3), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8), linear=True)\n    return _create_pvt2('pvt_v2_b2_li', pretrained=pretrained, **dict(model_args, **kwargs))\n\n",
  "\"\"\" The EfficientNet Family in PyTorch\n\nAn implementation of EfficienNet that covers variety of related models with efficient architectures:\n\n* EfficientNet-V2\n  - `EfficientNetV2: Smaller Models and Faster Training` - https://arxiv.org/abs/2104.00298\n\n* EfficientNet (B0-B8, L2 + Tensorflow pretrained AutoAug/RandAug/AdvProp/NoisyStudent weight ports)\n  - EfficientNet: Rethinking Model Scaling for CNNs - https://arxiv.org/abs/1905.11946\n  - CondConv: Conditionally Parameterized Convolutions for Efficient Inference - https://arxiv.org/abs/1904.04971\n  - Adversarial Examples Improve Image Recognition - https://arxiv.org/abs/1911.09665\n  - Self-training with Noisy Student improves ImageNet classification - https://arxiv.org/abs/1911.04252\n\n* MixNet (Small, Medium, and Large)\n  - MixConv: Mixed Depthwise Convolutional Kernels - https://arxiv.org/abs/1907.09595\n\n* MNasNet B1, A1 (SE), Small\n  - MnasNet: Platform-Aware Neural Architecture Search for Mobile - https://arxiv.org/abs/1807.11626\n\n* FBNet-C\n  - FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable NAS - https://arxiv.org/abs/1812.03443\n\n* Single-Path NAS Pixel1\n  - Single-Path NAS: Designing Hardware-Efficient ConvNets - https://arxiv.org/abs/1904.02877\n\n* TinyNet\n    - Model Rubik's Cube: Twisting Resolution, Depth and Width for TinyNets - https://arxiv.org/abs/2010.14819\n    - Definitions & weights borrowed from https://github.com/huawei-noah/CV-Backbones/tree/master/tinynet_pytorch\n\n* And likely more...\n\nThe majority of the above models (EfficientNet*, MixNet, MnasNet) and original weights were made available\nby Mingxing Tan, Quoc Le, and other members of their Google Brain team. Thanks for consistently releasing\nthe models and weights open source!\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\nfrom functools import partial\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.checkpoint import checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import create_conv2d, create_classifier, get_norm_act_layer, GroupNormAct\nfrom ._builder import build_model_with_cfg, pretrained_cfg_for_features\nfrom ._efficientnet_blocks import SqueezeExcite\nfrom ._efficientnet_builder import EfficientNetBuilder, decode_arch_def, efficientnet_init_weights, \\\n    round_channels, resolve_bn_args, resolve_act_layer, BN_EPS_TF_DEFAULT\nfrom ._features import FeatureInfo, FeatureHooks\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import generate_default_cfgs, register_model, register_model_deprecations\n\n__all__ = ['EfficientNet', 'EfficientNetFeatures']\n\n\nclass EfficientNet(nn.Module):\n    \"\"\" EfficientNet\n\n    A flexible and performant PyTorch implementation of efficient network architectures, including:\n      * EfficientNet-V2 Small, Medium, Large, XL & B0-B3\n      * EfficientNet B0-B8, L2\n      * EfficientNet-EdgeTPU\n      * EfficientNet-CondConv\n      * MixNet S, M, L, XL\n      * MnasNet A1, B1, and small\n      * MobileNet-V2\n      * FBNet C\n      * Single-Path NAS Pixel1\n      * TinyNet\n    \"\"\"\n\n    def __init__(\n            self,\n            block_args,\n            num_classes=1000,\n            num_features=1280,\n            in_chans=3,\n            stem_size=32,\n            fix_stem=False,\n            output_stride=32,\n            pad_type='',\n            round_chs_fn=round_channels,\n            act_layer=None,\n            norm_layer=None,\n            se_layer=None,\n            drop_rate=0.,\n            drop_path_rate=0.,\n            global_pool='avg'\n    ):\n        super(EfficientNet, self).__init__()\n        act_layer = act_layer or nn.ReLU\n        norm_layer = norm_layer or nn.BatchNorm2d\n        norm_act_layer = get_norm_act_layer(norm_layer, act_layer)\n        se_layer = se_layer or SqueezeExcite\n        self.num_classes = num_classes\n        self.num_features = num_features\n        self.drop_rate = drop_rate\n        self.grad_checkpointing = False\n\n        # Stem\n        if not fix_stem:\n            stem_size = round_chs_fn(stem_size)\n        self.conv_stem = create_conv2d(in_chans, stem_size, 3, stride=2, padding=pad_type)\n        self.bn1 = norm_act_layer(stem_size, inplace=True)\n\n        # Middle stages (IR/ER/DS Blocks)\n        builder = EfficientNetBuilder(\n            output_stride=output_stride,\n            pad_type=pad_type,\n            round_chs_fn=round_chs_fn,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n            se_layer=se_layer,\n            drop_path_rate=drop_path_rate,\n        )\n        self.blocks = nn.Sequential(*builder(stem_size, block_args))\n        self.feature_info = builder.features\n        head_chs = builder.in_chs\n\n        # Head + Pooling\n        self.conv_head = create_conv2d(head_chs, self.num_features, 1, padding=pad_type)\n        self.bn2 = norm_act_layer(self.num_features, inplace=True)\n        self.global_pool, self.classifier = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool)\n\n        efficientnet_init_weights(self)\n\n    def as_sequential(self):\n        layers = [self.conv_stem, self.bn1]\n        layers.extend(self.blocks)\n        layers.extend([self.conv_head, self.bn2, self.global_pool])\n        layers.extend([nn.Dropout(self.drop_rate), self.classifier])\n        return nn.Sequential(*layers)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^conv_stem|bn1',\n            blocks=[\n                (r'^blocks\\.(\\d+)' if coarse else r'^blocks\\.(\\d+)\\.(\\d+)', None),\n                (r'conv_head|bn2', (99999,))\n            ]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.classifier\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.global_pool, self.classifier = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x, flatten=True)\n        else:\n            x = self.blocks(x)\n        x = self.conv_head(x)\n        x = self.bn2(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        x = self.global_pool(x)\n        if self.drop_rate > 0.:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        return x if pre_logits else self.classifier(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\nclass EfficientNetFeatures(nn.Module):\n    \"\"\" EfficientNet Feature Extractor\n\n    A work-in-progress feature extraction module for EfficientNet, to use as a backbone for segmentation\n    and object detection models.\n    \"\"\"\n\n    def __init__(\n            self,\n            block_args,\n            out_indices=(0, 1, 2, 3, 4),\n            feature_location='bottleneck',\n            in_chans=3,\n            stem_size=32,\n            fix_stem=False,\n            output_stride=32,\n            pad_type='',\n            round_chs_fn=round_channels,\n            act_layer=None,\n            norm_layer=None,\n            se_layer=None,\n            drop_rate=0.,\n            drop_path_rate=0.\n    ):\n        super(EfficientNetFeatures, self).__init__()\n        act_layer = act_layer or nn.ReLU\n        norm_layer = norm_layer or nn.BatchNorm2d\n        norm_act_layer = get_norm_act_layer(norm_layer, act_layer)\n        se_layer = se_layer or SqueezeExcite\n        self.drop_rate = drop_rate\n        self.grad_checkpointing = False\n\n        # Stem\n        if not fix_stem:\n            stem_size = round_chs_fn(stem_size)\n        self.conv_stem = create_conv2d(in_chans, stem_size, 3, stride=2, padding=pad_type)\n        self.bn1 = norm_act_layer(stem_size, inplace=True)\n\n        # Middle stages (IR/ER/DS Blocks)\n        builder = EfficientNetBuilder(\n            output_stride=output_stride,\n            pad_type=pad_type,\n            round_chs_fn=round_chs_fn,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n            se_layer=se_layer,\n            drop_path_rate=drop_path_rate,\n            feature_location=feature_location,\n        )\n        self.blocks = nn.Sequential(*builder(stem_size, block_args))\n        self.feature_info = FeatureInfo(builder.features, out_indices)\n        self._stage_out_idx = {f['stage']: f['index'] for f in self.feature_info.get_dicts()}\n\n        efficientnet_init_weights(self)\n\n        # Register feature extraction hooks with FeatureHooks helper\n        self.feature_hooks = None\n        if feature_location != 'bottleneck':\n            hooks = self.feature_info.get_dicts(keys=('module', 'hook_type'))\n            self.feature_hooks = FeatureHooks(hooks, self.named_modules())\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    def forward(self, x) -> List[torch.Tensor]:\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        if self.feature_hooks is None:\n            features = []\n            if 0 in self._stage_out_idx:\n                features.append(x)  # add stem out\n            for i, b in enumerate(self.blocks):\n                if self.grad_checkpointing and not torch.jit.is_scripting():\n                    x = checkpoint(b, x)\n                else:\n                    x = b(x)\n                if i + 1 in self._stage_out_idx:\n                    features.append(x)\n            return features\n        else:\n            self.blocks(x)\n            out = self.feature_hooks.get_output(x.device)\n            return list(out.values())\n\n\ndef _create_effnet(variant, pretrained=False, **kwargs):\n    features_mode = ''\n    model_cls = EfficientNet\n    kwargs_filter = None\n    if kwargs.pop('features_only', False):\n        if 'feature_cfg' in kwargs:\n            features_mode = 'cfg'\n        else:\n            kwargs_filter = ('num_classes', 'num_features', 'head_conv', 'global_pool')\n            model_cls = EfficientNetFeatures\n            features_mode = 'cls'\n\n    model = build_model_with_cfg(\n        model_cls,\n        variant,\n        pretrained,\n        features_only=features_mode == 'cfg',\n        pretrained_strict=features_mode != 'cls',\n        kwargs_filter=kwargs_filter,\n        **kwargs,\n    )\n    if features_mode == 'cls':\n        model.pretrained_cfg = model.default_cfg = pretrained_cfg_for_features(model.pretrained_cfg)\n    return model\n\n\ndef _gen_mnasnet_a1(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\"Creates a mnasnet-a1 model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet\n    Paper: https://arxiv.org/pdf/1807.11626.pdf.\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    \"\"\"\n    arch_def = [\n        # stage 0, 112x112 in\n        ['ds_r1_k3_s1_e1_c16_noskip'],\n        # stage 1, 112x112 in\n        ['ir_r2_k3_s2_e6_c24'],\n        # stage 2, 56x56 in\n        ['ir_r3_k5_s2_e3_c40_se0.25'],\n        # stage 3, 28x28 in\n        ['ir_r4_k3_s2_e6_c80'],\n        # stage 4, 14x14in\n        ['ir_r2_k3_s1_e6_c112_se0.25'],\n        # stage 5, 14x14in\n        ['ir_r3_k5_s2_e6_c160_se0.25'],\n        # stage 6, 7x7 in\n        ['ir_r1_k3_s1_e6_c320'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=32,\n        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        **kwargs\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_mnasnet_b1(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\"Creates a mnasnet-b1 model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet\n    Paper: https://arxiv.org/pdf/1807.11626.pdf.\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    \"\"\"\n    arch_def = [\n        # stage 0, 112x112 in\n        ['ds_r1_k3_s1_c16_noskip'],\n        # stage 1, 112x112 in\n        ['ir_r3_k3_s2_e3_c24'],\n        # stage 2, 56x56 in\n        ['ir_r3_k5_s2_e3_c40'],\n        # stage 3, 28x28 in\n        ['ir_r3_k5_s2_e6_c80'],\n        # stage 4, 14x14in\n        ['ir_r2_k3_s1_e6_c96'],\n        # stage 5, 14x14in\n        ['ir_r4_k5_s2_e6_c192'],\n        # stage 6, 7x7 in\n        ['ir_r1_k3_s1_e6_c320_noskip']\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=32,\n        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        **kwargs\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_mnasnet_small(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\"Creates a mnasnet-b1 model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet\n    Paper: https://arxiv.org/pdf/1807.11626.pdf.\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    \"\"\"\n    arch_def = [\n        ['ds_r1_k3_s1_c8'],\n        ['ir_r1_k3_s2_e3_c16'],\n        ['ir_r2_k3_s2_e6_c16'],\n        ['ir_r4_k5_s2_e6_c32_se0.25'],\n        ['ir_r3_k3_s1_e6_c32_se0.25'],\n        ['ir_r3_k5_s2_e6_c88_se0.25'],\n        ['ir_r1_k3_s1_e6_c144']\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=8,\n        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        **kwargs\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_mobilenet_v2(\n        variant, channel_multiplier=1.0, depth_multiplier=1.0, fix_stem_head=False, pretrained=False, **kwargs):\n    \"\"\" Generate MobileNet-V2 network\n    Ref impl: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py\n    Paper: https://arxiv.org/abs/1801.04381\n    \"\"\"\n    arch_def = [\n        ['ds_r1_k3_s1_c16'],\n        ['ir_r2_k3_s2_e6_c24'],\n        ['ir_r3_k3_s2_e6_c32'],\n        ['ir_r4_k3_s2_e6_c64'],\n        ['ir_r3_k3_s1_e6_c96'],\n        ['ir_r3_k3_s2_e6_c160'],\n        ['ir_r1_k3_s1_e6_c320'],\n    ]\n    round_chs_fn = partial(round_channels, multiplier=channel_multiplier)\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier=depth_multiplier, fix_first_last=fix_stem_head),\n        num_features=1280 if fix_stem_head else max(1280, round_chs_fn(1280)),\n        stem_size=32,\n        fix_stem=fix_stem_head,\n        round_chs_fn=round_chs_fn,\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        act_layer=resolve_act_layer(kwargs, 'relu6'),\n        **kwargs\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_fbnetc(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\" FBNet-C\n\n        Paper: https://arxiv.org/abs/1812.03443\n        Ref Impl: https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/modeling/backbone/fbnet_modeldef.py\n\n        NOTE: the impl above does not relate to the 'C' variant here, that was derived from paper,\n        it was used to confirm some building block details\n    \"\"\"\n    arch_def = [\n        ['ir_r1_k3_s1_e1_c16'],\n        ['ir_r1_k3_s2_e6_c24', 'ir_r2_k3_s1_e1_c24'],\n        ['ir_r1_k5_s2_e6_c32', 'ir_r1_k5_s1_e3_c32', 'ir_r1_k5_s1_e6_c32', 'ir_r1_k3_s1_e6_c32'],\n        ['ir_r1_k5_s2_e6_c64', 'ir_r1_k5_s1_e3_c64', 'ir_r2_k5_s1_e6_c64'],\n        ['ir_r3_k5_s1_e6_c112', 'ir_r1_k5_s1_e3_c112'],\n        ['ir_r4_k5_s2_e6_c184'],\n        ['ir_r1_k3_s1_e6_c352'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=16,\n        num_features=1984,  # paper suggests this, but is not 100% clear\n        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        **kwargs\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_spnasnet(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\"Creates the Single-Path NAS model from search targeted for Pixel1 phone.\n\n    Paper: https://arxiv.org/abs/1904.02877\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    \"\"\"\n    arch_def = [\n        # stage 0, 112x112 in\n        ['ds_r1_k3_s1_c16_noskip'],\n        # stage 1, 112x112 in\n        ['ir_r3_k3_s2_e3_c24'],\n        # stage 2, 56x56 in\n        ['ir_r1_k5_s2_e6_c40', 'ir_r3_k3_s1_e3_c40'],\n        # stage 3, 28x28 in\n        ['ir_r1_k5_s2_e6_c80', 'ir_r3_k3_s1_e3_c80'],\n        # stage 4, 14x14in\n        ['ir_r1_k5_s1_e6_c96', 'ir_r3_k5_s1_e3_c96'],\n        # stage 5, 14x14in\n        ['ir_r4_k5_s2_e6_c192'],\n        # stage 6, 7x7 in\n        ['ir_r1_k3_s1_e6_c320_noskip']\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=32,\n        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        **kwargs\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_efficientnet(\n        variant, channel_multiplier=1.0, depth_multiplier=1.0, channel_divisor=8,\n        group_size=None, pretrained=False, **kwargs):\n    \"\"\"Creates an EfficientNet model.\n\n    Ref impl: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py\n    Paper: https://arxiv.org/abs/1905.11946\n\n    EfficientNet params\n    name: (channel_multiplier, depth_multiplier, resolution, dropout_rate)\n    'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n    'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n    'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n    'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n    'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n    'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n    'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n    'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n    'efficientnet-b8': (2.2, 3.6, 672, 0.5),\n    'efficientnet-l2': (4.3, 5.3, 800, 0.5),\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer\n      depth_multiplier: multiplier to number of repeats per stage\n\n    \"\"\"\n    arch_def = [\n        ['ds_r1_k3_s1_e1_c16_se0.25'],\n        ['ir_r2_k3_s2_e6_c24_se0.25'],\n        ['ir_r2_k5_s2_e6_c40_se0.25'],\n        ['ir_r3_k3_s2_e6_c80_se0.25'],\n        ['ir_r3_k5_s1_e6_c112_se0.25'],\n        ['ir_r4_k5_s2_e6_c192_se0.25'],\n        ['ir_r1_k3_s1_e6_c320_se0.25'],\n    ]\n    round_chs_fn = partial(round_channels, multiplier=channel_multiplier, divisor=channel_divisor)\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier, group_size=group_size),\n        num_features=round_chs_fn(1280),\n        stem_size=32,\n        round_chs_fn=round_chs_fn,\n        act_layer=resolve_act_layer(kwargs, 'swish'),\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        **kwargs,\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_efficientnet_edge(\n        variant, channel_multiplier=1.0, depth_multiplier=1.0, group_size=None, pretrained=False, **kwargs):\n    \"\"\" Creates an EfficientNet-EdgeTPU model\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/edgetpu\n    \"\"\"\n\n    arch_def = [\n        # NOTE `fc` is present to override a mismatch between stem channels and in chs not\n        # present in other models\n        ['er_r1_k3_s1_e4_c24_fc24_noskip'],\n        ['er_r2_k3_s2_e8_c32'],\n        ['er_r4_k3_s2_e8_c48'],\n        ['ir_r5_k5_s2_e8_c96'],\n        ['ir_r4_k5_s1_e8_c144'],\n        ['ir_r2_k5_s2_e8_c192'],\n    ]\n    round_chs_fn = partial(round_channels, multiplier=channel_multiplier)\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier, group_size=group_size),\n        num_features=round_chs_fn(1280),\n        stem_size=32,\n        round_chs_fn=round_chs_fn,\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        act_layer=resolve_act_layer(kwargs, 'relu'),\n        **kwargs,\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_efficientnet_condconv(\n        variant, channel_multiplier=1.0, depth_multiplier=1.0, experts_multiplier=1, pretrained=False, **kwargs):\n    \"\"\"Creates an EfficientNet-CondConv model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/condconv\n    \"\"\"\n    arch_def = [\n        ['ds_r1_k3_s1_e1_c16_se0.25'],\n        ['ir_r2_k3_s2_e6_c24_se0.25'],\n        ['ir_r2_k5_s2_e6_c40_se0.25'],\n        ['ir_r3_k3_s2_e6_c80_se0.25'],\n        ['ir_r3_k5_s1_e6_c112_se0.25_cc4'],\n        ['ir_r4_k5_s2_e6_c192_se0.25_cc4'],\n        ['ir_r1_k3_s1_e6_c320_se0.25_cc4'],\n    ]\n    # NOTE unlike official impl, this one uses `cc<x>` option where x is the base number of experts for each stage and\n    # the expert_multiplier increases that on a per-model basis as with depth/channel multipliers\n    round_chs_fn = partial(round_channels, multiplier=channel_multiplier)\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier, experts_multiplier=experts_multiplier),\n        num_features=round_chs_fn(1280),\n        stem_size=32,\n        round_chs_fn=round_chs_fn,\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        act_layer=resolve_act_layer(kwargs, 'swish'),\n        **kwargs,\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_efficientnet_lite(variant, channel_multiplier=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\"Creates an EfficientNet-Lite model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite\n    Paper: https://arxiv.org/abs/1905.11946\n\n    EfficientNet params\n    name: (channel_multiplier, depth_multiplier, resolution, dropout_rate)\n      'efficientnet-lite0': (1.0, 1.0, 224, 0.2),\n      'efficientnet-lite1': (1.0, 1.1, 240, 0.2),\n      'efficientnet-lite2': (1.1, 1.2, 260, 0.3),\n      'efficientnet-lite3': (1.2, 1.4, 280, 0.3),\n      'efficientnet-lite4': (1.4, 1.8, 300, 0.3),\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer\n      depth_multiplier: multiplier to number of repeats per stage\n    \"\"\"\n    arch_def = [\n        ['ds_r1_k3_s1_e1_c16'],\n        ['ir_r2_k3_s2_e6_c24'],\n        ['ir_r2_k5_s2_e6_c40'],\n        ['ir_r3_k3_s2_e6_c80'],\n        ['ir_r3_k5_s1_e6_c112'],\n        ['ir_r4_k5_s2_e6_c192'],\n        ['ir_r1_k3_s1_e6_c320'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier, fix_first_last=True),\n        num_features=1280,\n        stem_size=32,\n        fix_stem=True,\n        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n        act_layer=resolve_act_layer(kwargs, 'relu6'),\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        **kwargs,\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_efficientnetv2_base(\n        variant, channel_multiplier=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\" Creates an EfficientNet-V2 base model\n\n    Ref impl: https://github.com/google/automl/tree/master/efficientnetv2\n    Paper: `EfficientNetV2: Smaller Models and Faster Training` - https://arxiv.org/abs/2104.00298\n    \"\"\"\n    arch_def = [\n        ['cn_r1_k3_s1_e1_c16_skip'],\n        ['er_r2_k3_s2_e4_c32'],\n        ['er_r2_k3_s2_e4_c48'],\n        ['ir_r3_k3_s2_e4_c96_se0.25'],\n        ['ir_r5_k3_s1_e6_c112_se0.25'],\n        ['ir_r8_k3_s2_e6_c192_se0.25'],\n    ]\n    round_chs_fn = partial(round_channels, multiplier=channel_multiplier, round_limit=0.)\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier),\n        num_features=round_chs_fn(1280),\n        stem_size=32,\n        round_chs_fn=round_chs_fn,\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        act_layer=resolve_act_layer(kwargs, 'silu'),\n        **kwargs,\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_efficientnetv2_s(\n        variant, channel_multiplier=1.0, depth_multiplier=1.0, group_size=None, rw=False, pretrained=False, **kwargs):\n    \"\"\" Creates an EfficientNet-V2 Small model\n\n    Ref impl: https://github.com/google/automl/tree/master/efficientnetv2\n    Paper: `EfficientNetV2: Smaller Models and Faster Training` - https://arxiv.org/abs/2104.00298\n\n    NOTE: `rw` flag sets up 'small' variant to behave like my initial v2 small model,\n        before ref the impl was released.\n    \"\"\"\n    arch_def = [\n        ['cn_r2_k3_s1_e1_c24_skip'],\n        ['er_r4_k3_s2_e4_c48'],\n        ['er_r4_k3_s2_e4_c64'],\n        ['ir_r6_k3_s2_e4_c128_se0.25'],\n        ['ir_r9_k3_s1_e6_c160_se0.25'],\n        ['ir_r15_k3_s2_e6_c256_se0.25'],\n    ]\n    num_features = 1280\n    if rw:\n        # my original variant, based on paper figure differs from the official release\n        arch_def[0] = ['er_r2_k3_s1_e1_c24']\n        arch_def[-1] = ['ir_r15_k3_s2_e6_c272_se0.25']\n        num_features = 1792\n\n    round_chs_fn = partial(round_channels, multiplier=channel_multiplier)\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier, group_size=group_size),\n        num_features=round_chs_fn(num_features),\n        stem_size=24,\n        round_chs_fn=round_chs_fn,\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        act_layer=resolve_act_layer(kwargs, 'silu'),\n        **kwargs,\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_efficientnetv2_m(variant, channel_multiplier=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\" Creates an EfficientNet-V2 Medium model\n\n    Ref impl: https://github.com/google/automl/tree/master/efficientnetv2\n    Paper: `EfficientNetV2: Smaller Models and Faster Training` - https://arxiv.org/abs/2104.00298\n    \"\"\"\n\n    arch_def = [\n        ['cn_r3_k3_s1_e1_c24_skip'],\n        ['er_r5_k3_s2_e4_c48'],\n        ['er_r5_k3_s2_e4_c80'],\n        ['ir_r7_k3_s2_e4_c160_se0.25'],\n        ['ir_r14_k3_s1_e6_c176_se0.25'],\n        ['ir_r18_k3_s2_e6_c304_se0.25'],\n        ['ir_r5_k3_s1_e6_c512_se0.25'],\n    ]\n\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier),\n        num_features=1280,\n        stem_size=24,\n        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        act_layer=resolve_act_layer(kwargs, 'silu'),\n        **kwargs,\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_efficientnetv2_l(variant, channel_multiplier=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\" Creates an EfficientNet-V2 Large model\n\n    Ref impl: https://github.com/google/automl/tree/master/efficientnetv2\n    Paper: `EfficientNetV2: Smaller Models and Faster Training` - https://arxiv.org/abs/2104.00298\n    \"\"\"\n\n    arch_def = [\n        ['cn_r4_k3_s1_e1_c32_skip'],\n        ['er_r7_k3_s2_e4_c64'],\n        ['er_r7_k3_s2_e4_c96'],\n        ['ir_r10_k3_s2_e4_c192_se0.25'],\n        ['ir_r19_k3_s1_e6_c224_se0.25'],\n        ['ir_r25_k3_s2_e6_c384_se0.25'],\n        ['ir_r7_k3_s1_e6_c640_se0.25'],\n    ]\n\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier),\n        num_features=1280,\n        stem_size=32,\n        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        act_layer=resolve_act_layer(kwargs, 'silu'),\n        **kwargs,\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_efficientnetv2_xl(variant, channel_multiplier=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\" Creates an EfficientNet-V2 Xtra-Large model\n\n    Ref impl: https://github.com/google/automl/tree/master/efficientnetv2\n    Paper: `EfficientNetV2: Smaller Models and Faster Training` - https://arxiv.org/abs/2104.00298\n    \"\"\"\n\n    arch_def = [\n        ['cn_r4_k3_s1_e1_c32_skip'],\n        ['er_r8_k3_s2_e4_c64'],\n        ['er_r8_k3_s2_e4_c96'],\n        ['ir_r16_k3_s2_e4_c192_se0.25'],\n        ['ir_r24_k3_s1_e6_c256_se0.25'],\n        ['ir_r32_k3_s2_e6_c512_se0.25'],\n        ['ir_r8_k3_s1_e6_c640_se0.25'],\n    ]\n\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier),\n        num_features=1280,\n        stem_size=32,\n        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        act_layer=resolve_act_layer(kwargs, 'silu'),\n        **kwargs,\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_mixnet_s(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\"Creates a MixNet Small model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet\n    Paper: https://arxiv.org/abs/1907.09595\n    \"\"\"\n    arch_def = [\n        # stage 0, 112x112 in\n        ['ds_r1_k3_s1_e1_c16'],  # relu\n        # stage 1, 112x112 in\n        ['ir_r1_k3_a1.1_p1.1_s2_e6_c24', 'ir_r1_k3_a1.1_p1.1_s1_e3_c24'],  # relu\n        # stage 2, 56x56 in\n        ['ir_r1_k3.5.7_s2_e6_c40_se0.5_nsw', 'ir_r3_k3.5_a1.1_p1.1_s1_e6_c40_se0.5_nsw'],  # swish\n        # stage 3, 28x28 in\n        ['ir_r1_k3.5.7_p1.1_s2_e6_c80_se0.25_nsw', 'ir_r2_k3.5_p1.1_s1_e6_c80_se0.25_nsw'],  # swish\n        # stage 4, 14x14in\n        ['ir_r1_k3.5.7_a1.1_p1.1_s1_e6_c120_se0.5_nsw', 'ir_r2_k3.5.7.9_a1.1_p1.1_s1_e3_c120_se0.5_nsw'],  # swish\n        # stage 5, 14x14in\n        ['ir_r1_k3.5.7.9.11_s2_e6_c200_se0.5_nsw', 'ir_r2_k3.5.7.9_p1.1_s1_e6_c200_se0.5_nsw'],  # swish\n        # 7x7\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        num_features=1536,\n        stem_size=16,\n        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        **kwargs\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_mixnet_m(variant, channel_multiplier=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\"Creates a MixNet Medium-Large model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet\n    Paper: https://arxiv.org/abs/1907.09595\n    \"\"\"\n    arch_def = [\n        # stage 0, 112x112 in\n        ['ds_r1_k3_s1_e1_c24'],  # relu\n        # stage 1, 112x112 in\n        ['ir_r1_k3.5.7_a1.1_p1.1_s2_e6_c32', 'ir_r1_k3_a1.1_p1.1_s1_e3_c32'],  # relu\n        # stage 2, 56x56 in\n        ['ir_r1_k3.5.7.9_s2_e6_c40_se0.5_nsw', 'ir_r3_k3.5_a1.1_p1.1_s1_e6_c40_se0.5_nsw'],  # swish\n        # stage 3, 28x28 in\n        ['ir_r1_k3.5.7_s2_e6_c80_se0.25_nsw', 'ir_r3_k3.5.7.9_a1.1_p1.1_s1_e6_c80_se0.25_nsw'],  # swish\n        # stage 4, 14x14in\n        ['ir_r1_k3_s1_e6_c120_se0.5_nsw', 'ir_r3_k3.5.7.9_a1.1_p1.1_s1_e3_c120_se0.5_nsw'],  # swish\n        # stage 5, 14x14in\n        ['ir_r1_k3.5.7.9_s2_e6_c200_se0.5_nsw', 'ir_r3_k3.5.7.9_p1.1_s1_e6_c200_se0.5_nsw'],  # swish\n        # 7x7\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier, depth_trunc='round'),\n        num_features=1536,\n        stem_size=24,\n        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        **kwargs\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_tinynet(\n    variant, model_width=1.0, depth_multiplier=1.0, pretrained=False, **kwargs\n):\n    \"\"\"Creates a TinyNet model.\n    \"\"\"\n    arch_def = [\n        ['ds_r1_k3_s1_e1_c16_se0.25'], ['ir_r2_k3_s2_e6_c24_se0.25'],\n        ['ir_r2_k5_s2_e6_c40_se0.25'], ['ir_r3_k3_s2_e6_c80_se0.25'],\n        ['ir_r3_k5_s1_e6_c112_se0.25'], ['ir_r4_k5_s2_e6_c192_se0.25'],\n        ['ir_r1_k3_s1_e6_c320_se0.25'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier, depth_trunc='round'),\n        num_features=max(1280, round_channels(1280, model_width, 8, None)),\n        stem_size=32,\n        fix_stem=True,\n        round_chs_fn=partial(round_channels, multiplier=model_width),\n        act_layer=resolve_act_layer(kwargs, 'swish'),\n        norm_layer=kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        **kwargs,\n    )\n    model = _create_effnet(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'conv_stem', 'classifier': 'classifier',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'mnasnet_050.untrained': _cfg(),\n    'mnasnet_075.untrained': _cfg(),\n    'mnasnet_100.rmsp_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mnasnet_b1-74cb7081.pth',\n        hf_hub_id='timm/'),\n    'mnasnet_140.untrained': _cfg(),\n\n    'semnasnet_050.untrained': _cfg(),\n    'semnasnet_075.rmsp_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/semnasnet_075-18710866.pth',\n        hf_hub_id='timm/'),\n    'semnasnet_100.rmsp_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mnasnet_a1-d9418771.pth',\n        hf_hub_id='timm/'),\n    'semnasnet_140.untrained': _cfg(),\n    'mnasnet_small.lamb_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mnasnet_small_lamb-aff75073.pth',\n        hf_hub_id='timm/'),\n\n    'mobilenetv2_035.untrained': _cfg(),\n    'mobilenetv2_050.lamb_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_050-3d30d450.pth',\n        hf_hub_id='timm/',\n        interpolation='bicubic',\n    ),\n    'mobilenetv2_075.untrained': _cfg(),\n    'mobilenetv2_100.ra_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_100_ra-b33bc2c4.pth',\n        hf_hub_id='timm/'),\n    'mobilenetv2_110d.ra_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_110d_ra-77090ade.pth',\n        hf_hub_id='timm/'),\n    'mobilenetv2_120d.ra_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_120d_ra-5987e2ed.pth',\n        hf_hub_id='timm/'),\n    'mobilenetv2_140.ra_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_140_ra-21a4e913.pth',\n        hf_hub_id='timm/'),\n\n    'fbnetc_100.rmsp_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/fbnetc_100-c345b898.pth',\n        hf_hub_id='timm/',\n        interpolation='bilinear'),\n    'spnasnet_100.rmsp_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/spnasnet_100-048bc3f4.pth',\n        hf_hub_id='timm/',\n        interpolation='bilinear'),\n\n    # NOTE experimenting with alternate attention\n    'efficientnet_b0.ra_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b0_ra-3dd342df.pth',\n        hf_hub_id='timm/'),\n    'efficientnet_b1.ft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b1-533bc792.pth',\n        hf_hub_id='timm/',\n        test_input_size=(3, 256, 256), crop_pct=1.0),\n    'efficientnet_b2.ra_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b2_ra-bcdf34b7.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 256, 256), pool_size=(8, 8), test_input_size=(3, 288, 288), crop_pct=1.0),\n    'efficientnet_b3.ra2_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra2-cf984f9c.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 288, 288), pool_size=(9, 9), test_input_size=(3, 320, 320), crop_pct=1.0),\n    'efficientnet_b4.ra2_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b4_ra2_320-7eb33cd5.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 320, 320), pool_size=(10, 10), test_input_size=(3, 384, 384), crop_pct=1.0),\n    'efficientnet_b5.sw_in12k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 448, 448), pool_size=(14, 14), crop_pct=1.0, crop_mode='squash'),\n    'efficientnet_b5.sw_in12k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 416, 416), pool_size=(13, 13), crop_pct=0.95, num_classes=11821),\n    'efficientnet_b6.untrained': _cfg(\n        url='', input_size=(3, 528, 528), pool_size=(17, 17), crop_pct=0.942),\n    'efficientnet_b7.untrained': _cfg(\n        url='', input_size=(3, 600, 600), pool_size=(19, 19), crop_pct=0.949),\n    'efficientnet_b8.untrained': _cfg(\n        url='', input_size=(3, 672, 672), pool_size=(21, 21), crop_pct=0.954),\n    'efficientnet_l2.untrained': _cfg(\n        url='', input_size=(3, 800, 800), pool_size=(25, 25), crop_pct=0.961),\n\n    # FIXME experimental\n    'efficientnet_b0_gn.untrained': _cfg(),\n    'efficientnet_b0_g8_gn.untrained': _cfg(),\n    'efficientnet_b0_g16_evos.untrained': _cfg(),\n    'efficientnet_b3_gn.untrained': _cfg(\n        input_size=(3, 288, 288), pool_size=(9, 9), test_input_size=(3, 320, 320), crop_pct=1.0),\n    'efficientnet_b3_g8_gn.untrained': _cfg(\n        input_size=(3, 288, 288), pool_size=(9, 9), test_input_size=(3, 320, 320), crop_pct=1.0),\n\n    'efficientnet_es.ra_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_es_ra-f111e99c.pth',\n        hf_hub_id='timm/'),\n    'efficientnet_em.ra2_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_em_ra2-66250f76.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n    'efficientnet_el.ra_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_el-3b455510.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),\n\n    'efficientnet_es_pruned.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_es_pruned75-1b7248cf.pth',\n        hf_hub_id='timm/'),\n    'efficientnet_el_pruned.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_el_pruned70-ef2a2ccf.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),\n\n    'efficientnet_cc_b0_4e.untrained': _cfg(),\n    'efficientnet_cc_b0_8e.untrained': _cfg(),\n    'efficientnet_cc_b1_8e.untrained': _cfg(input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n\n    'efficientnet_lite0.ra_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_lite0_ra-37913777.pth',\n        hf_hub_id='timm/'),\n    'efficientnet_lite1.untrained': _cfg(\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n    'efficientnet_lite2.untrained': _cfg(\n        input_size=(3, 260, 260), pool_size=(9, 9), crop_pct=0.890),\n    'efficientnet_lite3.untrained': _cfg(\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),\n    'efficientnet_lite4.untrained': _cfg(\n        input_size=(3, 380, 380), pool_size=(12, 12), crop_pct=0.922),\n\n    'efficientnet_b1_pruned.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/effnetb1_pruned-bea43a3a.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), pool_size=(8, 8),\n        crop_pct=0.882, mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    'efficientnet_b2_pruned.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/effnetb2_pruned-08c1b27c.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 260, 260), pool_size=(9, 9),\n        crop_pct=0.890, mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    'efficientnet_b3_pruned.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/effnetb3_pruned-59ecf72d.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 300, 300), pool_size=(10, 10),\n        crop_pct=0.904, mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n\n    'efficientnetv2_rw_t.ra2_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnetv2_t_agc-3620981a.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 224, 224), test_input_size=(3, 288, 288), pool_size=(7, 7), crop_pct=1.0),\n    'gc_efficientnetv2_rw_t.agc_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gc_efficientnetv2_rw_t_agc-927a0bde.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 224, 224), test_input_size=(3, 288, 288), pool_size=(7, 7), crop_pct=1.0),\n    'efficientnetv2_rw_s.ra2_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_v2s_ra2_288-a6477665.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 288, 288), test_input_size=(3, 384, 384), pool_size=(9, 9), crop_pct=1.0),\n    'efficientnetv2_rw_m.agc_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnetv2_rw_m_agc-3d90cb1e.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 320, 320), test_input_size=(3, 416, 416), pool_size=(10, 10), crop_pct=1.0),\n\n    'efficientnetv2_s.untrained': _cfg(\n        input_size=(3, 288, 288), test_input_size=(3, 384, 384), pool_size=(9, 9), crop_pct=1.0),\n    'efficientnetv2_m.untrained': _cfg(\n        input_size=(3, 320, 320), test_input_size=(3, 416, 416), pool_size=(10, 10), crop_pct=1.0),\n    'efficientnetv2_l.untrained': _cfg(\n        input_size=(3, 384, 384), test_input_size=(3, 480, 480), pool_size=(12, 12), crop_pct=1.0),\n    'efficientnetv2_xl.untrained': _cfg(\n        input_size=(3, 384, 384), test_input_size=(3, 512, 512), pool_size=(12, 12), crop_pct=1.0),\n\n    'tf_efficientnet_b0.ns_jft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ns-c0e6a31c.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 224, 224)),\n    'tf_efficientnet_b1.ns_jft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_ns-99dd0c41.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n    'tf_efficientnet_b2.ns_jft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_ns-00306e48.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 260, 260), pool_size=(9, 9), crop_pct=0.890),\n    'tf_efficientnet_b3.ns_jft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b3_ns-9d44bf68.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),\n    'tf_efficientnet_b4.ns_jft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 380, 380), pool_size=(12, 12), crop_pct=0.922),\n    'tf_efficientnet_b5.ns_jft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_ns-6f26d0cf.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 456, 456), pool_size=(15, 15), crop_pct=0.934),\n    'tf_efficientnet_b6.ns_jft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b6_ns-51548356.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 528, 528), pool_size=(17, 17), crop_pct=0.942),\n    'tf_efficientnet_b7.ns_jft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ns-1dbc32de.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 600, 600), pool_size=(19, 19), crop_pct=0.949),\n    'tf_efficientnet_l2.ns_jft_in1k_475': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_l2_ns_475-bebbd00a.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 475, 475), pool_size=(15, 15), crop_pct=0.936),\n    'tf_efficientnet_l2.ns_jft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_l2_ns-df73bb44.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 800, 800), pool_size=(25, 25), crop_pct=0.96),\n\n    'tf_efficientnet_b0.ap_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ap-f262efe1.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD, input_size=(3, 224, 224)),\n    'tf_efficientnet_b1.ap_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_ap-44ef0a3d.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n    'tf_efficientnet_b2.ap_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_ap-2f8e7636.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 260, 260), pool_size=(9, 9), crop_pct=0.890),\n    'tf_efficientnet_b3.ap_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b3_ap-aad25bdd.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),\n    'tf_efficientnet_b4.ap_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ap-dedb23e6.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 380, 380), pool_size=(12, 12), crop_pct=0.922),\n    'tf_efficientnet_b5.ap_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_ap-9e82fae8.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 456, 456), pool_size=(15, 15), crop_pct=0.934),\n    'tf_efficientnet_b6.ap_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b6_ap-4ffb161f.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 528, 528), pool_size=(17, 17), crop_pct=0.942),\n    'tf_efficientnet_b7.ap_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ap-ddb28fec.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 600, 600), pool_size=(19, 19), crop_pct=0.949),\n    'tf_efficientnet_b8.ap_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b8_ap-00e169fa.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 672, 672), pool_size=(21, 21), crop_pct=0.954),\n\n    'tf_efficientnet_b5.ra_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_ra-9a3e5369.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 456, 456), pool_size=(15, 15), crop_pct=0.934),\n    'tf_efficientnet_b7.ra_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ra-6c08e654.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 600, 600), pool_size=(19, 19), crop_pct=0.949),\n    'tf_efficientnet_b8.ra_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b8_ra-572d5dd9.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 672, 672), pool_size=(21, 21), crop_pct=0.954),\n\n    'tf_efficientnet_b0.aa_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_aa-827b6e33.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 224, 224)),\n    'tf_efficientnet_b1.aa_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_aa-ea7a6ee0.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n    'tf_efficientnet_b2.aa_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_aa-60c94f97.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 260, 260), pool_size=(9, 9), crop_pct=0.890),\n    'tf_efficientnet_b3.aa_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b3_aa-84b4657e.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),\n    'tf_efficientnet_b4.aa_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_aa-818f208c.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 380, 380), pool_size=(12, 12), crop_pct=0.922),\n    'tf_efficientnet_b5.aa_in1k': _cfg(\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_aa-99018a74.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 456, 456), pool_size=(15, 15), crop_pct=0.934),\n    'tf_efficientnet_b6.aa_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b6_aa-80ba17e4.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 528, 528), pool_size=(17, 17), crop_pct=0.942),\n    'tf_efficientnet_b7.aa_in1k': _cfg(\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_aa-076e3472.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 600, 600), pool_size=(19, 19), crop_pct=0.949),\n\n    'tf_efficientnet_b0.in1k': _cfg(\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0-0af12548.pth',\n        #hf_hub_id='timm/',\n        input_size=(3, 224, 224)),\n    'tf_efficientnet_b1.in1k': _cfg(\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1-5c1377c4.pth',\n        #hf_hub_id='timm/',\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n    'tf_efficientnet_b2.in1k': _cfg(\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2-e393ef04.pth',\n        #hf_hub_id='timm/',\n        input_size=(3, 260, 260), pool_size=(9, 9), crop_pct=0.890),\n    'tf_efficientnet_b3.in1k': _cfg(\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b3-e3bd6955.pth',\n        #hf_hub_id='timm/',\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),\n    'tf_efficientnet_b4.in1k': _cfg(\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4-74ee3bed.pth',\n        #hf_hub_id='timm/',\n        input_size=(3, 380, 380), pool_size=(12, 12), crop_pct=0.922),\n    'tf_efficientnet_b5.in1k': _cfg(\n        url='https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5-c6949ce9.pth',\n        #hf_hub_id='timm/',\n        input_size=(3, 456, 456), pool_size=(15, 15), crop_pct=0.934),\n\n\n    'tf_efficientnet_es.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_es-ca1afbfe.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 224, 224), ),\n    'tf_efficientnet_em.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_em-e78cfe58.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n    'tf_efficientnet_el.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_el-5143854e.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),\n\n    'tf_efficientnet_cc_b0_4e.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_cc_b0_4e-4362b6b2.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    'tf_efficientnet_cc_b0_8e.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_cc_b0_8e-66184a25.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    'tf_efficientnet_cc_b1_8e.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_cc_b1_8e-f7c79ae1.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n\n    'tf_efficientnet_lite0.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite0-0aa007d2.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        interpolation='bicubic',  # should be bilinear but bicubic better match for TF bilinear at low res\n    ),\n    'tf_efficientnet_lite1.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite1-bde8b488.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882,\n        interpolation='bicubic',  # should be bilinear but bicubic better match for TF bilinear at low res\n    ),\n    'tf_efficientnet_lite2.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite2-dcccb7df.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 260, 260), pool_size=(9, 9), crop_pct=0.890,\n        interpolation='bicubic',  # should be bilinear but bicubic better match for TF bilinear at low res\n    ),\n    'tf_efficientnet_lite3.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite3-b733e338.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904, interpolation='bilinear'),\n    'tf_efficientnet_lite4.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite4-741542c3.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 380, 380), pool_size=(12, 12), crop_pct=0.920, interpolation='bilinear'),\n\n    'tf_efficientnetv2_s.in21k_ft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_s_21ft1k-d7dafa41.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 300, 300), test_input_size=(3, 384, 384), pool_size=(10, 10), crop_pct=1.0),\n    'tf_efficientnetv2_m.in21k_ft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_m_21ft1k-bf41664a.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 384, 384), test_input_size=(3, 480, 480), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'tf_efficientnetv2_l.in21k_ft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_l_21ft1k-60127a9d.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 384, 384), test_input_size=(3, 480, 480), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'tf_efficientnetv2_xl.in21k_ft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_xl_in21ft1k-06c35c48.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 384, 384), test_input_size=(3, 512, 512), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n\n    'tf_efficientnetv2_s.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_s-eb54923e.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 300, 300), test_input_size=(3, 384, 384), pool_size=(10, 10), crop_pct=1.0),\n    'tf_efficientnetv2_m.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_m-cc09e0cd.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 384, 384), test_input_size=(3, 480, 480), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'tf_efficientnetv2_l.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_l-d664b728.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 384, 384), test_input_size=(3, 480, 480), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n\n    'tf_efficientnetv2_s.in21k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_s_21k-6337ad01.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), num_classes=21843,\n        input_size=(3, 300, 300), test_input_size=(3, 384, 384), pool_size=(10, 10), crop_pct=1.0),\n    'tf_efficientnetv2_m.in21k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_m_21k-361418a2.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), num_classes=21843,\n        input_size=(3, 384, 384), test_input_size=(3, 480, 480), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'tf_efficientnetv2_l.in21k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_l_21k-91a19ec9.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), num_classes=21843,\n        input_size=(3, 384, 384), test_input_size=(3, 480, 480), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n    'tf_efficientnetv2_xl.in21k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_xl_in21k-fd7e8abf.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), num_classes=21843,\n        input_size=(3, 384, 384), test_input_size=(3, 512, 512), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n\n    'tf_efficientnetv2_b0.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_b0-c7cc451f.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 192, 192), test_input_size=(3, 224, 224), pool_size=(6, 6)),\n    'tf_efficientnetv2_b1.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_b1-be6e41b0.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 192, 192), test_input_size=(3, 240, 240), pool_size=(6, 6), crop_pct=0.882),\n    'tf_efficientnetv2_b2.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_b2-847de54e.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 208, 208), test_input_size=(3, 260, 260), pool_size=(7, 7), crop_pct=0.890),\n    'tf_efficientnetv2_b3.in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 240, 240), test_input_size=(3, 300, 300), pool_size=(8, 8), crop_pct=0.9, crop_mode='squash'),\n    'tf_efficientnetv2_b3.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_b3-57773f13.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), test_input_size=(3, 300, 300), pool_size=(8, 8), crop_pct=0.904),\n    'tf_efficientnetv2_b3.in21k': _cfg(\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD, num_classes=21843,\n        input_size=(3, 240, 240), test_input_size=(3, 300, 300), pool_size=(8, 8), crop_pct=0.904),\n\n    'mixnet_s.ft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mixnet_s-a907afbc.pth',\n        hf_hub_id='timm/'),\n    'mixnet_m.ft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mixnet_m-4647fc68.pth',\n        hf_hub_id='timm/'),\n    'mixnet_l.ft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mixnet_l-5a9a2ed8.pth',\n        hf_hub_id='timm/'),\n    'mixnet_xl.ra_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mixnet_xl_ra-aac3c00c.pth',\n        hf_hub_id='timm/'),\n    'mixnet_xxl.untrained': _cfg(),\n\n    'tf_mixnet_s.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mixnet_s-89d3354b.pth',\n        hf_hub_id='timm/'),\n    'tf_mixnet_m.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mixnet_m-0f4d8805.pth',\n        hf_hub_id='timm/'),\n    'tf_mixnet_l.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mixnet_l-6c92e0c8.pth',\n        hf_hub_id='timm/'),\n\n    \"tinynet_a.in1k\": _cfg(\n        input_size=(3, 192, 192), pool_size=(6, 6),  # int(224 * 0.86)\n        url='https://github.com/huawei-noah/CV-Backbones/releases/download/v1.2.0/tinynet_a.pth',\n        hf_hub_id='timm/'),\n    \"tinynet_b.in1k\": _cfg(\n        input_size=(3, 188, 188), pool_size=(6, 6),  # int(224 * 0.84)\n        url='https://github.com/huawei-noah/CV-Backbones/releases/download/v1.2.0/tinynet_b.pth',\n        hf_hub_id='timm/'),\n    \"tinynet_c.in1k\": _cfg(\n        input_size=(3, 184, 184), pool_size=(6, 6),  # int(224 * 0.825)\n        url='https://github.com/huawei-noah/CV-Backbones/releases/download/v1.2.0/tinynet_c.pth',\n        hf_hub_id='timm/'),\n    \"tinynet_d.in1k\": _cfg(\n        input_size=(3, 152, 152), pool_size=(5, 5),  # int(224 * 0.68)\n        url='https://github.com/huawei-noah/CV-Backbones/releases/download/v1.2.0/tinynet_d.pth',\n        hf_hub_id='timm/'),\n    \"tinynet_e.in1k\": _cfg(\n        input_size=(3, 106, 106), pool_size=(4, 4),  # int(224 * 0.475)\n        url='https://github.com/huawei-noah/CV-Backbones/releases/download/v1.2.0/tinynet_e.pth',\n        hf_hub_id='timm/'),\n})\n\n\n@register_model\ndef mnasnet_050(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MNASNet B1, depth multiplier of 0.5. \"\"\"\n    model = _gen_mnasnet_b1('mnasnet_050', 0.5, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mnasnet_075(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MNASNet B1, depth multiplier of 0.75. \"\"\"\n    model = _gen_mnasnet_b1('mnasnet_075', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mnasnet_100(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MNASNet B1, depth multiplier of 1.0. \"\"\"\n    model = _gen_mnasnet_b1('mnasnet_100', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mnasnet_b1(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MNASNet B1, depth multiplier of 1.0. \"\"\"\n    return mnasnet_100(pretrained, **kwargs)\n\n\n@register_model\ndef mnasnet_140(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MNASNet B1,  depth multiplier of 1.4 \"\"\"\n    model = _gen_mnasnet_b1('mnasnet_140', 1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef semnasnet_050(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MNASNet A1 (w/ SE), depth multiplier of 0.5 \"\"\"\n    model = _gen_mnasnet_a1('semnasnet_050', 0.5, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef semnasnet_075(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MNASNet A1 (w/ SE),  depth multiplier of 0.75. \"\"\"\n    model = _gen_mnasnet_a1('semnasnet_075', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef semnasnet_100(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MNASNet A1 (w/ SE), depth multiplier of 1.0. \"\"\"\n    model = _gen_mnasnet_a1('semnasnet_100', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mnasnet_a1(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MNASNet A1 (w/ SE), depth multiplier of 1.0. \"\"\"\n    return semnasnet_100(pretrained, **kwargs)\n\n\n@register_model\ndef semnasnet_140(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MNASNet A1 (w/ SE), depth multiplier of 1.4. \"\"\"\n    model = _gen_mnasnet_a1('semnasnet_140', 1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mnasnet_small(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MNASNet Small,  depth multiplier of 1.0. \"\"\"\n    model = _gen_mnasnet_small('mnasnet_small', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv2_035(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MobileNet V2 w/ 0.35 channel multiplier \"\"\"\n    model = _gen_mobilenet_v2('mobilenetv2_035', 0.35, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv2_050(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MobileNet V2 w/ 0.5 channel multiplier \"\"\"\n    model = _gen_mobilenet_v2('mobilenetv2_050', 0.5, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv2_075(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MobileNet V2 w/ 0.75 channel multiplier \"\"\"\n    model = _gen_mobilenet_v2('mobilenetv2_075', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv2_100(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MobileNet V2 w/ 1.0 channel multiplier \"\"\"\n    model = _gen_mobilenet_v2('mobilenetv2_100', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv2_140(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MobileNet V2 w/ 1.4 channel multiplier \"\"\"\n    model = _gen_mobilenet_v2('mobilenetv2_140', 1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv2_110d(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MobileNet V2 w/ 1.1 channel, 1.2 depth multipliers\"\"\"\n    model = _gen_mobilenet_v2(\n        'mobilenetv2_110d', 1.1, depth_multiplier=1.2, fix_stem_head=True, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv2_120d(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" MobileNet V2 w/ 1.2 channel, 1.4 depth multipliers \"\"\"\n    model = _gen_mobilenet_v2(\n        'mobilenetv2_120d', 1.2, depth_multiplier=1.4, fix_stem_head=True, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef fbnetc_100(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" FBNet-C \"\"\"\n    if pretrained:\n        # pretrained model trained with non-default BN epsilon\n        kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    model = _gen_fbnetc('fbnetc_100', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef spnasnet_100(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" Single-Path NAS Pixel1\"\"\"\n    model = _gen_spnasnet('spnasnet_100', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b0(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B0 \"\"\"\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        'efficientnet_b0', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b1(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B1 \"\"\"\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        'efficientnet_b1', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b2(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B2 \"\"\"\n    # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        'efficientnet_b2', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b2a(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B2 @ 288x288 w/ 1.0 test crop\"\"\"\n    # WARN this model def is deprecated, different train/test res + test crop handled by default_cfg now\n    return efficientnet_b2(pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef efficientnet_b3(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B3 \"\"\"\n    # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        'efficientnet_b3', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b3a(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B3 @ 320x320 w/ 1.0 test crop-pct \"\"\"\n    # WARN this model def is deprecated, different train/test res + test crop handled by default_cfg now\n    return efficientnet_b3(pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef efficientnet_b4(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B4 \"\"\"\n    # NOTE for train, drop_rate should be 0.4, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        'efficientnet_b4', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b5(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B5 \"\"\"\n    # NOTE for train, drop_rate should be 0.4, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        'efficientnet_b5', channel_multiplier=1.6, depth_multiplier=2.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b6(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B6 \"\"\"\n    # NOTE for train, drop_rate should be 0.5, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        'efficientnet_b6', channel_multiplier=1.8, depth_multiplier=2.6, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b7(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B7 \"\"\"\n    # NOTE for train, drop_rate should be 0.5, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        'efficientnet_b7', channel_multiplier=2.0, depth_multiplier=3.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b8(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B8 \"\"\"\n    # NOTE for train, drop_rate should be 0.5, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        'efficientnet_b8', channel_multiplier=2.2, depth_multiplier=3.6, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_l2(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-L2.\"\"\"\n    # NOTE for train, drop_rate should be 0.5, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        'efficientnet_l2', channel_multiplier=4.3, depth_multiplier=5.3, pretrained=pretrained, **kwargs)\n    return model\n\n\n# FIXME experimental group cong / GroupNorm / EvoNorm experiments\n@register_model\ndef efficientnet_b0_gn(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B0 + GroupNorm\"\"\"\n    model = _gen_efficientnet(\n        'efficientnet_b0_gn', norm_layer=partial(GroupNormAct, group_size=8), pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b0_g8_gn(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B0 w/ group conv + GroupNorm\"\"\"\n    model = _gen_efficientnet(\n        'efficientnet_b0_g8_gn', group_size=8, norm_layer=partial(GroupNormAct, group_size=8),\n        pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b0_g16_evos(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B0 w/ group 16 conv + EvoNorm\"\"\"\n    model = _gen_efficientnet(\n        'efficientnet_b0_g16_evos', group_size=16, channel_divisor=16,\n        pretrained=pretrained, **kwargs) #norm_layer=partial(EvoNorm2dS0, group_size=16),\n    return model\n\n\n@register_model\ndef efficientnet_b3_gn(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B3 w/ GroupNorm \"\"\"\n    # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        'efficientnet_b3_gn', channel_multiplier=1.2, depth_multiplier=1.4, channel_divisor=16,\n        norm_layer=partial(GroupNormAct, group_size=16), pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b3_g8_gn(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B3 w/ grouped conv + BN\"\"\"\n    # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        'efficientnet_b3_g8_gn', channel_multiplier=1.2, depth_multiplier=1.4, group_size=8, channel_divisor=16,\n        norm_layer=partial(GroupNormAct, group_size=16), pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_es(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Edge Small. \"\"\"\n    model = _gen_efficientnet_edge(\n        'efficientnet_es', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_es_pruned(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Edge Small Pruned. For more info: https://github.com/DeGirum/pruned-models/releases/tag/efficientnet_v1.0\"\"\"\n    model = _gen_efficientnet_edge(\n        'efficientnet_es_pruned', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n@register_model\ndef efficientnet_em(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Edge-Medium. \"\"\"\n    model = _gen_efficientnet_edge(\n        'efficientnet_em', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_el(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Edge-Large. \"\"\"\n    model = _gen_efficientnet_edge(\n        'efficientnet_el', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n@register_model\ndef efficientnet_el_pruned(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Edge-Large pruned. For more info: https://github.com/DeGirum/pruned-models/releases/tag/efficientnet_v1.0\"\"\"\n    model = _gen_efficientnet_edge(\n        'efficientnet_el_pruned', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n@register_model\ndef efficientnet_cc_b0_4e(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-CondConv-B0 w/ 8 Experts \"\"\"\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    model = _gen_efficientnet_condconv(\n        'efficientnet_cc_b0_4e', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_cc_b0_8e(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-CondConv-B0 w/ 8 Experts \"\"\"\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    model = _gen_efficientnet_condconv(\n        'efficientnet_cc_b0_8e', channel_multiplier=1.0, depth_multiplier=1.0, experts_multiplier=2,\n        pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_cc_b1_8e(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-CondConv-B1 w/ 8 Experts \"\"\"\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    model = _gen_efficientnet_condconv(\n        'efficientnet_cc_b1_8e', channel_multiplier=1.0, depth_multiplier=1.1, experts_multiplier=2,\n        pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_lite0(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Lite0 \"\"\"\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    model = _gen_efficientnet_lite(\n        'efficientnet_lite0', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_lite1(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Lite1 \"\"\"\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    model = _gen_efficientnet_lite(\n        'efficientnet_lite1', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_lite2(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Lite2 \"\"\"\n    # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2\n    model = _gen_efficientnet_lite(\n        'efficientnet_lite2', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_lite3(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Lite3 \"\"\"\n    # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2\n    model = _gen_efficientnet_lite(\n        'efficientnet_lite3', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_lite4(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Lite4 \"\"\"\n    # NOTE for train, drop_rate should be 0.4, drop_path_rate should be 0.2\n    model = _gen_efficientnet_lite(\n        'efficientnet_lite4', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b1_pruned(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B1 Pruned. The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf  \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    variant = 'efficientnet_b1_pruned'\n    model = _gen_efficientnet(\n        variant, channel_multiplier=1.0, depth_multiplier=1.1, pruned=True, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b2_pruned(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B2 Pruned. The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet(\n        'efficientnet_b2_pruned', channel_multiplier=1.1, depth_multiplier=1.2, pruned=True,\n        pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b3_pruned(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B3 Pruned. The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet(\n        'efficientnet_b3_pruned', channel_multiplier=1.2, depth_multiplier=1.4, pruned=True,\n        pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnetv2_rw_t(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-V2 Tiny (Custom variant, tiny not in paper). \"\"\"\n    model = _gen_efficientnetv2_s(\n        'efficientnetv2_rw_t', channel_multiplier=0.8, depth_multiplier=0.9, rw=False, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef gc_efficientnetv2_rw_t(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-V2 Tiny w/ Global Context Attn (Custom variant, tiny not in paper). \"\"\"\n    model = _gen_efficientnetv2_s(\n        'gc_efficientnetv2_rw_t', channel_multiplier=0.8, depth_multiplier=0.9,\n        rw=False, se_layer='gc', pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnetv2_rw_s(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-V2 Small (RW variant).\n    NOTE: This is my initial (pre official code release) w/ some differences.\n    See efficientnetv2_s and tf_efficientnetv2_s for versions that match the official w/ PyTorch vs TF padding\n    \"\"\"\n    model = _gen_efficientnetv2_s('efficientnetv2_rw_s', rw=True, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnetv2_rw_m(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-V2 Medium (RW variant).\n    \"\"\"\n    model = _gen_efficientnetv2_s(\n        'efficientnetv2_rw_m', channel_multiplier=1.2, depth_multiplier=(1.2,) * 4 + (1.6,) * 2, rw=True,\n        pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnetv2_s(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-V2 Small. \"\"\"\n    model = _gen_efficientnetv2_s('efficientnetv2_s', pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnetv2_m(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-V2 Medium. \"\"\"\n    model = _gen_efficientnetv2_m('efficientnetv2_m', pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnetv2_l(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-V2 Large. \"\"\"\n    model = _gen_efficientnetv2_l('efficientnetv2_l', pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnetv2_xl(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-V2 Xtra-Large. \"\"\"\n    model = _gen_efficientnetv2_xl('efficientnetv2_xl', pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b0(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B0. Tensorflow compatible variant  \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet(\n        'tf_efficientnet_b0', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b1(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B1. Tensorflow compatible variant  \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet(\n        'tf_efficientnet_b1', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b2(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B2. Tensorflow compatible variant  \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet(\n        'tf_efficientnet_b2', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b3(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B3. Tensorflow compatible variant \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet(\n        'tf_efficientnet_b3', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b4(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B4. Tensorflow compatible variant \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet(\n        'tf_efficientnet_b4', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b5(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B5. Tensorflow compatible variant \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet(\n        'tf_efficientnet_b5', channel_multiplier=1.6, depth_multiplier=2.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b6(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B6. Tensorflow compatible variant \"\"\"\n    # NOTE for train, drop_rate should be 0.5\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet(\n        'tf_efficientnet_b6', channel_multiplier=1.8, depth_multiplier=2.6, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b7(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B7. Tensorflow compatible variant \"\"\"\n    # NOTE for train, drop_rate should be 0.5\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet(\n        'tf_efficientnet_b7', channel_multiplier=2.0, depth_multiplier=3.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b8(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-B8. Tensorflow compatible variant \"\"\"\n    # NOTE for train, drop_rate should be 0.5\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet(\n        'tf_efficientnet_b8', channel_multiplier=2.2, depth_multiplier=3.6, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_l2(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-L2 NoisyStudent. Tensorflow compatible variant \"\"\"\n    # NOTE for train, drop_rate should be 0.5\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet(\n        'tf_efficientnet_l2', channel_multiplier=4.3, depth_multiplier=5.3, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_es(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Edge Small. Tensorflow compatible variant  \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet_edge(\n        'tf_efficientnet_es', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_em(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Edge-Medium. Tensorflow compatible variant  \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet_edge(\n        'tf_efficientnet_em', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_el(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Edge-Large. Tensorflow compatible variant  \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet_edge(\n        'tf_efficientnet_el', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_cc_b0_4e(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-CondConv-B0 w/ 4 Experts. Tensorflow compatible variant \"\"\"\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet_condconv(\n        'tf_efficientnet_cc_b0_4e', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_cc_b0_8e(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-CondConv-B0 w/ 8 Experts. Tensorflow compatible variant \"\"\"\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet_condconv(\n        'tf_efficientnet_cc_b0_8e', channel_multiplier=1.0, depth_multiplier=1.0, experts_multiplier=2,\n        pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_cc_b1_8e(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-CondConv-B1 w/ 8 Experts. Tensorflow compatible variant \"\"\"\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet_condconv(\n        'tf_efficientnet_cc_b1_8e', channel_multiplier=1.0, depth_multiplier=1.1, experts_multiplier=2,\n        pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_lite0(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Lite0 \"\"\"\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet_lite(\n        'tf_efficientnet_lite0', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_lite1(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Lite1 \"\"\"\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet_lite(\n        'tf_efficientnet_lite1', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_lite2(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Lite2 \"\"\"\n    # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet_lite(\n        'tf_efficientnet_lite2', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_lite3(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Lite3 \"\"\"\n    # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet_lite(\n        'tf_efficientnet_lite3', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_lite4(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-Lite4 \"\"\"\n    # NOTE for train, drop_rate should be 0.4, drop_path_rate should be 0.2\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnet_lite(\n        'tf_efficientnet_lite4', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnetv2_s(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-V2 Small. Tensorflow compatible variant  \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnetv2_s('tf_efficientnetv2_s', pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnetv2_m(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-V2 Medium. Tensorflow compatible variant  \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnetv2_m('tf_efficientnetv2_m', pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnetv2_l(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-V2 Large. Tensorflow compatible variant  \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnetv2_l('tf_efficientnetv2_l', pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnetv2_xl(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-V2 Xtra-Large. Tensorflow compatible variant\n    \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnetv2_xl('tf_efficientnetv2_xl', pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnetv2_b0(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-V2-B0. Tensorflow compatible variant  \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnetv2_base('tf_efficientnetv2_b0', pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnetv2_b1(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-V2-B1. Tensorflow compatible variant  \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnetv2_base(\n        'tf_efficientnetv2_b1', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnetv2_b2(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-V2-B2. Tensorflow compatible variant  \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnetv2_base(\n        'tf_efficientnetv2_b2', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnetv2_b3(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\" EfficientNet-V2-B3. Tensorflow compatible variant \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_efficientnetv2_base(\n        'tf_efficientnetv2_b3', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mixnet_s(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\"Creates a MixNet Small model.\n    \"\"\"\n    model = _gen_mixnet_s(\n        'mixnet_s', channel_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mixnet_m(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\"Creates a MixNet Medium model.\n    \"\"\"\n    model = _gen_mixnet_m(\n        'mixnet_m', channel_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mixnet_l(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\"Creates a MixNet Large model.\n    \"\"\"\n    model = _gen_mixnet_m(\n        'mixnet_l', channel_multiplier=1.3, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mixnet_xl(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\"Creates a MixNet Extra-Large model.\n    Not a paper spec, experimental def by RW w/ depth scaling.\n    \"\"\"\n    model = _gen_mixnet_m(\n        'mixnet_xl', channel_multiplier=1.6, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mixnet_xxl(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\"Creates a MixNet Double Extra Large model.\n    Not a paper spec, experimental def by RW w/ depth scaling.\n    \"\"\"\n    model = _gen_mixnet_m(\n        'mixnet_xxl', channel_multiplier=2.4, depth_multiplier=1.3, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mixnet_s(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\"Creates a MixNet Small model. Tensorflow compatible variant\n    \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_mixnet_s(\n        'tf_mixnet_s', channel_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mixnet_m(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\"Creates a MixNet Medium model. Tensorflow compatible variant\n    \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_mixnet_m(\n        'tf_mixnet_m', channel_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mixnet_l(pretrained=False, **kwargs) -> EfficientNet:\n    \"\"\"Creates a MixNet Large model. Tensorflow compatible variant\n    \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_mixnet_m(\n        'tf_mixnet_l', channel_multiplier=1.3, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tinynet_a(pretrained=False, **kwargs) -> EfficientNet:\n    model = _gen_tinynet('tinynet_a', 1.0, 1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tinynet_b(pretrained=False, **kwargs) -> EfficientNet:\n    model = _gen_tinynet('tinynet_b', 0.75, 1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tinynet_c(pretrained=False, **kwargs) -> EfficientNet:\n    model = _gen_tinynet('tinynet_c', 0.54, 0.85, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tinynet_d(pretrained=False, **kwargs) -> EfficientNet:\n    model = _gen_tinynet('tinynet_d', 0.54, 0.695, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tinynet_e(pretrained=False, **kwargs) -> EfficientNet:\n    model = _gen_tinynet('tinynet_e', 0.51, 0.6, pretrained=pretrained, **kwargs)\n    return model\n\n\nregister_model_deprecations(__name__, {\n    'tf_efficientnet_b0_ap': 'tf_efficientnet_b0.ap_in1k',\n    'tf_efficientnet_b1_ap': 'tf_efficientnet_b1.ap_in1k',\n    'tf_efficientnet_b2_ap': 'tf_efficientnet_b2.ap_in1k',\n    'tf_efficientnet_b3_ap': 'tf_efficientnet_b3.ap_in1k',\n    'tf_efficientnet_b4_ap': 'tf_efficientnet_b4.ap_in1k',\n    'tf_efficientnet_b5_ap': 'tf_efficientnet_b5.ap_in1k',\n    'tf_efficientnet_b6_ap': 'tf_efficientnet_b6.ap_in1k',\n    'tf_efficientnet_b7_ap': 'tf_efficientnet_b7.ap_in1k',\n    'tf_efficientnet_b8_ap': 'tf_efficientnet_b8.ap_in1k',\n    'tf_efficientnet_b0_ns': 'tf_efficientnet_b0.ns_jft_in1k',\n    'tf_efficientnet_b1_ns': 'tf_efficientnet_b1.ns_jft_in1k',\n    'tf_efficientnet_b2_ns': 'tf_efficientnet_b2.ns_jft_in1k',\n    'tf_efficientnet_b3_ns': 'tf_efficientnet_b3.ns_jft_in1k',\n    'tf_efficientnet_b4_ns': 'tf_efficientnet_b4.ns_jft_in1k',\n    'tf_efficientnet_b5_ns': 'tf_efficientnet_b5.ns_jft_in1k',\n    'tf_efficientnet_b6_ns': 'tf_efficientnet_b6.ns_jft_in1k',\n    'tf_efficientnet_b7_ns': 'tf_efficientnet_b7.ns_jft_in1k',\n    'tf_efficientnet_l2_ns_475': 'tf_efficientnet_l2.ns_jft_in1k_475',\n    'tf_efficientnet_l2_ns': 'tf_efficientnet_l2.ns_jft_in1k',\n    'tf_efficientnetv2_s_in21ft1k': 'tf_efficientnetv2_s.in21k_ft_in1k',\n    'tf_efficientnetv2_m_in21ft1k': 'tf_efficientnetv2_m.in21k_ft_in1k',\n    'tf_efficientnetv2_l_in21ft1k': 'tf_efficientnetv2_l.in21k_ft_in1k',\n    'tf_efficientnetv2_xl_in21ft1k': 'tf_efficientnetv2_xl.in21k_ft_in1k',\n    'tf_efficientnetv2_s_in21k': 'tf_efficientnetv2_s.in21k',\n    'tf_efficientnetv2_m_in21k': 'tf_efficientnetv2_m.in21k',\n    'tf_efficientnetv2_l_in21k': 'tf_efficientnetv2_l.in21k',\n    'tf_efficientnetv2_xl_in21k': 'tf_efficientnetv2_xl.in21k',\n})\n",
  "\"\"\" MobileNet V3\n\nA PyTorch impl of MobileNet-V3, compatible with TF weights from official impl.\n\nPaper: Searching for MobileNetV3 - https://arxiv.org/abs/1905.02244\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\nfrom functools import partial\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.checkpoint import checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import SelectAdaptivePool2d, Linear, create_conv2d, get_norm_act_layer\nfrom ._builder import build_model_with_cfg, pretrained_cfg_for_features\nfrom ._efficientnet_blocks import SqueezeExcite\nfrom ._efficientnet_builder import EfficientNetBuilder, decode_arch_def, efficientnet_init_weights, \\\n    round_channels, resolve_bn_args, resolve_act_layer, BN_EPS_TF_DEFAULT\nfrom ._features import FeatureInfo, FeatureHooks\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import generate_default_cfgs, register_model, register_model_deprecations\n\n__all__ = ['MobileNetV3', 'MobileNetV3Features']\n\n\nclass MobileNetV3(nn.Module):\n    \"\"\" MobiletNet-V3\n\n    Based on my EfficientNet implementation and building blocks, this model utilizes the MobileNet-v3 specific\n    'efficient head', where global pooling is done before the head convolution without a final batch-norm\n    layer before the classifier.\n\n    Paper: `Searching for MobileNetV3` - https://arxiv.org/abs/1905.02244\n\n    Other architectures utilizing MobileNet-V3 efficient head that are supported by this impl include:\n      * HardCoRe-NAS - https://arxiv.org/abs/2102.11646 (defn in hardcorenas.py uses this class)\n      * FBNet-V3 - https://arxiv.org/abs/2006.02049\n      * LCNet - https://arxiv.org/abs/2109.15099\n    \"\"\"\n\n    def __init__(\n            self,\n            block_args,\n            num_classes=1000,\n            in_chans=3,\n            stem_size=16,\n            fix_stem=False,\n            num_features=1280,\n            head_bias=True,\n            pad_type='',\n            act_layer=None,\n            norm_layer=None,\n            se_layer=None,\n            se_from_exp=True,\n            round_chs_fn=round_channels,\n            drop_rate=0.,\n            drop_path_rate=0.,\n            global_pool='avg',\n    ):\n        super(MobileNetV3, self).__init__()\n        act_layer = act_layer or nn.ReLU\n        norm_layer = norm_layer or nn.BatchNorm2d\n        norm_act_layer = get_norm_act_layer(norm_layer, act_layer)\n        se_layer = se_layer or SqueezeExcite\n        self.num_classes = num_classes\n        self.num_features = num_features\n        self.drop_rate = drop_rate\n        self.grad_checkpointing = False\n\n        # Stem\n        if not fix_stem:\n            stem_size = round_chs_fn(stem_size)\n        self.conv_stem = create_conv2d(in_chans, stem_size, 3, stride=2, padding=pad_type)\n        self.bn1 = norm_act_layer(stem_size, inplace=True)\n\n        # Middle stages (IR/ER/DS Blocks)\n        builder = EfficientNetBuilder(\n            output_stride=32,\n            pad_type=pad_type,\n            round_chs_fn=round_chs_fn,\n            se_from_exp=se_from_exp,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n            se_layer=se_layer,\n            drop_path_rate=drop_path_rate,\n        )\n        self.blocks = nn.Sequential(*builder(stem_size, block_args))\n        self.feature_info = builder.features\n        head_chs = builder.in_chs\n\n        # Head + Pooling\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        num_pooled_chs = head_chs * self.global_pool.feat_mult()\n        self.conv_head = create_conv2d(num_pooled_chs, self.num_features, 1, padding=pad_type, bias=head_bias)\n        self.act2 = act_layer(inplace=True)\n        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()  # don't flatten if pooling disabled\n        self.classifier = Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n        efficientnet_init_weights(self)\n\n    def as_sequential(self):\n        layers = [self.conv_stem, self.bn1]\n        layers.extend(self.blocks)\n        layers.extend([self.global_pool, self.conv_head, self.act2])\n        layers.extend([nn.Flatten(), nn.Dropout(self.drop_rate), self.classifier])\n        return nn.Sequential(*layers)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^conv_stem|bn1',\n            blocks=r'^blocks\\.(\\d+)' if coarse else r'^blocks\\.(\\d+)\\.(\\d+)'\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.classifier\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        # cannot meaningfully change pooling of efficient head after creation\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()  # don't flatten if pooling disabled\n        self.classifier = Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x, flatten=True)\n        else:\n            x = self.blocks(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        x = self.global_pool(x)\n        x = self.conv_head(x)\n        x = self.act2(x)\n        x = self.flatten(x)\n        if pre_logits:\n            return x\n        if self.drop_rate > 0.:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        return self.classifier(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\nclass MobileNetV3Features(nn.Module):\n    \"\"\" MobileNetV3 Feature Extractor\n\n    A work-in-progress feature extraction module for MobileNet-V3 to use as a backbone for segmentation\n    and object detection models.\n    \"\"\"\n\n    def __init__(\n            self,\n            block_args,\n            out_indices=(0, 1, 2, 3, 4),\n            feature_location='bottleneck',\n            in_chans=3,\n            stem_size=16,\n            fix_stem=False,\n            output_stride=32,\n            pad_type='',\n            round_chs_fn=round_channels,\n            se_from_exp=True,\n            act_layer=None,\n            norm_layer=None,\n            se_layer=None,\n            drop_rate=0.,\n            drop_path_rate=0.,\n    ):\n        super(MobileNetV3Features, self).__init__()\n        act_layer = act_layer or nn.ReLU\n        norm_layer = norm_layer or nn.BatchNorm2d\n        se_layer = se_layer or SqueezeExcite\n        self.drop_rate = drop_rate\n        self.grad_checkpointing = False\n\n        # Stem\n        if not fix_stem:\n            stem_size = round_chs_fn(stem_size)\n        self.conv_stem = create_conv2d(in_chans, stem_size, 3, stride=2, padding=pad_type)\n        self.bn1 = norm_layer(stem_size)\n        self.act1 = act_layer(inplace=True)\n\n        # Middle stages (IR/ER/DS Blocks)\n        builder = EfficientNetBuilder(\n            output_stride=output_stride,\n            pad_type=pad_type,\n            round_chs_fn=round_chs_fn,\n            se_from_exp=se_from_exp,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n            se_layer=se_layer,\n            drop_path_rate=drop_path_rate,\n            feature_location=feature_location,\n        )\n        self.blocks = nn.Sequential(*builder(stem_size, block_args))\n        self.feature_info = FeatureInfo(builder.features, out_indices)\n        self._stage_out_idx = {f['stage']: f['index'] for f in self.feature_info.get_dicts()}\n\n        efficientnet_init_weights(self)\n\n        # Register feature extraction hooks with FeatureHooks helper\n        self.feature_hooks = None\n        if feature_location != 'bottleneck':\n            hooks = self.feature_info.get_dicts(keys=('module', 'hook_type'))\n            self.feature_hooks = FeatureHooks(hooks, self.named_modules())\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    def forward(self, x) -> List[torch.Tensor]:\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        if self.feature_hooks is None:\n            features = []\n            if 0 in self._stage_out_idx:\n                features.append(x)  # add stem out\n            for i, b in enumerate(self.blocks):\n                if self.grad_checkpointing and not torch.jit.is_scripting():\n                    x = checkpoint(b, x)\n                else:\n                    x = b(x)\n                if i + 1 in self._stage_out_idx:\n                    features.append(x)\n            return features\n        else:\n            self.blocks(x)\n            out = self.feature_hooks.get_output(x.device)\n            return list(out.values())\n\n\ndef _create_mnv3(variant, pretrained=False, **kwargs):\n    features_mode = ''\n    model_cls = MobileNetV3\n    kwargs_filter = None\n    if kwargs.pop('features_only', False):\n        if 'feature_cfg' in kwargs:\n            features_mode = 'cfg'\n        else:\n            kwargs_filter = ('num_classes', 'num_features', 'head_conv', 'head_bias', 'global_pool')\n            model_cls = MobileNetV3Features\n            features_mode = 'cls'\n\n    model = build_model_with_cfg(\n        model_cls,\n        variant,\n        pretrained,\n        features_only=features_mode == 'cfg',\n        pretrained_strict=features_mode != 'cls',\n        kwargs_filter=kwargs_filter,\n        **kwargs,\n    )\n    if features_mode == 'cls':\n        model.default_cfg = pretrained_cfg_for_features(model.default_cfg)\n    return model\n\n\ndef _gen_mobilenet_v3_rw(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\"Creates a MobileNet-V3 model.\n\n    Ref impl: ?\n    Paper: https://arxiv.org/abs/1905.02244\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    \"\"\"\n    arch_def = [\n        # stage 0, 112x112 in\n        ['ds_r1_k3_s1_e1_c16_nre_noskip'],  # relu\n        # stage 1, 112x112 in\n        ['ir_r1_k3_s2_e4_c24_nre', 'ir_r1_k3_s1_e3_c24_nre'],  # relu\n        # stage 2, 56x56 in\n        ['ir_r3_k5_s2_e3_c40_se0.25_nre'],  # relu\n        # stage 3, 28x28 in\n        ['ir_r1_k3_s2_e6_c80', 'ir_r1_k3_s1_e2.5_c80', 'ir_r2_k3_s1_e2.3_c80'],  # hard-swish\n        # stage 4, 14x14in\n        ['ir_r2_k3_s1_e6_c112_se0.25'],  # hard-swish\n        # stage 5, 14x14in\n        ['ir_r3_k5_s2_e6_c160_se0.25'],  # hard-swish\n        # stage 6, 7x7 in\n        ['cn_r1_k1_s1_c960'],  # hard-swish\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        head_bias=False,\n        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n        norm_layer=partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        act_layer=resolve_act_layer(kwargs, 'hard_swish'),\n        se_layer=partial(SqueezeExcite, gate_layer='hard_sigmoid'),\n        **kwargs,\n    )\n    model = _create_mnv3(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_mobilenet_v3(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\"Creates a MobileNet-V3 model.\n\n    Ref impl: ?\n    Paper: https://arxiv.org/abs/1905.02244\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    \"\"\"\n    if 'small' in variant:\n        num_features = 1024\n        if 'minimal' in variant:\n            act_layer = resolve_act_layer(kwargs, 'relu')\n            arch_def = [\n                # stage 0, 112x112 in\n                ['ds_r1_k3_s2_e1_c16'],\n                # stage 1, 56x56 in\n                ['ir_r1_k3_s2_e4.5_c24', 'ir_r1_k3_s1_e3.67_c24'],\n                # stage 2, 28x28 in\n                ['ir_r1_k3_s2_e4_c40', 'ir_r2_k3_s1_e6_c40'],\n                # stage 3, 14x14 in\n                ['ir_r2_k3_s1_e3_c48'],\n                # stage 4, 14x14in\n                ['ir_r3_k3_s2_e6_c96'],\n                # stage 6, 7x7 in\n                ['cn_r1_k1_s1_c576'],\n            ]\n        else:\n            act_layer = resolve_act_layer(kwargs, 'hard_swish')\n            arch_def = [\n                # stage 0, 112x112 in\n                ['ds_r1_k3_s2_e1_c16_se0.25_nre'],  # relu\n                # stage 1, 56x56 in\n                ['ir_r1_k3_s2_e4.5_c24_nre', 'ir_r1_k3_s1_e3.67_c24_nre'],  # relu\n                # stage 2, 28x28 in\n                ['ir_r1_k5_s2_e4_c40_se0.25', 'ir_r2_k5_s1_e6_c40_se0.25'],  # hard-swish\n                # stage 3, 14x14 in\n                ['ir_r2_k5_s1_e3_c48_se0.25'],  # hard-swish\n                # stage 4, 14x14in\n                ['ir_r3_k5_s2_e6_c96_se0.25'],  # hard-swish\n                # stage 6, 7x7 in\n                ['cn_r1_k1_s1_c576'],  # hard-swish\n            ]\n    else:\n        num_features = 1280\n        if 'minimal' in variant:\n            act_layer = resolve_act_layer(kwargs, 'relu')\n            arch_def = [\n                # stage 0, 112x112 in\n                ['ds_r1_k3_s1_e1_c16'],\n                # stage 1, 112x112 in\n                ['ir_r1_k3_s2_e4_c24', 'ir_r1_k3_s1_e3_c24'],\n                # stage 2, 56x56 in\n                ['ir_r3_k3_s2_e3_c40'],\n                # stage 3, 28x28 in\n                ['ir_r1_k3_s2_e6_c80', 'ir_r1_k3_s1_e2.5_c80', 'ir_r2_k3_s1_e2.3_c80'],\n                # stage 4, 14x14in\n                ['ir_r2_k3_s1_e6_c112'],\n                # stage 5, 14x14in\n                ['ir_r3_k3_s2_e6_c160'],\n                # stage 6, 7x7 in\n                ['cn_r1_k1_s1_c960'],\n            ]\n        else:\n            act_layer = resolve_act_layer(kwargs, 'hard_swish')\n            arch_def = [\n                # stage 0, 112x112 in\n                ['ds_r1_k3_s1_e1_c16_nre'],  # relu\n                # stage 1, 112x112 in\n                ['ir_r1_k3_s2_e4_c24_nre', 'ir_r1_k3_s1_e3_c24_nre'],  # relu\n                # stage 2, 56x56 in\n                ['ir_r3_k5_s2_e3_c40_se0.25_nre'],  # relu\n                # stage 3, 28x28 in\n                ['ir_r1_k3_s2_e6_c80', 'ir_r1_k3_s1_e2.5_c80', 'ir_r2_k3_s1_e2.3_c80'],  # hard-swish\n                # stage 4, 14x14in\n                ['ir_r2_k3_s1_e6_c112_se0.25'],  # hard-swish\n                # stage 5, 14x14in\n                ['ir_r3_k5_s2_e6_c160_se0.25'],  # hard-swish\n                # stage 6, 7x7 in\n                ['cn_r1_k1_s1_c960'],  # hard-swish\n            ]\n    se_layer = partial(SqueezeExcite, gate_layer='hard_sigmoid', force_act_layer=nn.ReLU, rd_round_fn=round_channels)\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        num_features=num_features,\n        stem_size=16,\n        fix_stem=channel_multiplier < 0.75,\n        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n        norm_layer=partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        act_layer=act_layer,\n        se_layer=se_layer,\n        **kwargs,\n    )\n    model = _create_mnv3(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_fbnetv3(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\" FBNetV3\n    Paper: `FBNetV3: Joint Architecture-Recipe Search using Predictor Pretraining`\n        - https://arxiv.org/abs/2006.02049\n    FIXME untested, this is a preliminary impl of some FBNet-V3 variants.\n    \"\"\"\n    vl = variant.split('_')[-1]\n    if vl in ('a', 'b'):\n        stem_size = 16\n        arch_def = [\n            ['ds_r2_k3_s1_e1_c16'],\n            ['ir_r1_k5_s2_e4_c24', 'ir_r3_k5_s1_e2_c24'],\n            ['ir_r1_k5_s2_e5_c40_se0.25', 'ir_r4_k5_s1_e3_c40_se0.25'],\n            ['ir_r1_k5_s2_e5_c72', 'ir_r4_k3_s1_e3_c72'],\n            ['ir_r1_k3_s1_e5_c120_se0.25', 'ir_r5_k5_s1_e3_c120_se0.25'],\n            ['ir_r1_k3_s2_e6_c184_se0.25', 'ir_r5_k5_s1_e4_c184_se0.25', 'ir_r1_k5_s1_e6_c224_se0.25'],\n            ['cn_r1_k1_s1_c1344'],\n        ]\n    elif vl == 'd':\n        stem_size = 24\n        arch_def = [\n            ['ds_r2_k3_s1_e1_c16'],\n            ['ir_r1_k3_s2_e5_c24', 'ir_r5_k3_s1_e2_c24'],\n            ['ir_r1_k5_s2_e4_c40_se0.25', 'ir_r4_k3_s1_e3_c40_se0.25'],\n            ['ir_r1_k3_s2_e5_c72', 'ir_r4_k3_s1_e3_c72'],\n            ['ir_r1_k3_s1_e5_c128_se0.25', 'ir_r6_k5_s1_e3_c128_se0.25'],\n            ['ir_r1_k3_s2_e6_c208_se0.25', 'ir_r5_k5_s1_e5_c208_se0.25', 'ir_r1_k5_s1_e6_c240_se0.25'],\n            ['cn_r1_k1_s1_c1440'],\n        ]\n    elif vl == 'g':\n        stem_size = 32\n        arch_def = [\n            ['ds_r3_k3_s1_e1_c24'],\n            ['ir_r1_k5_s2_e4_c40', 'ir_r4_k5_s1_e2_c40'],\n            ['ir_r1_k5_s2_e4_c56_se0.25', 'ir_r4_k5_s1_e3_c56_se0.25'],\n            ['ir_r1_k5_s2_e5_c104', 'ir_r4_k3_s1_e3_c104'],\n            ['ir_r1_k3_s1_e5_c160_se0.25', 'ir_r8_k5_s1_e3_c160_se0.25'],\n            ['ir_r1_k3_s2_e6_c264_se0.25', 'ir_r6_k5_s1_e5_c264_se0.25', 'ir_r2_k5_s1_e6_c288_se0.25'],\n            ['cn_r1_k1_s1_c1728'],\n        ]\n    else:\n        raise NotImplemented\n    round_chs_fn = partial(round_channels, multiplier=channel_multiplier, round_limit=0.95)\n    se_layer = partial(SqueezeExcite, gate_layer='hard_sigmoid', rd_round_fn=round_chs_fn)\n    act_layer = resolve_act_layer(kwargs, 'hard_swish')\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        num_features=1984,\n        head_bias=False,\n        stem_size=stem_size,\n        round_chs_fn=round_chs_fn,\n        se_from_exp=False,\n        norm_layer=partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        act_layer=act_layer,\n        se_layer=se_layer,\n        **kwargs,\n    )\n    model = _create_mnv3(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_lcnet(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\" LCNet\n    Essentially a MobileNet-V3 crossed with a MobileNet-V1\n\n    Paper: `PP-LCNet: A Lightweight CPU Convolutional Neural Network` - https://arxiv.org/abs/2109.15099\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    \"\"\"\n    arch_def = [\n        # stage 0, 112x112 in\n        ['dsa_r1_k3_s1_c32'],\n        # stage 1, 112x112 in\n        ['dsa_r2_k3_s2_c64'],\n        # stage 2, 56x56 in\n        ['dsa_r2_k3_s2_c128'],\n        # stage 3, 28x28 in\n        ['dsa_r1_k3_s2_c256', 'dsa_r1_k5_s1_c256'],\n        # stage 4, 14x14in\n        ['dsa_r4_k5_s1_c256'],\n        # stage 5, 14x14in\n        ['dsa_r2_k5_s2_c512_se0.25'],\n        # 7x7\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=16,\n        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n        norm_layer=partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        act_layer=resolve_act_layer(kwargs, 'hard_swish'),\n        se_layer=partial(SqueezeExcite, gate_layer='hard_sigmoid', force_act_layer=nn.ReLU),\n        num_features=1280,\n        **kwargs,\n    )\n    model = _create_mnv3(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _gen_lcnet(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    \"\"\" LCNet\n    Essentially a MobileNet-V3 crossed with a MobileNet-V1\n\n    Paper: `PP-LCNet: A Lightweight CPU Convolutional Neural Network` - https://arxiv.org/abs/2109.15099\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    \"\"\"\n    arch_def = [\n        # stage 0, 112x112 in\n        ['dsa_r1_k3_s1_c32'],\n        # stage 1, 112x112 in\n        ['dsa_r2_k3_s2_c64'],\n        # stage 2, 56x56 in\n        ['dsa_r2_k3_s2_c128'],\n        # stage 3, 28x28 in\n        ['dsa_r1_k3_s2_c256', 'dsa_r1_k5_s1_c256'],\n        # stage 4, 14x14in\n        ['dsa_r4_k5_s1_c256'],\n        # stage 5, 14x14in\n        ['dsa_r2_k5_s2_c512_se0.25'],\n        # 7x7\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=16,\n        round_chs_fn=partial(round_channels, multiplier=channel_multiplier),\n        norm_layer=partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        act_layer=resolve_act_layer(kwargs, 'hard_swish'),\n        se_layer=partial(SqueezeExcite, gate_layer='hard_sigmoid', force_act_layer=nn.ReLU),\n        num_features=1280,\n        **kwargs,\n    )\n    model = _create_mnv3(variant, pretrained, **model_kwargs)\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bilinear',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'conv_stem', 'classifier': 'classifier',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'mobilenetv3_large_075.untrained': _cfg(url=''),\n    'mobilenetv3_large_100.ra_in1k': _cfg(\n        interpolation='bicubic',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_large_100_ra-f55367f5.pth',\n        hf_hub_id='timm/'),\n    'mobilenetv3_large_100.miil_in21k_ft_in1k': _cfg(\n        interpolation='bilinear', mean=(0., 0., 0.), std=(1., 1., 1.),\n        origin_url='https://github.com/Alibaba-MIIL/ImageNet21K',\n        paper_ids='arXiv:2104.10972v4',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/mobilenetv3_large_100_1k_miil_78_0-66471c13.pth',\n        hf_hub_id='timm/'),\n    'mobilenetv3_large_100.miil_in21k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/mobilenetv3_large_100_in21k_miil-d71cc17b.pth',\n        hf_hub_id='timm/',\n        origin_url='https://github.com/Alibaba-MIIL/ImageNet21K',\n        paper_ids='arXiv:2104.10972v4',\n        interpolation='bilinear', mean=(0., 0., 0.), std=(1., 1., 1.), num_classes=11221),\n\n    'mobilenetv3_small_050.lamb_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_small_050_lambc-4b7bbe87.pth',\n        hf_hub_id='timm/',\n        interpolation='bicubic'),\n    'mobilenetv3_small_075.lamb_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_small_075_lambc-384766db.pth',\n        hf_hub_id='timm/',\n        interpolation='bicubic'),\n    'mobilenetv3_small_100.lamb_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_small_100_lamb-266a294c.pth',\n        hf_hub_id='timm/',\n        interpolation='bicubic'),\n\n    'mobilenetv3_rw.rmsp_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_100-35495452.pth',\n        interpolation='bicubic'),\n\n    'tf_mobilenetv3_large_075.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_large_075-150ee8b0.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    'tf_mobilenetv3_large_100.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_large_100-427764d5.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    'tf_mobilenetv3_large_minimal_100.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_large_minimal_100-8596ae28.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    'tf_mobilenetv3_small_075.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_small_075-da427f52.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    'tf_mobilenetv3_small_100.in1k': _cfg(\n        url= 'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_small_100-37f49e2b.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    'tf_mobilenetv3_small_minimal_100.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_small_minimal_100-922a7843.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n\n    'fbnetv3_b.ra2_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/fbnetv3_b_224-ead5d2a1.pth',\n        hf_hub_id='timm/',\n        test_input_size=(3, 256, 256), crop_pct=0.95),\n    'fbnetv3_d.ra2_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/fbnetv3_d_224-c98bce42.pth',\n        hf_hub_id='timm/',\n        test_input_size=(3, 256, 256), crop_pct=0.95),\n    'fbnetv3_g.ra2_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/fbnetv3_g_240-0b1df83b.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), test_input_size=(3, 288, 288), crop_pct=0.95, pool_size=(8, 8)),\n\n    \"lcnet_035.untrained\": _cfg(),\n    \"lcnet_050.ra2_in1k\": _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/lcnet_050-f447553b.pth',\n        hf_hub_id='timm/',\n        interpolation='bicubic',\n    ),\n    \"lcnet_075.ra2_in1k\": _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/lcnet_075-318cad2c.pth',\n        hf_hub_id='timm/',\n        interpolation='bicubic',\n    ),\n    \"lcnet_100.ra2_in1k\": _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/lcnet_100-a929038c.pth',\n        hf_hub_id='timm/',\n        interpolation='bicubic',\n    ),\n    \"lcnet_150.untrained\": _cfg(),\n})\n\n\n@register_model\ndef mobilenetv3_large_075(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" MobileNet V3 \"\"\"\n    model = _gen_mobilenet_v3('mobilenetv3_large_075', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv3_large_100(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" MobileNet V3 \"\"\"\n    model = _gen_mobilenet_v3('mobilenetv3_large_100', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv3_small_050(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" MobileNet V3 \"\"\"\n    model = _gen_mobilenet_v3('mobilenetv3_small_050', 0.50, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv3_small_075(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" MobileNet V3 \"\"\"\n    model = _gen_mobilenet_v3('mobilenetv3_small_075', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv3_small_100(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" MobileNet V3 \"\"\"\n    model = _gen_mobilenet_v3('mobilenetv3_small_100', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv3_rw(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" MobileNet V3 \"\"\"\n    if pretrained:\n        # pretrained model trained with non-default BN epsilon\n        kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    model = _gen_mobilenet_v3_rw('mobilenetv3_rw', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mobilenetv3_large_075(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" MobileNet V3 \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_mobilenet_v3('tf_mobilenetv3_large_075', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mobilenetv3_large_100(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" MobileNet V3 \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_mobilenet_v3('tf_mobilenetv3_large_100', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mobilenetv3_large_minimal_100(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" MobileNet V3 \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_mobilenet_v3('tf_mobilenetv3_large_minimal_100', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mobilenetv3_small_075(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" MobileNet V3 \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_mobilenet_v3('tf_mobilenetv3_small_075', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mobilenetv3_small_100(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" MobileNet V3 \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_mobilenet_v3('tf_mobilenetv3_small_100', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mobilenetv3_small_minimal_100(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" MobileNet V3 \"\"\"\n    kwargs['bn_eps'] = BN_EPS_TF_DEFAULT\n    kwargs['pad_type'] = 'same'\n    model = _gen_mobilenet_v3('tf_mobilenetv3_small_minimal_100', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef fbnetv3_b(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" FBNetV3-B \"\"\"\n    model = _gen_fbnetv3('fbnetv3_b', pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef fbnetv3_d(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" FBNetV3-D \"\"\"\n    model = _gen_fbnetv3('fbnetv3_d', pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef fbnetv3_g(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" FBNetV3-G \"\"\"\n    model = _gen_fbnetv3('fbnetv3_g', pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef lcnet_035(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" PP-LCNet 0.35\"\"\"\n    model = _gen_lcnet('lcnet_035', 0.35, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef lcnet_050(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" PP-LCNet 0.5\"\"\"\n    model = _gen_lcnet('lcnet_050', 0.5, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef lcnet_075(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" PP-LCNet 1.0\"\"\"\n    model = _gen_lcnet('lcnet_075', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef lcnet_100(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" PP-LCNet 1.0\"\"\"\n    model = _gen_lcnet('lcnet_100', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef lcnet_150(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" PP-LCNet 1.5\"\"\"\n    model = _gen_lcnet('lcnet_150', 1.5, pretrained=pretrained, **kwargs)\n    return model\n\n\nregister_model_deprecations(__name__, {\n    'mobilenetv3_large_100_miil': 'mobilenetv3_large_100.miil_in21k_ft_in1k',\n    'mobilenetv3_large_100_miil_in21k': 'mobilenetv3_large_100.miil_in21k',\n})\n",
  "\"\"\" MLP-Mixer, ResMLP, and gMLP in PyTorch\n\nThis impl originally based on MLP-Mixer paper.\n\nOfficial JAX impl: https://github.com/google-research/vision_transformer/blob/linen/vit_jax/models_mixer.py\n\nPaper: 'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n\n@article{tolstikhin2021,\n  title={MLP-Mixer: An all-MLP Architecture for Vision},\n  author={Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner,\n        Thomas and Yung, Jessica and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},\n  journal={arXiv preprint arXiv:2105.01601},\n  year={2021}\n}\n\nAlso supporting ResMlp, and a preliminary (not verified) implementations of gMLP\n\nCode: https://github.com/facebookresearch/deit\nPaper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n@misc{touvron2021resmlp,\n      title={ResMLP: Feedforward networks for image classification with data-efficient training},\n      author={Hugo Touvron and Piotr Bojanowski and Mathilde Caron and Matthieu Cord and Alaaeldin El-Nouby and\n        Edouard Grave and Armand Joulin and Gabriel Synnaeve and Jakob Verbeek and Herv Jgou},\n      year={2021},\n      eprint={2105.03404},\n}\n\nPaper: `Pay Attention to MLPs` - https://arxiv.org/abs/2105.08050\n@misc{liu2021pay,\n      title={Pay Attention to MLPs},\n      author={Hanxiao Liu and Zihang Dai and David R. So and Quoc V. Le},\n      year={2021},\n      eprint={2105.08050},\n}\n\nA thank you to paper authors for releasing code and weights.\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import PatchEmbed, Mlp, GluMlp, GatedMlp, DropPath, lecun_normal_, to_2tuple\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import named_apply, checkpoint_seq\nfrom ._registry import generate_default_cfgs, register_model, register_model_deprecations\n\n__all__ = ['MixerBlock', 'MlpMixer']  # model_registry will add each entrypoint fn to this\n\n\nclass MixerBlock(nn.Module):\n    \"\"\" Residual Block w/ token mixing and channel MLPs\n    Based on: 'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n    \"\"\"\n    def __init__(\n            self,\n            dim,\n            seq_len,\n            mlp_ratio=(0.5, 4.0),\n            mlp_layer=Mlp,\n            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n            act_layer=nn.GELU,\n            drop=0.,\n            drop_path=0.,\n    ):\n        super().__init__()\n        tokens_dim, channels_dim = [int(x * dim) for x in to_2tuple(mlp_ratio)]\n        self.norm1 = norm_layer(dim)\n        self.mlp_tokens = mlp_layer(seq_len, tokens_dim, act_layer=act_layer, drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        self.mlp_channels = mlp_layer(dim, channels_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.mlp_tokens(self.norm1(x).transpose(1, 2)).transpose(1, 2))\n        x = x + self.drop_path(self.mlp_channels(self.norm2(x)))\n        return x\n\n\nclass Affine(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.alpha = nn.Parameter(torch.ones((1, 1, dim)))\n        self.beta = nn.Parameter(torch.zeros((1, 1, dim)))\n\n    def forward(self, x):\n        return torch.addcmul(self.beta, self.alpha, x)\n\n\nclass ResBlock(nn.Module):\n    \"\"\" Residual MLP block w/ LayerScale and Affine 'norm'\n\n    Based on: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n    \"\"\"\n    def __init__(\n            self,\n            dim,\n            seq_len,\n            mlp_ratio=4,\n            mlp_layer=Mlp,\n            norm_layer=Affine,\n            act_layer=nn.GELU,\n            init_values=1e-4,\n            drop=0.,\n            drop_path=0.,\n    ):\n        super().__init__()\n        channel_dim = int(dim * mlp_ratio)\n        self.norm1 = norm_layer(dim)\n        self.linear_tokens = nn.Linear(seq_len, seq_len)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        self.mlp_channels = mlp_layer(dim, channel_dim, act_layer=act_layer, drop=drop)\n        self.ls1 = nn.Parameter(init_values * torch.ones(dim))\n        self.ls2 = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        x = x + self.drop_path(self.ls1 * self.linear_tokens(self.norm1(x).transpose(1, 2)).transpose(1, 2))\n        x = x + self.drop_path(self.ls2 * self.mlp_channels(self.norm2(x)))\n        return x\n\n\nclass SpatialGatingUnit(nn.Module):\n    \"\"\" Spatial Gating Unit\n\n    Based on: `Pay Attention to MLPs` - https://arxiv.org/abs/2105.08050\n    \"\"\"\n    def __init__(self, dim, seq_len, norm_layer=nn.LayerNorm):\n        super().__init__()\n        gate_dim = dim // 2\n        self.norm = norm_layer(gate_dim)\n        self.proj = nn.Linear(seq_len, seq_len)\n\n    def init_weights(self):\n        # special init for the projection gate, called as override by base model init\n        nn.init.normal_(self.proj.weight, std=1e-6)\n        nn.init.ones_(self.proj.bias)\n\n    def forward(self, x):\n        u, v = x.chunk(2, dim=-1)\n        v = self.norm(v)\n        v = self.proj(v.transpose(-1, -2))\n        return u * v.transpose(-1, -2)\n\n\nclass SpatialGatingBlock(nn.Module):\n    \"\"\" Residual Block w/ Spatial Gating\n\n    Based on: `Pay Attention to MLPs` - https://arxiv.org/abs/2105.08050\n    \"\"\"\n    def __init__(\n            self,\n            dim,\n            seq_len,\n            mlp_ratio=4,\n            mlp_layer=GatedMlp,\n            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n            act_layer=nn.GELU,\n            drop=0.,\n            drop_path=0.,\n    ):\n        super().__init__()\n        channel_dim = int(dim * mlp_ratio)\n        self.norm = norm_layer(dim)\n        sgu = partial(SpatialGatingUnit, seq_len=seq_len)\n        self.mlp_channels = mlp_layer(dim, channel_dim, act_layer=act_layer, gate_layer=sgu, drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        x = x + self.drop_path(self.mlp_channels(self.norm(x)))\n        return x\n\n\nclass MlpMixer(nn.Module):\n\n    def __init__(\n            self,\n            num_classes=1000,\n            img_size=224,\n            in_chans=3,\n            patch_size=16,\n            num_blocks=8,\n            embed_dim=512,\n            mlp_ratio=(0.5, 4.0),\n            block_layer=MixerBlock,\n            mlp_layer=Mlp,\n            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n            act_layer=nn.GELU,\n            drop_rate=0.,\n            proj_drop_rate=0.,\n            drop_path_rate=0.,\n            nlhb=False,\n            stem_norm=False,\n            global_pool='avg',\n    ):\n        super().__init__()\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.grad_checkpointing = False\n\n        self.stem = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer if stem_norm else None,\n        )\n        # FIXME drop_path (stochastic depth scaling rule or all the same?)\n        self.blocks = nn.Sequential(*[\n            block_layer(\n                embed_dim,\n                self.stem.num_patches,\n                mlp_ratio,\n                mlp_layer=mlp_layer,\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                drop=proj_drop_rate,\n                drop_path=drop_path_rate,\n            )\n            for _ in range(num_blocks)])\n        self.norm = norm_layer(embed_dim)\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n\n        self.init_weights(nlhb=nlhb)\n\n    @torch.jit.ignore\n    def init_weights(self, nlhb=False):\n        head_bias = -math.log(self.num_classes) if nlhb else 0.\n        named_apply(partial(_init_weights, head_bias=head_bias), module=self)  # depth-first\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^stem',  # stem and embed\n            blocks=[(r'^blocks\\.(\\d+)', None), (r'^norm', (99999,))]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('', 'avg')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool == 'avg':\n            x = x.mean(dim=1)\n        x = self.head_drop(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _init_weights(module: nn.Module, name: str, head_bias: float = 0., flax=False):\n    \"\"\" Mixer weight initialization (trying to match Flax defaults)\n    \"\"\"\n    if isinstance(module, nn.Linear):\n        if name.startswith('head'):\n            nn.init.zeros_(module.weight)\n            nn.init.constant_(module.bias, head_bias)\n        else:\n            if flax:\n                # Flax defaults\n                lecun_normal_(module.weight)\n                if module.bias is not None:\n                    nn.init.zeros_(module.bias)\n            else:\n                # like MLP init in vit (my original init)\n                nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    if 'mlp' in name:\n                        nn.init.normal_(module.bias, std=1e-6)\n                    else:\n                        nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Conv2d):\n        lecun_normal_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d, nn.GroupNorm)):\n        nn.init.ones_(module.weight)\n        nn.init.zeros_(module.bias)\n    elif hasattr(module, 'init_weights'):\n        # NOTE if a parent module contains init_weights method, it can override the init of the\n        # child modules as this will be called in depth-first order.\n        module.init_weights()\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    \"\"\" Remap checkpoints if needed \"\"\"\n    if 'patch_embed.proj.weight' in state_dict:\n        # Remap FB ResMlp models -> timm\n        out_dict = {}\n        for k, v in state_dict.items():\n            k = k.replace('patch_embed.', 'stem.')\n            k = k.replace('attn.', 'linear_tokens.')\n            k = k.replace('mlp.', 'mlp_channels.')\n            k = k.replace('gamma_', 'ls')\n            if k.endswith('.alpha') or k.endswith('.beta'):\n                v = v.reshape(1, 1, -1)\n            out_dict[k] = v\n        return out_dict\n    return state_dict\n\n\ndef _create_mixer(variant, pretrained=False, **kwargs):\n    if kwargs.get('features_only', None):\n        raise RuntimeError('features_only not implemented for MLP-Mixer models.')\n\n    model = build_model_with_cfg(\n        MlpMixer,\n        variant,\n        pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        **kwargs,\n    )\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': 0.875, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n        'first_conv': 'stem.proj', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'mixer_s32_224.untrained': _cfg(),\n    'mixer_s16_224.untrained': _cfg(),\n    'mixer_b32_224.untrained': _cfg(),\n    'mixer_b16_224.goog_in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_mixer_b16_224-76587d61.pth',\n    ),\n    'mixer_b16_224.goog_in21k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_mixer_b16_224_in21k-617b3de2.pth',\n        num_classes=21843\n    ),\n    'mixer_l32_224.untrained': _cfg(),\n    'mixer_l16_224.goog_in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_mixer_l16_224-92f9adc4.pth',\n    ),\n    'mixer_l16_224.goog_in21k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_mixer_l16_224_in21k-846aa33c.pth',\n        num_classes=21843\n    ),\n\n    # Mixer ImageNet-21K-P pretraining\n    'mixer_b16_224.miil_in21k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/mixer_b16_224_miil_in21k-2a558a71.pth',\n        mean=(0., 0., 0.), std=(1., 1., 1.), crop_pct=0.875, interpolation='bilinear', num_classes=11221,\n    ),\n    'mixer_b16_224.miil_in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/mixer_b16_224_miil-9229a591.pth',\n        mean=(0., 0., 0.), std=(1., 1., 1.), crop_pct=0.875, interpolation='bilinear',\n    ),\n\n    'gmixer_12_224.untrained': _cfg(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n    'gmixer_24_224.ra3_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gmixer_24_224_raa-7daf7ae6.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n\n    'resmlp_12_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/resmlp_12_no_dist.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n    'resmlp_24_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/resmlp_24_no_dist.pth',\n        #url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resmlp_24_224_raa-a8256759.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n    'resmlp_36_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/resmlp_36_no_dist.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n    'resmlp_big_24_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/resmlpB_24_no_dist.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n\n    'resmlp_12_224.fb_distilled_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/resmlp_12_dist.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n    'resmlp_24_224.fb_distilled_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/resmlp_24_dist.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n    'resmlp_36_224.fb_distilled_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/resmlp_36_dist.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n    'resmlp_big_24_224.fb_distilled_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/resmlpB_24_dist.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n\n    'resmlp_big_24_224.fb_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/resmlpB_24_22k.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n\n    'resmlp_12_224.fb_dino': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/resmlp_12_dino.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n    'resmlp_24_224.fb_dino': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/resmlp_24_dino.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n\n    'gmlp_ti16_224.untrained': _cfg(),\n    'gmlp_s16_224.ra3_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gmlp_s16_224_raa-10536d42.pth',\n    ),\n    'gmlp_b16_224.untrained': _cfg(),\n})\n\n\n@register_model\ndef mixer_s32_224(pretrained=False, **kwargs) -> MlpMixer:\n    \"\"\" Mixer-S/32 224x224\n    Paper: 'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n    \"\"\"\n    model_args = dict(patch_size=32, num_blocks=8, embed_dim=512, **kwargs)\n    model = _create_mixer('mixer_s32_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef mixer_s16_224(pretrained=False, **kwargs) -> MlpMixer:\n    \"\"\" Mixer-S/16 224x224\n    Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n    \"\"\"\n    model_args = dict(patch_size=16, num_blocks=8, embed_dim=512, **kwargs)\n    model = _create_mixer('mixer_s16_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef mixer_b32_224(pretrained=False, **kwargs) -> MlpMixer:\n    \"\"\" Mixer-B/32 224x224\n    Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n    \"\"\"\n    model_args = dict(patch_size=32, num_blocks=12, embed_dim=768, **kwargs)\n    model = _create_mixer('mixer_b32_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef mixer_b16_224(pretrained=False, **kwargs) -> MlpMixer:\n    \"\"\" Mixer-B/16 224x224. ImageNet-1k pretrained weights.\n    Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n    \"\"\"\n    model_args = dict(patch_size=16, num_blocks=12, embed_dim=768, **kwargs)\n    model = _create_mixer('mixer_b16_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef mixer_l32_224(pretrained=False, **kwargs) -> MlpMixer:\n    \"\"\" Mixer-L/32 224x224.\n    Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n    \"\"\"\n    model_args = dict(patch_size=32, num_blocks=24, embed_dim=1024, **kwargs)\n    model = _create_mixer('mixer_l32_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef mixer_l16_224(pretrained=False, **kwargs) -> MlpMixer:\n    \"\"\" Mixer-L/16 224x224. ImageNet-1k pretrained weights.\n    Paper:  'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601\n    \"\"\"\n    model_args = dict(patch_size=16, num_blocks=24, embed_dim=1024, **kwargs)\n    model = _create_mixer('mixer_l16_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef gmixer_12_224(pretrained=False, **kwargs) -> MlpMixer:\n    \"\"\" Glu-Mixer-12 224x224\n    Experiment by Ross Wightman, adding SwiGLU to MLP-Mixer\n    \"\"\"\n    model_args = dict(\n        patch_size=16, num_blocks=12, embed_dim=384, mlp_ratio=(1.0, 4.0),\n        mlp_layer=GluMlp, act_layer=nn.SiLU, **kwargs)\n    model = _create_mixer('gmixer_12_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef gmixer_24_224(pretrained=False, **kwargs) -> MlpMixer:\n    \"\"\" Glu-Mixer-24 224x224\n    Experiment by Ross Wightman, adding SwiGLU to MLP-Mixer\n    \"\"\"\n    model_args = dict(\n        patch_size=16, num_blocks=24, embed_dim=384, mlp_ratio=(1.0, 4.0),\n        mlp_layer=GluMlp, act_layer=nn.SiLU, **kwargs)\n    model = _create_mixer('gmixer_24_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef resmlp_12_224(pretrained=False, **kwargs) -> MlpMixer:\n    \"\"\" ResMLP-12\n    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n    \"\"\"\n    model_args = dict(\n        patch_size=16, num_blocks=12, embed_dim=384, mlp_ratio=4, block_layer=ResBlock, norm_layer=Affine, **kwargs)\n    model = _create_mixer('resmlp_12_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef resmlp_24_224(pretrained=False, **kwargs) -> MlpMixer:\n    \"\"\" ResMLP-24\n    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n    \"\"\"\n    model_args = dict(\n        patch_size=16, num_blocks=24, embed_dim=384, mlp_ratio=4,\n        block_layer=partial(ResBlock, init_values=1e-5), norm_layer=Affine, **kwargs)\n    model = _create_mixer('resmlp_24_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef resmlp_36_224(pretrained=False, **kwargs) -> MlpMixer:\n    \"\"\" ResMLP-36\n    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n    \"\"\"\n    model_args = dict(\n        patch_size=16, num_blocks=36, embed_dim=384, mlp_ratio=4,\n        block_layer=partial(ResBlock, init_values=1e-6), norm_layer=Affine, **kwargs)\n    model = _create_mixer('resmlp_36_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef resmlp_big_24_224(pretrained=False, **kwargs) -> MlpMixer:\n    \"\"\" ResMLP-B-24\n    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404\n    \"\"\"\n    model_args = dict(\n        patch_size=8, num_blocks=24, embed_dim=768, mlp_ratio=4,\n        block_layer=partial(ResBlock, init_values=1e-6), norm_layer=Affine, **kwargs)\n    model = _create_mixer('resmlp_big_24_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef gmlp_ti16_224(pretrained=False, **kwargs) -> MlpMixer:\n    \"\"\" gMLP-Tiny\n    Paper: `Pay Attention to MLPs` - https://arxiv.org/abs/2105.08050\n    \"\"\"\n    model_args = dict(\n        patch_size=16, num_blocks=30, embed_dim=128, mlp_ratio=6, block_layer=SpatialGatingBlock,\n        mlp_layer=GatedMlp, **kwargs)\n    model = _create_mixer('gmlp_ti16_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef gmlp_s16_224(pretrained=False, **kwargs) -> MlpMixer:\n    \"\"\" gMLP-Small\n    Paper: `Pay Attention to MLPs` - https://arxiv.org/abs/2105.08050\n    \"\"\"\n    model_args = dict(\n        patch_size=16, num_blocks=30, embed_dim=256, mlp_ratio=6, block_layer=SpatialGatingBlock,\n        mlp_layer=GatedMlp, **kwargs)\n    model = _create_mixer('gmlp_s16_224', pretrained=pretrained, **model_args)\n    return model\n\n\n@register_model\ndef gmlp_b16_224(pretrained=False, **kwargs) -> MlpMixer:\n    \"\"\" gMLP-Base\n    Paper: `Pay Attention to MLPs` - https://arxiv.org/abs/2105.08050\n    \"\"\"\n    model_args = dict(\n        patch_size=16, num_blocks=30, embed_dim=512, mlp_ratio=6, block_layer=SpatialGatingBlock,\n        mlp_layer=GatedMlp, **kwargs)\n    model = _create_mixer('gmlp_b16_224', pretrained=pretrained, **model_args)\n    return model\n\n\nregister_model_deprecations(__name__, {\n    'mixer_b16_224_in21k': 'mixer_b16_224.goog_in21k_ft_in1k',\n    'mixer_l16_224_in21k': 'mixer_l16_224.goog_in21k_ft_in1k',\n    'mixer_b16_224_miil': 'mixer_b16_224.miil_in21k_ft_in1k',\n    'mixer_b16_224_miil_in21k': 'mixer_b16_224.miil_in21k',\n    'resmlp_12_distilled_224': 'resmlp_12_224.fb_distilled_in1k',\n    'resmlp_24_distilled_224': 'resmlp_24_224.fb_distilled_in1k',\n    'resmlp_36_distilled_224': 'resmlp_36_224.fb_distilled_in1k',\n    'resmlp_big_24_distilled_224': 'resmlp_big_24_224.fb_distilled_in1k',\n    'resmlp_big_24_224_in22ft1k': 'resmlp_big_24_224.fb_in22k_ft_in1k',\n    'resmlp_12_224_dino': 'resmlp_12_224',\n    'resmlp_24_224_dino': 'resmlp_24_224',\n})\n",
  "\"\"\" Class-Attention in Image Transformers (CaiT)\n\nPaper: 'Going deeper with Image Transformers' - https://arxiv.org/abs/2103.17239\n\nOriginal code and weights from https://github.com/facebookresearch/deit, copyright below\n\nModifications and additions for timm hacked together by / Copyright 2021, Ross Wightman\n\"\"\"\n# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import PatchEmbed, Mlp, DropPath, trunc_normal_, use_fused_attn\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['Cait', 'ClassAttn', 'LayerScaleBlockClassAttn', 'LayerScaleBlock', 'TalkingHeadAttn']\n\n\nclass ClassAttn(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications to do CA\n    fused_attn: torch.jit.Final[bool]\n\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n        self.fused_attn = use_fused_attn()\n\n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        q = self.q(x[:, 0]).unsqueeze(1).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        if self.fused_attn:\n            x_cls = torch.nn.functional.scaled_dot_product_attention(\n                q, k, v,\n                dropout_p=self.attn_drop.p,\n            )\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            x_cls = attn @ v\n\n        x_cls = x_cls.transpose(1, 2).reshape(B, 1, C)\n        x_cls = self.proj(x_cls)\n        x_cls = self.proj_drop(x_cls)\n\n        return x_cls\n\n\nclass LayerScaleBlockClassAttn(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications to add CA and LayerScale\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            attn_block=ClassAttn,\n            mlp_block=Mlp,\n            init_values=1e-4,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = attn_block(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n        )\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = mlp_block(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n        self.gamma_1 = nn.Parameter(init_values * torch.ones(dim))\n        self.gamma_2 = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x, x_cls):\n        u = torch.cat((x_cls, x), dim=1)\n        x_cls = x_cls + self.drop_path(self.gamma_1 * self.attn(self.norm1(u)))\n        x_cls = x_cls + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x_cls)))\n        return x_cls\n\n\nclass TalkingHeadAttn(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications to add Talking Heads Attention (https://arxiv.org/pdf/2003.02436v1.pdf)\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n\n        self.num_heads = num_heads\n\n        head_dim = dim // num_heads\n\n        self.scale = head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n\n        self.proj = nn.Linear(dim, dim)\n\n        self.proj_l = nn.Linear(num_heads, num_heads)\n        self.proj_w = nn.Linear(num_heads, num_heads)\n\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n\n        attn = q @ k.transpose(-2, -1)\n\n        attn = self.proj_l(attn.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n\n        attn = attn.softmax(dim=-1)\n\n        attn = self.proj_w(attn.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass LayerScaleBlock(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications to add layerScale\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            attn_block=TalkingHeadAttn,\n            mlp_block=Mlp,\n            init_values=1e-4,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = attn_block(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n        )\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = mlp_block(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n        self.gamma_1 = nn.Parameter(init_values * torch.ones(dim))\n        self.gamma_2 = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass Cait(nn.Module):\n    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n    # with slight modifications to adapt to our cait models\n    def __init__(\n            self,\n            img_size=224,\n            patch_size=16,\n            in_chans=3,\n            num_classes=1000,\n            global_pool='token',\n            embed_dim=768,\n            depth=12,\n            num_heads=12,\n            mlp_ratio=4.,\n            qkv_bias=True,\n            drop_rate=0.,\n            pos_drop_rate=0.,\n            proj_drop_rate=0.,\n            attn_drop_rate=0.,\n            drop_path_rate=0.,\n            block_layers=LayerScaleBlock,\n            block_layers_token=LayerScaleBlockClassAttn,\n            patch_layer=PatchEmbed,\n            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n            act_layer=nn.GELU,\n            attn_block=TalkingHeadAttn,\n            mlp_block=Mlp,\n            init_values=1e-4,\n            attn_block_token_only=ClassAttn,\n            mlp_block_token_only=Mlp,\n            depth_token_only=2,\n            mlp_ratio_token_only=4.0\n    ):\n        super().__init__()\n        assert global_pool in ('', 'token', 'avg')\n\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.num_features = self.embed_dim = embed_dim\n        self.grad_checkpointing = False\n\n        self.patch_embed = patch_layer(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n        )\n\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n\n        dpr = [drop_path_rate for i in range(depth)]\n        self.blocks = nn.Sequential(*[block_layers(\n            dim=embed_dim,\n            num_heads=num_heads,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            proj_drop=proj_drop_rate,\n            attn_drop=attn_drop_rate,\n            drop_path=dpr[i],\n            norm_layer=norm_layer,\n            act_layer=act_layer,\n            attn_block=attn_block,\n            mlp_block=mlp_block,\n            init_values=init_values,\n        ) for i in range(depth)])\n\n        self.blocks_token_only = nn.ModuleList([block_layers_token(\n            dim=embed_dim,\n            num_heads=num_heads,\n            mlp_ratio=mlp_ratio_token_only,\n            qkv_bias=qkv_bias,\n            norm_layer=norm_layer,\n            act_layer=act_layer,\n            attn_block=attn_block_token_only,\n            mlp_block=mlp_block_token_only,\n            init_values=init_values,\n        ) for _ in range(depth_token_only)])\n\n        self.norm = norm_layer(embed_dim)\n\n        self.feature_info = [dict(num_chs=embed_dim, reduction=0, module='head')]\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        def _matcher(name):\n            if any([name.startswith(n) for n in ('cls_token', 'pos_embed', 'patch_embed')]):\n                return 0\n            elif name.startswith('blocks.'):\n                return int(name.split('.')[1]) + 1\n            elif name.startswith('blocks_token_only.'):\n                # overlap token only blocks with last blocks\n                to_offset = len(self.blocks) - len(self.blocks_token_only) + 1\n                return int(name.split('.')[1]) + to_offset\n            elif name.startswith('norm.'):\n                return len(self.blocks)\n            else:\n                return float('inf')\n        return _matcher\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('', 'token', 'avg')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n        for i, blk in enumerate(self.blocks_token_only):\n            cls_tokens = blk(x, cls_tokens)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool:\n            x = x[:, 1:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]\n        x = self.head_drop(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef checkpoint_filter_fn(state_dict, model=None):\n    if 'model' in state_dict:\n        state_dict = state_dict['model']\n    checkpoint_no_module = {}\n    for k, v in state_dict.items():\n        checkpoint_no_module[k.replace('module.', '')] = v\n    return checkpoint_no_module\n\n\ndef _create_cait(variant, pretrained=False, **kwargs):\n    if kwargs.get('features_only', None):\n        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n\n    model = build_model_with_cfg(\n        Cait,\n        variant,\n        pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        **kwargs,\n    )\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 384, 384), 'pool_size': None,\n        'crop_pct': 1.0, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'cait_xxs24_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/XXS24_224.pth',\n        input_size=(3, 224, 224),\n    ),\n    'cait_xxs24_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/XXS24_384.pth',\n    ),\n    'cait_xxs36_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/XXS36_224.pth',\n        input_size=(3, 224, 224),\n    ),\n    'cait_xxs36_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/XXS36_384.pth',\n    ),\n    'cait_xs24_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/XS24_384.pth',\n    ),\n    'cait_s24_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/S24_224.pth',\n        input_size=(3, 224, 224),\n    ),\n    'cait_s24_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/S24_384.pth',\n    ),\n    'cait_s36_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/S36_384.pth',\n    ),\n    'cait_m36_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/M36_384.pth',\n    ),\n    'cait_m48_448.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/deit/M48_448.pth',\n        input_size=(3, 448, 448),\n    ),\n})\n\n\n@register_model\ndef cait_xxs24_224(pretrained=False, **kwargs) -> Cait:\n    model_args = dict(patch_size=16, embed_dim=192, depth=24, num_heads=4, init_values=1e-5)\n    model = _create_cait('cait_xxs24_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef cait_xxs24_384(pretrained=False, **kwargs) -> Cait:\n    model_args = dict(patch_size=16, embed_dim=192, depth=24, num_heads=4, init_values=1e-5)\n    model = _create_cait('cait_xxs24_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef cait_xxs36_224(pretrained=False, **kwargs) -> Cait:\n    model_args = dict(patch_size=16, embed_dim=192, depth=36, num_heads=4, init_values=1e-5)\n    model = _create_cait('cait_xxs36_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef cait_xxs36_384(pretrained=False, **kwargs) -> Cait:\n    model_args = dict(patch_size=16, embed_dim=192, depth=36, num_heads=4, init_values=1e-5)\n    model = _create_cait('cait_xxs36_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef cait_xs24_384(pretrained=False, **kwargs) -> Cait:\n    model_args = dict(patch_size=16, embed_dim=288, depth=24, num_heads=6, init_values=1e-5)\n    model = _create_cait('cait_xs24_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef cait_s24_224(pretrained=False, **kwargs) -> Cait:\n    model_args = dict(patch_size=16, embed_dim=384, depth=24, num_heads=8, init_values=1e-5)\n    model = _create_cait('cait_s24_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef cait_s24_384(pretrained=False, **kwargs) -> Cait:\n    model_args = dict(patch_size=16, embed_dim=384, depth=24, num_heads=8, init_values=1e-5)\n    model = _create_cait('cait_s24_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef cait_s36_384(pretrained=False, **kwargs) -> Cait:\n    model_args = dict(patch_size=16, embed_dim=384, depth=36, num_heads=8, init_values=1e-6)\n    model = _create_cait('cait_s36_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef cait_m36_384(pretrained=False, **kwargs) -> Cait:\n    model_args = dict(patch_size=16, embed_dim=768, depth=36, num_heads=16, init_values=1e-6)\n    model = _create_cait('cait_m36_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef cait_m48_448(pretrained=False, **kwargs) -> Cait:\n    model_args = dict(patch_size=16, embed_dim=768, depth=48, num_heads=16, init_values=1e-6)\n    model = _create_cait('cait_m48_448', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n",
  "\"\"\" PyTorch implementation of DualPathNetworks\nBased on original MXNet implementation https://github.com/cypw/DPNs with\nmany ideas from another PyTorch implementation https://github.com/oyam/pytorch-DPNs.\n\nThis implementation is compatible with the pretrained weights from cypw's MXNet implementation.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom collections import OrderedDict\nfrom functools import partial\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DPN_MEAN, IMAGENET_DPN_STD, IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import BatchNormAct2d, ConvNormAct, create_conv2d, create_classifier, get_norm_act_layer\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['DPN']\n\n\nclass CatBnAct(nn.Module):\n    def __init__(self, in_chs, norm_layer=BatchNormAct2d):\n        super(CatBnAct, self).__init__()\n        self.bn = norm_layer(in_chs, eps=0.001)\n\n    @torch.jit._overload_method  # noqa: F811\n    def forward(self, x):\n        # type: (Tuple[torch.Tensor, torch.Tensor]) -> (torch.Tensor)\n        pass\n\n    @torch.jit._overload_method  # noqa: F811\n    def forward(self, x):\n        # type: (torch.Tensor) -> (torch.Tensor)\n        pass\n\n    def forward(self, x):\n        if isinstance(x, tuple):\n            x = torch.cat(x, dim=1)\n        return self.bn(x)\n\n\nclass BnActConv2d(nn.Module):\n    def __init__(self, in_chs, out_chs, kernel_size, stride, groups=1, norm_layer=BatchNormAct2d):\n        super(BnActConv2d, self).__init__()\n        self.bn = norm_layer(in_chs, eps=0.001)\n        self.conv = create_conv2d(in_chs, out_chs, kernel_size, stride=stride, groups=groups)\n\n    def forward(self, x):\n        return self.conv(self.bn(x))\n\n\nclass DualPathBlock(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            num_1x1_a,\n            num_3x3_b,\n            num_1x1_c,\n            inc,\n            groups,\n            block_type='normal',\n            b=False,\n    ):\n        super(DualPathBlock, self).__init__()\n        self.num_1x1_c = num_1x1_c\n        self.inc = inc\n        self.b = b\n        if block_type == 'proj':\n            self.key_stride = 1\n            self.has_proj = True\n        elif block_type == 'down':\n            self.key_stride = 2\n            self.has_proj = True\n        else:\n            assert block_type == 'normal'\n            self.key_stride = 1\n            self.has_proj = False\n\n        self.c1x1_w_s1 = None\n        self.c1x1_w_s2 = None\n        if self.has_proj:\n            # Using different member names here to allow easier parameter key matching for conversion\n            if self.key_stride == 2:\n                self.c1x1_w_s2 = BnActConv2d(\n                    in_chs=in_chs, out_chs=num_1x1_c + 2 * inc, kernel_size=1, stride=2)\n            else:\n                self.c1x1_w_s1 = BnActConv2d(\n                    in_chs=in_chs, out_chs=num_1x1_c + 2 * inc, kernel_size=1, stride=1)\n\n        self.c1x1_a = BnActConv2d(in_chs=in_chs, out_chs=num_1x1_a, kernel_size=1, stride=1)\n        self.c3x3_b = BnActConv2d(\n            in_chs=num_1x1_a, out_chs=num_3x3_b, kernel_size=3, stride=self.key_stride, groups=groups)\n        if b:\n            self.c1x1_c = CatBnAct(in_chs=num_3x3_b)\n            self.c1x1_c1 = create_conv2d(num_3x3_b, num_1x1_c, kernel_size=1)\n            self.c1x1_c2 = create_conv2d(num_3x3_b, inc, kernel_size=1)\n        else:\n            self.c1x1_c = BnActConv2d(in_chs=num_3x3_b, out_chs=num_1x1_c + inc, kernel_size=1, stride=1)\n            self.c1x1_c1 = None\n            self.c1x1_c2 = None\n\n    @torch.jit._overload_method  # noqa: F811\n    def forward(self, x):\n        # type: (Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]\n        pass\n\n    @torch.jit._overload_method  # noqa: F811\n    def forward(self, x):\n        # type: (torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]\n        pass\n\n    def forward(self, x) -> Tuple[torch.Tensor, torch.Tensor]:\n        if isinstance(x, tuple):\n            x_in = torch.cat(x, dim=1)\n        else:\n            x_in = x\n        if self.c1x1_w_s1 is None and self.c1x1_w_s2 is None:\n            # self.has_proj == False, torchscript requires condition on module == None\n            x_s1 = x[0]\n            x_s2 = x[1]\n        else:\n            # self.has_proj == True\n            if self.c1x1_w_s1 is not None:\n                # self.key_stride = 1\n                x_s = self.c1x1_w_s1(x_in)\n            else:\n                # self.key_stride = 2\n                x_s = self.c1x1_w_s2(x_in)\n            x_s1 = x_s[:, :self.num_1x1_c, :, :]\n            x_s2 = x_s[:, self.num_1x1_c:, :, :]\n        x_in = self.c1x1_a(x_in)\n        x_in = self.c3x3_b(x_in)\n        x_in = self.c1x1_c(x_in)\n        if self.c1x1_c1 is not None:\n            # self.b == True, using None check for torchscript compat\n            out1 = self.c1x1_c1(x_in)\n            out2 = self.c1x1_c2(x_in)\n        else:\n            out1 = x_in[:, :self.num_1x1_c, :, :]\n            out2 = x_in[:, self.num_1x1_c:, :, :]\n        resid = x_s1 + out1\n        dense = torch.cat([x_s2, out2], dim=1)\n        return resid, dense\n\n\nclass DPN(nn.Module):\n    def __init__(\n            self,\n            k_sec=(3, 4, 20, 3),\n            inc_sec=(16, 32, 24, 128),\n            k_r=96,\n            groups=32,\n            num_classes=1000,\n            in_chans=3,\n            output_stride=32,\n            global_pool='avg',\n            small=False,\n            num_init_features=64,\n            b=False,\n            drop_rate=0.,\n            norm_layer='batchnorm2d',\n            act_layer='relu',\n            fc_act_layer='elu',\n    ):\n        super(DPN, self).__init__()\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        self.b = b\n        assert output_stride == 32  # FIXME look into dilation support\n\n        norm_layer = partial(get_norm_act_layer(norm_layer, act_layer=act_layer), eps=.001)\n        fc_norm_layer = partial(get_norm_act_layer(norm_layer, act_layer=fc_act_layer), eps=.001, inplace=False)\n        bw_factor = 1 if small else 4\n        blocks = OrderedDict()\n\n        # conv1\n        blocks['conv1_1'] = ConvNormAct(\n            in_chans, num_init_features, kernel_size=3 if small else 7, stride=2, norm_layer=norm_layer)\n        blocks['conv1_pool'] = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.feature_info = [dict(num_chs=num_init_features, reduction=2, module='features.conv1_1')]\n\n        # conv2\n        bw = 64 * bw_factor\n        inc = inc_sec[0]\n        r = (k_r * bw) // (64 * bw_factor)\n        blocks['conv2_1'] = DualPathBlock(num_init_features, r, r, bw, inc, groups, 'proj', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[0] + 1):\n            blocks['conv2_' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'normal', b)\n            in_chs += inc\n        self.feature_info += [dict(num_chs=in_chs, reduction=4, module=f'features.conv2_{k_sec[0]}')]\n\n        # conv3\n        bw = 128 * bw_factor\n        inc = inc_sec[1]\n        r = (k_r * bw) // (64 * bw_factor)\n        blocks['conv3_1'] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'down', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[1] + 1):\n            blocks['conv3_' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'normal', b)\n            in_chs += inc\n        self.feature_info += [dict(num_chs=in_chs, reduction=8, module=f'features.conv3_{k_sec[1]}')]\n\n        # conv4\n        bw = 256 * bw_factor\n        inc = inc_sec[2]\n        r = (k_r * bw) // (64 * bw_factor)\n        blocks['conv4_1'] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'down', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[2] + 1):\n            blocks['conv4_' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'normal', b)\n            in_chs += inc\n        self.feature_info += [dict(num_chs=in_chs, reduction=16, module=f'features.conv4_{k_sec[2]}')]\n\n        # conv5\n        bw = 512 * bw_factor\n        inc = inc_sec[3]\n        r = (k_r * bw) // (64 * bw_factor)\n        blocks['conv5_1'] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'down', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[3] + 1):\n            blocks['conv5_' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'normal', b)\n            in_chs += inc\n        self.feature_info += [dict(num_chs=in_chs, reduction=32, module=f'features.conv5_{k_sec[3]}')]\n\n        blocks['conv5_bn_ac'] = CatBnAct(in_chs, norm_layer=fc_norm_layer)\n\n        self.num_features = in_chs\n        self.features = nn.Sequential(blocks)\n\n        # Using 1x1 conv for the FC layer to allow the extra pooling scheme\n        self.global_pool, self.classifier = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool, use_conv=True)\n        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^features\\.conv1',\n            blocks=[\n                (r'^features\\.conv(\\d+)' if coarse else r'^features\\.conv(\\d+)_(\\d+)', None),\n                (r'^features\\.conv5_bn_ac', (99999,))\n            ]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, 'gradient checkpointing not supported'\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.classifier\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.global_pool, self.classifier = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool, use_conv=True)\n        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()\n\n    def forward_features(self, x):\n        return self.features(x)\n\n    def forward_head(self, x, pre_logits: bool = False):\n        x = self.global_pool(x)\n        if self.drop_rate > 0.:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        if pre_logits:\n            return self.flatten(x)\n        x = self.classifier(x)\n        return self.flatten(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_dpn(variant, pretrained=False, **kwargs):\n    return build_model_with_cfg(\n        DPN,\n        variant,\n        pretrained,\n        feature_cfg=dict(feature_concat=True, flatten_sequential=True),\n        **kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DPN_MEAN, 'std': IMAGENET_DPN_STD,\n        'first_conv': 'features.conv1_1.conv', 'classifier': 'classifier',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'dpn48b.untrained': _cfg(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n    'dpn68.mx_in1k': _cfg(hf_hub_id='timm/'),\n    'dpn68b.ra_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD,\n        crop_pct=0.95, test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'dpn68b.mx_in1k': _cfg(hf_hub_id='timm/'),\n    'dpn92.mx_in1k': _cfg(hf_hub_id='timm/'),\n    'dpn98.mx_in1k': _cfg(hf_hub_id='timm/'),\n    'dpn131.mx_in1k': _cfg(hf_hub_id='timm/'),\n    'dpn107.mx_in1k': _cfg(hf_hub_id='timm/')\n})\n\n\n@register_model\ndef dpn48b(pretrained=False, **kwargs) -> DPN:\n    model_kwargs = dict(\n        small=True, num_init_features=10, k_r=128, groups=32,\n        b=True, k_sec=(3, 4, 6, 3), inc_sec=(16, 32, 32, 64), act_layer='silu')\n    return _create_dpn('dpn48b', pretrained=pretrained, **dict(model_kwargs, **kwargs))\n\n\n@register_model\ndef dpn68(pretrained=False, **kwargs) -> DPN:\n    model_kwargs = dict(\n        small=True, num_init_features=10, k_r=128, groups=32,\n        k_sec=(3, 4, 12, 3), inc_sec=(16, 32, 32, 64))\n    return _create_dpn('dpn68', pretrained=pretrained, **dict(model_kwargs, **kwargs))\n\n\n@register_model\ndef dpn68b(pretrained=False, **kwargs) -> DPN:\n    model_kwargs = dict(\n        small=True, num_init_features=10, k_r=128, groups=32,\n        b=True, k_sec=(3, 4, 12, 3), inc_sec=(16, 32, 32, 64))\n    return _create_dpn('dpn68b', pretrained=pretrained, **dict(model_kwargs, **kwargs))\n\n\n@register_model\ndef dpn92(pretrained=False, **kwargs) -> DPN:\n    model_kwargs = dict(\n        num_init_features=64, k_r=96, groups=32,\n        k_sec=(3, 4, 20, 3), inc_sec=(16, 32, 24, 128))\n    return _create_dpn('dpn92', pretrained=pretrained, **dict(model_kwargs, **kwargs))\n\n\n@register_model\ndef dpn98(pretrained=False, **kwargs) -> DPN:\n    model_kwargs = dict(\n        num_init_features=96, k_r=160, groups=40,\n        k_sec=(3, 6, 20, 3), inc_sec=(16, 32, 32, 128))\n    return _create_dpn('dpn98', pretrained=pretrained, **dict(model_kwargs, **kwargs))\n\n\n@register_model\ndef dpn131(pretrained=False, **kwargs) -> DPN:\n    model_kwargs = dict(\n        num_init_features=128, k_r=160, groups=40,\n        k_sec=(4, 8, 28, 3), inc_sec=(16, 32, 32, 128))\n    return _create_dpn('dpn131', pretrained=pretrained, **dict(model_kwargs, **kwargs))\n\n\n@register_model\ndef dpn107(pretrained=False, **kwargs) -> DPN:\n    model_kwargs = dict(\n        num_init_features=128, k_r=200, groups=50,\n        k_sec=(4, 8, 20, 3), inc_sec=(20, 64, 64, 128))\n    return _create_dpn('dpn107', pretrained=pretrained, **dict(model_kwargs, **kwargs))\n",
  "\"\"\" VoVNet (V1 & V2)\n\nPapers:\n* `An Energy and GPU-Computation Efficient Backbone Network` - https://arxiv.org/abs/1904.09730\n* `CenterMask : Real-Time Anchor-Free Instance Segmentation` - https://arxiv.org/abs/1911.06667\n\nLooked at  https://github.com/youngwanLEE/vovnet-detectron2 &\nhttps://github.com/stigma0617/VoVNet.pytorch/blob/master/models_vovnet/vovnet.py\nfor some reference, rewrote most of the code.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import ConvNormAct, SeparableConvNormAct, BatchNormAct2d, ClassifierHead, DropPath, \\\n    create_attn, create_norm_act_layer\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['VovNet']  # model_registry will add each entrypoint fn to this\n\n\nclass SequentialAppendList(nn.Sequential):\n    def __init__(self, *args):\n        super(SequentialAppendList, self).__init__(*args)\n\n    def forward(self, x: torch.Tensor, concat_list: List[torch.Tensor]) -> torch.Tensor:\n        for i, module in enumerate(self):\n            if i == 0:\n                concat_list.append(module(x))\n            else:\n                concat_list.append(module(concat_list[-1]))\n        x = torch.cat(concat_list, dim=1)\n        return x\n\n\nclass OsaBlock(nn.Module):\n\n    def __init__(\n            self,\n            in_chs,\n            mid_chs,\n            out_chs,\n            layer_per_block,\n            residual=False,\n            depthwise=False,\n            attn='',\n            norm_layer=BatchNormAct2d,\n            act_layer=nn.ReLU,\n            drop_path=None,\n    ):\n        super(OsaBlock, self).__init__()\n\n        self.residual = residual\n        self.depthwise = depthwise\n        conv_kwargs = dict(norm_layer=norm_layer, act_layer=act_layer)\n\n        next_in_chs = in_chs\n        if self.depthwise and next_in_chs != mid_chs:\n            assert not residual\n            self.conv_reduction = ConvNormAct(next_in_chs, mid_chs, 1, **conv_kwargs)\n        else:\n            self.conv_reduction = None\n\n        mid_convs = []\n        for i in range(layer_per_block):\n            if self.depthwise:\n                conv = SeparableConvNormAct(mid_chs, mid_chs, **conv_kwargs)\n            else:\n                conv = ConvNormAct(next_in_chs, mid_chs, 3, **conv_kwargs)\n            next_in_chs = mid_chs\n            mid_convs.append(conv)\n        self.conv_mid = SequentialAppendList(*mid_convs)\n\n        # feature aggregation\n        next_in_chs = in_chs + layer_per_block * mid_chs\n        self.conv_concat = ConvNormAct(next_in_chs, out_chs, **conv_kwargs)\n\n        self.attn = create_attn(attn, out_chs) if attn else None\n\n        self.drop_path = drop_path\n\n    def forward(self, x):\n        output = [x]\n        if self.conv_reduction is not None:\n            x = self.conv_reduction(x)\n        x = self.conv_mid(x, output)\n        x = self.conv_concat(x)\n        if self.attn is not None:\n            x = self.attn(x)\n        if self.drop_path is not None:\n            x = self.drop_path(x)\n        if self.residual:\n            x = x + output[0]\n        return x\n\n\nclass OsaStage(nn.Module):\n\n    def __init__(\n            self,\n            in_chs,\n            mid_chs,\n            out_chs,\n            block_per_stage,\n            layer_per_block,\n            downsample=True,\n            residual=True,\n            depthwise=False,\n            attn='ese',\n            norm_layer=BatchNormAct2d,\n            act_layer=nn.ReLU,\n            drop_path_rates=None,\n    ):\n        super(OsaStage, self).__init__()\n        self.grad_checkpointing = False\n\n        if downsample:\n            self.pool = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n        else:\n            self.pool = None\n\n        blocks = []\n        for i in range(block_per_stage):\n            last_block = i == block_per_stage - 1\n            if drop_path_rates is not None and drop_path_rates[i] > 0.:\n                drop_path = DropPath(drop_path_rates[i])\n            else:\n                drop_path = None\n            blocks += [OsaBlock(\n                in_chs, mid_chs, out_chs, layer_per_block, residual=residual and i > 0, depthwise=depthwise,\n                attn=attn if last_block else '', norm_layer=norm_layer, act_layer=act_layer, drop_path=drop_path)\n            ]\n            in_chs = out_chs\n        self.blocks = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        if self.pool is not None:\n            x = self.pool(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        return x\n\n\nclass VovNet(nn.Module):\n\n    def __init__(\n            self,\n            cfg,\n            in_chans=3,\n            num_classes=1000,\n            global_pool='avg',\n            output_stride=32,\n            norm_layer=BatchNormAct2d,\n            act_layer=nn.ReLU,\n            drop_rate=0.,\n            drop_path_rate=0.,\n            **kwargs,\n    ):\n        \"\"\"\n        Args:\n            cfg (dict): Model architecture configuration\n            in_chans (int): Number of input channels (default: 3)\n            num_classes (int): Number of classifier classes (default: 1000)\n            global_pool (str): Global pooling type (default: 'avg')\n            output_stride (int): Output stride of network, one of (8, 16, 32) (default: 32)\n            norm_layer (Union[str, nn.Module]): normalization layer\n            act_layer (Union[str, nn.Module]): activation layer\n            drop_rate (float): Dropout rate (default: 0.)\n            drop_path_rate (float): Stochastic depth drop-path rate (default: 0.)\n            kwargs (dict): Extra kwargs overlayed onto cfg\n        \"\"\"\n        super(VovNet, self).__init__()\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        assert output_stride == 32  # FIXME support dilation\n\n        cfg = dict(cfg, **kwargs)\n        stem_stride = cfg.get(\"stem_stride\", 4)\n        stem_chs = cfg[\"stem_chs\"]\n        stage_conv_chs = cfg[\"stage_conv_chs\"]\n        stage_out_chs = cfg[\"stage_out_chs\"]\n        block_per_stage = cfg[\"block_per_stage\"]\n        layer_per_block = cfg[\"layer_per_block\"]\n        conv_kwargs = dict(norm_layer=norm_layer, act_layer=act_layer)\n\n        # Stem module\n        last_stem_stride = stem_stride // 2\n        conv_type = SeparableConvNormAct if cfg[\"depthwise\"] else ConvNormAct\n        self.stem = nn.Sequential(*[\n            ConvNormAct(in_chans, stem_chs[0], 3, stride=2, **conv_kwargs),\n            conv_type(stem_chs[0], stem_chs[1], 3, stride=1, **conv_kwargs),\n            conv_type(stem_chs[1], stem_chs[2], 3, stride=last_stem_stride, **conv_kwargs),\n        ])\n        self.feature_info = [dict(\n            num_chs=stem_chs[1], reduction=2, module=f'stem.{1 if stem_stride == 4 else 2}')]\n        current_stride = stem_stride\n\n        # OSA stages\n        stage_dpr = torch.split(torch.linspace(0, drop_path_rate, sum(block_per_stage)), block_per_stage)\n        in_ch_list = stem_chs[-1:] + stage_out_chs[:-1]\n        stage_args = dict(residual=cfg[\"residual\"], depthwise=cfg[\"depthwise\"], attn=cfg[\"attn\"], **conv_kwargs)\n        stages = []\n        for i in range(4):  # num_stages\n            downsample = stem_stride == 2 or i > 0  # first stage has no stride/downsample if stem_stride is 4\n            stages += [OsaStage(\n                in_ch_list[i],\n                stage_conv_chs[i],\n                stage_out_chs[i],\n                block_per_stage[i],\n                layer_per_block,\n                downsample=downsample,\n                drop_path_rates=stage_dpr[i],\n                **stage_args,\n            )]\n            self.num_features = stage_out_chs[i]\n            current_stride *= 2 if downsample else 1\n            self.feature_info += [dict(num_chs=self.num_features, reduction=current_stride, module=f'stages.{i}')]\n\n        self.stages = nn.Sequential(*stages)\n\n        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=drop_rate)\n\n        for n, m in self.named_modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.Linear):\n                nn.init.zeros_(m.bias)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^stem',\n            blocks=r'^stages\\.(\\d+)' if coarse else r'^stages\\.(\\d+).blocks\\.(\\d+)',\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        for s in self.stages:\n            s.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate)\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        return self.stages(x)\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=pre_logits)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\n# model cfgs adapted from https://github.com/youngwanLEE/vovnet-detectron2 &\n# https://github.com/stigma0617/VoVNet.pytorch/blob/master/models_vovnet/vovnet.py\nmodel_cfgs = dict(\n    vovnet39a=dict(\n        stem_chs=[64, 64, 128],\n        stage_conv_chs=[128, 160, 192, 224],\n        stage_out_chs=[256, 512, 768, 1024],\n        layer_per_block=5,\n        block_per_stage=[1, 1, 2, 2],\n        residual=False,\n        depthwise=False,\n        attn='',\n    ),\n    vovnet57a=dict(\n        stem_chs=[64, 64, 128],\n        stage_conv_chs=[128, 160, 192, 224],\n        stage_out_chs=[256, 512, 768, 1024],\n        layer_per_block=5,\n        block_per_stage=[1, 1, 4, 3],\n        residual=False,\n        depthwise=False,\n        attn='',\n\n    ),\n    ese_vovnet19b_slim_dw=dict(\n        stem_chs=[64, 64, 64],\n        stage_conv_chs=[64, 80, 96, 112],\n        stage_out_chs=[112, 256, 384, 512],\n        layer_per_block=3,\n        block_per_stage=[1, 1, 1, 1],\n        residual=True,\n        depthwise=True,\n        attn='ese',\n\n    ),\n    ese_vovnet19b_dw=dict(\n        stem_chs=[64, 64, 64],\n        stage_conv_chs=[128, 160, 192, 224],\n        stage_out_chs=[256, 512, 768, 1024],\n        layer_per_block=3,\n        block_per_stage=[1, 1, 1, 1],\n        residual=True,\n        depthwise=True,\n        attn='ese',\n    ),\n    ese_vovnet19b_slim=dict(\n        stem_chs=[64, 64, 128],\n        stage_conv_chs=[64, 80, 96, 112],\n        stage_out_chs=[112, 256, 384, 512],\n        layer_per_block=3,\n        block_per_stage=[1, 1, 1, 1],\n        residual=True,\n        depthwise=False,\n        attn='ese',\n    ),\n    ese_vovnet19b=dict(\n        stem_chs=[64, 64, 128],\n        stage_conv_chs=[128, 160, 192, 224],\n        stage_out_chs=[256, 512, 768, 1024],\n        layer_per_block=3,\n        block_per_stage=[1, 1, 1, 1],\n        residual=True,\n        depthwise=False,\n        attn='ese',\n\n    ),\n    ese_vovnet39b=dict(\n        stem_chs=[64, 64, 128],\n        stage_conv_chs=[128, 160, 192, 224],\n        stage_out_chs=[256, 512, 768, 1024],\n        layer_per_block=5,\n        block_per_stage=[1, 1, 2, 2],\n        residual=True,\n        depthwise=False,\n        attn='ese',\n    ),\n    ese_vovnet57b=dict(\n        stem_chs=[64, 64, 128],\n        stage_conv_chs=[128, 160, 192, 224],\n        stage_out_chs=[256, 512, 768, 1024],\n        layer_per_block=5,\n        block_per_stage=[1, 1, 4, 3],\n        residual=True,\n        depthwise=False,\n        attn='ese',\n\n    ),\n    ese_vovnet99b=dict(\n        stem_chs=[64, 64, 128],\n        stage_conv_chs=[128, 160, 192, 224],\n        stage_out_chs=[256, 512, 768, 1024],\n        layer_per_block=5,\n        block_per_stage=[1, 3, 9, 3],\n        residual=True,\n        depthwise=False,\n        attn='ese',\n    ),\n    eca_vovnet39b=dict(\n        stem_chs=[64, 64, 128],\n        stage_conv_chs=[128, 160, 192, 224],\n        stage_out_chs=[256, 512, 768, 1024],\n        layer_per_block=5,\n        block_per_stage=[1, 1, 2, 2],\n        residual=True,\n        depthwise=False,\n        attn='eca',\n    ),\n)\nmodel_cfgs['ese_vovnet39b_evos'] = model_cfgs['ese_vovnet39b']\n\n\ndef _create_vovnet(variant, pretrained=False, **kwargs):\n    return build_model_with_cfg(\n        VovNet,\n        variant,\n        pretrained,\n        model_cfg=model_cfgs[variant],\n        feature_cfg=dict(flatten_sequential=True),\n        **kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.0.conv', 'classifier': 'head.fc', **kwargs,\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'vovnet39a.untrained': _cfg(url=''),\n    'vovnet57a.untrained': _cfg(url=''),\n    'ese_vovnet19b_slim_dw.untrained': _cfg(url=''),\n    'ese_vovnet19b_dw.ra_in1k': _cfg(\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=0.95),\n    'ese_vovnet19b_slim.untrained': _cfg(url=''),\n    'ese_vovnet39b.ra_in1k': _cfg(\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=0.95),\n    'ese_vovnet57b.untrained': _cfg(url=''),\n    'ese_vovnet99b.untrained': _cfg(url=''),\n    'eca_vovnet39b.untrained': _cfg(url=''),\n    'ese_vovnet39b_evos.untrained': _cfg(url=''),\n})\n\n\n@register_model\ndef vovnet39a(pretrained=False, **kwargs) -> VovNet:\n    return _create_vovnet('vovnet39a', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef vovnet57a(pretrained=False, **kwargs) -> VovNet:\n    return _create_vovnet('vovnet57a', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef ese_vovnet19b_slim_dw(pretrained=False, **kwargs) -> VovNet:\n    return _create_vovnet('ese_vovnet19b_slim_dw', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef ese_vovnet19b_dw(pretrained=False, **kwargs) -> VovNet:\n    return _create_vovnet('ese_vovnet19b_dw', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef ese_vovnet19b_slim(pretrained=False, **kwargs) -> VovNet:\n    return _create_vovnet('ese_vovnet19b_slim', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef ese_vovnet39b(pretrained=False, **kwargs) -> VovNet:\n    return _create_vovnet('ese_vovnet39b', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef ese_vovnet57b(pretrained=False, **kwargs) -> VovNet:\n    return _create_vovnet('ese_vovnet57b', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef ese_vovnet99b(pretrained=False, **kwargs) -> VovNet:\n    return _create_vovnet('ese_vovnet99b', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef eca_vovnet39b(pretrained=False, **kwargs) -> VovNet:\n    return _create_vovnet('eca_vovnet39b', pretrained=pretrained, **kwargs)\n\n\n# Experimental Models\n\n@register_model\ndef ese_vovnet39b_evos(pretrained=False, **kwargs) -> VovNet:\n    def norm_act_fn(num_features, **nkwargs):\n        return create_norm_act_layer('evonorms0', num_features, jit=False, **nkwargs)\n    return _create_vovnet('ese_vovnet39b_evos', pretrained=pretrained, norm_layer=norm_act_fn, **kwargs)\n",
  "\"\"\" ConViT Model\n\n@article{d2021convit,\n  title={ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases},\n  author={d'Ascoli, St{\\'e}phane and Touvron, Hugo and Leavitt, Matthew and Morcos, Ari and Biroli, Giulio and Sagun, Levent},\n  journal={arXiv preprint arXiv:2103.10697},\n  year={2021}\n}\n\nPaper link: https://arxiv.org/abs/2103.10697\nOriginal code: https://github.com/facebookresearch/convit, original copyright below\n\nModifications and additions for timm hacked together by / Copyright 2021, Ross Wightman\n\"\"\"\n# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the CC-by-NC license found in the\n# LICENSE file in the root directory of this source tree.\n#\n'''These modules are adapted from those of timm, see\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n'''\n\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import DropPath, trunc_normal_, PatchEmbed, Mlp, LayerNorm\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_module\nfrom ._registry import register_model, generate_default_cfgs\nfrom .vision_transformer_hybrid import HybridEmbed\n\n\n__all__ = ['ConVit']\n\n\n@register_notrace_module  # reason: FX can't symbolically trace control flow in forward method\nclass GPSA(nn.Module):\n    def __init__(\n            self,\n            dim,\n            num_heads=8,\n            qkv_bias=False,\n            attn_drop=0.,\n            proj_drop=0.,\n            locality_strength=1.,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        self.dim = dim\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n        self.locality_strength = locality_strength\n\n        self.qk = nn.Linear(dim, dim * 2, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.pos_proj = nn.Linear(3, num_heads)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.gating_param = nn.Parameter(torch.ones(self.num_heads))\n        self.rel_indices: torch.Tensor = torch.zeros(1, 1, 1, 3)  # silly torchscript hack, won't work with None\n\n    def forward(self, x):\n        B, N, C = x.shape\n        if self.rel_indices is None or self.rel_indices.shape[1] != N:\n            self.rel_indices = self.get_rel_indices(N)\n        attn = self.get_attention(x)\n        v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def get_attention(self, x):\n        B, N, C = x.shape\n        qk = self.qk(x).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k = qk[0], qk[1]\n        pos_score = self.rel_indices.expand(B, -1, -1, -1)\n        pos_score = self.pos_proj(pos_score).permute(0, 3, 1, 2)\n        patch_score = (q @ k.transpose(-2, -1)) * self.scale\n        patch_score = patch_score.softmax(dim=-1)\n        pos_score = pos_score.softmax(dim=-1)\n\n        gating = self.gating_param.view(1, -1, 1, 1)\n        attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score\n        attn /= attn.sum(dim=-1).unsqueeze(-1)\n        attn = self.attn_drop(attn)\n        return attn\n\n    def get_attention_map(self, x, return_map=False):\n        attn_map = self.get_attention(x).mean(0)  # average over batch\n        distances = self.rel_indices.squeeze()[:, :, -1] ** .5\n        dist = torch.einsum('nm,hnm->h', (distances, attn_map)) / distances.size(0)\n        if return_map:\n            return dist, attn_map\n        else:\n            return dist\n\n    def local_init(self):\n        self.v.weight.data.copy_(torch.eye(self.dim))\n        locality_distance = 1  # max(1,1/locality_strength**.5)\n\n        kernel_size = int(self.num_heads ** .5)\n        center = (kernel_size - 1) / 2 if kernel_size % 2 == 0 else kernel_size // 2\n        for h1 in range(kernel_size):\n            for h2 in range(kernel_size):\n                position = h1 + kernel_size * h2\n                self.pos_proj.weight.data[position, 2] = -1\n                self.pos_proj.weight.data[position, 1] = 2 * (h1 - center) * locality_distance\n                self.pos_proj.weight.data[position, 0] = 2 * (h2 - center) * locality_distance\n        self.pos_proj.weight.data *= self.locality_strength\n\n    def get_rel_indices(self, num_patches: int) -> torch.Tensor:\n        img_size = int(num_patches ** .5)\n        rel_indices = torch.zeros(1, num_patches, num_patches, 3)\n        ind = torch.arange(img_size).view(1, -1) - torch.arange(img_size).view(-1, 1)\n        indx = ind.repeat(img_size, img_size)\n        indy = ind.repeat_interleave(img_size, dim=0).repeat_interleave(img_size, dim=1)\n        indd = indx ** 2 + indy ** 2\n        rel_indices[:, :, :, 2] = indd.unsqueeze(0)\n        rel_indices[:, :, :, 1] = indy.unsqueeze(0)\n        rel_indices[:, :, :, 0] = indx.unsqueeze(0)\n        device = self.qk.weight.device\n        return rel_indices.to(device)\n\n\nclass MHSA(nn.Module):\n    def __init__(\n            self,\n            dim,\n            num_heads=8,\n            qkv_bias=False,\n            attn_drop=0.,\n            proj_drop=0.,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def get_attention_map(self, x, return_map=False):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        attn_map = (q @ k.transpose(-2, -1)) * self.scale\n        attn_map = attn_map.softmax(dim=-1).mean(0)\n\n        img_size = int(N ** .5)\n        ind = torch.arange(img_size).view(1, -1) - torch.arange(img_size).view(-1, 1)\n        indx = ind.repeat(img_size, img_size)\n        indy = ind.repeat_interleave(img_size, dim=0).repeat_interleave(img_size, dim=1)\n        indd = indx ** 2 + indy ** 2\n        distances = indd ** .5\n        distances = distances.to(x.device)\n\n        dist = torch.einsum('nm,hnm->h', (distances, attn_map)) / N\n        if return_map:\n            return dist, attn_map\n        else:\n            return dist\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=LayerNorm,\n            use_gpsa=True,\n            locality_strength=1.,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.use_gpsa = use_gpsa\n        if self.use_gpsa:\n            self.attn = GPSA(\n                dim,\n                num_heads=num_heads,\n                qkv_bias=qkv_bias,\n                attn_drop=attn_drop,\n                proj_drop=proj_drop,\n                locality_strength=locality_strength,\n            )\n        else:\n            self.attn = MHSA(\n                dim,\n                num_heads=num_heads,\n                qkv_bias=qkv_bias,\n                attn_drop=attn_drop,\n                proj_drop=proj_drop,\n            )\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass ConVit(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(\n            self,\n            img_size=224,\n            patch_size=16,\n            in_chans=3,\n            num_classes=1000,\n            global_pool='token',\n            embed_dim=768,\n            depth=12,\n            num_heads=12,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            drop_rate=0.,\n            pos_drop_rate=0.,\n            proj_drop_rate=0.,\n            attn_drop_rate=0.,\n            drop_path_rate=0.,\n            hybrid_backbone=None,\n            norm_layer=LayerNorm,\n            local_up_to_layer=3,\n            locality_strength=1.,\n            use_pos_embed=True,\n    ):\n        super().__init__()\n        assert global_pool in ('', 'avg', 'token')\n        embed_dim *= num_heads\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.local_up_to_layer = local_up_to_layer\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.locality_strength = locality_strength\n        self.use_pos_embed = use_pos_embed\n\n        if hybrid_backbone is not None:\n            self.patch_embed = HybridEmbed(\n                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n        else:\n            self.patch_embed = PatchEmbed(\n                img_size=img_size,\n                patch_size=patch_size,\n                in_chans=in_chans,\n                embed_dim=embed_dim,\n            )\n        num_patches = self.patch_embed.num_patches\n        self.num_patches = num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n\n        if self.use_pos_embed:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n            trunc_normal_(self.pos_embed, std=.02)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n                use_gpsa=i < local_up_to_layer,\n                locality_strength=locality_strength,\n            ) for i in range(depth)])\n        self.norm = norm_layer(embed_dim)\n\n        # Classifier head\n        self.feature_info = [dict(num_chs=embed_dim, reduction=0, module='head')]\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n        for n, m in self.named_modules():\n            if hasattr(m, 'local_init'):\n                m.local_init()\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^cls_token|pos_embed|patch_embed',  # stem and embed\n            blocks=[(r'^blocks\\.(\\d+)', None), (r'^norm', (99999,))]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, 'gradient checkpointing not supported'\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('', 'token', 'avg')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        if self.use_pos_embed:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n        for u, blk in enumerate(self.blocks):\n            if u == self.local_up_to_layer:\n                x = torch.cat((cls_tokens, x), dim=1)\n            x = blk(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool:\n            x = x[:, 1:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]\n        x = self.head_drop(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_convit(variant, pretrained=False, **kwargs):\n    if kwargs.get('features_only', None):\n        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n\n    return build_model_with_cfg(ConVit, variant, pretrained, **kwargs)\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD, 'fixed_input_size': True,\n        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    # ConViT\n    'convit_tiny.fb_in1k': _cfg(hf_hub_id='timm/'),\n    'convit_small.fb_in1k': _cfg(hf_hub_id='timm/'),\n    'convit_base.fb_in1k': _cfg(hf_hub_id='timm/')\n})\n\n\n@register_model\ndef convit_tiny(pretrained=False, **kwargs) -> ConVit:\n    model_args = dict(\n        local_up_to_layer=10, locality_strength=1.0, embed_dim=48, num_heads=4)\n    model = _create_convit(variant='convit_tiny', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convit_small(pretrained=False, **kwargs) -> ConVit:\n    model_args = dict(\n        local_up_to_layer=10, locality_strength=1.0, embed_dim=48, num_heads=9)\n    model = _create_convit(variant='convit_small', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef convit_base(pretrained=False, **kwargs) -> ConVit:\n    model_args = dict(\n        local_up_to_layer=10, locality_strength=1.0, embed_dim=48, num_heads=16)\n    model = _create_convit(variant='convit_base', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n",
  "import hashlib\nimport json\nimport logging\nimport os\nimport sys\nfrom functools import partial\nfrom pathlib import Path\nfrom tempfile import TemporaryDirectory\nfrom typing import Iterable, Optional, Union\n\nimport torch\nfrom torch.hub import HASH_REGEX, download_url_to_file, urlparse\n\ntry:\n    from torch.hub import get_dir\nexcept ImportError:\n    from torch.hub import _get_torch_home as get_dir\n\ntry:\n    import safetensors.torch\n    _has_safetensors = True\nexcept ImportError:\n    _has_safetensors = False\n\nif sys.version_info >= (3, 8):\n    from typing import Literal\nelse:\n    from typing_extensions import Literal\n\nfrom timm import __version__\nfrom timm.models._pretrained import filter_pretrained_cfg\n\ntry:\n    from huggingface_hub import (\n        create_repo, get_hf_file_metadata,\n        hf_hub_download, hf_hub_url,\n        repo_type_and_id_from_hf_id, upload_folder)\n    from huggingface_hub.utils import EntryNotFoundError\n    hf_hub_download = partial(hf_hub_download, library_name=\"timm\", library_version=__version__)\n    _has_hf_hub = True\nexcept ImportError:\n    hf_hub_download = None\n    _has_hf_hub = False\n\n_logger = logging.getLogger(__name__)\n\n__all__ = ['get_cache_dir', 'download_cached_file', 'has_hf_hub', 'hf_split', 'load_model_config_from_hf',\n           'load_state_dict_from_hf', 'save_for_hf', 'push_to_hf_hub']\n\n# Default name for a weights file hosted on the Huggingface Hub.\nHF_WEIGHTS_NAME = \"pytorch_model.bin\"  # default pytorch pkl\nHF_SAFE_WEIGHTS_NAME = \"model.safetensors\"  # safetensors version\nHF_OPEN_CLIP_WEIGHTS_NAME = \"open_clip_pytorch_model.bin\"  # default pytorch pkl\nHF_OPEN_CLIP_SAFE_WEIGHTS_NAME = \"open_clip_model.safetensors\"  # safetensors version\n\n\ndef get_cache_dir(child_dir=''):\n    \"\"\"\n    Returns the location of the directory where models are cached (and creates it if necessary).\n    \"\"\"\n    # Issue warning to move data if old env is set\n    if os.getenv('TORCH_MODEL_ZOO'):\n        _logger.warning('TORCH_MODEL_ZOO is deprecated, please use env TORCH_HOME instead')\n\n    hub_dir = get_dir()\n    child_dir = () if not child_dir else (child_dir,)\n    model_dir = os.path.join(hub_dir, 'checkpoints', *child_dir)\n    os.makedirs(model_dir, exist_ok=True)\n    return model_dir\n\n\ndef download_cached_file(url, check_hash=True, progress=False):\n    if isinstance(url, (list, tuple)):\n        url, filename = url\n    else:\n        parts = urlparse(url)\n        filename = os.path.basename(parts.path)\n    cached_file = os.path.join(get_cache_dir(), filename)\n    if not os.path.exists(cached_file):\n        _logger.info('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\n        hash_prefix = None\n        if check_hash:\n            r = HASH_REGEX.search(filename)  # r is Optional[Match[str]]\n            hash_prefix = r.group(1) if r else None\n        download_url_to_file(url, cached_file, hash_prefix, progress=progress)\n    return cached_file\n\n\ndef check_cached_file(url, check_hash=True):\n    if isinstance(url, (list, tuple)):\n        url, filename = url\n    else:\n        parts = urlparse(url)\n        filename = os.path.basename(parts.path)\n    cached_file = os.path.join(get_cache_dir(), filename)\n    if os.path.exists(cached_file):\n        if check_hash:\n            r = HASH_REGEX.search(filename)  # r is Optional[Match[str]]\n            hash_prefix = r.group(1) if r else None\n            if hash_prefix:\n                with open(cached_file, 'rb') as f:\n                    hd = hashlib.sha256(f.read()).hexdigest()\n                    if hd[:len(hash_prefix)] != hash_prefix:\n                        return False\n        return True\n    return False\n\n\ndef has_hf_hub(necessary=False):\n    if not _has_hf_hub and necessary:\n        # if no HF Hub module installed, and it is necessary to continue, raise error\n        raise RuntimeError(\n            'Hugging Face hub model specified but package not installed. Run `pip install huggingface_hub`.')\n    return _has_hf_hub\n\n\ndef hf_split(hf_id: str):\n    # FIXME I may change @ -> # and be parsed as fragment in a URI model name scheme\n    rev_split = hf_id.split('@')\n    assert 0 < len(rev_split) <= 2, 'hf_hub id should only contain one @ character to identify revision.'\n    hf_model_id = rev_split[0]\n    hf_revision = rev_split[-1] if len(rev_split) > 1 else None\n    return hf_model_id, hf_revision\n\n\ndef load_cfg_from_json(json_file: Union[str, os.PathLike]):\n    with open(json_file, \"r\", encoding=\"utf-8\") as reader:\n        text = reader.read()\n    return json.loads(text)\n\n\ndef download_from_hf(model_id: str, filename: str):\n    hf_model_id, hf_revision = hf_split(model_id)\n    return hf_hub_download(hf_model_id, filename, revision=hf_revision)\n\n\ndef load_model_config_from_hf(model_id: str):\n    assert has_hf_hub(True)\n    cached_file = download_from_hf(model_id, 'config.json')\n\n    hf_config = load_cfg_from_json(cached_file)\n    if 'pretrained_cfg' not in hf_config:\n        # old form, pull pretrain_cfg out of the base dict\n        pretrained_cfg = hf_config\n        hf_config = {}\n        hf_config['architecture'] = pretrained_cfg.pop('architecture')\n        hf_config['num_features'] = pretrained_cfg.pop('num_features', None)\n        if 'labels' in pretrained_cfg:  # deprecated name for 'label_names'\n            pretrained_cfg['label_names'] = pretrained_cfg.pop('labels')\n        hf_config['pretrained_cfg'] = pretrained_cfg\n\n    # NOTE currently discarding parent config as only arch name and pretrained_cfg used in timm right now\n    pretrained_cfg = hf_config['pretrained_cfg']\n    pretrained_cfg['hf_hub_id'] = model_id  # insert hf_hub id for pretrained weight load during model creation\n    pretrained_cfg['source'] = 'hf-hub'\n\n    # model should be created with base config num_classes if its exist\n    if 'num_classes' in hf_config:\n        pretrained_cfg['num_classes'] = hf_config['num_classes']\n\n    # label meta-data in base config overrides saved pretrained_cfg on load\n    if 'label_names' in hf_config:\n        pretrained_cfg['label_names'] = hf_config.pop('label_names')\n    if 'label_descriptions' in hf_config:\n        pretrained_cfg['label_descriptions'] = hf_config.pop('label_descriptions')\n\n    model_name = hf_config['architecture']\n    return pretrained_cfg, model_name\n\n\ndef load_state_dict_from_hf(model_id: str, filename: str = HF_WEIGHTS_NAME):\n    assert has_hf_hub(True)\n    hf_model_id, hf_revision = hf_split(model_id)\n\n    # Look for .safetensors alternatives and load from it if it exists\n    if _has_safetensors:\n        for safe_filename in _get_safe_alternatives(filename):\n            try:\n                cached_safe_file = hf_hub_download(repo_id=hf_model_id, filename=safe_filename, revision=hf_revision)\n                _logger.info(\n                    f\"[{model_id}] Safe alternative available for '{filename}' \"\n                    f\"(as '{safe_filename}'). Loading weights using safetensors.\")\n                return safetensors.torch.load_file(cached_safe_file, device=\"cpu\")\n            except EntryNotFoundError:\n                pass\n\n    # Otherwise, load using pytorch.load\n    cached_file = hf_hub_download(hf_model_id, filename=filename, revision=hf_revision)\n    _logger.debug(f\"[{model_id}] Safe alternative not found for '{filename}'. Loading weights using default pytorch.\")\n    return torch.load(cached_file, map_location='cpu')\n\n\ndef save_config_for_hf(\n        model,\n        config_path: str,\n        model_config: Optional[dict] = None\n):\n    model_config = model_config or {}\n    hf_config = {}\n    pretrained_cfg = filter_pretrained_cfg(model.pretrained_cfg, remove_source=True, remove_null=True)\n    # set some values at root config level\n    hf_config['architecture'] = pretrained_cfg.pop('architecture')\n    hf_config['num_classes'] = model_config.get('num_classes', model.num_classes)\n    hf_config['num_features'] = model_config.get('num_features', model.num_features)\n    global_pool_type = model_config.get('global_pool', getattr(model, 'global_pool', None))\n    if isinstance(global_pool_type, str) and global_pool_type:\n        hf_config['global_pool'] = global_pool_type\n\n    if 'labels' in model_config:\n        _logger.warning(\n            \"'labels' as a config field for is deprecated. Please use 'label_names' and 'label_descriptions'.\"\n            \" Renaming provided 'labels' field to 'label_names'.\")\n        model_config.setdefault('label_names', model_config.pop('labels'))\n\n    label_names = model_config.pop('label_names', None)\n    if label_names:\n        assert isinstance(label_names, (dict, list, tuple))\n        # map label id (classifier index) -> unique label name (ie synset for ImageNet, MID for OpenImages)\n        # can be a dict id: name if there are id gaps, or tuple/list if no gaps.\n        hf_config['label_names'] = label_names\n\n    label_descriptions = model_config.pop('label_descriptions', None)\n    if label_descriptions:\n        assert isinstance(label_descriptions, dict)\n        # maps label names -> descriptions\n        hf_config['label_descriptions'] = label_descriptions\n\n    hf_config['pretrained_cfg'] = pretrained_cfg\n    hf_config.update(model_config)\n\n    with config_path.open('w') as f:\n        json.dump(hf_config, f, indent=2)\n\n\ndef save_for_hf(\n        model,\n        save_directory: str,\n        model_config: Optional[dict] = None,\n        safe_serialization: Union[bool, Literal[\"both\"]] = False,\n):\n    assert has_hf_hub(True)\n    save_directory = Path(save_directory)\n    save_directory.mkdir(exist_ok=True, parents=True)\n\n    # Save model weights, either safely (using safetensors), or using legacy pytorch approach or both.\n    tensors = model.state_dict()\n    if safe_serialization is True or safe_serialization == \"both\":\n        assert _has_safetensors, \"`pip install safetensors` to use .safetensors\"\n        safetensors.torch.save_file(tensors, save_directory / HF_SAFE_WEIGHTS_NAME)\n    if safe_serialization is False or safe_serialization == \"both\":\n        torch.save(tensors, save_directory / HF_WEIGHTS_NAME)\n\n    config_path = save_directory / 'config.json'\n    save_config_for_hf(model, config_path, model_config=model_config)\n\n\ndef push_to_hf_hub(\n        model,\n        repo_id: str,\n        commit_message: str = 'Add model',\n        token: Optional[str] = None,\n        revision: Optional[str] = None,\n        private: bool = False,\n        create_pr: bool = False,\n        model_config: Optional[dict] = None,\n        model_card: Optional[dict] = None,\n        safe_serialization: Union[bool, Literal[\"both\"]] = False,\n):\n    \"\"\"\n    Arguments:\n        (...)\n        safe_serialization (`bool` or `\"both\"`, *optional*, defaults to `False`):\n            Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).\n            Can be set to `\"both\"` in order to push both safe and unsafe weights.\n    \"\"\"\n    # Create repo if it doesn't exist yet\n    repo_url = create_repo(repo_id, token=token, private=private, exist_ok=True)\n\n    # Infer complete repo_id from repo_url\n    # Can be different from the input `repo_id` if repo_owner was implicit\n    _, repo_owner, repo_name = repo_type_and_id_from_hf_id(repo_url)\n    repo_id = f\"{repo_owner}/{repo_name}\"\n\n    # Check if README file already exist in repo\n    try:\n        get_hf_file_metadata(hf_hub_url(repo_id=repo_id, filename=\"README.md\", revision=revision))\n        has_readme = True\n    except EntryNotFoundError:\n        has_readme = False\n\n    # Dump model and push to Hub\n    with TemporaryDirectory() as tmpdir:\n        # Save model weights and config.\n        save_for_hf(model, tmpdir, model_config=model_config, safe_serialization=safe_serialization)\n\n        # Add readme if it does not exist\n        if not has_readme:\n            model_card = model_card or {}\n            model_name = repo_id.split('/')[-1]\n            readme_path = Path(tmpdir) / \"README.md\"\n            readme_text = generate_readme(model_card, model_name)\n            readme_path.write_text(readme_text)\n\n        # Upload model and return\n        return upload_folder(\n            repo_id=repo_id,\n            folder_path=tmpdir,\n            revision=revision,\n            create_pr=create_pr,\n            commit_message=commit_message,\n        )\n\n\ndef generate_readme(model_card: dict, model_name: str):\n    readme_text = \"---\\n\"\n    readme_text += \"tags:\\n- image-classification\\n- timm\\n\"\n    readme_text += \"library_name: timm\\n\"\n    readme_text += f\"license: {model_card.get('license', 'apache-2.0')}\\n\"\n    if 'details' in model_card and 'Dataset' in model_card['details']:\n        readme_text += 'datasets:\\n'\n        if isinstance(model_card['details']['Dataset'], (tuple, list)):\n            for d in model_card['details']['Dataset']:\n                readme_text += f\"- {d.lower()}\\n\"\n        else:\n            readme_text += f\"- {model_card['details']['Dataset'].lower()}\\n\"\n        if 'Pretrain Dataset' in model_card['details']:\n            if isinstance(model_card['details']['Pretrain Dataset'], (tuple, list)):\n                for d in model_card['details']['Pretrain Dataset']:\n                    readme_text += f\"- {d.lower()}\\n\"\n            else:\n                readme_text += f\"- {model_card['details']['Pretrain Dataset'].lower()}\\n\"\n    readme_text += \"---\\n\"\n    readme_text += f\"# Model card for {model_name}\\n\"\n    if 'description' in model_card:\n        readme_text += f\"\\n{model_card['description']}\\n\"\n    if 'details' in model_card:\n        readme_text += f\"\\n## Model Details\\n\"\n        for k, v in model_card['details'].items():\n            if isinstance(v, (list, tuple)):\n                readme_text += f\"- **{k}:**\\n\"\n                for vi in v:\n                    readme_text += f\"  - {vi}\\n\"\n            elif isinstance(v, dict):\n                readme_text += f\"- **{k}:**\\n\"\n                for ki, vi in v.items():\n                    readme_text += f\"  - {ki}: {vi}\\n\"\n            else:\n                readme_text += f\"- **{k}:** {v}\\n\"\n    if 'usage' in model_card:\n        readme_text += f\"\\n## Model Usage\\n\"\n        readme_text += model_card['usage']\n        readme_text += '\\n'\n\n    if 'comparison' in model_card:\n        readme_text += f\"\\n## Model Comparison\\n\"\n        readme_text += model_card['comparison']\n        readme_text += '\\n'\n\n    if 'citation' in model_card:\n        readme_text += f\"\\n## Citation\\n\"\n        if not isinstance(model_card['citation'], (list, tuple)):\n            citations = [model_card['citation']]\n        else:\n            citations = model_card['citation']\n        for c in citations:\n            readme_text += f\"```bibtex\\n{c}\\n```\\n\"\n    return readme_text\n\n\ndef _get_safe_alternatives(filename: str) -> Iterable[str]:\n    \"\"\"Returns potential safetensors alternatives for a given filename.\n\n    Use case:\n        When downloading a model from the Huggingface Hub, we first look if a .safetensors file exists and if yes, we use it.\n        Main use case is filename \"pytorch_model.bin\" => check for \"model.safetensors\" or \"pytorch_model.safetensors\".\n    \"\"\"\n    if filename == HF_WEIGHTS_NAME:\n        yield HF_SAFE_WEIGHTS_NAME\n    # if filename == HF_OPEN_CLIP_WEIGHTS_NAME:  # FIXME tracking safetensors yet\n    #     yield HF_OPEN_CLIP_SAFE_WEIGHTS_NAME\n    if filename not in (HF_WEIGHTS_NAME, HF_OPEN_CLIP_WEIGHTS_NAME) and filename.endswith(\".bin\"):\n        yield filename[:-4] + \".safetensors\"\n",
  "\"\"\" Cross-Covariance Image Transformer (XCiT) in PyTorch\n\nPaper:\n    - https://arxiv.org/abs/2106.09681\n\nSame as the official implementation, with some minor adaptations, original copyright below\n    - https://github.com/facebookresearch/xcit/blob/master/xcit.py\n\nModifications and additions for timm hacked together by / Copyright 2021, Ross Wightman\n\"\"\"\n# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\n\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import DropPath, trunc_normal_, to_2tuple\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_module\nfrom ._registry import register_model, generate_default_cfgs, register_model_deprecations\nfrom .cait import ClassAttn\nfrom .vision_transformer import Mlp\n\n__all__ = ['Xcit']  # model_registry will add each entrypoint fn to this\n\n\n@register_notrace_module  # reason: FX can't symbolically trace torch.arange in forward method\nclass PositionalEncodingFourier(nn.Module):\n    \"\"\"\n    Positional encoding relying on a fourier kernel matching the one used in the \"Attention is all you Need\" paper.\n    Based on the official XCiT code\n        - https://github.com/facebookresearch/xcit/blob/master/xcit.py\n    \"\"\"\n\n    def __init__(self, hidden_dim=32, dim=768, temperature=10000):\n        super().__init__()\n        self.token_projection = nn.Conv2d(hidden_dim * 2, dim, kernel_size=1)\n        self.scale = 2 * math.pi\n        self.temperature = temperature\n        self.hidden_dim = hidden_dim\n        self.dim = dim\n        self.eps = 1e-6\n\n    def forward(self, B: int, H: int, W: int):\n        device = self.token_projection.weight.device\n        y_embed = torch.arange(1, H+1, dtype=torch.float32, device=device).unsqueeze(1).repeat(1, 1, W)\n        x_embed = torch.arange(1, W+1, dtype=torch.float32, device=device).repeat(1, H, 1)\n        y_embed = y_embed / (y_embed[:, -1:, :] + self.eps) * self.scale\n        x_embed = x_embed / (x_embed[:, :, -1:] + self.eps) * self.scale\n        dim_t = torch.arange(self.hidden_dim, dtype=torch.float32, device=device)\n        dim_t = self.temperature ** (2 * torch.div(dim_t, 2, rounding_mode='floor') / self.hidden_dim)\n        pos_x = x_embed[:, :, :, None] / dim_t\n        pos_y = y_embed[:, :, :, None] / dim_t\n        pos_x = torch.stack([pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()], dim=4).flatten(3)\n        pos_y = torch.stack([pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()], dim=4).flatten(3)\n        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n        pos = self.token_projection(pos)\n        return pos.repeat(B, 1, 1, 1)  # (B, C, H, W)\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution + batch norm\"\"\"\n    return torch.nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False),\n        nn.BatchNorm2d(out_planes)\n    )\n\n\nclass ConvPatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding using multiple convolutional layers\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, act_layer=nn.GELU):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        num_patches = (img_size[1] // patch_size) * (img_size[0] // patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        if patch_size == 16:\n            self.proj = torch.nn.Sequential(\n                conv3x3(in_chans, embed_dim // 8, 2),\n                act_layer(),\n                conv3x3(embed_dim // 8, embed_dim // 4, 2),\n                act_layer(),\n                conv3x3(embed_dim // 4, embed_dim // 2, 2),\n                act_layer(),\n                conv3x3(embed_dim // 2, embed_dim, 2),\n            )\n        elif patch_size == 8:\n            self.proj = torch.nn.Sequential(\n                conv3x3(in_chans, embed_dim // 4, 2),\n                act_layer(),\n                conv3x3(embed_dim // 4, embed_dim // 2, 2),\n                act_layer(),\n                conv3x3(embed_dim // 2, embed_dim, 2),\n            )\n        else:\n            raise('For convolutional projection, patch size has to be in [8, 16]')\n\n    def forward(self, x):\n        x = self.proj(x)\n        Hp, Wp = x.shape[2], x.shape[3]\n        x = x.flatten(2).transpose(1, 2)  # (B, N, C)\n        return x, (Hp, Wp)\n\n\nclass LPI(nn.Module):\n    \"\"\"\n    Local Patch Interaction module that allows explicit communication between tokens in 3x3 windows to augment the\n    implicit communication performed by the block diagonal scatter attention. Implemented using 2 layers of separable\n    3x3 convolutions with GeLU and BatchNorm2d\n    \"\"\"\n\n    def __init__(self, in_features, out_features=None, act_layer=nn.GELU, kernel_size=3):\n        super().__init__()\n        out_features = out_features or in_features\n\n        padding = kernel_size // 2\n\n        self.conv1 = torch.nn.Conv2d(\n            in_features, in_features, kernel_size=kernel_size, padding=padding, groups=in_features)\n        self.act = act_layer()\n        self.bn = nn.BatchNorm2d(in_features)\n        self.conv2 = torch.nn.Conv2d(\n            in_features, out_features, kernel_size=kernel_size, padding=padding, groups=out_features)\n\n    def forward(self, x, H: int, W: int):\n        B, N, C = x.shape\n        x = x.permute(0, 2, 1).reshape(B, C, H, W)\n        x = self.conv1(x)\n        x = self.act(x)\n        x = self.bn(x)\n        x = self.conv2(x)\n        x = x.reshape(B, C, N).permute(0, 2, 1)\n        return x\n\n\nclass ClassAttentionBlock(nn.Module):\n    \"\"\"Class Attention Layer as in CaiT https://arxiv.org/abs/2103.17239\"\"\"\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            eta=1.,\n            tokens_norm=False,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n\n        self.attn = ClassAttn(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=proj_drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=proj_drop)\n\n        if eta is not None:  # LayerScale Initialization (no layerscale when None)\n            self.gamma1 = nn.Parameter(eta * torch.ones(dim))\n            self.gamma2 = nn.Parameter(eta * torch.ones(dim))\n        else:\n            self.gamma1, self.gamma2 = 1.0, 1.0\n\n        # See https://github.com/rwightman/pytorch-image-models/pull/747#issuecomment-877795721\n        self.tokens_norm = tokens_norm\n\n    def forward(self, x):\n        x_norm1 = self.norm1(x)\n        x_attn = torch.cat([self.attn(x_norm1), x_norm1[:, 1:]], dim=1)\n        x = x + self.drop_path(self.gamma1 * x_attn)\n        if self.tokens_norm:\n            x = self.norm2(x)\n        else:\n            x = torch.cat([self.norm2(x[:, 0:1]), x[:, 1:]], dim=1)\n        x_res = x\n        cls_token = x[:, 0:1]\n        cls_token = self.gamma2 * self.mlp(cls_token)\n        x = torch.cat([cls_token, x[:, 1:]], dim=1)\n        x = x_res + self.drop_path(x)\n        return x\n\n\nclass XCA(nn.Module):\n    \"\"\" Cross-Covariance Attention (XCA)\n    Operation where the channels are updated using a weighted sum. The weights are obtained from the (softmax\n    normalized) Cross-covariance matrix (Q^T \\\\cdot K \\\\in d_h \\\\times d_h)\n    \"\"\"\n\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        # Result of next line is (qkv, B, num (H)eads,  (C')hannels per head, N)\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 4, 1)\n        q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)\n        \n        # Paper section 3.2 l2-Normalization and temperature scaling\n        q = torch.nn.functional.normalize(q, dim=-1)\n        k = torch.nn.functional.normalize(k, dim=-1)\n        attn = (q @ k.transpose(-2, -1)) * self.temperature\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        # (B, H, C', N), permute -> (B, N, H, C')\n        x = (attn @ v).permute(0, 3, 1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'temperature'}\n\n\nclass XCABlock(nn.Module):\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            eta=1.,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = XCA(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=proj_drop)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm3 = norm_layer(dim)\n        self.local_mp = LPI(in_features=dim, act_layer=act_layer)\n\n        self.norm2 = norm_layer(dim)\n        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=proj_drop)\n\n        self.gamma1 = nn.Parameter(eta * torch.ones(dim))\n        self.gamma3 = nn.Parameter(eta * torch.ones(dim))\n        self.gamma2 = nn.Parameter(eta * torch.ones(dim))\n\n    def forward(self, x, H: int, W: int):\n        x = x + self.drop_path(self.gamma1 * self.attn(self.norm1(x)))\n        # NOTE official code has 3 then 2, so keeping it the same to be consistent with loaded weights\n        # See https://github.com/rwightman/pytorch-image-models/pull/747#issuecomment-877795721\n        x = x + self.drop_path(self.gamma3 * self.local_mp(self.norm3(x), H, W))\n        x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass Xcit(nn.Module):\n    \"\"\"\n    Based on timm and DeiT code bases\n    https://github.com/rwightman/pytorch-image-models/tree/master/timm\n    https://github.com/facebookresearch/deit/\n    \"\"\"\n\n    def __init__(\n            self,\n            img_size=224,\n            patch_size=16,\n            in_chans=3,\n            num_classes=1000,\n            global_pool='token',\n            embed_dim=768,\n            depth=12,\n            num_heads=12,\n            mlp_ratio=4.,\n            qkv_bias=True,\n            drop_rate=0.,\n            pos_drop_rate=0.,\n            proj_drop_rate=0.,\n            attn_drop_rate=0.,\n            drop_path_rate=0.,\n            act_layer=None,\n            norm_layer=None,\n            cls_attn_layers=2,\n            use_pos_embed=True,\n            eta=1.,\n            tokens_norm=False,\n    ):\n        \"\"\"\n        Args:\n            img_size (int, tuple): input image size\n            patch_size (int): patch size\n            in_chans (int): number of input channels\n            num_classes (int): number of classes for classification head\n            embed_dim (int): embedding dimension\n            depth (int): depth of transformer\n            num_heads (int): number of attention heads\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n            qkv_bias (bool): enable bias for qkv if True\n            drop_rate (float): dropout rate after positional embedding, and in XCA/CA projection + MLP\n            pos_drop_rate: position embedding dropout rate\n            proj_drop_rate (float): projection dropout rate\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate (constant across all layers)\n            norm_layer: (nn.Module): normalization layer\n            cls_attn_layers: (int) Depth of Class attention layers\n            use_pos_embed: (bool) whether to use positional encoding\n            eta: (float) layerscale initialization value\n            tokens_norm: (bool) Whether to normalize all tokens or just the cls_token in the CA\n\n        Notes:\n            - Although `layer_norm` is user specifiable, there are hard-coded `BatchNorm2d`s in the local patch\n              interaction (class LPI) and the patch embedding (class ConvPatchEmbed)\n        \"\"\"\n        super().__init__()\n        assert global_pool in ('', 'avg', 'token')\n        img_size = to_2tuple(img_size)\n        assert (img_size[0] % patch_size == 0) and (img_size[0] % patch_size == 0), \\\n            '`patch_size` should divide image dimensions evenly'\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        act_layer = act_layer or nn.GELU\n\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim\n        self.global_pool = global_pool\n        self.grad_checkpointing = False\n\n        self.patch_embed = ConvPatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            act_layer=act_layer,\n        )\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_pos_embed:\n            self.pos_embed = PositionalEncodingFourier(dim=embed_dim)\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n\n        self.blocks = nn.ModuleList([\n            XCABlock(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=drop_path_rate,\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n                eta=eta,\n            )\n            for _ in range(depth)])\n\n        self.cls_attn_blocks = nn.ModuleList([\n            ClassAttentionBlock(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                proj_drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n                eta=eta,\n                tokens_norm=tokens_norm,\n            )\n            for _ in range(cls_attn_layers)])\n\n        # Classifier head\n        self.norm = norm_layer(embed_dim)\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n        # Init weights\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^cls_token|pos_embed|patch_embed',  # stem and embed\n            blocks=r'^blocks\\.(\\d+)',\n            cls_attn_blocks=[(r'^cls_attn_blocks\\.(\\d+)', None), (r'^norm', (99999,))]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=''):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('', 'avg', 'token')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        B = x.shape[0]\n        # x is (B, N, C). (Hp, Hw) is (height in units of patches, width in units of patches)\n        x, (Hp, Wp) = self.patch_embed(x)\n\n        if self.pos_embed is not None:\n            # `pos_embed` (B, C, Hp, Wp), reshape -> (B, C, N), permute -> (B, N, C)\n            pos_encoding = self.pos_embed(B, Hp, Wp).reshape(B, -1, x.shape[1]).permute(0, 2, 1)\n            x = x + pos_encoding\n        x = self.pos_drop(x)\n\n        for blk in self.blocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint(blk, x, Hp, Wp)\n            else:\n                x = blk(x, Hp, Wp)\n\n        x = torch.cat((self.cls_token.expand(B, -1, -1), x), dim=1)\n\n        for blk in self.cls_attn_blocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint(blk, x)\n            else:\n                x = blk(x)\n\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool:\n            x = x[:, 1:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]\n        x = self.head_drop(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    if 'model' in state_dict:\n        state_dict = state_dict['model']\n    # For consistency with timm's transformer models while being compatible with official weights source we rename\n    # pos_embeder to pos_embed. Also account for use_pos_embed == False\n    use_pos_embed = getattr(model, 'pos_embed', None) is not None\n    pos_embed_keys = [k for k in state_dict if k.startswith('pos_embed')]\n    for k in pos_embed_keys:\n        if use_pos_embed:\n            state_dict[k.replace('pos_embeder.', 'pos_embed.')] = state_dict.pop(k)\n        else:\n            del state_dict[k]\n    # timm's implementation of class attention in CaiT is slightly more efficient as it does not compute query vectors\n    # for all tokens, just the class token. To use official weights source we must split qkv into q, k, v\n    if 'cls_attn_blocks.0.attn.qkv.weight' in state_dict and 'cls_attn_blocks.0.attn.q.weight' in model.state_dict():\n        num_ca_blocks = len(model.cls_attn_blocks)\n        for i in range(num_ca_blocks):\n            qkv_weight = state_dict.pop(f'cls_attn_blocks.{i}.attn.qkv.weight')\n            qkv_weight = qkv_weight.reshape(3, -1, qkv_weight.shape[-1])\n            for j, subscript in enumerate('qkv'):\n                state_dict[f'cls_attn_blocks.{i}.attn.{subscript}.weight'] = qkv_weight[j]\n            qkv_bias = state_dict.pop(f'cls_attn_blocks.{i}.attn.qkv.bias', None)\n            if qkv_bias is not None:\n                qkv_bias = qkv_bias.reshape(3, -1)\n                for j, subscript in enumerate('qkv'):\n                    state_dict[f'cls_attn_blocks.{i}.attn.{subscript}.bias'] = qkv_bias[j]\n    return state_dict\n\n\ndef _create_xcit(variant, pretrained=False, default_cfg=None, **kwargs):\n    model = build_model_with_cfg(\n        Xcit,\n        variant,\n        pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        **kwargs,\n    )\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': 1.0, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'patch_embed.proj.0.0', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    # Patch size 16\n    'xcit_nano_12_p16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p16_224.pth'),\n    'xcit_nano_12_p16_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p16_224_dist.pth'),\n    'xcit_nano_12_p16_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p16_384_dist.pth', input_size=(3, 384, 384)),\n    'xcit_tiny_12_p16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p16_224.pth'),\n    'xcit_tiny_12_p16_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p16_224_dist.pth'),\n    'xcit_tiny_12_p16_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p16_384_dist.pth', input_size=(3, 384, 384)),\n    'xcit_tiny_24_p16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p16_224.pth'),\n    'xcit_tiny_24_p16_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p16_224_dist.pth'),\n    'xcit_tiny_24_p16_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p16_384_dist.pth', input_size=(3, 384, 384)),\n    'xcit_small_12_p16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p16_224.pth'),\n    'xcit_small_12_p16_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p16_224_dist.pth'),\n    'xcit_small_12_p16_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p16_384_dist.pth', input_size=(3, 384, 384)),\n    'xcit_small_24_p16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p16_224.pth'),\n    'xcit_small_24_p16_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p16_224_dist.pth'),\n    'xcit_small_24_p16_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p16_384_dist.pth', input_size=(3, 384, 384)),\n    'xcit_medium_24_p16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p16_224.pth'),\n    'xcit_medium_24_p16_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p16_224_dist.pth'),\n    'xcit_medium_24_p16_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p16_384_dist.pth', input_size=(3, 384, 384)),\n    'xcit_large_24_p16_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p16_224.pth'),\n    'xcit_large_24_p16_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p16_224_dist.pth'),\n    'xcit_large_24_p16_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p16_384_dist.pth', input_size=(3, 384, 384)),\n\n    # Patch size 8\n    'xcit_nano_12_p8_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p8_224.pth'),\n    'xcit_nano_12_p8_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p8_224_dist.pth'),\n    'xcit_nano_12_p8_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_nano_12_p8_384_dist.pth', input_size=(3, 384, 384)),\n    'xcit_tiny_12_p8_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p8_224.pth'),\n    'xcit_tiny_12_p8_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p8_224_dist.pth'),\n    'xcit_tiny_12_p8_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_12_p8_384_dist.pth', input_size=(3, 384, 384)),\n    'xcit_tiny_24_p8_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p8_224.pth'),\n    'xcit_tiny_24_p8_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p8_224_dist.pth'),\n    'xcit_tiny_24_p8_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_tiny_24_p8_384_dist.pth', input_size=(3, 384, 384)),\n    'xcit_small_12_p8_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p8_224.pth'),\n    'xcit_small_12_p8_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p8_224_dist.pth'),\n    'xcit_small_12_p8_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_12_p8_384_dist.pth', input_size=(3, 384, 384)),\n    'xcit_small_24_p8_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p8_224.pth'),\n    'xcit_small_24_p8_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p8_224_dist.pth'),\n    'xcit_small_24_p8_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_small_24_p8_384_dist.pth', input_size=(3, 384, 384)),\n    'xcit_medium_24_p8_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p8_224.pth'),\n    'xcit_medium_24_p8_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p8_224_dist.pth'),\n    'xcit_medium_24_p8_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_medium_24_p8_384_dist.pth', input_size=(3, 384, 384)),\n    'xcit_large_24_p8_224.fb_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p8_224.pth'),\n    'xcit_large_24_p8_224.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p8_224_dist.pth'),\n    'xcit_large_24_p8_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        url='https://dl.fbaipublicfiles.com/xcit/xcit_large_24_p8_384_dist.pth', input_size=(3, 384, 384)),\n})\n\n\n@register_model\ndef xcit_nano_12_p16_224(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=16, embed_dim=128, depth=12, num_heads=4, eta=1.0, tokens_norm=False)\n    model = _create_xcit('xcit_nano_12_p16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_nano_12_p16_384(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=16, embed_dim=128, depth=12, num_heads=4, eta=1.0, tokens_norm=False, img_size=384)\n    model = _create_xcit('xcit_nano_12_p16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_tiny_12_p16_224(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=16, embed_dim=192, depth=12, num_heads=4, eta=1.0, tokens_norm=True)\n    model = _create_xcit('xcit_tiny_12_p16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_tiny_12_p16_384(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=16, embed_dim=192, depth=12, num_heads=4, eta=1.0, tokens_norm=True)\n    model = _create_xcit('xcit_tiny_12_p16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_small_12_p16_224(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=16, embed_dim=384, depth=12, num_heads=8, eta=1.0, tokens_norm=True)\n    model = _create_xcit('xcit_small_12_p16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_small_12_p16_384(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=16, embed_dim=384, depth=12, num_heads=8, eta=1.0, tokens_norm=True)\n    model = _create_xcit('xcit_small_12_p16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_tiny_24_p16_224(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=16, embed_dim=192, depth=24, num_heads=4, eta=1e-5, tokens_norm=True)\n    model = _create_xcit('xcit_tiny_24_p16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_tiny_24_p16_384(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=16, embed_dim=192, depth=24, num_heads=4, eta=1e-5, tokens_norm=True)\n    model = _create_xcit('xcit_tiny_24_p16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_small_24_p16_224(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=16, embed_dim=384, depth=24, num_heads=8, eta=1e-5, tokens_norm=True)\n    model = _create_xcit('xcit_small_24_p16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_small_24_p16_384(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=16, embed_dim=384, depth=24, num_heads=8, eta=1e-5, tokens_norm=True)\n    model = _create_xcit('xcit_small_24_p16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_medium_24_p16_224(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=16, embed_dim=512, depth=24, num_heads=8, eta=1e-5, tokens_norm=True)\n    model = _create_xcit('xcit_medium_24_p16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_medium_24_p16_384(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=16, embed_dim=512, depth=24, num_heads=8, eta=1e-5, tokens_norm=True)\n    model = _create_xcit('xcit_medium_24_p16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_large_24_p16_224(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=16, embed_dim=768, depth=24, num_heads=16, eta=1e-5, tokens_norm=True)\n    model = _create_xcit('xcit_large_24_p16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_large_24_p16_384(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=16, embed_dim=768, depth=24, num_heads=16, eta=1e-5, tokens_norm=True)\n    model = _create_xcit('xcit_large_24_p16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n# Patch size 8x8 models\n@register_model\ndef xcit_nano_12_p8_224(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=8, embed_dim=128, depth=12, num_heads=4, eta=1.0, tokens_norm=False)\n    model = _create_xcit('xcit_nano_12_p8_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_nano_12_p8_384(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=8, embed_dim=128, depth=12, num_heads=4, eta=1.0, tokens_norm=False)\n    model = _create_xcit('xcit_nano_12_p8_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_tiny_12_p8_224(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=8, embed_dim=192, depth=12, num_heads=4, eta=1.0, tokens_norm=True)\n    model = _create_xcit('xcit_tiny_12_p8_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_tiny_12_p8_384(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=8, embed_dim=192, depth=12, num_heads=4, eta=1.0, tokens_norm=True)\n    model = _create_xcit('xcit_tiny_12_p8_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_small_12_p8_224(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=8, embed_dim=384, depth=12, num_heads=8, eta=1.0, tokens_norm=True)\n    model = _create_xcit('xcit_small_12_p8_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_small_12_p8_384(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=8, embed_dim=384, depth=12, num_heads=8, eta=1.0, tokens_norm=True)\n    model = _create_xcit('xcit_small_12_p8_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_tiny_24_p8_224(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=8, embed_dim=192, depth=24, num_heads=4, eta=1e-5, tokens_norm=True)\n    model = _create_xcit('xcit_tiny_24_p8_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_tiny_24_p8_384(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=8, embed_dim=192, depth=24, num_heads=4, eta=1e-5, tokens_norm=True)\n    model = _create_xcit('xcit_tiny_24_p8_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_small_24_p8_224(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=8, embed_dim=384, depth=24, num_heads=8, eta=1e-5, tokens_norm=True)\n    model = _create_xcit('xcit_small_24_p8_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_small_24_p8_384(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=8, embed_dim=384, depth=24, num_heads=8, eta=1e-5, tokens_norm=True)\n    model = _create_xcit('xcit_small_24_p8_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_medium_24_p8_224(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=8, embed_dim=512, depth=24, num_heads=8, eta=1e-5, tokens_norm=True)\n    model = _create_xcit('xcit_medium_24_p8_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_medium_24_p8_384(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=8, embed_dim=512, depth=24, num_heads=8, eta=1e-5, tokens_norm=True)\n    model = _create_xcit('xcit_medium_24_p8_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_large_24_p8_224(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=8, embed_dim=768, depth=24, num_heads=16, eta=1e-5, tokens_norm=True)\n    model = _create_xcit('xcit_large_24_p8_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef xcit_large_24_p8_384(pretrained=False, **kwargs) -> Xcit:\n    model_args = dict(\n        patch_size=8, embed_dim=768, depth=24, num_heads=16, eta=1e-5, tokens_norm=True)\n    model = _create_xcit('xcit_large_24_p8_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\nregister_model_deprecations(__name__, {\n    # Patch size 16\n    'xcit_nano_12_p16_224_dist': 'xcit_nano_12_p16_224.fb_dist_in1k',\n    'xcit_nano_12_p16_384_dist': 'xcit_nano_12_p16_384.fb_dist_in1k',\n    'xcit_tiny_12_p16_224_dist': 'xcit_tiny_12_p16_224.fb_dist_in1k',\n    'xcit_tiny_12_p16_384_dist': 'xcit_tiny_12_p16_384.fb_dist_in1k',\n    'xcit_tiny_24_p16_224_dist': 'xcit_tiny_24_p16_224.fb_dist_in1k',\n    'xcit_tiny_24_p16_384_dist': 'xcit_tiny_24_p16_384.fb_dist_in1k',\n    'xcit_small_12_p16_224_dist': 'xcit_small_12_p16_224.fb_dist_in1k',\n    'xcit_small_12_p16_384_dist': 'xcit_small_12_p16_384.fb_dist_in1k',\n    'xcit_small_24_p16_224_dist': 'xcit_small_24_p16_224.fb_dist_in1k',\n    'xcit_medium_24_p16_224_dist': 'xcit_medium_24_p16_224.fb_dist_in1k',\n    'xcit_medium_24_p16_384_dist': 'xcit_medium_24_p16_384.fb_dist_in1k',\n    'xcit_large_24_p16_224_dist': 'xcit_large_24_p16_224.fb_dist_in1k',\n    'xcit_large_24_p16_384_dist': 'xcit_large_24_p16_384.fb_dist_in1k',\n\n    # Patch size 8\n    'xcit_nano_12_p8_224_dist': 'xcit_nano_12_p8_224.fb_dist_in1k',\n    'xcit_nano_12_p8_384_dist': 'xcit_nano_12_p8_384.fb_dist_in1k',\n    'xcit_tiny_12_p8_224_dist': 'xcit_tiny_12_p8_224.fb_dist_in1k',\n    'xcit_tiny_12_p8_384_dist': 'xcit_tiny_12_p8_384.fb_dist_in1k',\n    'xcit_tiny_24_p8_224_dist': 'xcit_tiny_24_p8_224.fb_dist_in1k',\n    'xcit_tiny_24_p8_384_dist': 'xcit_tiny_24_p8_384.fb_dist_in1k',\n    'xcit_small_12_p8_224_dist': 'xcit_small_12_p8_224.fb_dist_in1k',\n    'xcit_small_12_p8_384_dist': 'xcit_small_12_p8_384.fb_dist_in1k',\n    'xcit_small_24_p8_224_dist': 'xcit_small_24_p8_224.fb_dist_in1k',\n    'xcit_small_24_p8_384_dist': 'xcit_small_24_p8_384.fb_dist_in1k',\n    'xcit_medium_24_p8_224_dist': 'xcit_medium_24_p8_224.fb_dist_in1k',\n    'xcit_medium_24_p8_384_dist': 'xcit_medium_24_p8_384.fb_dist_in1k',\n    'xcit_large_24_p8_224_dist': 'xcit_large_24_p8_224.fb_dist_in1k',\n    'xcit_large_24_p8_384_dist': 'xcit_large_24_p8_384.fb_dist_in1k',\n})\n",
  "\"\"\"PyTorch SelecSLS Net example for ImageNet Classification\nLicense: CC BY 4.0 (https://creativecommons.org/licenses/by/4.0/legalcode)\nAuthor: Dushyant Mehta (@mehtadushy)\n\nSelecSLS (core) Network Architecture as proposed in \"XNect: Real-time Multi-person 3D\nHuman Pose Estimation with a Single RGB Camera, Mehta et al.\"\nhttps://arxiv.org/abs/1907.00837\n\nBased on ResNet implementation in https://github.com/rwightman/pytorch-image-models\nand SelecSLS Net implementation in https://github.com/mehtadushy/SelecSLS-Pytorch\n\"\"\"\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import create_classifier\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['SelecSls']  # model_registry will add each entrypoint fn to this\n\n\nclass SequentialList(nn.Sequential):\n\n    def __init__(self, *args):\n        super(SequentialList, self).__init__(*args)\n\n    @torch.jit._overload_method  # noqa: F811\n    def forward(self, x):\n        # type: (List[torch.Tensor]) -> (List[torch.Tensor])\n        pass\n\n    @torch.jit._overload_method  # noqa: F811\n    def forward(self, x):\n        # type: (torch.Tensor) -> (List[torch.Tensor])\n        pass\n\n    def forward(self, x) -> List[torch.Tensor]:\n        for module in self:\n            x = module(x)\n        return x\n\n\nclass SelectSeq(nn.Module):\n    def __init__(self, mode='index', index=0):\n        super(SelectSeq, self).__init__()\n        self.mode = mode\n        self.index = index\n\n    @torch.jit._overload_method  # noqa: F811\n    def forward(self, x):\n        # type: (List[torch.Tensor]) -> (torch.Tensor)\n        pass\n\n    @torch.jit._overload_method  # noqa: F811\n    def forward(self, x):\n        # type: (Tuple[torch.Tensor]) -> (torch.Tensor)\n        pass\n\n    def forward(self, x) -> torch.Tensor:\n        if self.mode == 'index':\n            return x[self.index]\n        else:\n            return torch.cat(x, dim=1)\n\n\ndef conv_bn(in_chs, out_chs, k=3, stride=1, padding=None, dilation=1):\n    if padding is None:\n        padding = ((stride - 1) + dilation * (k - 1)) // 2\n    return nn.Sequential(\n        nn.Conv2d(in_chs, out_chs, k, stride, padding=padding, dilation=dilation, bias=False),\n        nn.BatchNorm2d(out_chs),\n        nn.ReLU(inplace=True)\n    )\n\n\nclass SelecSlsBlock(nn.Module):\n    def __init__(self, in_chs, skip_chs, mid_chs, out_chs, is_first, stride, dilation=1):\n        super(SelecSlsBlock, self).__init__()\n        self.stride = stride\n        self.is_first = is_first\n        assert stride in [1, 2]\n\n        # Process input with 4 conv blocks with the same number of input and output channels\n        self.conv1 = conv_bn(in_chs, mid_chs, 3, stride, dilation=dilation)\n        self.conv2 = conv_bn(mid_chs, mid_chs, 1)\n        self.conv3 = conv_bn(mid_chs, mid_chs // 2, 3)\n        self.conv4 = conv_bn(mid_chs // 2, mid_chs, 1)\n        self.conv5 = conv_bn(mid_chs, mid_chs // 2, 3)\n        self.conv6 = conv_bn(2 * mid_chs + (0 if is_first else skip_chs), out_chs, 1)\n\n    def forward(self, x: List[torch.Tensor]) -> List[torch.Tensor]:\n        if not isinstance(x, list):\n            x = [x]\n        assert len(x) in [1, 2]\n\n        d1 = self.conv1(x[0])\n        d2 = self.conv3(self.conv2(d1))\n        d3 = self.conv5(self.conv4(d2))\n        if self.is_first:\n            out = self.conv6(torch.cat([d1, d2, d3], 1))\n            return [out, out]\n        else:\n            return [self.conv6(torch.cat([d1, d2, d3, x[1]], 1)), x[1]]\n\n\nclass SelecSls(nn.Module):\n    \"\"\"SelecSls42 / SelecSls60 / SelecSls84\n\n    Parameters\n    ----------\n    cfg : network config dictionary specifying block type, feature, and head args\n    num_classes : int, default 1000\n        Number of classification classes.\n    in_chans : int, default 3\n        Number of input (color) channels.\n    drop_rate : float, default 0.\n        Dropout probability before classifier, for training\n    global_pool : str, default 'avg'\n        Global pooling type. One of 'avg', 'max', 'avgmax', 'catavgmax'\n    \"\"\"\n\n    def __init__(self, cfg, num_classes=1000, in_chans=3, drop_rate=0.0, global_pool='avg'):\n        self.num_classes = num_classes\n        super(SelecSls, self).__init__()\n\n        self.stem = conv_bn(in_chans, 32, stride=2)\n        self.features = SequentialList(*[cfg['block'](*block_args) for block_args in cfg['features']])\n        self.from_seq = SelectSeq()  # from List[tensor] -> Tensor in module compatible way\n        self.head = nn.Sequential(*[conv_bn(*conv_args) for conv_args in cfg['head']])\n        self.num_features = cfg['num_features']\n        self.feature_info = cfg['feature_info']\n\n        self.global_pool, self.head_drop, self.fc = create_classifier(\n            self.num_features,\n            self.num_classes,\n            pool_type=global_pool,\n            drop_rate=drop_rate,\n        )\n\n        for n, m in self.named_modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^stem',\n            blocks=r'^features\\.(\\d+)',\n            blocks_head=r'^head'\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, 'gradient checkpointing not supported'\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.fc\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.global_pool, self.fc = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        x = self.features(x)\n        x = self.head(self.from_seq(x))\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        x = self.global_pool(x)\n        x = self.head_drop(x)\n        return x if pre_logits else self.fc(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_SelecSls(variant, pretrained, **kwargs):\n    cfg = {}\n    feature_info = [dict(num_chs=32, reduction=2, module='stem.2')]\n    if variant.startswith('SelecSls42'):\n        cfg['block'] = SelecSlsBlock\n        # Define configuration of the network after the initial neck\n        cfg['features'] = [\n            # in_chs, skip_chs, mid_chs, out_chs, is_first, stride\n            (32, 0, 64, 64, True, 2),\n            (64, 64, 64, 128, False, 1),\n            (128, 0, 144, 144, True, 2),\n            (144, 144, 144, 288, False, 1),\n            (288, 0, 304, 304, True, 2),\n            (304, 304, 304, 480, False, 1),\n        ]\n        feature_info.extend([\n            dict(num_chs=128, reduction=4, module='features.1'),\n            dict(num_chs=288, reduction=8, module='features.3'),\n            dict(num_chs=480, reduction=16, module='features.5'),\n        ])\n        # Head can be replaced with alternative configurations depending on the problem\n        feature_info.append(dict(num_chs=1024, reduction=32, module='head.1'))\n        if variant == 'SelecSls42b':\n            cfg['head'] = [\n                (480, 960, 3, 2),\n                (960, 1024, 3, 1),\n                (1024, 1280, 3, 2),\n                (1280, 1024, 1, 1),\n            ]\n            feature_info.append(dict(num_chs=1024, reduction=64, module='head.3'))\n            cfg['num_features'] = 1024\n        else:\n            cfg['head'] = [\n                (480, 960, 3, 2),\n                (960, 1024, 3, 1),\n                (1024, 1024, 3, 2),\n                (1024, 1280, 1, 1),\n            ]\n            feature_info.append(dict(num_chs=1280, reduction=64, module='head.3'))\n            cfg['num_features'] = 1280\n\n    elif variant.startswith('SelecSls60'):\n        cfg['block'] = SelecSlsBlock\n        # Define configuration of the network after the initial neck\n        cfg['features'] = [\n            # in_chs, skip_chs, mid_chs, out_chs, is_first, stride\n            (32, 0, 64, 64, True, 2),\n            (64, 64, 64, 128, False, 1),\n            (128, 0, 128, 128, True, 2),\n            (128, 128, 128, 128, False, 1),\n            (128, 128, 128, 288, False, 1),\n            (288, 0, 288, 288, True, 2),\n            (288, 288, 288, 288, False, 1),\n            (288, 288, 288, 288, False, 1),\n            (288, 288, 288, 416, False, 1),\n        ]\n        feature_info.extend([\n            dict(num_chs=128, reduction=4, module='features.1'),\n            dict(num_chs=288, reduction=8, module='features.4'),\n            dict(num_chs=416, reduction=16, module='features.8'),\n        ])\n        # Head can be replaced with alternative configurations depending on the problem\n        feature_info.append(dict(num_chs=1024, reduction=32, module='head.1'))\n        if variant == 'SelecSls60b':\n            cfg['head'] = [\n                (416, 756, 3, 2),\n                (756, 1024, 3, 1),\n                (1024, 1280, 3, 2),\n                (1280, 1024, 1, 1),\n            ]\n            feature_info.append(dict(num_chs=1024, reduction=64, module='head.3'))\n            cfg['num_features'] = 1024\n        else:\n            cfg['head'] = [\n                (416, 756, 3, 2),\n                (756, 1024, 3, 1),\n                (1024, 1024, 3, 2),\n                (1024, 1280, 1, 1),\n            ]\n            feature_info.append(dict(num_chs=1280, reduction=64, module='head.3'))\n            cfg['num_features'] = 1280\n\n    elif variant == 'SelecSls84':\n        cfg['block'] = SelecSlsBlock\n        # Define configuration of the network after the initial neck\n        cfg['features'] = [\n            # in_chs, skip_chs, mid_chs, out_chs, is_first, stride\n            (32, 0, 64, 64, True, 2),\n            (64, 64, 64, 144, False, 1),\n            (144, 0, 144, 144, True, 2),\n            (144, 144, 144, 144, False, 1),\n            (144, 144, 144, 144, False, 1),\n            (144, 144, 144, 144, False, 1),\n            (144, 144, 144, 304, False, 1),\n            (304, 0, 304, 304, True, 2),\n            (304, 304, 304, 304, False, 1),\n            (304, 304, 304, 304, False, 1),\n            (304, 304, 304, 304, False, 1),\n            (304, 304, 304, 304, False, 1),\n            (304, 304, 304, 512, False, 1),\n        ]\n        feature_info.extend([\n            dict(num_chs=144, reduction=4, module='features.1'),\n            dict(num_chs=304, reduction=8, module='features.6'),\n            dict(num_chs=512, reduction=16, module='features.12'),\n        ])\n        # Head can be replaced with alternative configurations depending on the problem\n        cfg['head'] = [\n            (512, 960, 3, 2),\n            (960, 1024, 3, 1),\n            (1024, 1024, 3, 2),\n            (1024, 1280, 3, 1),\n        ]\n        cfg['num_features'] = 1280\n        feature_info.extend([\n            dict(num_chs=1024, reduction=32, module='head.1'),\n            dict(num_chs=1280, reduction=64, module='head.3')\n        ])\n    else:\n        raise ValueError('Invalid net configuration ' + variant + ' !!!')\n    cfg['feature_info'] = feature_info\n\n    # this model can do 6 feature levels by default, unlike most others, leave as 0-4 to avoid surprises?\n    return build_model_with_cfg(\n        SelecSls,\n        variant,\n        pretrained,\n        model_cfg=cfg,\n        feature_cfg=dict(out_indices=(0, 1, 2, 3, 4), flatten_sequential=True),\n        **kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (4, 4),\n        'crop_pct': 0.875, 'interpolation': 'bilinear',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.0', 'classifier': 'fc',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'SelecSls42.untrained': _cfg(\n        interpolation='bicubic'),\n    'SelecSls42b.in1k': _cfg(\n        hf_hub_id='timm/',\n        interpolation='bicubic'),\n    'SelecSls60.in1k': _cfg(\n        hf_hub_id='timm/',\n        interpolation='bicubic'),\n    'SelecSls60b.in1k': _cfg(\n        hf_hub_id='timm/',\n        interpolation='bicubic'),\n    'SelecSls84.untrained': _cfg(\n        interpolation='bicubic'),\n})\n\n\n@register_model\ndef SelecSls42(pretrained=False, **kwargs) -> SelecSls:\n    \"\"\"Constructs a SelecSls42 model.\n    \"\"\"\n    return _create_SelecSls('SelecSls42', pretrained, **kwargs)\n\n\n@register_model\ndef SelecSls42b(pretrained=False, **kwargs) -> SelecSls:\n    \"\"\"Constructs a SelecSls42_B model.\n    \"\"\"\n    return _create_SelecSls('SelecSls42b', pretrained, **kwargs)\n\n\n@register_model\ndef SelecSls60(pretrained=False, **kwargs) -> SelecSls:\n    \"\"\"Constructs a SelecSls60 model.\n    \"\"\"\n    return _create_SelecSls('SelecSls60', pretrained, **kwargs)\n\n\n@register_model\ndef SelecSls60b(pretrained=False, **kwargs) -> SelecSls:\n    \"\"\"Constructs a SelecSls60_B model.\n    \"\"\"\n    return _create_SelecSls('SelecSls60b', pretrained, **kwargs)\n\n\n@register_model\ndef SelecSls84(pretrained=False, **kwargs) -> SelecSls:\n    \"\"\"Constructs a SelecSls84 model.\n    \"\"\"\n    return _create_SelecSls('SelecSls84', pretrained, **kwargs)\n",
  "import os\nimport pkgutil\nfrom copy import deepcopy\n\nfrom torch import nn as nn\n\nfrom timm.layers import Conv2dSame, BatchNormAct2d, Linear\n\n__all__ = ['extract_layer', 'set_layer', 'adapt_model_from_string', 'adapt_model_from_file']\n\n\ndef extract_layer(model, layer):\n    layer = layer.split('.')\n    module = model\n    if hasattr(model, 'module') and layer[0] != 'module':\n        module = model.module\n    if not hasattr(model, 'module') and layer[0] == 'module':\n        layer = layer[1:]\n    for l in layer:\n        if hasattr(module, l):\n            if not l.isdigit():\n                module = getattr(module, l)\n            else:\n                module = module[int(l)]\n        else:\n            return module\n    return module\n\n\ndef set_layer(model, layer, val):\n    layer = layer.split('.')\n    module = model\n    if hasattr(model, 'module') and layer[0] != 'module':\n        module = model.module\n    lst_index = 0\n    module2 = module\n    for l in layer:\n        if hasattr(module2, l):\n            if not l.isdigit():\n                module2 = getattr(module2, l)\n            else:\n                module2 = module2[int(l)]\n            lst_index += 1\n    lst_index -= 1\n    for l in layer[:lst_index]:\n        if not l.isdigit():\n            module = getattr(module, l)\n        else:\n            module = module[int(l)]\n    l = layer[lst_index]\n    setattr(module, l, val)\n\n\ndef adapt_model_from_string(parent_module, model_string):\n    separator = '***'\n    state_dict = {}\n    lst_shape = model_string.split(separator)\n    for k in lst_shape:\n        k = k.split(':')\n        key = k[0]\n        shape = k[1][1:-1].split(',')\n        if shape[0] != '':\n            state_dict[key] = [int(i) for i in shape]\n\n    new_module = deepcopy(parent_module)\n    for n, m in parent_module.named_modules():\n        old_module = extract_layer(parent_module, n)\n        if isinstance(old_module, nn.Conv2d) or isinstance(old_module, Conv2dSame):\n            if isinstance(old_module, Conv2dSame):\n                conv = Conv2dSame\n            else:\n                conv = nn.Conv2d\n            s = state_dict[n + '.weight']\n            in_channels = s[1]\n            out_channels = s[0]\n            g = 1\n            if old_module.groups > 1:\n                in_channels = out_channels\n                g = in_channels\n            new_conv = conv(\n                in_channels=in_channels, out_channels=out_channels, kernel_size=old_module.kernel_size,\n                bias=old_module.bias is not None, padding=old_module.padding, dilation=old_module.dilation,\n                groups=g, stride=old_module.stride)\n            set_layer(new_module, n, new_conv)\n        elif isinstance(old_module, BatchNormAct2d):\n            new_bn = BatchNormAct2d(\n                state_dict[n + '.weight'][0], eps=old_module.eps, momentum=old_module.momentum,\n                affine=old_module.affine, track_running_stats=True)\n            new_bn.drop = old_module.drop\n            new_bn.act = old_module.act\n            set_layer(new_module, n, new_bn)\n        elif isinstance(old_module, nn.BatchNorm2d):\n            new_bn = nn.BatchNorm2d(\n                num_features=state_dict[n + '.weight'][0], eps=old_module.eps, momentum=old_module.momentum,\n                affine=old_module.affine, track_running_stats=True)\n            set_layer(new_module, n, new_bn)\n        elif isinstance(old_module, nn.Linear):\n            # FIXME extra checks to ensure this is actually the FC classifier layer and not a diff Linear layer?\n            num_features = state_dict[n + '.weight'][1]\n            new_fc = Linear(\n                in_features=num_features, out_features=old_module.out_features, bias=old_module.bias is not None)\n            set_layer(new_module, n, new_fc)\n            if hasattr(new_module, 'num_features'):\n                new_module.num_features = num_features\n    new_module.eval()\n    parent_module.eval()\n\n    return new_module\n\n\ndef adapt_model_from_file(parent_module, model_variant):\n    adapt_data = pkgutil.get_data(__name__, os.path.join('_pruned', model_variant + '.txt'))\n    return adapt_model_from_string(parent_module, adapt_data.decode('utf-8').strip())\n",
  "\"\"\" Pytorch Inception-Resnet-V2 implementation\nSourced from https://github.com/Cadene/tensorflow-model-zoo.torch (MIT License) which is\nbased upon Google's Tensorflow implementation and pretrained weights (Apache 2.0 License)\n\"\"\"\nfrom functools import partial\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import create_classifier, ConvNormAct\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import flatten_modules\nfrom ._registry import register_model, generate_default_cfgs, register_model_deprecations\n\n__all__ = ['InceptionResnetV2']\n\n\nclass Mixed_5b(nn.Module):\n    def __init__(self, conv_block=None):\n        super(Mixed_5b, self).__init__()\n        conv_block = conv_block or ConvNormAct\n\n        self.branch0 = conv_block(192, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            conv_block(192, 48, kernel_size=1, stride=1),\n            conv_block(48, 64, kernel_size=5, stride=1, padding=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            conv_block(192, 64, kernel_size=1, stride=1),\n            conv_block(64, 96, kernel_size=3, stride=1, padding=1),\n            conv_block(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            conv_block(192, 64, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block35(nn.Module):\n    def __init__(self, scale=1.0, conv_block=None):\n        super(Block35, self).__init__()\n        self.scale = scale\n        conv_block = conv_block or ConvNormAct\n\n        self.branch0 = conv_block(320, 32, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            conv_block(320, 32, kernel_size=1, stride=1),\n            conv_block(32, 32, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            conv_block(320, 32, kernel_size=1, stride=1),\n            conv_block(32, 48, kernel_size=3, stride=1, padding=1),\n            conv_block(48, 64, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.conv2d = nn.Conv2d(128, 320, kernel_size=1, stride=1)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.act(out)\n        return out\n\n\nclass Mixed_6a(nn.Module):\n    def __init__(self, conv_block=None):\n        super(Mixed_6a, self).__init__()\n        conv_block = conv_block or ConvNormAct\n\n        self.branch0 = conv_block(320, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            conv_block(320, 256, kernel_size=1, stride=1),\n            conv_block(256, 256, kernel_size=3, stride=1, padding=1),\n            conv_block(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Block17(nn.Module):\n    def __init__(self, scale=1.0, conv_block=None):\n        super(Block17, self).__init__()\n        self.scale = scale\n        conv_block = conv_block or ConvNormAct\n\n        self.branch0 = conv_block(1088, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            conv_block(1088, 128, kernel_size=1, stride=1),\n            conv_block(128, 160, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            conv_block(160, 192, kernel_size=(7, 1), stride=1, padding=(3, 0))\n        )\n\n        self.conv2d = nn.Conv2d(384, 1088, kernel_size=1, stride=1)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.act(out)\n        return out\n\n\nclass Mixed_7a(nn.Module):\n    def __init__(self, conv_block=None):\n        super(Mixed_7a, self).__init__()\n        conv_block = conv_block or ConvNormAct\n\n        self.branch0 = nn.Sequential(\n            conv_block(1088, 256, kernel_size=1, stride=1),\n            conv_block(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            conv_block(1088, 256, kernel_size=1, stride=1),\n            conv_block(256, 288, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            conv_block(1088, 256, kernel_size=1, stride=1),\n            conv_block(256, 288, kernel_size=3, stride=1, padding=1),\n            conv_block(288, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch3 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block8(nn.Module):\n\n    def __init__(self, scale=1.0, no_relu=False, conv_block=None):\n        super(Block8, self).__init__()\n        self.scale = scale\n        conv_block = conv_block or ConvNormAct\n\n        self.branch0 = conv_block(2080, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            conv_block(2080, 192, kernel_size=1, stride=1),\n            conv_block(192, 224, kernel_size=(1, 3), stride=1, padding=(0, 1)),\n            conv_block(224, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))\n        )\n\n        self.conv2d = nn.Conv2d(448, 2080, kernel_size=1, stride=1)\n        self.relu = None if no_relu else nn.ReLU()\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        if self.relu is not None:\n            out = self.relu(out)\n        return out\n\n\nclass InceptionResnetV2(nn.Module):\n    def __init__(\n            self,\n            num_classes=1000,\n            in_chans=3,\n            drop_rate=0.,\n            output_stride=32,\n            global_pool='avg',\n            norm_layer='batchnorm2d',\n            norm_eps=1e-3,\n            act_layer='relu',\n    ):\n        super(InceptionResnetV2, self).__init__()\n        self.num_classes = num_classes\n        self.num_features = 1536\n        assert output_stride == 32\n        conv_block = partial(\n            ConvNormAct,\n            padding=0,\n            norm_layer=norm_layer,\n            act_layer=act_layer,\n            norm_kwargs=dict(eps=norm_eps),\n            act_kwargs=dict(inplace=True),\n        )\n\n        self.conv2d_1a = conv_block(in_chans, 32, kernel_size=3, stride=2)\n        self.conv2d_2a = conv_block(32, 32, kernel_size=3, stride=1)\n        self.conv2d_2b = conv_block(32, 64, kernel_size=3, stride=1, padding=1)\n        self.feature_info = [dict(num_chs=64, reduction=2, module='conv2d_2b')]\n\n        self.maxpool_3a = nn.MaxPool2d(3, stride=2)\n        self.conv2d_3b = conv_block(64, 80, kernel_size=1, stride=1)\n        self.conv2d_4a = conv_block(80, 192, kernel_size=3, stride=1)\n        self.feature_info += [dict(num_chs=192, reduction=4, module='conv2d_4a')]\n\n        self.maxpool_5a = nn.MaxPool2d(3, stride=2)\n        self.mixed_5b = Mixed_5b(conv_block=conv_block)\n        self.repeat = nn.Sequential(*[Block35(scale=0.17, conv_block=conv_block) for _ in range(10)])\n        self.feature_info += [dict(num_chs=320, reduction=8, module='repeat')]\n\n        self.mixed_6a = Mixed_6a(conv_block=conv_block)\n        self.repeat_1 = nn.Sequential(*[Block17(scale=0.10, conv_block=conv_block) for _ in range(20)])\n        self.feature_info += [dict(num_chs=1088, reduction=16, module='repeat_1')]\n\n        self.mixed_7a = Mixed_7a(conv_block=conv_block)\n        self.repeat_2 = nn.Sequential(*[Block8(scale=0.20, conv_block=conv_block) for _ in range(9)])\n\n        self.block8 = Block8(no_relu=True, conv_block=conv_block)\n        self.conv2d_7b = conv_block(2080, self.num_features, kernel_size=1, stride=1)\n        self.feature_info += [dict(num_chs=self.num_features, reduction=32, module='conv2d_7b')]\n\n        self.global_pool, self.head_drop, self.classif = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool, drop_rate=drop_rate)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        module_map = {k: i for i, (k, _) in enumerate(flatten_modules(self.named_children(), prefix=()))}\n        module_map.pop(('classif',))\n\n        def _matcher(name):\n            if any([name.startswith(n) for n in ('conv2d_1', 'conv2d_2')]):\n                return 0\n            elif any([name.startswith(n) for n in ('conv2d_3', 'conv2d_4')]):\n                return 1\n            elif any([name.startswith(n) for n in ('block8', 'conv2d_7')]):\n                return len(module_map) + 1\n            else:\n                for k in module_map.keys():\n                    if k == tuple(name.split('.')[:len(k)]):\n                        return module_map[k]\n                return float('inf')\n        return _matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, \"checkpointing not supported\"\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.classif\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.global_pool, self.classif = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        x = self.conv2d_1a(x)\n        x = self.conv2d_2a(x)\n        x = self.conv2d_2b(x)\n        x = self.maxpool_3a(x)\n        x = self.conv2d_3b(x)\n        x = self.conv2d_4a(x)\n        x = self.maxpool_5a(x)\n        x = self.mixed_5b(x)\n        x = self.repeat(x)\n        x = self.mixed_6a(x)\n        x = self.repeat_1(x)\n        x = self.mixed_7a(x)\n        x = self.repeat_2(x)\n        x = self.block8(x)\n        x = self.conv2d_7b(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        x = self.global_pool(x)\n        x = self.head_drop(x)\n        return x if pre_logits else self.classif(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_inception_resnet_v2(variant, pretrained=False, **kwargs):\n    return build_model_with_cfg(InceptionResnetV2, variant, pretrained, **kwargs)\n\n\ndefault_cfgs = generate_default_cfgs({\n    # ported from http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz\n    'inception_resnet_v2.tf_in1k': {\n        'hf_hub_id': 'timm/',\n        'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (8, 8),\n        'crop_pct': 0.8975, 'interpolation': 'bicubic',\n        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,\n        'first_conv': 'conv2d_1a.conv', 'classifier': 'classif',\n    },\n    # As per https://arxiv.org/abs/1705.07204 and\n    # ported from http://download.tensorflow.org/models/ens_adv_inception_resnet_v2_2017_08_18.tar.gz\n    'inception_resnet_v2.tf_ens_adv_in1k': {\n        'hf_hub_id': 'timm/',\n        'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (8, 8),\n        'crop_pct': 0.8975, 'interpolation': 'bicubic',\n        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,\n        'first_conv': 'conv2d_1a.conv', 'classifier': 'classif',\n    }\n})\n\n\n@register_model\ndef inception_resnet_v2(pretrained=False, **kwargs) -> InceptionResnetV2:\n    return _create_inception_resnet_v2('inception_resnet_v2', pretrained=pretrained, **kwargs)\n\n\nregister_model_deprecations(__name__, {\n    'ens_adv_inception_resnet_v2': 'inception_resnet_v2.tf_ens_adv_in1k',\n})",
  "\"\"\" Inception-V3\n\nOriginally from torchvision Inception3 model\nLicensed BSD-Clause 3 https://github.com/pytorch/vision/blob/master/LICENSE\n\"\"\"\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_STD, IMAGENET_DEFAULT_MEAN, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import trunc_normal_, create_classifier, Linear, ConvNormAct\nfrom ._builder import build_model_with_cfg\nfrom ._builder import resolve_pretrained_cfg\nfrom ._manipulate import flatten_modules\nfrom ._registry import register_model, generate_default_cfgs, register_model_deprecations\n\n__all__ = ['InceptionV3']  # model_registry will add each entrypoint fn to this\n\n\nclass InceptionA(nn.Module):\n\n    def __init__(self, in_channels, pool_features, conv_block=None):\n        super(InceptionA, self).__init__()\n        conv_block = conv_block or ConvNormAct\n        self.branch1x1 = conv_block(in_channels, 64, kernel_size=1)\n\n        self.branch5x5_1 = conv_block(in_channels, 48, kernel_size=1)\n        self.branch5x5_2 = conv_block(48, 64, kernel_size=5, padding=2)\n\n        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, padding=1)\n\n        self.branch_pool = conv_block(in_channels, pool_features, kernel_size=1)\n\n    def _forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n        return outputs\n\n    def forward(self, x):\n        outputs = self._forward(x)\n        return torch.cat(outputs, 1)\n\n\nclass InceptionB(nn.Module):\n\n    def __init__(self, in_channels, conv_block=None):\n        super(InceptionB, self).__init__()\n        conv_block = conv_block or ConvNormAct\n        self.branch3x3 = conv_block(in_channels, 384, kernel_size=3, stride=2)\n\n        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, stride=2)\n\n    def _forward(self, x):\n        branch3x3 = self.branch3x3(x)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n\n        outputs = [branch3x3, branch3x3dbl, branch_pool]\n        return outputs\n\n    def forward(self, x):\n        outputs = self._forward(x)\n        return torch.cat(outputs, 1)\n\n\nclass InceptionC(nn.Module):\n\n    def __init__(self, in_channels, channels_7x7, conv_block=None):\n        super(InceptionC, self).__init__()\n        conv_block = conv_block or ConvNormAct\n        self.branch1x1 = conv_block(in_channels, 192, kernel_size=1)\n\n        c7 = channels_7x7\n        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=1)\n        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7_3 = conv_block(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n\n        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=1)\n        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7dbl_5 = conv_block(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n\n        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n\n    def _forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch7x7 = self.branch7x7_1(x)\n        branch7x7 = self.branch7x7_2(branch7x7)\n        branch7x7 = self.branch7x7_3(branch7x7)\n\n        branch7x7dbl = self.branch7x7dbl_1(x)\n        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n        return outputs\n\n    def forward(self, x):\n        outputs = self._forward(x)\n        return torch.cat(outputs, 1)\n\n\nclass InceptionD(nn.Module):\n\n    def __init__(self, in_channels, conv_block=None):\n        super(InceptionD, self).__init__()\n        conv_block = conv_block or ConvNormAct\n        self.branch3x3_1 = conv_block(in_channels, 192, kernel_size=1)\n        self.branch3x3_2 = conv_block(192, 320, kernel_size=3, stride=2)\n\n        self.branch7x7x3_1 = conv_block(in_channels, 192, kernel_size=1)\n        self.branch7x7x3_2 = conv_block(192, 192, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7x3_3 = conv_block(192, 192, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7x3_4 = conv_block(192, 192, kernel_size=3, stride=2)\n\n    def _forward(self, x):\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = self.branch3x3_2(branch3x3)\n\n        branch7x7x3 = self.branch7x7x3_1(x)\n        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n        outputs = [branch3x3, branch7x7x3, branch_pool]\n        return outputs\n\n    def forward(self, x):\n        outputs = self._forward(x)\n        return torch.cat(outputs, 1)\n\n\nclass InceptionE(nn.Module):\n\n    def __init__(self, in_channels, conv_block=None):\n        super(InceptionE, self).__init__()\n        conv_block = conv_block or ConvNormAct\n        self.branch1x1 = conv_block(in_channels, 320, kernel_size=1)\n\n        self.branch3x3_1 = conv_block(in_channels, 384, kernel_size=1)\n        self.branch3x3_2a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3_2b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch3x3dbl_1 = conv_block(in_channels, 448, kernel_size=1)\n        self.branch3x3dbl_2 = conv_block(448, 384, kernel_size=3, padding=1)\n        self.branch3x3dbl_3a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3dbl_3b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n\n    def _forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3),\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = [\n            self.branch3x3dbl_3a(branch3x3dbl),\n            self.branch3x3dbl_3b(branch3x3dbl),\n        ]\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n        return outputs\n\n    def forward(self, x):\n        outputs = self._forward(x)\n        return torch.cat(outputs, 1)\n\n\nclass InceptionAux(nn.Module):\n\n    def __init__(self, in_channels, num_classes, conv_block=None):\n        super(InceptionAux, self).__init__()\n        conv_block = conv_block or ConvNormAct\n        self.conv0 = conv_block(in_channels, 128, kernel_size=1)\n        self.conv1 = conv_block(128, 768, kernel_size=5)\n        self.conv1.stddev = 0.01\n        self.fc = Linear(768, num_classes)\n        self.fc.stddev = 0.001\n\n    def forward(self, x):\n        # N x 768 x 17 x 17\n        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n        # N x 768 x 5 x 5\n        x = self.conv0(x)\n        # N x 128 x 5 x 5\n        x = self.conv1(x)\n        # N x 768 x 1 x 1\n        # Adaptive average pooling\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        # N x 768 x 1 x 1\n        x = torch.flatten(x, 1)\n        # N x 768\n        x = self.fc(x)\n        # N x 1000\n        return x\n\n\nclass InceptionV3(nn.Module):\n    \"\"\"Inception-V3\n    \"\"\"\n    aux_logits: torch.jit.Final[bool]\n\n    def __init__(\n            self,\n            num_classes=1000,\n            in_chans=3,\n            drop_rate=0.,\n            global_pool='avg',\n            aux_logits=False,\n            norm_layer='batchnorm2d',\n            norm_eps=1e-3,\n            act_layer='relu',\n    ):\n        super(InceptionV3, self).__init__()\n        self.num_classes = num_classes\n        self.aux_logits = aux_logits\n        conv_block = partial(\n            ConvNormAct,\n            padding=0,\n            norm_layer=norm_layer,\n            act_layer=act_layer,\n            norm_kwargs=dict(eps=norm_eps),\n            act_kwargs=dict(inplace=True),\n        )\n\n        self.Conv2d_1a_3x3 = conv_block(in_chans, 32, kernel_size=3, stride=2)\n        self.Conv2d_2a_3x3 = conv_block(32, 32, kernel_size=3)\n        self.Conv2d_2b_3x3 = conv_block(32, 64, kernel_size=3, padding=1)\n        self.Pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n        self.Conv2d_3b_1x1 = conv_block(64, 80, kernel_size=1)\n        self.Conv2d_4a_3x3 = conv_block(80, 192, kernel_size=3)\n        self.Pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n        self.Mixed_5b = InceptionA(192, pool_features=32, conv_block=conv_block)\n        self.Mixed_5c = InceptionA(256, pool_features=64, conv_block=conv_block)\n        self.Mixed_5d = InceptionA(288, pool_features=64, conv_block=conv_block)\n        self.Mixed_6a = InceptionB(288, conv_block=conv_block)\n        self.Mixed_6b = InceptionC(768, channels_7x7=128, conv_block=conv_block)\n        self.Mixed_6c = InceptionC(768, channels_7x7=160, conv_block=conv_block)\n        self.Mixed_6d = InceptionC(768, channels_7x7=160, conv_block=conv_block)\n        self.Mixed_6e = InceptionC(768, channels_7x7=192, conv_block=conv_block)\n        if aux_logits:\n            self.AuxLogits = InceptionAux(768, num_classes, conv_block=conv_block)\n        else:\n            self.AuxLogits = None\n        self.Mixed_7a = InceptionD(768, conv_block=conv_block)\n        self.Mixed_7b = InceptionE(1280, conv_block=conv_block)\n        self.Mixed_7c = InceptionE(2048, conv_block=conv_block)\n        self.feature_info = [\n            dict(num_chs=64, reduction=2, module='Conv2d_2b_3x3'),\n            dict(num_chs=192, reduction=4, module='Conv2d_4a_3x3'),\n            dict(num_chs=288, reduction=8, module='Mixed_5d'),\n            dict(num_chs=768, reduction=16, module='Mixed_6e'),\n            dict(num_chs=2048, reduction=32, module='Mixed_7c'),\n        ]\n\n        self.num_features = 2048\n        self.global_pool, self.head_drop, self.fc = create_classifier(\n            self.num_features,\n            self.num_classes,\n            pool_type=global_pool,\n            drop_rate=drop_rate,\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n                trunc_normal_(m.weight, std=stddev)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        module_map = {k: i for i, (k, _) in enumerate(flatten_modules(self.named_children(), prefix=()))}\n        module_map.pop(('fc',))\n\n        def _matcher(name):\n            if any([name.startswith(n) for n in ('Conv2d_1', 'Conv2d_2')]):\n                return 0\n            elif any([name.startswith(n) for n in ('Conv2d_3', 'Conv2d_4')]):\n                return 1\n            else:\n                for k in module_map.keys():\n                    if k == tuple(name.split('.')[:len(k)]):\n                        return module_map[k]\n                return float('inf')\n        return _matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, 'gradient checkpointing not supported'\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.fc\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.global_pool, self.fc = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)\n\n    def forward_preaux(self, x):\n        x = self.Conv2d_1a_3x3(x)  # N x 32 x 149 x 149\n        x = self.Conv2d_2a_3x3(x)  # N x 32 x 147 x 147\n        x = self.Conv2d_2b_3x3(x)  # N x 64 x 147 x 147\n        x = self.Pool1(x)  # N x 64 x 73 x 73\n        x = self.Conv2d_3b_1x1(x)  # N x 80 x 73 x 73\n        x = self.Conv2d_4a_3x3(x)  # N x 192 x 71 x 71\n        x = self.Pool2(x)  # N x 192 x 35 x 35\n        x = self.Mixed_5b(x)  # N x 256 x 35 x 35\n        x = self.Mixed_5c(x)  # N x 288 x 35 x 35\n        x = self.Mixed_5d(x)  # N x 288 x 35 x 35\n        x = self.Mixed_6a(x)  # N x 768 x 17 x 17\n        x = self.Mixed_6b(x)  # N x 768 x 17 x 17\n        x = self.Mixed_6c(x)  # N x 768 x 17 x 17\n        x = self.Mixed_6d(x)  # N x 768 x 17 x 17\n        x = self.Mixed_6e(x)  # N x 768 x 17 x 17\n        return x\n\n    def forward_postaux(self, x):\n        x = self.Mixed_7a(x)  # N x 1280 x 8 x 8\n        x = self.Mixed_7b(x)  # N x 2048 x 8 x 8\n        x = self.Mixed_7c(x)  # N x 2048 x 8 x 8\n        return x\n\n    def forward_features(self, x):\n        x = self.forward_preaux(x)\n        if self.aux_logits:\n            aux = self.AuxLogits(x)\n            x = self.forward_postaux(x)\n            return x, aux\n        x = self.forward_postaux(x)\n        return x\n\n    def forward_head(self, x):\n        x = self.global_pool(x)\n        x = self.head_drop(x)\n        x = self.fc(x)\n        return x\n\n    def forward(self, x):\n        if self.aux_logits:\n            x, aux = self.forward_features(x)\n            x = self.forward_head(x)\n            return x, aux\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_inception_v3(variant, pretrained=False, **kwargs):\n    pretrained_cfg = resolve_pretrained_cfg(variant, pretrained_cfg=kwargs.pop('pretrained_cfg', None))\n    aux_logits = kwargs.get('aux_logits', False)\n    has_aux_logits = False\n    if pretrained_cfg:\n        # only torchvision pretrained weights have aux logits\n        has_aux_logits = pretrained_cfg.tag == 'tv_in1k'\n    if aux_logits:\n        assert not kwargs.pop('features_only', False)\n        load_strict = has_aux_logits\n    else:\n        load_strict = not has_aux_logits\n\n    return build_model_with_cfg(\n        InceptionV3,\n        variant,\n        pretrained,\n        pretrained_cfg=pretrained_cfg,\n        pretrained_strict=load_strict,\n        **kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (8, 8),\n        'crop_pct': 0.875, 'interpolation': 'bicubic',\n        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,\n        'first_conv': 'Conv2d_1a_3x3.conv', 'classifier': 'fc',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    # original PyTorch weights, ported from Tensorflow but modified\n    'inception_v3.tv_in1k': _cfg(\n        # NOTE checkpoint has aux logit layer weights\n        hf_hub_id='timm/',\n        url='https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'),\n    # my port of Tensorflow SLIM weights (http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz)\n    'inception_v3.tf_in1k': _cfg(hf_hub_id='timm/'),\n    # my port of Tensorflow adversarially trained Inception V3 from\n    # http://download.tensorflow.org/models/adv_inception_v3_2017_08_18.tar.gz\n    'inception_v3.tf_adv_in1k': _cfg(hf_hub_id='timm/'),\n    # from gluon pretrained models, best performing in terms of accuracy/loss metrics\n    # https://gluon-cv.mxnet.io/model_zoo/classification.html\n    'inception_v3.gluon_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=IMAGENET_DEFAULT_MEAN,  # also works well with inception defaults\n        std=IMAGENET_DEFAULT_STD,  # also works well with inception defaults\n    )\n})\n\n\n@register_model\ndef inception_v3(pretrained=False, **kwargs) -> InceptionV3:\n    model = _create_inception_v3('inception_v3', pretrained=pretrained, **kwargs)\n    return model\n\n\nregister_model_deprecations(__name__, {\n    'tf_inception_v3': 'inception_v3.tf_in1k',\n    'adv_inception_v3': 'inception_v3.tf_adv_in1k',\n    'gluon_inception_v3': 'inception_v3.gluon_in1k',\n})",
  "\"\"\" Bring-Your-Own-Attention Network\n\nA flexible network w/ dataclass based config for stacking NN blocks including\nself-attention (or similar) layers.\n\nCurrently used to implement experimental variants of:\n  * Bottleneck Transformers\n  * Lambda ResNets\n  * HaloNets\n\nConsider all of the models definitions here as experimental WIP and likely to change.\n\nHacked together by / copyright Ross Wightman, 2021.\n\"\"\"\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\nfrom .byobnet import ByoBlockCfg, ByoModelCfg, ByobNet, interleave_blocks\n\n__all__ = []\n\n\nmodel_cfgs = dict(\n\n    botnet26t=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=0, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), d=2, c=1024, s=2, gs=0, br=0.25),\n            ByoBlockCfg(type='self_attn', d=2, c=2048, s=2, gs=0, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='maxpool',\n        fixed_input_size=True,\n        self_attn_layer='bottleneck',\n        self_attn_kwargs=dict()\n    ),\n    sebotnet33ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), every=[2], d=3, c=512, s=2, gs=0, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), every=[2], d=3, c=1024, s=2, gs=0, br=0.25),\n            ByoBlockCfg('self_attn', d=2, c=1536, s=2, gs=0, br=0.333),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='',\n        act_layer='silu',\n        num_features=1280,\n        attn_layer='se',\n        self_attn_layer='bottleneck',\n        self_attn_kwargs=dict()\n    ),\n    botnet50ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=3, c=256, s=1, gs=0, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), every=4, d=4, c=512, s=2, gs=0, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), d=6, c=1024, s=2, gs=0, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), d=3, c=2048, s=2, gs=0, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='maxpool',\n        act_layer='silu',\n        fixed_input_size=True,\n        self_attn_layer='bottleneck',\n        self_attn_kwargs=dict()\n    ),\n    eca_botnext26ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=16, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=16, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), d=2, c=1024, s=2, gs=16, br=0.25),\n            ByoBlockCfg(type='self_attn', d=2, c=2048, s=2, gs=16, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='maxpool',\n        fixed_input_size=True,\n        act_layer='silu',\n        attn_layer='eca',\n        self_attn_layer='bottleneck',\n        self_attn_kwargs=dict(dim_head=16)\n    ),\n\n    halonet_h1=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='self_attn', d=3, c=64, s=1, gs=0, br=1.0),\n            ByoBlockCfg(type='self_attn', d=3, c=128, s=2, gs=0, br=1.0),\n            ByoBlockCfg(type='self_attn', d=10, c=256, s=2, gs=0, br=1.0),\n            ByoBlockCfg(type='self_attn', d=3, c=512, s=2, gs=0, br=1.0),\n        ),\n        stem_chs=64,\n        stem_type='7x7',\n        stem_pool='maxpool',\n\n        self_attn_layer='halo',\n        self_attn_kwargs=dict(block_size=8, halo_size=3),\n    ),\n    halonet26t=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=0, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), d=2, c=1024, s=2, gs=0, br=0.25),\n            ByoBlockCfg(type='self_attn', d=2, c=2048, s=2, gs=0, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='maxpool',\n        self_attn_layer='halo',\n        self_attn_kwargs=dict(block_size=8, halo_size=2)\n    ),\n    sehalonet33ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), every=[2], d=3, c=512, s=2, gs=0, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), every=[2], d=3, c=1024, s=2, gs=0, br=0.25),\n            ByoBlockCfg('self_attn', d=2, c=1536, s=2, gs=0, br=0.333),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='',\n        act_layer='silu',\n        num_features=1280,\n        attn_layer='se',\n        self_attn_layer='halo',\n        self_attn_kwargs=dict(block_size=8, halo_size=3)\n    ),\n    halonet50ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=3, c=256, s=1, gs=0, br=0.25),\n            interleave_blocks(\n                types=('bottle', 'self_attn'), every=4, d=4, c=512, s=2, gs=0, br=0.25,\n                self_attn_layer='halo', self_attn_kwargs=dict(block_size=8, halo_size=3, num_heads=4)),\n            interleave_blocks(types=('bottle', 'self_attn'), d=6, c=1024, s=2, gs=0, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), d=3, c=2048, s=2, gs=0, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='maxpool',\n        act_layer='silu',\n        self_attn_layer='halo',\n        self_attn_kwargs=dict(block_size=8, halo_size=3)\n    ),\n    eca_halonext26ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=16, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=16, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), d=2, c=1024, s=2, gs=16, br=0.25),\n            ByoBlockCfg(type='self_attn', d=2, c=2048, s=2, gs=16, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='maxpool',\n        act_layer='silu',\n        attn_layer='eca',\n        self_attn_layer='halo',\n        self_attn_kwargs=dict(block_size=8, halo_size=2, dim_head=16)\n    ),\n\n    lambda_resnet26t=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=0, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), d=2, c=1024, s=2, gs=0, br=0.25),\n            ByoBlockCfg(type='self_attn', d=2, c=2048, s=2, gs=0, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='maxpool',\n        self_attn_layer='lambda',\n        self_attn_kwargs=dict(r=9)\n    ),\n    lambda_resnet50ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=3, c=256, s=1, gs=0, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), every=4, d=4, c=512, s=2, gs=0, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), d=6, c=1024, s=2, gs=0, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), d=3, c=2048, s=2, gs=0, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='maxpool',\n        act_layer='silu',\n        self_attn_layer='lambda',\n        self_attn_kwargs=dict(r=9)\n    ),\n    lambda_resnet26rpt_256=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=256, s=1, gs=0, br=0.25),\n            ByoBlockCfg(type='bottle', d=2, c=512, s=2, gs=0, br=0.25),\n            interleave_blocks(types=('bottle', 'self_attn'), d=2, c=1024, s=2, gs=0, br=0.25),\n            ByoBlockCfg(type='self_attn', d=2, c=2048, s=2, gs=0, br=0.25),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='maxpool',\n        self_attn_layer='lambda',\n        self_attn_kwargs=dict(r=None)\n    ),\n\n    # experimental\n    haloregnetz_b=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=2, c=48, s=2, gs=16, br=3),\n            ByoBlockCfg(type='bottle', d=6, c=96, s=2, gs=16, br=3),\n            interleave_blocks(types=('bottle', 'self_attn'), every=3, d=12, c=192, s=2, gs=16, br=3),\n            ByoBlockCfg('self_attn', d=2, c=288, s=2, gs=16, br=3),\n        ),\n        stem_chs=32,\n        stem_pool='',\n        downsample='',\n        num_features=1536,\n        act_layer='silu',\n        attn_layer='se',\n        attn_kwargs=dict(rd_ratio=0.25),\n        block_kwargs=dict(bottle_in=True, linear_out=True),\n        self_attn_layer='halo',\n        self_attn_kwargs=dict(block_size=7, halo_size=2, qk_ratio=0.33)\n    ),\n\n    # experimental\n    lamhalobotnet50ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=3, c=256, s=1, gs=0, br=0.25),\n            interleave_blocks(\n                types=('bottle', 'self_attn'), d=4, c=512, s=2, gs=0, br=0.25,\n                self_attn_layer='lambda', self_attn_kwargs=dict(r=13)),\n            interleave_blocks(\n                types=('bottle', 'self_attn'), d=6, c=1024, s=2, gs=0, br=0.25,\n                self_attn_layer='halo', self_attn_kwargs=dict(halo_size=3)),\n            interleave_blocks(\n                types=('bottle', 'self_attn'), d=3, c=2048, s=2, gs=0, br=0.25,\n                self_attn_layer='bottleneck', self_attn_kwargs=dict()),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='',\n        act_layer='silu',\n    ),\n    halo2botnet50ts=ByoModelCfg(\n        blocks=(\n            ByoBlockCfg(type='bottle', d=3, c=256, s=1, gs=0, br=0.25),\n            interleave_blocks(\n                types=('bottle', 'self_attn'), d=4, c=512, s=2, gs=0, br=0.25,\n                self_attn_layer='halo', self_attn_kwargs=dict(halo_size=3)),\n            interleave_blocks(\n                types=('bottle', 'self_attn'), d=6, c=1024, s=2, gs=0, br=0.25,\n                self_attn_layer='halo', self_attn_kwargs=dict(halo_size=3)),\n            interleave_blocks(\n                types=('bottle', 'self_attn'), d=3, c=2048, s=2, gs=0, br=0.25,\n                self_attn_layer='bottleneck', self_attn_kwargs=dict()),\n        ),\n        stem_chs=64,\n        stem_type='tiered',\n        stem_pool='',\n        act_layer='silu',\n    ),\n)\n\n\ndef _create_byoanet(variant, cfg_variant=None, pretrained=False, **kwargs):\n    return build_model_with_cfg(\n        ByobNet, variant, pretrained,\n        model_cfg=model_cfgs[variant] if not cfg_variant else model_cfgs[cfg_variant],\n        feature_cfg=dict(flatten_sequential=True),\n        **kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.95, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.conv1.conv', 'classifier': 'head.fc',\n        'fixed_input_size': False, 'min_input_size': (3, 224, 224),\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    # GPU-Efficient (ResNet) weights\n    'botnet26t_256.c1_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/botnet26t_c1_256-167a0e9f.pth',\n        hf_hub_id='timm/',\n        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8)),\n    'sebotnet33ts_256.a1h_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/sebotnet33ts_a1h2_256-957e3c3e.pth',\n        hf_hub_id='timm/',\n        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.94),\n    'botnet50ts_256.untrained': _cfg(\n        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8)),\n    'eca_botnext26ts_256.c1_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/eca_botnext26ts_c_256-95a898f6.pth',\n        hf_hub_id='timm/',\n        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8)),\n\n    'halonet_h1.untrained': _cfg(input_size=(3, 256, 256), pool_size=(8, 8), min_input_size=(3, 256, 256)),\n    'halonet26t.a1h_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/halonet26t_a1h_256-3083328c.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 256, 256), pool_size=(8, 8), min_input_size=(3, 256, 256)),\n    'sehalonet33ts.ra2_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/sehalonet33ts_256-87e053f9.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 256, 256), pool_size=(8, 8), min_input_size=(3, 256, 256), crop_pct=0.94),\n    'halonet50ts.a1h_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/halonet50ts_a1h2_256-f3a3daee.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 256, 256), pool_size=(8, 8), min_input_size=(3, 256, 256), crop_pct=0.94),\n    'eca_halonext26ts.c1_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/eca_halonext26ts_c_256-06906299.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 256, 256), pool_size=(8, 8), min_input_size=(3, 256, 256), crop_pct=0.94),\n\n    'lambda_resnet26t.c1_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/lambda_resnet26t_c_256-e5a5c857.pth',\n        hf_hub_id='timm/',\n        min_input_size=(3, 128, 128), input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.94),\n    'lambda_resnet50ts.a1h_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/lambda_resnet50ts_a1h_256-b87370f7.pth',\n        hf_hub_id='timm/',\n        min_input_size=(3, 128, 128), input_size=(3, 256, 256), pool_size=(8, 8)),\n    'lambda_resnet26rpt_256.c1_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/lambda_resnet26rpt_c_256-ab00292d.pth',\n        hf_hub_id='timm/',\n        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8), crop_pct=0.94),\n\n    'haloregnetz_b.ra3_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/haloregnetz_c_raa_256-c8ad7616.pth',\n        hf_hub_id='timm/',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        first_conv='stem.conv', input_size=(3, 224, 224), pool_size=(7, 7), min_input_size=(3, 224, 224), crop_pct=0.94),\n\n    'lamhalobotnet50ts_256.a1h_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/lamhalobotnet50ts_a1h2_256-fe3d9445.pth',\n        hf_hub_id='timm/',\n        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8)),\n    'halo2botnet50ts_256.a1h_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-attn-weights/halo2botnet50ts_a1h2_256-fd9c11a3.pth',\n        hf_hub_id='timm/',\n        fixed_input_size=True, input_size=(3, 256, 256), pool_size=(8, 8)),\n})\n\n\n@register_model\ndef botnet26t_256(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" Bottleneck Transformer w/ ResNet26-T backbone.\n    \"\"\"\n    kwargs.setdefault('img_size', 256)\n    return _create_byoanet('botnet26t_256', 'botnet26t', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef sebotnet33ts_256(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" Bottleneck Transformer w/ a ResNet33-t backbone, SE attn for non Halo blocks, SiLU,\n    \"\"\"\n    return _create_byoanet('sebotnet33ts_256', 'sebotnet33ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef botnet50ts_256(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" Bottleneck Transformer w/ ResNet50-T backbone, silu act.\n    \"\"\"\n    kwargs.setdefault('img_size', 256)\n    return _create_byoanet('botnet50ts_256', 'botnet50ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef eca_botnext26ts_256(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" Bottleneck Transformer w/ ResNet26-T backbone, silu act.\n    \"\"\"\n    kwargs.setdefault('img_size', 256)\n    return _create_byoanet('eca_botnext26ts_256', 'eca_botnext26ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef halonet_h1(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" HaloNet-H1. Halo attention in all stages as per the paper.\n    NOTE: This runs very slowly!\n    \"\"\"\n    return _create_byoanet('halonet_h1', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef halonet26t(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" HaloNet w/ a ResNet26-t backbone. Halo attention in final two stages\n    \"\"\"\n    return _create_byoanet('halonet26t', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef sehalonet33ts(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" HaloNet w/ a ResNet33-t backbone, SE attn for non Halo blocks, SiLU, 1-2 Halo in stage 2,3,4.\n    \"\"\"\n    return _create_byoanet('sehalonet33ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef halonet50ts(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" HaloNet w/ a ResNet50-t backbone, silu act. Halo attention in final two stages\n    \"\"\"\n    return _create_byoanet('halonet50ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef eca_halonext26ts(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" HaloNet w/ a ResNet26-t backbone, silu act. Halo attention in final two stages\n    \"\"\"\n    return _create_byoanet('eca_halonext26ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef lambda_resnet26t(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" Lambda-ResNet-26-T. Lambda layers w/ conv pos in last two stages.\n    \"\"\"\n    return _create_byoanet('lambda_resnet26t', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef lambda_resnet50ts(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" Lambda-ResNet-50-TS. SiLU act. Lambda layers w/ conv pos in last two stages.\n    \"\"\"\n    return _create_byoanet('lambda_resnet50ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef lambda_resnet26rpt_256(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" Lambda-ResNet-26-R-T. Lambda layers w/ rel pos embed in last two stages.\n    \"\"\"\n    kwargs.setdefault('img_size', 256)\n    return _create_byoanet('lambda_resnet26rpt_256', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef haloregnetz_b(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" Halo + RegNetZ\n    \"\"\"\n    return _create_byoanet('haloregnetz_b', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef lamhalobotnet50ts_256(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" Combo Attention (Lambda + Halo + Bot) Network\n    \"\"\"\n    return _create_byoanet('lamhalobotnet50ts_256', 'lamhalobotnet50ts', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef halo2botnet50ts_256(pretrained=False, **kwargs) -> ByobNet:\n    \"\"\" Combo Attention (Halo + Halo + Bot) Network\n    \"\"\"\n    return _create_byoanet('halo2botnet50ts_256', 'halo2botnet50ts', pretrained=pretrained, **kwargs)\n",
  "\"\"\" LeViT\n\nPaper: `LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference`\n    - https://arxiv.org/abs/2104.01136\n\n@article{graham2021levit,\n  title={LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference},\n  author={Benjamin Graham and Alaaeldin El-Nouby and Hugo Touvron and Pierre Stock and Armand Joulin and Herv\\'e J\\'egou and Matthijs Douze},\n  journal={arXiv preprint arXiv:22104.01136},\n  year={2021}\n}\n\nAdapted from official impl at https://github.com/facebookresearch/LeViT, original copyright bellow.\n\nThis version combines both conv/linear models and fixes torchscript compatibility.\n\nModifications and additions for timm hacked together by / Copyright 2021, Ross Wightman\n\"\"\"\n\n# Copyright (c) 2015-present, Facebook, Inc.\n# All rights reserved.\n\n# Modified from\n# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n# Copyright 2020 Ross Wightman, Apache-2.0 License\nfrom collections import OrderedDict\nfrom functools import partial\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_STD, IMAGENET_DEFAULT_MEAN\nfrom timm.layers import to_ntuple, to_2tuple, get_act_layer, DropPath, trunc_normal_\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import generate_default_cfgs, register_model\n\n__all__ = ['Levit']\n\n\nclass ConvNorm(nn.Module):\n    def __init__(\n            self, in_chs, out_chs, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bn_weight_init=1):\n        super().__init__()\n        self.linear = nn.Conv2d(in_chs, out_chs, kernel_size, stride, padding, dilation, groups, bias=False)\n        self.bn = nn.BatchNorm2d(out_chs)\n\n        nn.init.constant_(self.bn.weight, bn_weight_init)\n\n    @torch.no_grad()\n    def fuse(self):\n        c, bn = self.linear, self.bn\n        w = bn.weight / (bn.running_var + bn.eps) ** 0.5\n        w = c.weight * w[:, None, None, None]\n        b = bn.bias - bn.running_mean * bn.weight / (bn.running_var + bn.eps) ** 0.5\n        m = nn.Conv2d(\n            w.size(1), w.size(0), w.shape[2:], stride=self.linear.stride,\n            padding=self.linear.padding, dilation=self.linear.dilation, groups=self.linear.groups)\n        m.weight.data.copy_(w)\n        m.bias.data.copy_(b)\n        return m\n\n    def forward(self, x):\n        return self.bn(self.linear(x))\n\n\nclass LinearNorm(nn.Module):\n    def __init__(self, in_features, out_features, bn_weight_init=1):\n        super().__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=False)\n        self.bn = nn.BatchNorm1d(out_features)\n\n        nn.init.constant_(self.bn.weight, bn_weight_init)\n\n    @torch.no_grad()\n    def fuse(self):\n        l, bn = self.linear, self.bn\n        w = bn.weight / (bn.running_var + bn.eps) ** 0.5\n        w = l.weight * w[:, None]\n        b = bn.bias - bn.running_mean * bn.weight / (bn.running_var + bn.eps) ** 0.5\n        m = nn.Linear(w.size(1), w.size(0))\n        m.weight.data.copy_(w)\n        m.bias.data.copy_(b)\n        return m\n\n    def forward(self, x):\n        x = self.linear(x)\n        return self.bn(x.flatten(0, 1)).reshape_as(x)\n\n\nclass NormLinear(nn.Module):\n    def __init__(self, in_features, out_features, bias=True, std=0.02, drop=0.):\n        super().__init__()\n        self.bn = nn.BatchNorm1d(in_features)\n        self.drop = nn.Dropout(drop)\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n\n        trunc_normal_(self.linear.weight, std=std)\n        if self.linear.bias is not None:\n            nn.init.constant_(self.linear.bias, 0)\n\n    @torch.no_grad()\n    def fuse(self):\n        bn, l = self.bn, self.linear\n        w = bn.weight / (bn.running_var + bn.eps) ** 0.5\n        b = bn.bias - self.bn.running_mean * self.bn.weight / (bn.running_var + bn.eps) ** 0.5\n        w = l.weight * w[None, :]\n        if l.bias is None:\n            b = b @ self.linear.weight.T\n        else:\n            b = (l.weight @ b[:, None]).view(-1) + self.linear.bias\n        m = nn.Linear(w.size(1), w.size(0))\n        m.weight.data.copy_(w)\n        m.bias.data.copy_(b)\n        return m\n\n    def forward(self, x):\n        return self.linear(self.drop(self.bn(x)))\n\n\nclass Stem8(nn.Sequential):\n    def __init__(self, in_chs, out_chs, act_layer):\n        super().__init__()\n        self.stride = 8\n\n        self.add_module('conv1', ConvNorm(in_chs, out_chs // 4, 3, stride=2, padding=1))\n        self.add_module('act1', act_layer())\n        self.add_module('conv2', ConvNorm(out_chs // 4, out_chs // 2, 3, stride=2, padding=1))\n        self.add_module('act2', act_layer())\n        self.add_module('conv3', ConvNorm(out_chs // 2, out_chs, 3, stride=2, padding=1))\n\n\nclass Stem16(nn.Sequential):\n    def __init__(self, in_chs, out_chs, act_layer):\n        super().__init__()\n        self.stride = 16\n\n        self.add_module('conv1', ConvNorm(in_chs, out_chs // 8, 3, stride=2, padding=1))\n        self.add_module('act1', act_layer())\n        self.add_module('conv2', ConvNorm(out_chs // 8, out_chs // 4, 3, stride=2, padding=1))\n        self.add_module('act2', act_layer())\n        self.add_module('conv3', ConvNorm(out_chs // 4, out_chs // 2, 3, stride=2, padding=1))\n        self.add_module('act3', act_layer())\n        self.add_module('conv4', ConvNorm(out_chs // 2, out_chs, 3, stride=2, padding=1))\n\n\nclass Downsample(nn.Module):\n    def __init__(self, stride, resolution, use_pool=False):\n        super().__init__()\n        self.stride = stride\n        self.resolution = to_2tuple(resolution)\n        self.pool = nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False) if use_pool else None\n\n    def forward(self, x):\n        B, N, C = x.shape\n        x = x.view(B, self.resolution[0], self.resolution[1], C)\n        if self.pool is not None:\n            x = self.pool(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n        else:\n            x = x[:, ::self.stride, ::self.stride]\n        return x.reshape(B, -1, C)\n\n\nclass Attention(nn.Module):\n    attention_bias_cache: Dict[str, torch.Tensor]\n\n    def __init__(\n            self,\n            dim,\n            key_dim,\n            num_heads=8,\n            attn_ratio=4.,\n            resolution=14,\n            use_conv=False,\n            act_layer=nn.SiLU,\n    ):\n        super().__init__()\n        ln_layer = ConvNorm if use_conv else LinearNorm\n        resolution = to_2tuple(resolution)\n\n        self.use_conv = use_conv\n        self.num_heads = num_heads\n        self.scale = key_dim ** -0.5\n        self.key_dim = key_dim\n        self.key_attn_dim = key_dim * num_heads\n        self.val_dim = int(attn_ratio * key_dim)\n        self.val_attn_dim = int(attn_ratio * key_dim) * num_heads\n\n        self.qkv = ln_layer(dim, self.val_attn_dim + self.key_attn_dim * 2)\n        self.proj = nn.Sequential(OrderedDict([\n            ('act', act_layer()),\n            ('ln', ln_layer(self.val_attn_dim, dim, bn_weight_init=0))\n        ]))\n\n        self.attention_biases = nn.Parameter(torch.zeros(num_heads, resolution[0] * resolution[1]))\n        pos = torch.stack(torch.meshgrid(torch.arange(resolution[0]), torch.arange(resolution[1]))).flatten(1)\n        rel_pos = (pos[..., :, None] - pos[..., None, :]).abs()\n        rel_pos = (rel_pos[0] * resolution[1]) + rel_pos[1]\n        self.register_buffer('attention_bias_idxs', rel_pos, persistent=False)\n        self.attention_bias_cache = {}\n\n    @torch.no_grad()\n    def train(self, mode=True):\n        super().train(mode)\n        if mode and self.attention_bias_cache:\n            self.attention_bias_cache = {}  # clear ab cache\n\n    def get_attention_biases(self, device: torch.device) -> torch.Tensor:\n        if torch.jit.is_tracing() or self.training:\n            return self.attention_biases[:, self.attention_bias_idxs]\n        else:\n            device_key = str(device)\n            if device_key not in self.attention_bias_cache:\n                self.attention_bias_cache[device_key] = self.attention_biases[:, self.attention_bias_idxs]\n            return self.attention_bias_cache[device_key]\n\n    def forward(self, x):  # x (B,C,H,W)\n        if self.use_conv:\n            B, C, H, W = x.shape\n            q, k, v = self.qkv(x).view(\n                B, self.num_heads, -1, H * W).split([self.key_dim, self.key_dim, self.val_dim], dim=2)\n\n            attn = (q.transpose(-2, -1) @ k) * self.scale + self.get_attention_biases(x.device)\n            attn = attn.softmax(dim=-1)\n\n            x = (v @ attn.transpose(-2, -1)).view(B, -1, H, W)\n        else:\n            B, N, C = x.shape\n            q, k, v = self.qkv(x).view(\n                B, N, self.num_heads, -1).split([self.key_dim, self.key_dim, self.val_dim], dim=3)\n            q = q.permute(0, 2, 1, 3)\n            k = k.permute(0, 2, 3, 1)\n            v = v.permute(0, 2, 1, 3)\n\n            attn = q @ k * self.scale + self.get_attention_biases(x.device)\n            attn = attn.softmax(dim=-1)\n\n            x = (attn @ v).transpose(1, 2).reshape(B, N, self.val_attn_dim)\n        x = self.proj(x)\n        return x\n\n\nclass AttentionDownsample(nn.Module):\n    attention_bias_cache: Dict[str, torch.Tensor]\n\n    def __init__(\n            self,\n            in_dim,\n            out_dim,\n            key_dim,\n            num_heads=8,\n            attn_ratio=2.0,\n            stride=2,\n            resolution=14,\n            use_conv=False,\n            use_pool=False,\n            act_layer=nn.SiLU,\n    ):\n        super().__init__()\n        resolution = to_2tuple(resolution)\n\n        self.stride = stride\n        self.resolution = resolution\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n        self.key_attn_dim = key_dim * num_heads\n        self.val_dim = int(attn_ratio * key_dim)\n        self.val_attn_dim = self.val_dim * self.num_heads\n        self.scale = key_dim ** -0.5\n        self.use_conv = use_conv\n\n        if self.use_conv:\n            ln_layer = ConvNorm\n            sub_layer = partial(\n                nn.AvgPool2d,\n                kernel_size=3 if use_pool else 1, padding=1 if use_pool else 0, count_include_pad=False)\n        else:\n            ln_layer = LinearNorm\n            sub_layer = partial(Downsample, resolution=resolution, use_pool=use_pool)\n\n        self.kv = ln_layer(in_dim, self.val_attn_dim + self.key_attn_dim)\n        self.q = nn.Sequential(OrderedDict([\n            ('down', sub_layer(stride=stride)),\n            ('ln', ln_layer(in_dim, self.key_attn_dim))\n        ]))\n        self.proj = nn.Sequential(OrderedDict([\n            ('act', act_layer()),\n            ('ln', ln_layer(self.val_attn_dim, out_dim))\n        ]))\n\n        self.attention_biases = nn.Parameter(torch.zeros(num_heads, resolution[0] * resolution[1]))\n        k_pos = torch.stack(torch.meshgrid(torch.arange(resolution[0]), torch.arange(resolution[1]))).flatten(1)\n        q_pos = torch.stack(torch.meshgrid(\n            torch.arange(0, resolution[0], step=stride),\n            torch.arange(0, resolution[1], step=stride))).flatten(1)\n        rel_pos = (q_pos[..., :, None] - k_pos[..., None, :]).abs()\n        rel_pos = (rel_pos[0] * resolution[1]) + rel_pos[1]\n        self.register_buffer('attention_bias_idxs', rel_pos, persistent=False)\n\n        self.attention_bias_cache = {}  # per-device attention_biases cache\n\n    @torch.no_grad()\n    def train(self, mode=True):\n        super().train(mode)\n        if mode and self.attention_bias_cache:\n            self.attention_bias_cache = {}  # clear ab cache\n\n    def get_attention_biases(self, device: torch.device) -> torch.Tensor:\n        if torch.jit.is_tracing() or self.training:\n            return self.attention_biases[:, self.attention_bias_idxs]\n        else:\n            device_key = str(device)\n            if device_key not in self.attention_bias_cache:\n                self.attention_bias_cache[device_key] = self.attention_biases[:, self.attention_bias_idxs]\n            return self.attention_bias_cache[device_key]\n\n    def forward(self, x):\n        if self.use_conv:\n            B, C, H, W = x.shape\n            HH, WW = (H - 1) // self.stride + 1, (W - 1) // self.stride + 1\n            k, v = self.kv(x).view(B, self.num_heads, -1, H * W).split([self.key_dim, self.val_dim], dim=2)\n            q = self.q(x).view(B, self.num_heads, self.key_dim, -1)\n\n            attn = (q.transpose(-2, -1) @ k) * self.scale + self.get_attention_biases(x.device)\n            attn = attn.softmax(dim=-1)\n\n            x = (v @ attn.transpose(-2, -1)).reshape(B, self.val_attn_dim, HH, WW)\n        else:\n            B, N, C = x.shape\n            k, v = self.kv(x).view(B, N, self.num_heads, -1).split([self.key_dim, self.val_dim], dim=3)\n            k = k.permute(0, 2, 3, 1)  # BHCN\n            v = v.permute(0, 2, 1, 3)  # BHNC\n            q = self.q(x).view(B, -1, self.num_heads, self.key_dim).permute(0, 2, 1, 3)\n\n            attn = q @ k * self.scale + self.get_attention_biases(x.device)\n            attn = attn.softmax(dim=-1)\n\n            x = (attn @ v).transpose(1, 2).reshape(B, -1, self.val_attn_dim)\n        x = self.proj(x)\n        return x\n\n\nclass LevitMlp(nn.Module):\n    \"\"\" MLP for Levit w/ normalization + ability to switch btw conv and linear\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            use_conv=False,\n            act_layer=nn.SiLU,\n            drop=0.\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        ln_layer = ConvNorm if use_conv else LinearNorm\n\n        self.ln1 = ln_layer(in_features, hidden_features)\n        self.act = act_layer()\n        self.drop = nn.Dropout(drop)\n        self.ln2 = ln_layer(hidden_features, out_features, bn_weight_init=0)\n\n    def forward(self, x):\n        x = self.ln1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.ln2(x)\n        return x\n\n\nclass LevitDownsample(nn.Module):\n    def __init__(\n            self,\n            in_dim,\n            out_dim,\n            key_dim,\n            num_heads=8,\n            attn_ratio=4.,\n            mlp_ratio=2.,\n            act_layer=nn.SiLU,\n            attn_act_layer=None,\n            resolution=14,\n            use_conv=False,\n            use_pool=False,\n            drop_path=0.,\n    ):\n        super().__init__()\n        attn_act_layer = attn_act_layer or act_layer\n\n        self.attn_downsample = AttentionDownsample(\n            in_dim=in_dim,\n            out_dim=out_dim,\n            key_dim=key_dim,\n            num_heads=num_heads,\n            attn_ratio=attn_ratio,\n            act_layer=attn_act_layer,\n            resolution=resolution,\n            use_conv=use_conv,\n            use_pool=use_pool,\n        )\n\n        self.mlp = LevitMlp(\n            out_dim,\n            int(out_dim * mlp_ratio),\n            use_conv=use_conv,\n            act_layer=act_layer\n        )\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        x = self.attn_downsample(x)\n        x = x + self.drop_path(self.mlp(x))\n        return x\n\n\nclass LevitBlock(nn.Module):\n    def __init__(\n            self,\n            dim,\n            key_dim,\n            num_heads=8,\n            attn_ratio=4.,\n            mlp_ratio=2.,\n            resolution=14,\n            use_conv=False,\n            act_layer=nn.SiLU,\n            attn_act_layer=None,\n            drop_path=0.,\n    ):\n        super().__init__()\n        attn_act_layer = attn_act_layer or act_layer\n\n        self.attn = Attention(\n            dim=dim,\n            key_dim=key_dim,\n            num_heads=num_heads,\n            attn_ratio=attn_ratio,\n            resolution=resolution,\n            use_conv=use_conv,\n            act_layer=attn_act_layer,\n            )\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.mlp = LevitMlp(\n            dim,\n            int(dim * mlp_ratio),\n            use_conv=use_conv,\n            act_layer=act_layer\n        )\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        x = x + self.drop_path1(self.attn(x))\n        x = x + self.drop_path2(self.mlp(x))\n        return x\n\n\nclass LevitStage(nn.Module):\n    def __init__(\n            self,\n            in_dim,\n            out_dim,\n            key_dim,\n            depth=4,\n            num_heads=8,\n            attn_ratio=4.0,\n            mlp_ratio=4.0,\n            act_layer=nn.SiLU,\n            attn_act_layer=None,\n            resolution=14,\n            downsample='',\n            use_conv=False,\n            drop_path=0.,\n    ):\n        super().__init__()\n        resolution = to_2tuple(resolution)\n\n        if downsample:\n            self.downsample = LevitDownsample(\n                in_dim,\n                out_dim,\n                key_dim=key_dim,\n                num_heads=in_dim // key_dim,\n                attn_ratio=4.,\n                mlp_ratio=2.,\n                act_layer=act_layer,\n                attn_act_layer=attn_act_layer,\n                resolution=resolution,\n                use_conv=use_conv,\n                drop_path=drop_path,\n            )\n            resolution = [(r - 1) // 2 + 1 for r in resolution]\n        else:\n            assert in_dim == out_dim\n            self.downsample = nn.Identity()\n\n        blocks = []\n        for _ in range(depth):\n            blocks += [LevitBlock(\n                out_dim,\n                key_dim,\n                num_heads=num_heads,\n                attn_ratio=attn_ratio,\n                mlp_ratio=mlp_ratio,\n                act_layer=act_layer,\n                attn_act_layer=attn_act_layer,\n                resolution=resolution,\n                use_conv=use_conv,\n                drop_path=drop_path,\n            )]\n        self.blocks = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        x = self.downsample(x)\n        x = self.blocks(x)\n        return x\n\n\nclass Levit(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n\n    NOTE: distillation is defaulted to True since pretrained weights use it, will cause problems\n    w/ train scripts that don't take tuple outputs,\n    \"\"\"\n\n    def __init__(\n            self,\n            img_size=224,\n            in_chans=3,\n            num_classes=1000,\n            embed_dim=(192,),\n            key_dim=64,\n            depth=(12,),\n            num_heads=(3,),\n            attn_ratio=2.,\n            mlp_ratio=2.,\n            stem_backbone=None,\n            stem_stride=None,\n            stem_type='s16',\n            down_op='subsample',\n            act_layer='hard_swish',\n            attn_act_layer=None,\n            use_conv=False,\n            global_pool='avg',\n            drop_rate=0.,\n            drop_path_rate=0.):\n        super().__init__()\n        act_layer = get_act_layer(act_layer)\n        attn_act_layer = get_act_layer(attn_act_layer or act_layer)\n        self.use_conv = use_conv\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.num_features = embed_dim[-1]\n        self.embed_dim = embed_dim\n        self.drop_rate = drop_rate\n        self.grad_checkpointing = False\n        self.feature_info = []\n\n        num_stages = len(embed_dim)\n        assert len(depth) == num_stages\n        num_heads = to_ntuple(num_stages)(num_heads)\n        attn_ratio = to_ntuple(num_stages)(attn_ratio)\n        mlp_ratio = to_ntuple(num_stages)(mlp_ratio)\n\n        if stem_backbone is not None:\n            assert stem_stride >= 2\n            self.stem = stem_backbone\n            stride = stem_stride\n        else:\n            assert stem_type in ('s16', 's8')\n            if stem_type == 's16':\n                self.stem = Stem16(in_chans, embed_dim[0], act_layer=act_layer)\n            else:\n                self.stem = Stem8(in_chans, embed_dim[0], act_layer=act_layer)\n            stride = self.stem.stride\n        resolution = tuple([i // p for i, p in zip(to_2tuple(img_size), to_2tuple(stride))])\n\n        in_dim = embed_dim[0]\n        stages = []\n        for i in range(num_stages):\n            stage_stride = 2 if i > 0 else 1\n            stages += [LevitStage(\n                in_dim,\n                embed_dim[i],\n                key_dim,\n                depth=depth[i],\n                num_heads=num_heads[i],\n                attn_ratio=attn_ratio[i],\n                mlp_ratio=mlp_ratio[i],\n                act_layer=act_layer,\n                attn_act_layer=attn_act_layer,\n                resolution=resolution,\n                use_conv=use_conv,\n                downsample=down_op if stage_stride == 2 else '',\n                drop_path=drop_path_rate\n            )]\n            stride *= stage_stride\n            resolution = tuple([(r - 1) // stage_stride + 1 for r in resolution])\n            self.feature_info += [dict(num_chs=embed_dim[i], reduction=stride, module=f'stages.{i}')]\n            in_dim = embed_dim[i]\n        self.stages = nn.Sequential(*stages)\n\n        # Classifier head\n        self.head = NormLinear(embed_dim[-1], num_classes, drop=drop_rate) if num_classes > 0 else nn.Identity()\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {x for x in self.state_dict().keys() if 'attention_biases' in x}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^cls_token|pos_embed|patch_embed',  # stem and embed\n            blocks=[(r'^blocks\\.(\\d+)', None), (r'^norm', (99999,))]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=None, distillation=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = NormLinear(\n            self.embed_dim[-1], num_classes, drop=self.drop_rate) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        if not self.use_conv:\n            x = x.flatten(2).transpose(1, 2)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.stages, x)\n        else:\n            x = self.stages(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool == 'avg':\n            x = x.mean(dim=(-2, -1)) if self.use_conv else x.mean(dim=1)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\nclass LevitDistilled(Levit):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.head_dist = NormLinear(self.num_features, self.num_classes) if self.num_classes > 0 else nn.Identity()\n        self.distilled_training = False  # must set this True to train w/ distillation token\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head, self.head_dist\n\n    def reset_classifier(self, num_classes, global_pool=None, distillation=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = NormLinear(\n            self.num_features, num_classes, drop=self.drop_rate) if num_classes > 0 else nn.Identity()\n        self.head_dist = NormLinear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    @torch.jit.ignore\n    def set_distilled_training(self, enable=True):\n        self.distilled_training = enable\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool == 'avg':\n            x = x.mean(dim=(-2, -1)) if self.use_conv else x.mean(dim=1)\n        if pre_logits:\n            return x\n        x, x_dist = self.head(x), self.head_dist(x)\n        if self.distilled_training and self.training and not torch.jit.is_scripting():\n            # only return separate classification predictions when training in distilled mode\n            return x, x_dist\n        else:\n            # during standard train/finetune, inference average the classifier predictions\n            return (x + x_dist) / 2\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    if 'model' in state_dict:\n        state_dict = state_dict['model']\n\n    # filter out attn biases, should not have been persistent\n    state_dict = {k: v for k, v in state_dict.items() if 'attention_bias_idxs' not in k}\n\n    D = model.state_dict()\n    out_dict = {}\n    for ka, kb, va, vb in zip(D.keys(), state_dict.keys(), D.values(), state_dict.values()):\n        if va.ndim == 4 and vb.ndim == 2:\n            vb = vb[:, :, None, None]\n        if va.shape != vb.shape:\n            # head or first-conv shapes may change for fine-tune\n            assert 'head' in ka or 'stem.conv1.linear' in ka\n        out_dict[ka] = vb\n\n    return out_dict\n\n\nmodel_cfgs = dict(\n    levit_128s=dict(\n        embed_dim=(128, 256, 384), key_dim=16, num_heads=(4, 6, 8), depth=(2, 3, 4)),\n    levit_128=dict(\n        embed_dim=(128, 256, 384), key_dim=16, num_heads=(4, 8, 12), depth=(4, 4, 4)),\n    levit_192=dict(\n        embed_dim=(192, 288, 384), key_dim=32, num_heads=(3, 5, 6), depth=(4, 4, 4)),\n    levit_256=dict(\n        embed_dim=(256, 384, 512), key_dim=32, num_heads=(4, 6, 8), depth=(4, 4, 4)),\n    levit_384=dict(\n        embed_dim=(384, 512, 768), key_dim=32, num_heads=(6, 9, 12), depth=(4, 4, 4)),\n\n    # stride-8 stem experiments\n    levit_384_s8=dict(\n        embed_dim=(384, 512, 768), key_dim=32, num_heads=(6, 9, 12), depth=(4, 4, 4),\n        act_layer='silu', stem_type='s8'),\n    levit_512_s8=dict(\n        embed_dim=(512, 640, 896), key_dim=64, num_heads=(8, 10, 14), depth=(4, 4, 4),\n        act_layer='silu', stem_type='s8'),\n\n    # wider experiments\n    levit_512=dict(\n        embed_dim=(512, 768, 1024), key_dim=64, num_heads=(8, 12, 16), depth=(4, 4, 4), act_layer='silu'),\n\n    # deeper experiments\n    levit_256d=dict(\n        embed_dim=(256, 384, 512), key_dim=32, num_heads=(4, 6, 8), depth=(4, 8, 6), act_layer='silu'),\n    levit_512d=dict(\n        embed_dim=(512, 640, 768), key_dim=64, num_heads=(8, 10, 12), depth=(4, 8, 6), act_layer='silu'),\n)\n\n\ndef create_levit(variant, cfg_variant=None, pretrained=False, distilled=True, **kwargs):\n    is_conv = '_conv' in variant\n    out_indices = kwargs.pop('out_indices', (0, 1, 2))\n    if kwargs.get('features_only', None):\n        if not is_conv:\n            raise RuntimeError('features_only not implemented for LeVit in non-convolutional mode.')\n    if cfg_variant is None:\n        if variant in model_cfgs:\n            cfg_variant = variant\n        elif is_conv:\n            cfg_variant = variant.replace('_conv', '')\n\n    model_cfg = dict(model_cfgs[cfg_variant], **kwargs)\n    model = build_model_with_cfg(\n        LevitDistilled if distilled else Levit,\n        variant,\n        pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),\n        **model_cfg,\n    )\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.conv1.linear', 'classifier': ('head.linear', 'head_dist.linear'),\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    # weights in nn.Linear mode\n    'levit_128s.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n    'levit_128.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n    'levit_192.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n    'levit_256.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n    'levit_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n\n    # weights in nn.Conv2d mode\n    'levit_conv_128s.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        pool_size=(4, 4),\n    ),\n    'levit_conv_128.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        pool_size=(4, 4),\n    ),\n    'levit_conv_192.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        pool_size=(4, 4),\n    ),\n    'levit_conv_256.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        pool_size=(4, 4),\n    ),\n    'levit_conv_384.fb_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n        pool_size=(4, 4),\n    ),\n\n    'levit_384_s8.untrained': _cfg(classifier='head.linear'),\n    'levit_512_s8.untrained': _cfg(classifier='head.linear'),\n    'levit_512.untrained': _cfg(classifier='head.linear'),\n    'levit_256d.untrained': _cfg(classifier='head.linear'),\n    'levit_512d.untrained': _cfg(classifier='head.linear'),\n\n    'levit_conv_384_s8.untrained': _cfg(classifier='head.linear'),\n    'levit_conv_512_s8.untrained': _cfg(classifier='head.linear'),\n    'levit_conv_512.untrained': _cfg(classifier='head.linear'),\n    'levit_conv_256d.untrained': _cfg(classifier='head.linear'),\n    'levit_conv_512d.untrained': _cfg(classifier='head.linear'),\n})\n\n\n@register_model\ndef levit_128s(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_128s', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef levit_128(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_128', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef levit_192(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_192', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef levit_256(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_256', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef levit_384(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_384', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef levit_384_s8(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_384_s8', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef levit_512_s8(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_512_s8', pretrained=pretrained, distilled=False, **kwargs)\n\n\n@register_model\ndef levit_512(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_512', pretrained=pretrained, distilled=False, **kwargs)\n\n\n@register_model\ndef levit_256d(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_256d', pretrained=pretrained, distilled=False, **kwargs)\n\n\n@register_model\ndef levit_512d(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_512d', pretrained=pretrained, distilled=False, **kwargs)\n\n\n@register_model\ndef levit_conv_128s(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_conv_128s', pretrained=pretrained, use_conv=True, **kwargs)\n\n\n@register_model\ndef levit_conv_128(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_conv_128', pretrained=pretrained, use_conv=True, **kwargs)\n\n\n@register_model\ndef levit_conv_192(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_conv_192', pretrained=pretrained, use_conv=True, **kwargs)\n\n\n@register_model\ndef levit_conv_256(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_conv_256', pretrained=pretrained, use_conv=True, **kwargs)\n\n\n@register_model\ndef levit_conv_384(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_conv_384', pretrained=pretrained, use_conv=True, **kwargs)\n\n\n@register_model\ndef levit_conv_384_s8(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_conv_384_s8', pretrained=pretrained, use_conv=True, **kwargs)\n\n\n@register_model\ndef levit_conv_512_s8(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_conv_512_s8', pretrained=pretrained, use_conv=True, distilled=False, **kwargs)\n\n\n@register_model\ndef levit_conv_512(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_conv_512', pretrained=pretrained, use_conv=True, distilled=False, **kwargs)\n\n\n@register_model\ndef levit_conv_256d(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_conv_256d', pretrained=pretrained, use_conv=True, distilled=False, **kwargs)\n\n\n@register_model\ndef levit_conv_512d(pretrained=False, **kwargs) -> Levit:\n    return create_levit('levit_conv_512d', pretrained=pretrained, use_conv=True, distilled=False, **kwargs)\n\n",
  "from ._builder import *\nfrom ._helpers import *\nfrom ._manipulate import *\nfrom ._prune import *\n\nimport warnings\nwarnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", DeprecationWarning)\n",
  "from ._features import *\n\nimport warnings\nwarnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", DeprecationWarning)\n",
  "import dataclasses\nimport logging\nimport os\nfrom copy import deepcopy\nfrom typing import Optional, Dict, Callable, Any, Tuple\n\nfrom torch import nn as nn\nfrom torch.hub import load_state_dict_from_url\n\nfrom timm.models._features import FeatureListNet, FeatureHookNet\nfrom timm.models._features_fx import FeatureGraphNet\nfrom timm.models._helpers import load_state_dict\nfrom timm.models._hub import has_hf_hub, download_cached_file, check_cached_file, load_state_dict_from_hf\nfrom timm.models._manipulate import adapt_input_conv\nfrom timm.models._pretrained import PretrainedCfg\nfrom timm.models._prune import adapt_model_from_file\nfrom timm.models._registry import get_pretrained_cfg\n\n_logger = logging.getLogger(__name__)\n\n# Global variables for rarely used pretrained checkpoint download progress and hash check.\n# Use set_pretrained_download_progress / set_pretrained_check_hash functions to toggle.\n_DOWNLOAD_PROGRESS = False\n_CHECK_HASH = False\n_USE_OLD_CACHE = int(os.environ.get('TIMM_USE_OLD_CACHE', 0)) > 0\n\n__all__ = ['set_pretrained_download_progress', 'set_pretrained_check_hash', 'load_custom_pretrained', 'load_pretrained',\n           'pretrained_cfg_for_features', 'resolve_pretrained_cfg', 'build_model_with_cfg']\n\n\ndef _resolve_pretrained_source(pretrained_cfg):\n    cfg_source = pretrained_cfg.get('source', '')\n    pretrained_url = pretrained_cfg.get('url', None)\n    pretrained_file = pretrained_cfg.get('file', None)\n    pretrained_sd = pretrained_cfg.get('state_dict', None)\n    hf_hub_id = pretrained_cfg.get('hf_hub_id', None)\n\n    # resolve where to load pretrained weights from\n    load_from = ''\n    pretrained_loc = ''\n    if cfg_source == 'hf-hub' and has_hf_hub(necessary=True):\n        # hf-hub specified as source via model identifier\n        load_from = 'hf-hub'\n        assert hf_hub_id\n        pretrained_loc = hf_hub_id\n    else:\n        # default source == timm or unspecified\n        if pretrained_sd:\n            # direct state_dict pass through is the highest priority\n            load_from = 'state_dict'\n            pretrained_loc = pretrained_sd\n            assert isinstance(pretrained_loc, dict)\n        elif pretrained_file:\n            # file load override is the second-highest priority if set\n            load_from = 'file'\n            pretrained_loc = pretrained_file\n        else:\n            old_cache_valid = False\n            if _USE_OLD_CACHE:\n                # prioritized old cached weights if exists and env var enabled\n                old_cache_valid = check_cached_file(pretrained_url) if pretrained_url else False\n            if not old_cache_valid and hf_hub_id and has_hf_hub(necessary=True):\n                # hf-hub available as alternate weight source in default_cfg\n                load_from = 'hf-hub'\n                pretrained_loc = hf_hub_id\n            elif pretrained_url:\n                load_from = 'url'\n                pretrained_loc = pretrained_url\n\n    if load_from == 'hf-hub' and pretrained_cfg.get('hf_hub_filename', None):\n        # if a filename override is set, return tuple for location w/ (hub_id, filename)\n        pretrained_loc = pretrained_loc, pretrained_cfg['hf_hub_filename']\n    return load_from, pretrained_loc\n\n\ndef set_pretrained_download_progress(enable=True):\n    \"\"\" Set download progress for pretrained weights on/off (globally). \"\"\"\n    global _DOWNLOAD_PROGRESS\n    _DOWNLOAD_PROGRESS = enable\n\n\ndef set_pretrained_check_hash(enable=True):\n    \"\"\" Set hash checking for pretrained weights on/off (globally). \"\"\"\n    global _CHECK_HASH\n    _CHECK_HASH = enable\n\n\ndef load_custom_pretrained(\n        model: nn.Module,\n        pretrained_cfg: Optional[Dict] = None,\n        load_fn: Optional[Callable] = None,\n):\n    r\"\"\"Loads a custom (read non .pth) weight file\n\n    Downloads checkpoint file into cache-dir like torch.hub based loaders, but calls\n    a passed in custom load fun, or the `load_pretrained` model member fn.\n\n    If the object is already present in `model_dir`, it's deserialized and returned.\n    The default value of `model_dir` is ``<hub_dir>/checkpoints`` where\n    `hub_dir` is the directory returned by :func:`~torch.hub.get_dir`.\n\n    Args:\n        model: The instantiated model to load weights into\n        pretrained_cfg (dict): Default pretrained model cfg\n        load_fn: An external standalone fn that loads weights into provided model, otherwise a fn named\n            'laod_pretrained' on the model will be called if it exists\n    \"\"\"\n    pretrained_cfg = pretrained_cfg or getattr(model, 'pretrained_cfg', None)\n    if not pretrained_cfg:\n        _logger.warning(\"Invalid pretrained config, cannot load weights.\")\n        return\n\n    load_from, pretrained_loc = _resolve_pretrained_source(pretrained_cfg)\n    if not load_from:\n        _logger.warning(\"No pretrained weights exist for this model. Using random initialization.\")\n        return\n    if load_from == 'hf-hub':\n        _logger.warning(\"Hugging Face hub not currently supported for custom load pretrained models.\")\n    elif load_from == 'url':\n        pretrained_loc = download_cached_file(\n            pretrained_loc,\n            check_hash=_CHECK_HASH,\n            progress=_DOWNLOAD_PROGRESS,\n        )\n\n    if load_fn is not None:\n        load_fn(model, pretrained_loc)\n    elif hasattr(model, 'load_pretrained'):\n        model.load_pretrained(pretrained_loc)\n    else:\n        _logger.warning(\"Valid function to load pretrained weights is not available, using random initialization.\")\n\n\ndef load_pretrained(\n        model: nn.Module,\n        pretrained_cfg: Optional[Dict] = None,\n        num_classes: int = 1000,\n        in_chans: int = 3,\n        filter_fn: Optional[Callable] = None,\n        strict: bool = True,\n):\n    \"\"\" Load pretrained checkpoint\n\n    Args:\n        model (nn.Module) : PyTorch model module\n        pretrained_cfg (Optional[Dict]): configuration for pretrained weights / target dataset\n        num_classes (int): num_classes for target model\n        in_chans (int): in_chans for target model\n        filter_fn (Optional[Callable]): state_dict filter fn for load (takes state_dict, model as args)\n        strict (bool): strict load of checkpoint\n\n    \"\"\"\n    pretrained_cfg = pretrained_cfg or getattr(model, 'pretrained_cfg', None)\n    if not pretrained_cfg:\n        raise RuntimeError(\"Invalid pretrained config, cannot load weights. Use `pretrained=False` for random init.\")\n\n    load_from, pretrained_loc = _resolve_pretrained_source(pretrained_cfg)\n    if load_from == 'state_dict':\n        _logger.info(f'Loading pretrained weights from state dict')\n        state_dict = pretrained_loc  # pretrained_loc is the actual state dict for this override\n    elif load_from == 'file':\n        _logger.info(f'Loading pretrained weights from file ({pretrained_loc})')\n        state_dict = load_state_dict(pretrained_loc)\n    elif load_from == 'url':\n        _logger.info(f'Loading pretrained weights from url ({pretrained_loc})')\n        if pretrained_cfg.get('custom_load', False):\n            pretrained_loc = download_cached_file(\n                pretrained_loc,\n                progress=_DOWNLOAD_PROGRESS,\n                check_hash=_CHECK_HASH,\n            )\n            model.load_pretrained(pretrained_loc)\n            return\n        else:\n            state_dict = load_state_dict_from_url(\n                pretrained_loc,\n                map_location='cpu',\n                progress=_DOWNLOAD_PROGRESS,\n                check_hash=_CHECK_HASH,\n            )\n    elif load_from == 'hf-hub':\n        _logger.info(f'Loading pretrained weights from Hugging Face hub ({pretrained_loc})')\n        if isinstance(pretrained_loc, (list, tuple)):\n            state_dict = load_state_dict_from_hf(*pretrained_loc)\n        else:\n            state_dict = load_state_dict_from_hf(pretrained_loc)\n    else:\n        model_name = pretrained_cfg.get('architecture', 'this model')\n        raise RuntimeError(f\"No pretrained weights exist for {model_name}. Use `pretrained=False` for random init.\")\n\n    if filter_fn is not None:\n        try:\n            state_dict = filter_fn(state_dict, model)\n        except TypeError as e:\n            # for backwards compat with filter fn that take one arg\n            state_dict = filter_fn(state_dict)\n\n    input_convs = pretrained_cfg.get('first_conv', None)\n    if input_convs is not None and in_chans != 3:\n        if isinstance(input_convs, str):\n            input_convs = (input_convs,)\n        for input_conv_name in input_convs:\n            weight_name = input_conv_name + '.weight'\n            try:\n                state_dict[weight_name] = adapt_input_conv(in_chans, state_dict[weight_name])\n                _logger.info(\n                    f'Converted input conv {input_conv_name} pretrained weights from 3 to {in_chans} channel(s)')\n            except NotImplementedError as e:\n                del state_dict[weight_name]\n                strict = False\n                _logger.warning(\n                    f'Unable to convert pretrained {input_conv_name} weights, using random init for this layer.')\n\n    classifiers = pretrained_cfg.get('classifier', None)\n    label_offset = pretrained_cfg.get('label_offset', 0)\n    if classifiers is not None:\n        if isinstance(classifiers, str):\n            classifiers = (classifiers,)\n        if num_classes != pretrained_cfg['num_classes']:\n            for classifier_name in classifiers:\n                # completely discard fully connected if model num_classes doesn't match pretrained weights\n                state_dict.pop(classifier_name + '.weight', None)\n                state_dict.pop(classifier_name + '.bias', None)\n            strict = False\n        elif label_offset > 0:\n            for classifier_name in classifiers:\n                # special case for pretrained weights with an extra background class in pretrained weights\n                classifier_weight = state_dict[classifier_name + '.weight']\n                state_dict[classifier_name + '.weight'] = classifier_weight[label_offset:]\n                classifier_bias = state_dict[classifier_name + '.bias']\n                state_dict[classifier_name + '.bias'] = classifier_bias[label_offset:]\n\n    model.load_state_dict(state_dict, strict=strict)\n\n\ndef pretrained_cfg_for_features(pretrained_cfg):\n    pretrained_cfg = deepcopy(pretrained_cfg)\n    # remove default pretrained cfg fields that don't have much relevance for feature backbone\n    to_remove = ('num_classes', 'classifier', 'global_pool')  # add default final pool size?\n    for tr in to_remove:\n        pretrained_cfg.pop(tr, None)\n    return pretrained_cfg\n\n\ndef _filter_kwargs(kwargs, names):\n    if not kwargs or not names:\n        return\n    for n in names:\n        kwargs.pop(n, None)\n\n\ndef _update_default_kwargs(pretrained_cfg, kwargs, kwargs_filter):\n    \"\"\" Update the default_cfg and kwargs before passing to model\n\n    Args:\n        pretrained_cfg: input pretrained cfg (updated in-place)\n        kwargs: keyword args passed to model build fn (updated in-place)\n        kwargs_filter: keyword arg keys that must be removed before model __init__\n    \"\"\"\n    # Set model __init__ args that can be determined by default_cfg (if not already passed as kwargs)\n    default_kwarg_names = ('num_classes', 'global_pool', 'in_chans')\n    if pretrained_cfg.get('fixed_input_size', False):\n        # if fixed_input_size exists and is True, model takes an img_size arg that fixes its input size\n        default_kwarg_names += ('img_size',)\n\n    for n in default_kwarg_names:\n        # for legacy reasons, model __init__args uses img_size + in_chans as separate args while\n        # pretrained_cfg has one input_size=(C, H ,W) entry\n        if n == 'img_size':\n            input_size = pretrained_cfg.get('input_size', None)\n            if input_size is not None:\n                assert len(input_size) == 3\n                kwargs.setdefault(n, input_size[-2:])\n        elif n == 'in_chans':\n            input_size = pretrained_cfg.get('input_size', None)\n            if input_size is not None:\n                assert len(input_size) == 3\n                kwargs.setdefault(n, input_size[0])\n        else:\n            default_val = pretrained_cfg.get(n, None)\n            if default_val is not None:\n                kwargs.setdefault(n, pretrained_cfg[n])\n\n    # Filter keyword args for task specific model variants (some 'features only' models, etc.)\n    _filter_kwargs(kwargs, names=kwargs_filter)\n\n\ndef resolve_pretrained_cfg(\n        variant: str,\n        pretrained_cfg=None,\n        pretrained_cfg_overlay=None,\n) -> PretrainedCfg:\n    model_with_tag = variant\n    pretrained_tag = None\n    if pretrained_cfg:\n        if isinstance(pretrained_cfg, dict):\n            # pretrained_cfg dict passed as arg, validate by converting to PretrainedCfg\n            pretrained_cfg = PretrainedCfg(**pretrained_cfg)\n        elif isinstance(pretrained_cfg, str):\n            pretrained_tag = pretrained_cfg\n            pretrained_cfg = None\n\n    # fallback to looking up pretrained cfg in model registry by variant identifier\n    if not pretrained_cfg:\n        if pretrained_tag:\n            model_with_tag = '.'.join([variant, pretrained_tag])\n        pretrained_cfg = get_pretrained_cfg(model_with_tag)\n\n    if not pretrained_cfg:\n        _logger.warning(\n            f\"No pretrained configuration specified for {model_with_tag} model. Using a default.\"\n            f\" Please add a config to the model pretrained_cfg registry or pass explicitly.\")\n        pretrained_cfg = PretrainedCfg()  # instance with defaults\n\n    pretrained_cfg_overlay = pretrained_cfg_overlay or {}\n    if not pretrained_cfg.architecture:\n        pretrained_cfg_overlay.setdefault('architecture', variant)\n    pretrained_cfg = dataclasses.replace(pretrained_cfg, **pretrained_cfg_overlay)\n\n    return pretrained_cfg\n\n\ndef build_model_with_cfg(\n        model_cls: Callable,\n        variant: str,\n        pretrained: bool,\n        pretrained_cfg: Optional[Dict] = None,\n        pretrained_cfg_overlay: Optional[Dict] = None,\n        model_cfg: Optional[Any] = None,\n        feature_cfg: Optional[Dict] = None,\n        pretrained_strict: bool = True,\n        pretrained_filter_fn: Optional[Callable] = None,\n        kwargs_filter: Optional[Tuple[str]] = None,\n        **kwargs,\n):\n    \"\"\" Build model with specified default_cfg and optional model_cfg\n\n    This helper fn aids in the construction of a model including:\n      * handling default_cfg and associated pretrained weight loading\n      * passing through optional model_cfg for models with config based arch spec\n      * features_only model adaptation\n      * pruning config / model adaptation\n\n    Args:\n        model_cls (nn.Module): model class\n        variant (str): model variant name\n        pretrained (bool): load pretrained weights\n        pretrained_cfg (dict): model's pretrained weight/task config\n        model_cfg (Optional[Dict]): model's architecture config\n        feature_cfg (Optional[Dict]: feature extraction adapter config\n        pretrained_strict (bool): load pretrained weights strictly\n        pretrained_filter_fn (Optional[Callable]): filter callable for pretrained weights\n        kwargs_filter (Optional[Tuple]): kwargs to filter before passing to model\n        **kwargs: model args passed through to model __init__\n    \"\"\"\n    pruned = kwargs.pop('pruned', False)\n    features = False\n    feature_cfg = feature_cfg or {}\n\n    # resolve and update model pretrained config and model kwargs\n    pretrained_cfg = resolve_pretrained_cfg(\n        variant,\n        pretrained_cfg=pretrained_cfg,\n        pretrained_cfg_overlay=pretrained_cfg_overlay\n    )\n\n    # FIXME converting back to dict, PretrainedCfg use should be propagated further, but not into model\n    pretrained_cfg = pretrained_cfg.to_dict()\n\n    _update_default_kwargs(pretrained_cfg, kwargs, kwargs_filter)\n\n    # Setup for feature extraction wrapper done at end of this fn\n    if kwargs.pop('features_only', False):\n        features = True\n        feature_cfg.setdefault('out_indices', (0, 1, 2, 3, 4))\n        if 'out_indices' in kwargs:\n            feature_cfg['out_indices'] = kwargs.pop('out_indices')\n\n    # Instantiate the model\n    if model_cfg is None:\n        model = model_cls(**kwargs)\n    else:\n        model = model_cls(cfg=model_cfg, **kwargs)\n    model.pretrained_cfg = pretrained_cfg\n    model.default_cfg = model.pretrained_cfg  # alias for backwards compat\n\n    if pruned:\n        model = adapt_model_from_file(model, variant)\n\n    # For classification models, check class attr, then kwargs, then default to 1k, otherwise 0 for feats\n    num_classes_pretrained = 0 if features else getattr(model, 'num_classes', kwargs.get('num_classes', 1000))\n    if pretrained:\n        load_pretrained(\n            model,\n            pretrained_cfg=pretrained_cfg,\n            num_classes=num_classes_pretrained,\n            in_chans=kwargs.get('in_chans', 3),\n            filter_fn=pretrained_filter_fn,\n            strict=pretrained_strict,\n        )\n\n    # Wrap the model in a feature extraction module if enabled\n    if features:\n        feature_cls = FeatureListNet\n        output_fmt = getattr(model, 'output_fmt', None)\n        if output_fmt is not None:\n            feature_cfg.setdefault('output_fmt', output_fmt)\n        if 'feature_cls' in feature_cfg:\n            feature_cls = feature_cfg.pop('feature_cls')\n            if isinstance(feature_cls, str):\n                feature_cls = feature_cls.lower()\n                if 'hook' in feature_cls:\n                    feature_cls = FeatureHookNet\n                elif feature_cls == 'fx':\n                    feature_cls = FeatureGraphNet\n                else:\n                    assert False, f'Unknown feature class {feature_cls}'\n        model = feature_cls(model, **feature_cfg)\n        model.pretrained_cfg = pretrained_cfg_for_features(pretrained_cfg)  # add back pretrained cfg\n        model.default_cfg = model.pretrained_cfg  # alias for rename backwards compat (default_cfg -> pretrained_cfg)\n\n    return model\n",
  "\"\"\" EVA\n\nEVA from https://github.com/baaivision/EVA , paper: https://arxiv.org/abs/2211.07636\n\n@article{EVA,\n  title={EVA: Exploring the Limits of Masked Visual Representation Learning at Scale},\n  author={Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang,\n  Tiejun and Wang, Xinlong and Cao, Yue},\n  journal={arXiv preprint arXiv:2211.07636},\n  year={2022}\n}\n\nEVA-02: A Visual Representation for Neon Genesis - https://arxiv.org/abs/2303.11331\n@article{EVA02,\n  title={EVA-02: A Visual Representation for Neon Genesis},\n  author={Fang, Yuxin and Sun, Quan and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},\n  journal={arXiv preprint arXiv:2303.11331},\n  year={2023}\n}\n\nThis file contains EVA & EVA02 model implementations evolved from BEiT, additional models in vision_transformer.py.\n\nModifications by / Copyright 2023 Ross Wightman, original copyrights below\n\"\"\"\n# EVA models Copyright (c) 2022 BAAI-Vision\n# EVA02 models Copyright (c) 2023 BAAI-Vision\n\nimport math\nfrom typing import Callable, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.checkpoint import checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\nfrom timm.layers import PatchEmbed, Mlp, GluMlp, SwiGLU, LayerNorm, DropPath, PatchDropout, RotaryEmbeddingCat, \\\n    apply_rot_embed_cat, apply_keep_indices_nlc, trunc_normal_, resample_patch_embed, resample_abs_pos_embed, \\\n    to_2tuple, use_fused_attn\n\nfrom ._builder import build_model_with_cfg\nfrom ._registry import generate_default_cfgs, register_model\n\n__all__ = ['Eva']\n\n\nclass EvaAttention(nn.Module):\n    fused_attn: torch.jit.Final[bool]\n\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int = 8,\n            qkv_bias: bool = True,\n            qkv_fused: bool = True,\n            attn_drop: float = 0.,\n            proj_drop: float = 0.,\n            attn_head_dim: Optional[int] = None,\n            norm_layer: Optional[Callable] = None,\n    ):\n        \"\"\"\n\n        Args:\n            dim:\n            num_heads:\n            qkv_bias:\n            qkv_fused:\n            attn_drop:\n            proj_drop:\n            attn_head_dim:\n            norm_layer:\n        \"\"\"\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = head_dim ** -0.5\n        self.fused_attn = use_fused_attn()\n\n        if qkv_fused:\n            self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n            self.q_proj = self.k_proj = self.v_proj = None\n            if qkv_bias:\n                self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n                self.register_buffer('k_bias', torch.zeros(all_head_dim), persistent=False)\n                self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n            else:\n                self.q_bias = self.k_bias = self.v_bias = None\n        else:\n            self.q_proj = nn.Linear(dim, all_head_dim, bias=qkv_bias)\n            self.k_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.v_proj = nn.Linear(dim, all_head_dim, bias=qkv_bias)\n            self.qkv = None\n            self.q_bias = self.k_bias = self.v_bias = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.norm = norm_layer(all_head_dim) if norm_layer is not None else nn.Identity()\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(\n            self,\n            x,\n            rope: Optional[torch.Tensor] = None,\n            attn_mask: Optional[torch.Tensor] = None,\n    ):\n        B, N, C = x.shape\n\n        if self.qkv is not None:\n            qkv_bias = torch.cat((self.q_bias, self.k_bias, self.v_bias)) if self.q_bias is not None else None\n            qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n            qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n            q, k, v = qkv.unbind(0)  # B, num_heads, N, head_dim\n        else:\n            q = self.q_proj(x).reshape(B, N, self.num_heads, -1).transpose(1, 2)  # B, num_heads, N, C\n            k = self.k_proj(x).reshape(B, N, self.num_heads, -1).transpose(1, 2)\n            v = self.v_proj(x).reshape(B, N, self.num_heads, -1).transpose(1, 2)\n\n        if rope is not None:\n            q = torch.cat([q[:, :, :1, :], apply_rot_embed_cat(q[:, :, 1:, :], rope)], 2).type_as(v)\n            k = torch.cat([k[:, :, :1, :], apply_rot_embed_cat(k[:, :, 1:, :], rope)], 2).type_as(v)\n\n        if self.fused_attn:\n            x = F.scaled_dot_product_attention(\n                q, k, v,\n                attn_mask=attn_mask,\n                dropout_p=self.attn_drop.p,\n            )\n        else:\n            q = q * self.scale\n            attn = (q @ k.transpose(-2, -1))\n            attn = attn.softmax(dim=-1)\n            if attn_mask is not None:\n                attn_mask = attn_mask.to(torch.bool)\n                attn = attn.masked_fill(~attn_mask[:, None, None, :], float(\"-inf\"))\n            attn = self.attn_drop(attn)\n            x = attn @ v\n\n        x = x.transpose(1, 2).reshape(B, N, C)\n        x = self.norm(x)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass EvaBlock(nn.Module):\n\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int,\n            qkv_bias: bool = True,\n            qkv_fused: bool = True,\n            mlp_ratio: float = 4.,\n            swiglu_mlp: bool = False,\n            scale_mlp: bool = False,\n            scale_attn_inner: bool = False,\n            proj_drop: float = 0.,\n            attn_drop: float = 0.,\n            drop_path: float = 0.,\n            init_values: Optional[float] = None,\n            act_layer: Callable = nn.GELU,\n            norm_layer: Callable = LayerNorm,\n            attn_head_dim: Optional[int] = None,\n    ):\n        \"\"\"\n\n        Args:\n            dim:\n            num_heads:\n            qkv_bias:\n            qkv_fused:\n            mlp_ratio:\n            swiglu_mlp:\n            scale_mlp:\n            scale_attn_inner:\n            proj_drop:\n            attn_drop:\n            drop_path:\n            init_values:\n            act_layer:\n            norm_layer:\n            attn_head_dim:\n        \"\"\"\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = EvaAttention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qkv_fused=qkv_fused,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n            attn_head_dim=attn_head_dim,\n            norm_layer=norm_layer if scale_attn_inner else None,\n        )\n        self.gamma_1 = nn.Parameter(init_values * torch.ones(dim)) if init_values is not None else None\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        hidden_features = int(dim * mlp_ratio)\n        if swiglu_mlp:\n            if scale_mlp:\n                # when norm in SwiGLU used, an impl with separate fc for gate & x is used\n                self.mlp = SwiGLU(\n                    in_features=dim,\n                    hidden_features=hidden_features,\n                    norm_layer=norm_layer if scale_mlp else None,\n                    drop=proj_drop,\n                )\n            else:\n                # w/o any extra norm, an impl with packed weights is used, matches existing GluMLP\n                self.mlp = GluMlp(\n                    in_features=dim,\n                    hidden_features=hidden_features * 2,\n                    norm_layer=norm_layer if scale_mlp else None,\n                    act_layer=nn.SiLU,\n                    gate_last=False,\n                    drop=proj_drop,\n                )\n        else:\n            self.mlp = Mlp(\n                in_features=dim,\n                hidden_features=hidden_features,\n                act_layer=act_layer,\n                norm_layer=norm_layer if scale_mlp else None,\n                drop=proj_drop,\n            )\n        self.gamma_2 = nn.Parameter(init_values * torch.ones(dim)) if init_values is not None else None\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x, rope: Optional[torch.Tensor] = None, attn_mask: Optional[torch.Tensor] = None):\n        if self.gamma_1 is None:\n            x = x + self.drop_path1(self.attn(self.norm1(x), rope=rope, attn_mask=attn_mask))\n            x = x + self.drop_path2(self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path1(self.gamma_1 * self.attn(self.norm1(x), rope=rope, attn_mask=attn_mask))\n            x = x + self.drop_path2(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass EvaBlockPostNorm(nn.Module):\n    \"\"\" EVA block w/ post-norm and support for swiglu, MLP norm scale, ROPE. \"\"\"\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int,\n            qkv_bias: bool = True,\n            qkv_fused: bool = True,\n            mlp_ratio: float = 4.,\n            swiglu_mlp: bool = False,\n            scale_mlp: bool = False,\n            scale_attn_inner: bool = False,\n            proj_drop: float = 0.,\n            attn_drop: float = 0.,\n            drop_path: float = 0.,\n            init_values: Optional[float] = None,  # ignore for post-norm\n            act_layer: Callable = nn.GELU,\n            norm_layer: Callable = nn.LayerNorm,\n            attn_head_dim: Optional[int] = None,\n    ):\n        \"\"\"\n\n        Args:\n            dim:\n            num_heads:\n            qkv_bias:\n            qkv_fused:\n            mlp_ratio:\n            swiglu_mlp:\n            scale_mlp:\n            scale_attn_inner:\n            proj_drop:\n            attn_drop:\n            drop_path:\n            init_values:\n            act_layer:\n            norm_layer:\n            attn_head_dim:\n        \"\"\"\n        super().__init__()\n        self.attn = EvaAttention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qkv_fused=qkv_fused,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n            attn_head_dim=attn_head_dim,\n            norm_layer=norm_layer if scale_attn_inner else None,\n        )\n        self.norm1 = norm_layer(dim)\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        hidden_features = int(dim * mlp_ratio)\n        if swiglu_mlp:\n            if scale_mlp:\n                # when norm in SwiGLU used, an impl with separate fc for gate & x is used\n                self.mlp = SwiGLU(\n                    in_features=dim,\n                    hidden_features=hidden_features,\n                    norm_layer=norm_layer if scale_mlp else None,\n                    drop=proj_drop,\n                )\n            else:\n                # w/o any extra norm, an impl with packed fc1 weights is used, matches existing GluMLP\n                self.mlp = GluMlp(\n                    in_features=dim,\n                    hidden_features=hidden_features * 2,\n                    norm_layer=norm_layer if scale_mlp else None,\n                    act_layer=nn.SiLU,\n                    gate_last=False,\n                    drop=proj_drop,\n                )\n        else:\n            self.mlp = Mlp(\n                in_features=dim,\n                hidden_features=hidden_features,\n                act_layer=act_layer,\n                norm_layer=norm_layer if scale_mlp else None,\n                drop=proj_drop,\n            )\n        self.norm2 = norm_layer(dim)\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x, rope: Optional[torch.Tensor] = None, attn_mask: Optional[torch.Tensor] = None):\n        x = x + self.drop_path1(self.norm1(self.attn(x, rope=rope, attn_mask=attn_mask)))\n        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n        return x\n\n\nclass Eva(nn.Module):\n    \"\"\" Eva Vision Transformer w/ Abs & Rotary Pos Embed\n\n    This class implements the EVA and EVA02 models that were based on the BEiT ViT variant\n      * EVA - abs pos embed, global avg pool\n      * EVA02 - abs + rope pos embed, global avg pool, SwiGLU, scale Norm in MLP (ala normformer)\n    \"\"\"\n\n    def __init__(\n            self,\n            img_size: Union[int, Tuple[int, int]] = 224,\n            patch_size: Union[int, Tuple[int, int]] = 16,\n            in_chans: int = 3,\n            num_classes: int = 1000,\n            global_pool: str = 'avg',\n            embed_dim: int = 768,\n            depth: int = 12,\n            num_heads: int = 12,\n            qkv_bias: bool = True,\n            qkv_fused: bool = True,\n            mlp_ratio: float = 4.,\n            swiglu_mlp: bool = False,\n            scale_mlp: bool = False,\n            scale_attn_inner: bool = False,\n            drop_rate: float = 0.,\n            pos_drop_rate: float = 0.,\n            patch_drop_rate: float = 0.,\n            proj_drop_rate: float = 0.,\n            attn_drop_rate: float = 0.,\n            drop_path_rate: float = 0.,\n            norm_layer: Callable = LayerNorm,\n            init_values: Optional[float] = None,\n            class_token: bool = True,\n            use_abs_pos_emb: bool = True,\n            use_rot_pos_emb: bool = False,\n            use_post_norm: bool = False,\n            ref_feat_shape: Optional[Union[Tuple[int, int], int]] = None,\n            head_init_scale: float = 0.001,\n    ):\n        \"\"\"\n\n        Args:\n            img_size:\n            patch_size:\n            in_chans:\n            num_classes:\n            global_pool:\n            embed_dim:\n            depth:\n            num_heads:\n            qkv_bias:\n            qkv_fused:\n            mlp_ratio:\n            swiglu_mlp:\n            scale_mlp:\n            scale_attn_inner:\n            drop_rate:\n            pos_drop_rate:\n            proj_drop_rate:\n            attn_drop_rate:\n            drop_path_rate:\n            norm_layer:\n            init_values:\n            class_token:\n            use_abs_pos_emb:\n            use_rot_pos_emb:\n            use_post_norm:\n            ref_feat_shape:\n            head_init_scale:\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.num_prefix_tokens = 1 if class_token else 0\n        self.grad_checkpointing = False\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n        )\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if class_token else None\n\n        self.pos_embed = nn.Parameter(\n            torch.zeros(1, num_patches + self.num_prefix_tokens, embed_dim)) if use_abs_pos_emb else None\n        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n        if patch_drop_rate > 0:\n            self.patch_drop = PatchDropout(\n                patch_drop_rate,\n                num_prefix_tokens=self.num_prefix_tokens,\n                return_indices=True,\n            )\n        else:\n            self.patch_drop = None\n\n        if use_rot_pos_emb:\n            ref_feat_shape = to_2tuple(ref_feat_shape) if ref_feat_shape is not None else None\n            self.rope = RotaryEmbeddingCat(\n                embed_dim // num_heads,\n                in_pixels=False,\n                feat_shape=self.patch_embed.grid_size,\n                ref_feat_shape=ref_feat_shape,\n            )\n        else:\n            self.rope = None\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        block_fn = EvaBlockPostNorm if use_post_norm else EvaBlock\n        self.blocks = nn.ModuleList([\n            block_fn(\n                dim=embed_dim,\n                num_heads=num_heads,\n                qkv_bias=qkv_bias,\n                qkv_fused=qkv_fused,\n                mlp_ratio=mlp_ratio,\n                swiglu_mlp=swiglu_mlp,\n                scale_mlp=scale_mlp,\n                scale_attn_inner=scale_attn_inner,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n                init_values=init_values,\n            )\n            for i in range(depth)])\n\n        use_fc_norm = self.global_pool == 'avg'\n        self.norm = nn.Identity() if use_fc_norm else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_fc_norm else nn.Identity()\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        self.apply(self._init_weights)\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n\n        self.fix_init_weight()\n        if isinstance(self.head, nn.Linear):\n            trunc_normal_(self.head.weight, std=.02)\n            self.head.weight.data.mul_(head_init_scale)\n            self.head.bias.data.mul_(head_init_scale)\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        nwd = {'pos_embed', 'cls_token'}\n        return nwd\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^cls_token|pos_embed|patch_embed',  # stem and embed\n            blocks=[(r'^blocks\\.(\\d+)', None), (r'^norm', (99999,))],\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n\n        if self.cls_token is not None:\n            x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n\n        # apply abs position embedding\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        # obtain shared rotary position embedding and apply patch dropout\n        rot_pos_embed = self.rope.get_embed() if self.rope is not None else None\n        if self.patch_drop is not None:\n            x, keep_indices = self.patch_drop(x)\n            if rot_pos_embed is not None and keep_indices is not None:\n                rot_pos_embed = apply_keep_indices_nlc(x, rot_pos_embed, keep_indices)\n\n        for blk in self.blocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint(blk, x, rope=rot_pos_embed)\n            else:\n                x = blk(x, rope=rot_pos_embed)\n\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool:\n            x = x[:, self.num_prefix_tokens:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]\n        x = self.fc_norm(x)\n        x = self.head_drop(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef checkpoint_filter_fn(\n        state_dict,\n        model,\n        interpolation='bicubic',\n        antialias=True,\n):\n    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n    out_dict = {}\n    state_dict = state_dict.get('model_ema', state_dict)\n    state_dict = state_dict.get('model', state_dict)\n    state_dict = state_dict.get('module', state_dict)\n    state_dict = state_dict.get('state_dict', state_dict)\n    # prefix for loading OpenCLIP compatible weights\n    if 'visual.trunk.pos_embed' in state_dict:\n        prefix = 'visual.trunk.'\n    elif 'visual.pos_embed' in state_dict:\n        prefix = 'visual.'\n    else:\n        prefix = ''\n    mim_weights = prefix + 'mask_token' in state_dict\n    no_qkv = prefix + 'blocks.0.attn.q_proj.weight' in state_dict\n\n    len_prefix = len(prefix)\n    for k, v in state_dict.items():\n        if prefix:\n            if k.startswith(prefix):\n                k = k[len_prefix:]\n            else:\n                continue\n\n        if 'rope' in k:\n            # fixed embedding no need to load buffer from checkpoint\n            continue\n\n        if 'patch_embed.proj.weight' in k:\n            _, _, H, W = model.patch_embed.proj.weight.shape\n            if v.shape[-1] != W or v.shape[-2] != H:\n                v = resample_patch_embed(\n                    v,\n                    (H, W),\n                    interpolation=interpolation,\n                    antialias=antialias,\n                    verbose=True,\n                )\n        elif k == 'pos_embed' and v.shape[1] != model.pos_embed.shape[1]:\n            # To resize pos embedding when using model at different size from pretrained weights\n            num_prefix_tokens = 0 if getattr(model, 'no_embed_class', False) else getattr(model, 'num_prefix_tokens', 1)\n            v = resample_abs_pos_embed(\n                v,\n                new_size=model.patch_embed.grid_size,\n                num_prefix_tokens=num_prefix_tokens,\n                interpolation=interpolation,\n                antialias=antialias,\n                verbose=True,\n            )\n\n        k = k.replace('mlp.ffn_ln', 'mlp.norm')\n        k = k.replace('attn.inner_attn_ln', 'attn.norm')\n        k = k.replace('mlp.w12', 'mlp.fc1')\n        k = k.replace('mlp.w1', 'mlp.fc1_g')\n        k = k.replace('mlp.w2', 'mlp.fc1_x')\n        k = k.replace('mlp.w3', 'mlp.fc2')\n        if no_qkv:\n            k = k.replace('q_bias', 'q_proj.bias')\n            k = k.replace('v_bias', 'v_proj.bias')\n\n        if mim_weights and k in ('mask_token', 'lm_head.weight', 'lm_head.bias', 'norm.weight', 'norm.bias'):\n            if k == 'norm.weight' or k == 'norm.bias':\n                # try moving norm -> fc norm on fine-tune, probably a better starting point than new init\n                k = k.replace('norm', 'fc_norm')\n            else:\n                # skip pretrain mask token & head weights\n                continue\n\n        out_dict[k] = v\n\n    return out_dict\n\n\ndef _create_eva(variant, pretrained=False, **kwargs):\n    if kwargs.get('features_only', None):\n        raise RuntimeError('features_only not implemented for Eva models.')\n\n    model = build_model_with_cfg(\n        Eva, variant, pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        **kwargs)\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': OPENAI_CLIP_MEAN, 'std': OPENAI_CLIP_STD,\n        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n        'license': 'mit', **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n\n    # EVA 01 CLIP fine-tuned on imagenet-1k\n    'eva_giant_patch14_224.clip_ft_in1k': _cfg(\n        # hf_hub_id='BAAI/EVA', hf_hub_filename='eva_clip_vis_enc_sz224_ftcls_89p1.pt',\n        hf_hub_id='timm/',\n    ),\n    'eva_giant_patch14_336.clip_ft_in1k': _cfg(\n        # hf_hub_id='BAAI/EVA', hf_hub_filename='eva_clip_vis_enc_sz336_ftcls_89p4.pt',\n        hf_hub_id='timm/',\n        input_size=(3, 336, 336), crop_pct=1.0, crop_mode='squash'),\n\n    # MIM EVA 01 pretrain, ft on in22k -> in1k\n    'eva_giant_patch14_336.m30m_ft_in22k_in1k': _cfg(\n        # hf_hub_id='BAAI/EVA', hf_hub_filename='eva_21k_1k_336px_psz14_ema_89p6.pt',\n        hf_hub_id='timm/',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD,\n        input_size=(3, 336, 336), crop_pct=1.0, crop_mode='squash'),\n    'eva_giant_patch14_560.m30m_ft_in22k_in1k': _cfg(\n        # hf_hub_id='BAAI/EVA', hf_hub_filename='eva_21k_1k_560px_psz14_ema_89p7.pt',\n        hf_hub_id='timm/',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD,\n        input_size=(3, 560, 560), crop_pct=1.0, crop_mode='squash'),\n\n    # in22k or m38m MIM pretrain w/ intermediate in22k fine-tune and final in1k fine-tune\n    'eva02_base_patch14_448.mim_in22k_ft_in22k_in1k': _cfg(\n        # hf_hub_id='Yuxin-CV/EVA-02', hf_hub_filename='eva02/cls/in21k_to_in1k/eva02_B_pt_in21k_medft_in21k_ft_in1k_p14.pt',\n        hf_hub_id='timm/',\n        input_size=(3, 448, 448), crop_pct=1.0, crop_mode='squash',\n    ),\n    'eva02_large_patch14_448.mim_in22k_ft_in22k_in1k': _cfg(\n        # hf_hub_id='Yuxin-CV/EVA-02', hf_hub_filename='eva02/cls/in21k_to_in1k/eva02_L_pt_in21k_medft_in21k_ft_in1k_p14.pt',\n        hf_hub_id='timm/',\n        input_size=(3, 448, 448), crop_pct=1.0, crop_mode='squash',\n    ),\n    'eva02_large_patch14_448.mim_m38m_ft_in22k_in1k': _cfg(\n        hf_hub_id='timm/',\n        #hf_hub_id='Yuxin-CV/EVA-02', hf_hub_filename='eva02/cls/in21k_to_in1k/eva02_L_pt_m38m_medft_in21k_ft_in1k_p14.pt',\n        input_size=(3, 448, 448), crop_pct=1.0, crop_mode='squash',\n    ),\n\n    # in22k or m3m MIM pretrain w/ in1k fine-tune\n    'eva02_tiny_patch14_336.mim_in22k_ft_in1k': _cfg(\n        #hf_hub_id='Yuxin-CV/EVA-02', hf_hub_filename='eva02/cls/in1k/eva02_Ti_pt_in21k_ft_in1k_p14.pt',\n        hf_hub_id='timm/',\n        input_size=(3, 336, 336), crop_pct=1.0,\n    ),\n    'eva02_small_patch14_336.mim_in22k_ft_in1k': _cfg(\n        #hf_hub_id='Yuxin-CV/EVA-02', hf_hub_filename='eva02/cls/in1k/eva02_S_pt_in21k_ft_in1k_p14.pt',\n        hf_hub_id='timm/',\n        input_size=(3, 336, 336), crop_pct=1.0,\n    ),\n    'eva02_base_patch14_448.mim_in22k_ft_in1k': _cfg(\n        #hf_hub_id='Yuxin-CV/EVA-02', hf_hub_filename='eva02/cls/in1k/eva02_B_pt_in21k_ft_in1k_p14.pt',\n        hf_hub_id='timm/',\n        input_size=(3, 448, 448), crop_pct=1.0,\n    ),\n    'eva02_large_patch14_448.mim_in22k_ft_in1k': _cfg(\n        #hf_hub_id='Yuxin-CV/EVA-02', hf_hub_filename='eva02/cls/in1k/eva02_L_pt_in21k_ft_in1k_p14.pt',\n        hf_hub_id='timm/',\n        input_size=(3, 448, 448), crop_pct=1.0,\n    ),\n    'eva02_large_patch14_448.mim_m38m_ft_in1k': _cfg(\n        #hf_hub_id='Yuxin-CV/EVA-02', hf_hub_filename='eva02/cls/in1k/eva02_L_pt_m38m_ft_in1k_p14.pt',\n        hf_hub_id='timm/',\n        input_size=(3, 448, 448), crop_pct=1.0,\n    ),\n\n    # in22k or m3m MIM pretrain w/ in22k fine-tune\n    'eva02_base_patch14_448.mim_in22k_ft_in22k': _cfg(\n        #hf_hub_id='Yuxin-CV/EVA-02', hf_hub_filename='eva02/cls/in21k/eva02_B_pt_in21k_medft_in21k_p14.pt',\n        hf_hub_id='timm/',\n        input_size=(3, 448, 448), crop_pct=1.0, crop_mode='squash', num_classes=21841,\n    ),\n    'eva02_large_patch14_448.mim_in22k_ft_in22k': _cfg(\n        #hf_hub_id='Yuxin-CV/EVA-02', hf_hub_filename='eva02/cls/in21k/eva02_L_pt_in21k_medft_in21k_p14.pt',\n        hf_hub_id='timm/',\n        input_size=(3, 448, 448), crop_pct=1.0, crop_mode='squash', num_classes=21841,\n    ),\n    'eva02_large_patch14_448.mim_m38m_ft_in22k': _cfg(\n        #hf_hub_id='Yuxin-CV/EVA-02', hf_hub_filename='eva02/cls/in21k/eva02_L_pt_m38m_medft_in21k_p14.pt',\n        hf_hub_id='timm/',\n        input_size=(3, 448, 448), crop_pct=1.0, crop_mode='squash', num_classes=21841,\n    ),\n\n    # in22k or m38m MIM pretrain\n    'eva02_tiny_patch14_224.mim_in22k': _cfg(\n        # hf_hub_id='Yuxin-CV/EVA-02', hf_hub_filename='eva02/pt/eva02_Ti_pt_in21k_p14.pt',\n        hf_hub_id='timm/',\n        num_classes=0,\n    ),\n    'eva02_small_patch14_224.mim_in22k': _cfg(\n        #hf_hub_id='Yuxin-CV/EVA-02', hf_hub_filename='eva02/pt/eva02_S_pt_in21k_p14.pt',\n        hf_hub_id='timm/',\n        num_classes=0,\n    ),\n    'eva02_base_patch14_224.mim_in22k': _cfg(\n        #hf_hub_id='Yuxin-CV/EVA-02', hf_hub_filename='eva02/pt/eva02_B_pt_in21k_p14.pt',\n        hf_hub_id='timm/',\n        num_classes=0,\n    ),\n    'eva02_large_patch14_224.mim_in22k': _cfg(\n        #hf_hub_id='Yuxin-CV/EVA-02', hf_hub_filename='eva02/pt/eva02_L_pt_in21k_p14.pt',\n        hf_hub_id='timm/',\n        num_classes=0,\n    ),\n    'eva02_large_patch14_224.mim_m38m': _cfg(\n        #hf_hub_id='Yuxin-CV/EVA-02', hf_hub_filename='eva02/pt/eva02_L_pt_m38m_p14.pt',\n        hf_hub_id='timm/',\n        num_classes=0,\n    ),\n\n    # EVA01 and EVA02 CLIP image towers\n    'eva_giant_patch14_clip_224.laion400m': _cfg(\n        # hf_hub_id='QuanSun/EVA-CLIP', hf_hub_filename='EVA01_CLIP_g_14_plus_psz14_s11B.pt',\n        hf_hub_id='timm/eva_giant_patch14_clip_224.laion400m_s11b_b41k',  # float16 weights\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        num_classes=1024,\n    ),\n    'eva_giant_patch14_clip_224.merged2b': _cfg(\n        # hf_hub_id='QuanSun/EVA-CLIP', hf_hub_filename='EVA01_CLIP_g_14_plus_psz14_s11B.pt',\n        hf_hub_id='timm/eva_giant_patch14_plus_clip_224.merged2b_s11b_b114k',  # float16 weights\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        num_classes=1024,\n    ),\n    'eva02_base_patch16_clip_224.merged2b': _cfg(\n        # hf_hub_id='QuanSun/EVA-CLIP', hf_hub_filename='EVA02_CLIP_L_psz14_s4B.pt',\n        hf_hub_id='timm/eva02_base_patch16_clip_224.merged2b_s8b_b131k',  # float16 weights\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        num_classes=512,\n    ),\n    'eva02_large_patch14_clip_224.merged2b': _cfg(\n        # hf_hub_id='QuanSun/EVA-CLIP', hf_hub_filename='EVA02_CLIP_L_psz14_s4B.pt',\n        hf_hub_id='timm/eva02_large_patch14_clip_224.merged2b_s4b_b131k',  # float16 weights\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        num_classes=768,\n    ),\n    'eva02_large_patch14_clip_336.merged2b': _cfg(\n        # hf_hub_id='QuanSun/EVA-CLIP', hf_hub_filename='EVA02_CLIP_L_psz14_s4B.pt',\n        hf_hub_id='timm/eva02_large_patch14_clip_336.merged2b_s6b_b61k',  # float16 weights\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        input_size=(3, 336, 336), crop_pct=1.0,\n        num_classes=768,\n    ),\n    'eva02_enormous_patch14_clip_224.laion2b': _cfg(\n        # hf_hub_id='QuanSun/EVA-CLIP', hf_hub_filename='EVA02_CLIP_E_psz14_plus_s9B.pt',\n        hf_hub_id='timm/eva02_enormous_patch14_clip_224.laion2b_s4b_b115k',  # float16 weights\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        num_classes=1024,\n    ),\n    'eva02_enormous_patch14_clip_224.laion2b_plus': _cfg(\n        # hf_hub_id='QuanSun/EVA-CLIP', hf_hub_filename='EVA02_CLIP_E_psz14_plus_s9B.pt',\n        hf_hub_id='timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k',  # bfloat16 weights\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        num_classes=1024,\n    ),\n    'eva02_enormous_patch14_clip_224.pretrain': _cfg(\n        # hf_hub_id='QuanSun/EVA-CLIP', hf_hub_filename='EVA02_E_psz14.pt',\n        num_classes=0,\n    ),\n\n})\n\n\n@register_model\ndef eva_giant_patch14_224(pretrained=False, **kwargs) -> Eva:\n    \"\"\" EVA-g model https://arxiv.org/abs/2211.07636 \"\"\"\n    model_args = dict(patch_size=14, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=6144 / 1408)\n    model = _create_eva('eva_giant_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva_giant_patch14_336(pretrained=False, **kwargs) -> Eva:\n    \"\"\" EVA-g model https://arxiv.org/abs/2211.07636 \"\"\"\n    model_args = dict(patch_size=14, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=6144 / 1408)\n    model = _create_eva('eva_giant_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva_giant_patch14_560(pretrained=False, **kwargs) -> Eva:\n    \"\"\" EVA-g model https://arxiv.org/abs/2211.07636 \"\"\"\n    model_args = dict(patch_size=14, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=6144 / 1408)\n    model = _create_eva('eva_giant_patch14_560', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva02_tiny_patch14_224(pretrained=False, **kwargs) -> Eva:\n    model_args = dict(\n        img_size=224,\n        patch_size=14,\n        embed_dim=192,\n        depth=12,\n        num_heads=3,\n        mlp_ratio=4 * 2 / 3,\n        swiglu_mlp=True,\n        use_rot_pos_emb=True,\n        ref_feat_shape=(16, 16),  # 224/14\n    )\n    model = _create_eva('eva02_tiny_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva02_small_patch14_224(pretrained=False, **kwargs) -> Eva:\n    model_args = dict(\n        img_size=224,\n        patch_size=14,\n        embed_dim=384,\n        depth=12,\n        num_heads=6,\n        mlp_ratio=4 * 2 / 3,\n        swiglu_mlp=True,\n        use_rot_pos_emb=True,\n        ref_feat_shape=(16, 16),  # 224/14\n    )\n    model = _create_eva('eva02_small_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva02_base_patch14_224(pretrained=False, **kwargs) -> Eva:\n    model_args = dict(\n        img_size=224,\n        patch_size=14,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        qkv_fused=False,\n        mlp_ratio=4 * 2 / 3,\n        swiglu_mlp=True,\n        scale_mlp=True,\n        use_rot_pos_emb=True,\n        ref_feat_shape=(16, 16),  # 224/14\n    )\n    model = _create_eva('eva02_base_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva02_large_patch14_224(pretrained=False, **kwargs) -> Eva:\n    model_args = dict(\n        img_size=224,\n        patch_size=14,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4 * 2 / 3,\n        qkv_fused=False,\n        swiglu_mlp=True,\n        scale_mlp=True,\n        use_rot_pos_emb=True,\n        ref_feat_shape=(16, 16),  # 224/14\n    )\n    model = _create_eva('eva02_large_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva02_tiny_patch14_336(pretrained=False, **kwargs) -> Eva:\n    model_args = dict(\n        img_size=336,\n        patch_size=14,\n        embed_dim=192,\n        depth=12,\n        num_heads=3,\n        mlp_ratio=4 * 2 / 3,\n        swiglu_mlp=True,\n        use_rot_pos_emb=True,\n        ref_feat_shape=(16, 16),  # 224/14\n    )\n    model = _create_eva('eva02_tiny_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva02_small_patch14_336(pretrained=False, **kwargs) -> Eva:\n    model_args = dict(\n        img_size=336,\n        patch_size=14,\n        embed_dim=384,\n        depth=12,\n        num_heads=6,\n        mlp_ratio=4 * 2 / 3,\n        swiglu_mlp=True,\n        use_rot_pos_emb=True,\n        ref_feat_shape=(16, 16),  # 224/14\n    )\n    model = _create_eva('eva02_small_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva02_base_patch14_448(pretrained=False, **kwargs) -> Eva:\n    model_args = dict(\n        img_size=448,\n        patch_size=14,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        qkv_fused=False,\n        mlp_ratio=4 * 2 / 3,\n        swiglu_mlp=True,\n        scale_mlp=True,\n        use_rot_pos_emb=True,\n        ref_feat_shape=(16, 16),  # 224/14\n    )\n    model = _create_eva('eva02_base_patch14_448', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva02_large_patch14_448(pretrained=False, **kwargs) -> Eva:\n    model_args = dict(\n        img_size=448,\n        patch_size=14,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4 * 2 / 3,\n        qkv_fused=False,\n        swiglu_mlp=True,\n        scale_mlp=True,\n        use_rot_pos_emb=True,\n        ref_feat_shape=(16, 16),  # 224/14\n    )\n    model = _create_eva('eva02_large_patch14_448', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva_giant_patch14_clip_224(pretrained=False, **kwargs) -> Eva:\n    \"\"\" EVA-g CLIP model (only difference from non-CLIP is the pooling)  \"\"\"\n    model_args = dict(\n        patch_size=14, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=6144 / 1408,\n        global_pool=kwargs.pop('global_pool', 'token'))\n    model = _create_eva('eva_giant_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva02_base_patch16_clip_224(pretrained=False, **kwargs) -> Eva:\n    \"\"\" A EVA-CLIP specific variant that adds additional attn scale layernorm to eva02_base \"\"\"\n    model_args = dict(\n        img_size=224,\n        patch_size=16,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        qkv_fused=False,\n        mlp_ratio=4 * 2 / 3,\n        swiglu_mlp=True,\n        scale_mlp=True,\n        scale_attn_inner=True,\n        use_rot_pos_emb=True,\n        ref_feat_shape=(16, 16),  # 224/14\n        global_pool=kwargs.pop('global_pool', 'token'),\n    )\n    model = _create_eva('eva02_base_patch16_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva02_large_patch14_clip_224(pretrained=False, **kwargs) -> Eva:\n    \"\"\" A EVA-CLIP specific variant that adds additional attn scale layernorm to eva02_large \"\"\"\n    model_args = dict(\n        img_size=224,\n        patch_size=14,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4 * 2 / 3,\n        qkv_fused=False,\n        swiglu_mlp=True,\n        scale_mlp=True,\n        scale_attn_inner=True,\n        use_rot_pos_emb=True,\n        ref_feat_shape=(16, 16),  # 224/14\n        global_pool=kwargs.pop('global_pool', 'token'),\n    )\n    model = _create_eva('eva02_large_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva02_large_patch14_clip_336(pretrained=False, **kwargs) -> Eva:\n    \"\"\" A EVA-CLIP specific variant that adds additional attn scale layernorm to eva02_large \"\"\"\n    model_args = dict(\n        img_size=336,\n        patch_size=14,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4 * 2 / 3,\n        qkv_fused=False,\n        swiglu_mlp=True,\n        scale_mlp=True,\n        scale_attn_inner=True,\n        use_rot_pos_emb=True,\n        ref_feat_shape=(16, 16),  # 224/14\n        global_pool=kwargs.pop('global_pool', 'token'),\n    )\n    model = _create_eva('eva02_large_patch14_clip_336', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva02_enormous_patch14_clip_224(pretrained=False, **kwargs) -> Eva:\n    \"\"\" A EVA-CLIP specific variant that uses residual post-norm in blocks \"\"\"\n    model_args = dict(\n        img_size=224,\n        patch_size=14,\n        embed_dim=1792,\n        depth=64,\n        num_heads=16,\n        mlp_ratio=15360 / 1792,\n        use_post_norm=True,\n        global_pool=kwargs.pop('global_pool', 'token'),\n    )\n    model = _create_eva('eva02_enormous_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n",
  "\"\"\" Multi-Scale Vision Transformer v2\n\n@inproceedings{li2021improved,\n  title={MViTv2: Improved multiscale vision transformers for classification and detection},\n  author={Li, Yanghao and Wu, Chao-Yuan and Fan, Haoqi and Mangalam, Karttikeya and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph},\n  booktitle={CVPR},\n  year={2022}\n}\n\nCode adapted from original Apache 2.0 licensed impl at https://github.com/facebookresearch/mvit\nOriginal copyright below.\n\nModifications and timm support by / Copyright 2022, Ross Wightman\n\"\"\"\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved. All Rights Reserved.\nimport operator\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom functools import partial, reduce\nfrom typing import Union, List, Tuple, Optional\n\nimport torch\nimport torch.utils.checkpoint as checkpoint\nfrom torch import nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import Mlp, DropPath, trunc_normal_tf_, get_norm_layer, to_2tuple\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_function\nfrom ._registry import register_model, register_model_deprecations, generate_default_cfgs\n\n__all__ = ['MultiScaleVit', 'MultiScaleVitCfg']  # model_registry will add each entrypoint fn to this\n\n\n@dataclass\nclass MultiScaleVitCfg:\n    depths: Tuple[int, ...] = (2, 3, 16, 3)\n    embed_dim: Union[int, Tuple[int, ...]] = 96\n    num_heads: Union[int, Tuple[int, ...]] = 1\n    mlp_ratio: float = 4.\n    pool_first: bool = False\n    expand_attn: bool = True\n    qkv_bias: bool = True\n    use_cls_token: bool = False\n    use_abs_pos: bool = False\n    residual_pooling: bool = True\n    mode: str = 'conv'\n    kernel_qkv: Tuple[int, int] = (3, 3)\n    stride_q: Optional[Tuple[Tuple[int, int]]] = ((1, 1), (2, 2), (2, 2), (2, 2))\n    stride_kv: Optional[Tuple[Tuple[int, int]]] = None\n    stride_kv_adaptive: Optional[Tuple[int, int]] = (4, 4)\n    patch_kernel: Tuple[int, int] = (7, 7)\n    patch_stride: Tuple[int, int] = (4, 4)\n    patch_padding: Tuple[int, int] = (3, 3)\n    pool_type: str = 'max'\n    rel_pos_type: str = 'spatial'\n    act_layer: Union[str, Tuple[str, str]] = 'gelu'\n    norm_layer: Union[str, Tuple[str, str]] = 'layernorm'\n    norm_eps: float = 1e-6\n\n    def __post_init__(self):\n        num_stages = len(self.depths)\n        if not isinstance(self.embed_dim, (tuple, list)):\n            self.embed_dim = tuple(self.embed_dim * 2 ** i for i in range(num_stages))\n        assert len(self.embed_dim) == num_stages\n\n        if not isinstance(self.num_heads, (tuple, list)):\n            self.num_heads = tuple(self.num_heads * 2 ** i for i in range(num_stages))\n        assert len(self.num_heads) == num_stages\n\n        if self.stride_kv_adaptive is not None and self.stride_kv is None:\n            _stride_kv = self.stride_kv_adaptive\n            pool_kv_stride = []\n            for i in range(num_stages):\n                if min(self.stride_q[i]) > 1:\n                    _stride_kv = [\n                        max(_stride_kv[d] // self.stride_q[i][d], 1)\n                        for d in range(len(_stride_kv))\n                    ]\n                pool_kv_stride.append(tuple(_stride_kv))\n            self.stride_kv = tuple(pool_kv_stride)\n\n\ndef prod(iterable):\n    return reduce(operator.mul, iterable, 1)\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"\n    PatchEmbed.\n    \"\"\"\n\n    def __init__(\n            self,\n            dim_in=3,\n            dim_out=768,\n            kernel=(7, 7),\n            stride=(4, 4),\n            padding=(3, 3),\n    ):\n        super().__init__()\n\n        self.proj = nn.Conv2d(\n            dim_in,\n            dim_out,\n            kernel_size=kernel,\n            stride=stride,\n            padding=padding,\n        )\n\n    def forward(self, x) -> Tuple[torch.Tensor, List[int]]:\n        x = self.proj(x)\n        # B C H W -> B HW C\n        return x.flatten(2).transpose(1, 2), x.shape[-2:]\n\n\n@register_notrace_function\ndef reshape_pre_pool(\n        x,\n        feat_size: List[int],\n        has_cls_token: bool = True\n) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n    H, W = feat_size\n    if has_cls_token:\n        cls_tok, x = x[:, :, :1, :], x[:, :, 1:, :]\n    else:\n        cls_tok = None\n    x = x.reshape(-1, H, W, x.shape[-1]).permute(0, 3, 1, 2).contiguous()\n    return x, cls_tok\n\n\n@register_notrace_function\ndef reshape_post_pool(\n        x,\n        num_heads: int,\n        cls_tok: Optional[torch.Tensor] = None\n) -> Tuple[torch.Tensor, List[int]]:\n    feat_size = [x.shape[2], x.shape[3]]\n    L_pooled = x.shape[2] * x.shape[3]\n    x = x.reshape(-1, num_heads, x.shape[1], L_pooled).transpose(2, 3)\n    if cls_tok is not None:\n        x = torch.cat((cls_tok, x), dim=2)\n    return x, feat_size\n\n\n@register_notrace_function\ndef cal_rel_pos_type(\n        attn: torch.Tensor,\n        q: torch.Tensor,\n        has_cls_token: bool,\n        q_size: List[int],\n        k_size: List[int],\n        rel_pos_h: torch.Tensor,\n        rel_pos_w: torch.Tensor,\n):\n    \"\"\"\n    Spatial Relative Positional Embeddings.\n    \"\"\"\n    sp_idx = 1 if has_cls_token else 0\n    q_h, q_w = q_size\n    k_h, k_w = k_size\n\n    # Scale up rel pos if shapes for q and k are different.\n    q_h_ratio = max(k_h / q_h, 1.0)\n    k_h_ratio = max(q_h / k_h, 1.0)\n    dist_h = (\n            torch.arange(q_h, device=q.device).unsqueeze(-1) * q_h_ratio -\n            torch.arange(k_h, device=q.device).unsqueeze(0) * k_h_ratio\n    )\n    dist_h += (k_h - 1) * k_h_ratio\n    q_w_ratio = max(k_w / q_w, 1.0)\n    k_w_ratio = max(q_w / k_w, 1.0)\n    dist_w = (\n            torch.arange(q_w, device=q.device).unsqueeze(-1) * q_w_ratio -\n            torch.arange(k_w, device=q.device).unsqueeze(0) * k_w_ratio\n    )\n    dist_w += (k_w - 1) * k_w_ratio\n\n    rel_h = rel_pos_h[dist_h.long()]\n    rel_w = rel_pos_w[dist_w.long()]\n\n    B, n_head, q_N, dim = q.shape\n\n    r_q = q[:, :, sp_idx:].reshape(B, n_head, q_h, q_w, dim)\n    rel_h = torch.einsum(\"byhwc,hkc->byhwk\", r_q, rel_h)\n    rel_w = torch.einsum(\"byhwc,wkc->byhwk\", r_q, rel_w)\n\n    attn[:, :, sp_idx:, sp_idx:] = (\n        attn[:, :, sp_idx:, sp_idx:].view(B, -1, q_h, q_w, k_h, k_w)\n        + rel_h.unsqueeze(-1)\n        + rel_w.unsqueeze(-2)\n    ).view(B, -1, q_h * q_w, k_h * k_w)\n\n    return attn\n\n\nclass MultiScaleAttentionPoolFirst(nn.Module):\n    def __init__(\n            self,\n            dim,\n            dim_out,\n            feat_size,\n            num_heads=8,\n            qkv_bias=True,\n            mode=\"conv\",\n            kernel_q=(1, 1),\n            kernel_kv=(1, 1),\n            stride_q=(1, 1),\n            stride_kv=(1, 1),\n            has_cls_token=True,\n            rel_pos_type='spatial',\n            residual_pooling=True,\n            norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        self.dim_out = dim_out\n        self.head_dim = dim_out // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.has_cls_token = has_cls_token\n        padding_q = tuple([int(q // 2) for q in kernel_q])\n        padding_kv = tuple([int(kv // 2) for kv in kernel_kv])\n\n        self.q = nn.Linear(dim, dim_out, bias=qkv_bias)\n        self.k = nn.Linear(dim, dim_out, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim_out, bias=qkv_bias)\n        self.proj = nn.Linear(dim_out, dim_out)\n\n        # Skip pooling with kernel and stride size of (1, 1, 1).\n        if prod(kernel_q) == 1 and prod(stride_q) == 1:\n            kernel_q = None\n        if prod(kernel_kv) == 1 and prod(stride_kv) == 1:\n            kernel_kv = None\n        self.mode = mode\n        self.unshared = mode == 'conv_unshared'\n        self.pool_q, self.pool_k, self.pool_v = None, None, None\n        self.norm_q, self.norm_k, self.norm_v = None, None, None\n        if mode in (\"avg\", \"max\"):\n            pool_op = nn.MaxPool2d if mode == \"max\" else nn.AvgPool2d\n            if kernel_q:\n                self.pool_q = pool_op(kernel_q, stride_q, padding_q)\n            if kernel_kv:\n                self.pool_k = pool_op(kernel_kv, stride_kv, padding_kv)\n                self.pool_v = pool_op(kernel_kv, stride_kv, padding_kv)\n        elif mode == \"conv\" or mode == \"conv_unshared\":\n            dim_conv = dim // num_heads if mode == \"conv\" else dim\n            if kernel_q:\n                self.pool_q = nn.Conv2d(\n                    dim_conv,\n                    dim_conv,\n                    kernel_q,\n                    stride=stride_q,\n                    padding=padding_q,\n                    groups=dim_conv,\n                    bias=False,\n                )\n                self.norm_q = norm_layer(dim_conv)\n            if kernel_kv:\n                self.pool_k = nn.Conv2d(\n                    dim_conv,\n                    dim_conv,\n                    kernel_kv,\n                    stride=stride_kv,\n                    padding=padding_kv,\n                    groups=dim_conv,\n                    bias=False,\n                )\n                self.norm_k = norm_layer(dim_conv)\n                self.pool_v = nn.Conv2d(\n                    dim_conv,\n                    dim_conv,\n                    kernel_kv,\n                    stride=stride_kv,\n                    padding=padding_kv,\n                    groups=dim_conv,\n                    bias=False,\n                )\n                self.norm_v = norm_layer(dim_conv)\n        else:\n            raise NotImplementedError(f\"Unsupported model {mode}\")\n\n        # relative pos embedding\n        self.rel_pos_type = rel_pos_type\n        if self.rel_pos_type == 'spatial':\n            assert feat_size[0] == feat_size[1]\n            size = feat_size[0]\n            q_size = size // stride_q[1] if len(stride_q) > 0 else size\n            kv_size = size // stride_kv[1] if len(stride_kv) > 0 else size\n            rel_sp_dim = 2 * max(q_size, kv_size) - 1\n\n            self.rel_pos_h = nn.Parameter(torch.zeros(rel_sp_dim, self.head_dim))\n            self.rel_pos_w = nn.Parameter(torch.zeros(rel_sp_dim, self.head_dim))\n            trunc_normal_tf_(self.rel_pos_h, std=0.02)\n            trunc_normal_tf_(self.rel_pos_w, std=0.02)\n\n        self.residual_pooling = residual_pooling\n\n    def forward(self, x, feat_size: List[int]):\n        B, N, _ = x.shape\n\n        fold_dim = 1 if self.unshared else self.num_heads\n        x = x.reshape(B, N, fold_dim, -1).permute(0, 2, 1, 3)\n        q = k = v = x\n\n        if self.pool_q is not None:\n            q, q_tok = reshape_pre_pool(q, feat_size, self.has_cls_token)\n            q = self.pool_q(q)\n            q, q_size = reshape_post_pool(q, self.num_heads, q_tok)\n        else:\n            q_size = feat_size\n        if self.norm_q is not None:\n            q = self.norm_q(q)\n\n        if self.pool_k is not None:\n            k, k_tok = reshape_pre_pool(k, feat_size, self.has_cls_token)\n            k = self.pool_k(k)\n            k, k_size = reshape_post_pool(k, self.num_heads, k_tok)\n        else:\n            k_size = feat_size\n        if self.norm_k is not None:\n            k = self.norm_k(k)\n\n        if self.pool_v is not None:\n            v, v_tok = reshape_pre_pool(v, feat_size, self.has_cls_token)\n            v = self.pool_v(v)\n            v, v_size = reshape_post_pool(v, self.num_heads, v_tok)\n        else:\n            v_size = feat_size\n        if self.norm_v is not None:\n            v = self.norm_v(v)\n\n        q_N = q_size[0] * q_size[1] + int(self.has_cls_token)\n        q = q.transpose(1, 2).reshape(B, q_N, -1)\n        q = self.q(q).reshape(B, q_N, self.num_heads, -1).transpose(1, 2)\n\n        k_N = k_size[0] * k_size[1] + int(self.has_cls_token)\n        k = k.transpose(1, 2).reshape(B, k_N, -1)\n        k = self.k(k).reshape(B, k_N, self.num_heads, -1)\n\n        v_N = v_size[0] * v_size[1] + int(self.has_cls_token)\n        v = v.transpose(1, 2).reshape(B, v_N, -1)\n        v = self.v(v).reshape(B, v_N, self.num_heads, -1).transpose(1, 2)\n\n        attn = (q * self.scale) @ k\n        if self.rel_pos_type == 'spatial':\n            attn = cal_rel_pos_type(\n                attn,\n                q,\n                self.has_cls_token,\n                q_size,\n                k_size,\n                self.rel_pos_h,\n                self.rel_pos_w,\n            )\n        attn = attn.softmax(dim=-1)\n        x = attn @ v\n\n        if self.residual_pooling:\n            x = x + q\n\n        x = x.transpose(1, 2).reshape(B, -1, self.dim_out)\n        x = self.proj(x)\n\n        return x, q_size\n\n\nclass MultiScaleAttention(nn.Module):\n    def __init__(\n            self,\n            dim,\n            dim_out,\n            feat_size,\n            num_heads=8,\n            qkv_bias=True,\n            mode=\"conv\",\n            kernel_q=(1, 1),\n            kernel_kv=(1, 1),\n            stride_q=(1, 1),\n            stride_kv=(1, 1),\n            has_cls_token=True,\n            rel_pos_type='spatial',\n            residual_pooling=True,\n            norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        self.dim_out = dim_out\n        self.head_dim = dim_out // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.has_cls_token = has_cls_token\n        padding_q = tuple([int(q // 2) for q in kernel_q])\n        padding_kv = tuple([int(kv // 2) for kv in kernel_kv])\n\n        self.qkv = nn.Linear(dim, dim_out * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim_out, dim_out)\n\n        # Skip pooling with kernel and stride size of (1, 1, 1).\n        if prod(kernel_q) == 1 and prod(stride_q) == 1:\n            kernel_q = None\n        if prod(kernel_kv) == 1 and prod(stride_kv) == 1:\n            kernel_kv = None\n        self.mode = mode\n        self.unshared = mode == 'conv_unshared'\n        self.norm_q, self.norm_k, self.norm_v = None, None, None\n        self.pool_q, self.pool_k, self.pool_v = None, None, None\n        if mode in (\"avg\", \"max\"):\n            pool_op = nn.MaxPool2d if mode == \"max\" else nn.AvgPool2d\n            if kernel_q:\n                self.pool_q = pool_op(kernel_q, stride_q, padding_q)\n            if kernel_kv:\n                self.pool_k = pool_op(kernel_kv, stride_kv, padding_kv)\n                self.pool_v = pool_op(kernel_kv, stride_kv, padding_kv)\n        elif mode == \"conv\" or mode == \"conv_unshared\":\n            dim_conv = dim_out // num_heads if mode == \"conv\" else dim_out\n            if kernel_q:\n                self.pool_q = nn.Conv2d(\n                    dim_conv,\n                    dim_conv,\n                    kernel_q,\n                    stride=stride_q,\n                    padding=padding_q,\n                    groups=dim_conv,\n                    bias=False,\n                )\n                self.norm_q = norm_layer(dim_conv)\n            if kernel_kv:\n                self.pool_k = nn.Conv2d(\n                    dim_conv,\n                    dim_conv,\n                    kernel_kv,\n                    stride=stride_kv,\n                    padding=padding_kv,\n                    groups=dim_conv,\n                    bias=False,\n                )\n                self.norm_k = norm_layer(dim_conv)\n                self.pool_v = nn.Conv2d(\n                    dim_conv,\n                    dim_conv,\n                    kernel_kv,\n                    stride=stride_kv,\n                    padding=padding_kv,\n                    groups=dim_conv,\n                    bias=False,\n                )\n                self.norm_v = norm_layer(dim_conv)\n        else:\n            raise NotImplementedError(f\"Unsupported model {mode}\")\n\n        # relative pos embedding\n        self.rel_pos_type = rel_pos_type\n        if self.rel_pos_type == 'spatial':\n            assert feat_size[0] == feat_size[1]\n            size = feat_size[0]\n            q_size = size // stride_q[1] if len(stride_q) > 0 else size\n            kv_size = size // stride_kv[1] if len(stride_kv) > 0 else size\n            rel_sp_dim = 2 * max(q_size, kv_size) - 1\n\n            self.rel_pos_h = nn.Parameter(torch.zeros(rel_sp_dim, self.head_dim))\n            self.rel_pos_w = nn.Parameter(torch.zeros(rel_sp_dim, self.head_dim))\n            trunc_normal_tf_(self.rel_pos_h, std=0.02)\n            trunc_normal_tf_(self.rel_pos_w, std=0.02)\n\n        self.residual_pooling = residual_pooling\n\n    def forward(self, x, feat_size: List[int]):\n        B, N, _ = x.shape\n\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(dim=0)\n\n        if self.pool_q is not None:\n            q, q_tok = reshape_pre_pool(q, feat_size, self.has_cls_token)\n            q = self.pool_q(q)\n            q, q_size = reshape_post_pool(q, self.num_heads, q_tok)\n        else:\n            q_size = feat_size\n        if self.norm_q is not None:\n            q = self.norm_q(q)\n\n        if self.pool_k is not None:\n            k, k_tok = reshape_pre_pool(k, feat_size, self.has_cls_token)\n            k = self.pool_k(k)\n            k, k_size = reshape_post_pool(k, self.num_heads, k_tok)\n        else:\n            k_size = feat_size\n        if self.norm_k is not None:\n            k = self.norm_k(k)\n\n        if self.pool_v is not None:\n            v, v_tok = reshape_pre_pool(v, feat_size, self.has_cls_token)\n            v = self.pool_v(v)\n            v, _ = reshape_post_pool(v, self.num_heads, v_tok)\n        if self.norm_v is not None:\n            v = self.norm_v(v)\n\n        attn = (q * self.scale) @ k.transpose(-2, -1)\n        if self.rel_pos_type == 'spatial':\n            attn = cal_rel_pos_type(\n                attn,\n                q,\n                self.has_cls_token,\n                q_size,\n                k_size,\n                self.rel_pos_h,\n                self.rel_pos_w,\n            )\n        attn = attn.softmax(dim=-1)\n        x = attn @ v\n\n        if self.residual_pooling:\n            x = x + q\n\n        x = x.transpose(1, 2).reshape(B, -1, self.dim_out)\n        x = self.proj(x)\n\n        return x, q_size\n\n\nclass MultiScaleBlock(nn.Module):\n    def __init__(\n            self,\n            dim,\n            dim_out,\n            num_heads,\n            feat_size,\n            mlp_ratio=4.0,\n            qkv_bias=True,\n            drop_path=0.0,\n            norm_layer=nn.LayerNorm,\n            kernel_q=(1, 1),\n            kernel_kv=(1, 1),\n            stride_q=(1, 1),\n            stride_kv=(1, 1),\n            mode=\"conv\",\n            has_cls_token=True,\n            expand_attn=False,\n            pool_first=False,\n            rel_pos_type='spatial',\n            residual_pooling=True,\n    ):\n        super().__init__()\n        proj_needed = dim != dim_out\n        self.dim = dim\n        self.dim_out = dim_out\n        self.has_cls_token = has_cls_token\n\n        self.norm1 = norm_layer(dim)\n\n        self.shortcut_proj_attn = nn.Linear(dim, dim_out) if proj_needed and expand_attn else None\n        if stride_q and prod(stride_q) > 1:\n            kernel_skip = [s + 1 if s > 1 else s for s in stride_q]\n            stride_skip = stride_q\n            padding_skip = [int(skip // 2) for skip in kernel_skip]\n            self.shortcut_pool_attn = nn.MaxPool2d(kernel_skip, stride_skip, padding_skip)\n        else:\n            self.shortcut_pool_attn = None\n\n        att_dim = dim_out if expand_attn else dim\n        attn_layer = MultiScaleAttentionPoolFirst if pool_first else MultiScaleAttention\n        self.attn = attn_layer(\n            dim,\n            att_dim,\n            num_heads=num_heads,\n            feat_size=feat_size,\n            qkv_bias=qkv_bias,\n            kernel_q=kernel_q,\n            kernel_kv=kernel_kv,\n            stride_q=stride_q,\n            stride_kv=stride_kv,\n            norm_layer=norm_layer,\n            has_cls_token=has_cls_token,\n            mode=mode,\n            rel_pos_type=rel_pos_type,\n            residual_pooling=residual_pooling,\n        )\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n        self.norm2 = norm_layer(att_dim)\n        mlp_dim_out = dim_out\n        self.shortcut_proj_mlp = nn.Linear(dim, dim_out) if proj_needed and not expand_attn else None\n        self.mlp = Mlp(\n            in_features=att_dim,\n            hidden_features=int(att_dim * mlp_ratio),\n            out_features=mlp_dim_out,\n        )\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n    def _shortcut_pool(self, x, feat_size: List[int]):\n        if self.shortcut_pool_attn is None:\n            return x\n        if self.has_cls_token:\n            cls_tok, x = x[:, :1, :], x[:, 1:, :]\n        else:\n            cls_tok = None\n        B, L, C = x.shape\n        H, W = feat_size\n        x = x.reshape(B, H, W, C).permute(0, 3, 1, 2).contiguous()\n        x = self.shortcut_pool_attn(x)\n        x = x.reshape(B, C, -1).transpose(1, 2)\n        if cls_tok is not None:\n            x = torch.cat((cls_tok, x), dim=1)\n        return x\n\n    def forward(self, x, feat_size: List[int]):\n        x_norm = self.norm1(x)\n        # NOTE as per the original impl, this seems odd, but shortcut uses un-normalized input if no proj\n        x_shortcut = x if self.shortcut_proj_attn is None else self.shortcut_proj_attn(x_norm)\n        x_shortcut = self._shortcut_pool(x_shortcut, feat_size)\n        x, feat_size_new = self.attn(x_norm, feat_size)\n        x = x_shortcut + self.drop_path1(x)\n\n        x_norm = self.norm2(x)\n        x_shortcut = x if self.shortcut_proj_mlp is None else self.shortcut_proj_mlp(x_norm)\n        x = x_shortcut + self.drop_path2(self.mlp(x_norm))\n        return x, feat_size_new\n\n\nclass MultiScaleVitStage(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            dim_out,\n            depth,\n            num_heads,\n            feat_size,\n            mlp_ratio=4.0,\n            qkv_bias=True,\n            mode=\"conv\",\n            kernel_q=(1, 1),\n            kernel_kv=(1, 1),\n            stride_q=(1, 1),\n            stride_kv=(1, 1),\n            has_cls_token=True,\n            expand_attn=False,\n            pool_first=False,\n            rel_pos_type='spatial',\n            residual_pooling=True,\n            norm_layer=nn.LayerNorm,\n            drop_path=0.0,\n    ):\n        super().__init__()\n        self.grad_checkpointing = False\n\n        self.blocks = nn.ModuleList()\n        if expand_attn:\n            out_dims = (dim_out,) * depth\n        else:\n            out_dims = (dim,) * (depth - 1) + (dim_out,)\n\n        for i in range(depth):\n            attention_block = MultiScaleBlock(\n                dim=dim,\n                dim_out=out_dims[i],\n                num_heads=num_heads,\n                feat_size=feat_size,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                kernel_q=kernel_q,\n                kernel_kv=kernel_kv,\n                stride_q=stride_q if i == 0 else (1, 1),\n                stride_kv=stride_kv,\n                mode=mode,\n                has_cls_token=has_cls_token,\n                pool_first=pool_first,\n                rel_pos_type=rel_pos_type,\n                residual_pooling=residual_pooling,\n                expand_attn=expand_attn,\n                norm_layer=norm_layer,\n                drop_path=drop_path[i] if isinstance(drop_path, (list, tuple)) else drop_path,\n            )\n            dim = out_dims[i]\n            self.blocks.append(attention_block)\n            if i == 0:\n                feat_size = tuple([size // stride for size, stride in zip(feat_size, stride_q)])\n\n        self.feat_size = feat_size\n\n    def forward(self, x, feat_size: List[int]):\n        for blk in self.blocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x, feat_size = checkpoint.checkpoint(blk, x, feat_size)\n            else:\n                x, feat_size = blk(x, feat_size)\n        return x, feat_size\n\n\nclass MultiScaleVit(nn.Module):\n    \"\"\"\n    Improved Multiscale Vision Transformers for Classification and Detection\n    Yanghao Li*, Chao-Yuan Wu*, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik,\n        Christoph Feichtenhofer*\n    https://arxiv.org/abs/2112.01526\n\n    Multiscale Vision Transformers\n    Haoqi Fan*, Bo Xiong*, Karttikeya Mangalam*, Yanghao Li*, Zhicheng Yan, Jitendra Malik,\n        Christoph Feichtenhofer*\n    https://arxiv.org/abs/2104.11227\n    \"\"\"\n\n    def __init__(\n            self,\n            cfg: MultiScaleVitCfg,\n            img_size: Tuple[int, int] = (224, 224),\n            in_chans: int = 3,\n            global_pool: Optional[str] = None,\n            num_classes: int = 1000,\n            drop_path_rate: float = 0.,\n            drop_rate: float = 0.,\n    ):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        norm_layer = partial(get_norm_layer(cfg.norm_layer), eps=cfg.norm_eps)\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        if global_pool is None:\n            global_pool = 'token' if cfg.use_cls_token else 'avg'\n        self.global_pool = global_pool\n        self.depths = tuple(cfg.depths)\n        self.expand_attn = cfg.expand_attn\n\n        embed_dim = cfg.embed_dim[0]\n        self.patch_embed = PatchEmbed(\n            dim_in=in_chans,\n            dim_out=embed_dim,\n            kernel=cfg.patch_kernel,\n            stride=cfg.patch_stride,\n            padding=cfg.patch_padding,\n        )\n        patch_dims = (img_size[0] // cfg.patch_stride[0], img_size[1] // cfg.patch_stride[1])\n        num_patches = prod(patch_dims)\n\n        if cfg.use_cls_token:\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n            self.num_prefix_tokens = 1\n            pos_embed_dim = num_patches + 1\n        else:\n            self.num_prefix_tokens = 0\n            self.cls_token = None\n            pos_embed_dim = num_patches\n\n        if cfg.use_abs_pos:\n            self.pos_embed = nn.Parameter(torch.zeros(1, pos_embed_dim, embed_dim))\n        else:\n            self.pos_embed = None\n\n        num_stages = len(cfg.embed_dim)\n        feat_size = patch_dims\n        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(cfg.depths)).split(cfg.depths)]\n        self.stages = nn.ModuleList()\n        for i in range(num_stages):\n            if cfg.expand_attn:\n                dim_out = cfg.embed_dim[i]\n            else:\n                dim_out = cfg.embed_dim[min(i + 1, num_stages - 1)]\n            stage = MultiScaleVitStage(\n                dim=embed_dim,\n                dim_out=dim_out,\n                depth=cfg.depths[i],\n                num_heads=cfg.num_heads[i],\n                feat_size=feat_size,\n                mlp_ratio=cfg.mlp_ratio,\n                qkv_bias=cfg.qkv_bias,\n                mode=cfg.mode,\n                pool_first=cfg.pool_first,\n                expand_attn=cfg.expand_attn,\n                kernel_q=cfg.kernel_qkv,\n                kernel_kv=cfg.kernel_qkv,\n                stride_q=cfg.stride_q[i],\n                stride_kv=cfg.stride_kv[i],\n                has_cls_token=cfg.use_cls_token,\n                rel_pos_type=cfg.rel_pos_type,\n                residual_pooling=cfg.residual_pooling,\n                norm_layer=norm_layer,\n                drop_path=dpr[i],\n            )\n            embed_dim = dim_out\n            feat_size = stage.feat_size\n            self.stages.append(stage)\n\n        self.num_features = embed_dim\n        self.norm = norm_layer(embed_dim)\n        self.head = nn.Sequential(OrderedDict([\n            ('drop', nn.Dropout(self.drop_rate)),\n            ('fc', nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity())\n        ]))\n\n        if self.pos_embed is not None:\n            trunc_normal_tf_(self.pos_embed, std=0.02)\n        if self.cls_token is not None:\n            trunc_normal_tf_(self.cls_token, std=0.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_tf_(m.weight, std=0.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {k for k, _ in self.named_parameters()\n                if any(n in k for n in [\"pos_embed\", \"rel_pos_h\", \"rel_pos_w\", \"cls_token\"])}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^patch_embed',  # stem and embed\n            blocks=[(r'^stages\\.(\\d+)', None), (r'^norm', (99999,))]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        for s in self.stages:\n            s.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = nn.Sequential(OrderedDict([\n            ('drop', nn.Dropout(self.drop_rate)),\n            ('fc', nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity())\n        ]))\n\n    def forward_features(self, x):\n        x, feat_size = self.patch_embed(x)\n        B, N, C = x.shape\n\n        if self.cls_token is not None:\n            cls_tokens = self.cls_token.expand(B, -1, -1)\n            x = torch.cat((cls_tokens, x), dim=1)\n\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n\n        for stage in self.stages:\n            x, feat_size = stage(x, feat_size)\n\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool:\n            if self.global_pool == 'avg':\n                x = x[:, self.num_prefix_tokens:].mean(1)\n            else:\n                x = x[:, 0]\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    if 'stages.0.blocks.0.norm1.weight' in state_dict:\n        return state_dict\n\n    import re\n    if 'model_state' in state_dict:\n        state_dict = state_dict['model_state']\n\n    depths = getattr(model, 'depths', None)\n    expand_attn = getattr(model, 'expand_attn', True)\n    assert depths is not None, 'model requires depth attribute to remap checkpoints'\n    depth_map = {}\n    block_idx = 0\n    for stage_idx, d in enumerate(depths):\n        depth_map.update({i: (stage_idx, i - block_idx) for i in range(block_idx, block_idx + d)})\n        block_idx += d\n\n    out_dict = {}\n    for k, v in state_dict.items():\n        k = re.sub(\n            r'blocks\\.(\\d+)',\n            lambda x: f'stages.{depth_map[int(x.group(1))][0]}.blocks.{depth_map[int(x.group(1))][1]}',\n            k)\n\n        if expand_attn:\n            k = re.sub(r'stages\\.(\\d+).blocks\\.(\\d+).proj', f'stages.\\\\1.blocks.\\\\2.shortcut_proj_attn', k)\n        else:\n            k = re.sub(r'stages\\.(\\d+).blocks\\.(\\d+).proj', f'stages.\\\\1.blocks.\\\\2.shortcut_proj_mlp', k)\n        if 'head' in k:\n            k = k.replace('head.projection', 'head.fc')\n        out_dict[k] = v\n\n    # for k, v in state_dict.items():\n    #     if model.pos_embed is not None and k == 'pos_embed' and v.shape[1] != model.pos_embed.shape[1]:\n    #         # To resize pos embedding when using model at different size from pretrained weights\n    #         v = resize_pos_embed(\n    #             v,\n    #             model.pos_embed,\n    #             0 if getattr(model, 'no_embed_class') else getattr(model, 'num_prefix_tokens', 1),\n    #             model.patch_embed.grid_size\n    #         )\n\n    return out_dict\n\n\nmodel_cfgs = dict(\n    mvitv2_tiny=MultiScaleVitCfg(\n        depths=(1, 2, 5, 2),\n    ),\n    mvitv2_small=MultiScaleVitCfg(\n        depths=(1, 2, 11, 2),\n    ),\n    mvitv2_base=MultiScaleVitCfg(\n        depths=(2, 3, 16, 3),\n    ),\n    mvitv2_large=MultiScaleVitCfg(\n        depths=(2, 6, 36, 4),\n        embed_dim=144,\n        num_heads=2,\n        expand_attn=False,\n    ),\n\n    mvitv2_small_cls=MultiScaleVitCfg(\n        depths=(1, 2, 11, 2),\n        use_cls_token=True,\n    ),\n    mvitv2_base_cls=MultiScaleVitCfg(\n        depths=(2, 3, 16, 3),\n        use_cls_token=True,\n    ),\n    mvitv2_large_cls=MultiScaleVitCfg(\n        depths=(2, 6, 36, 4),\n        embed_dim=144,\n        num_heads=2,\n        use_cls_token=True,\n        expand_attn=True,\n    ),\n    mvitv2_huge_cls=MultiScaleVitCfg(\n        depths=(4, 8, 60, 8),\n        embed_dim=192,\n        num_heads=3,\n        use_cls_token=True,\n        expand_attn=True,\n    ),\n)\n\n\ndef _create_mvitv2(variant, cfg_variant=None, pretrained=False, **kwargs):\n    return build_model_with_cfg(\n        MultiScaleVit,\n        variant,\n        pretrained,\n        model_cfg=model_cfgs[variant] if not cfg_variant else model_cfgs[cfg_variant],\n        pretrained_filter_fn=checkpoint_filter_fn,\n        feature_cfg=dict(flatten_sequential=True),\n        **kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'patch_embed.proj', 'classifier': 'head.fc',\n        'fixed_input_size': True,\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'mvitv2_tiny.fb_in1k': _cfg(url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_T_in1k.pyth'),\n    'mvitv2_small.fb_in1k': _cfg(url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_S_in1k.pyth'),\n    'mvitv2_base.fb_in1k': _cfg(url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_B_in1k.pyth'),\n    'mvitv2_large.fb_in1k': _cfg(url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_L_in1k.pyth'),\n\n    'mvitv2_small_cls': _cfg(url=''),\n    'mvitv2_base_cls.fb_inw21k': _cfg(\n        url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_B_in21k.pyth',\n        num_classes=19168),\n    'mvitv2_large_cls.fb_inw21k': _cfg(\n        url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_L_in21k.pyth',\n        num_classes=19168),\n    'mvitv2_huge_cls.fb_inw21k': _cfg(\n        url='https://dl.fbaipublicfiles.com/mvit/mvitv2_models/MViTv2_H_in21k.pyth',\n        num_classes=19168),\n})\n\n\n@register_model\ndef mvitv2_tiny(pretrained=False, **kwargs) -> MultiScaleVit:\n    return _create_mvitv2('mvitv2_tiny', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef mvitv2_small(pretrained=False, **kwargs) -> MultiScaleVit:\n    return _create_mvitv2('mvitv2_small', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef mvitv2_base(pretrained=False, **kwargs) -> MultiScaleVit:\n    return _create_mvitv2('mvitv2_base', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef mvitv2_large(pretrained=False, **kwargs) -> MultiScaleVit:\n    return _create_mvitv2('mvitv2_large', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef mvitv2_small_cls(pretrained=False, **kwargs) -> MultiScaleVit:\n    return _create_mvitv2('mvitv2_small_cls', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef mvitv2_base_cls(pretrained=False, **kwargs) -> MultiScaleVit:\n    return _create_mvitv2('mvitv2_base_cls', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef mvitv2_large_cls(pretrained=False, **kwargs) -> MultiScaleVit:\n    return _create_mvitv2('mvitv2_large_cls', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef mvitv2_huge_cls(pretrained=False, **kwargs) -> MultiScaleVit:\n    return _create_mvitv2('mvitv2_huge_cls', pretrained=pretrained, **kwargs)\n",
  "\"\"\" CrossViT Model\n\n@inproceedings{\n    chen2021crossvit,\n    title={{CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification}},\n    author={Chun-Fu (Richard) Chen and Quanfu Fan and Rameswar Panda},\n    booktitle={International Conference on Computer Vision (ICCV)},\n    year={2021}\n}\n\nPaper link: https://arxiv.org/abs/2103.14899\nOriginal code: https://github.com/IBM/CrossViT/blob/main/models/crossvit.py\n\nNOTE: model names have been renamed from originals to represent actual input res all *_224 -> *_240 and *_384 -> *_408\n\nModifications and additions for timm hacked together by / Copyright 2021, Ross Wightman\n\"\"\"\n\n# Copyright IBM All Rights Reserved.\n# SPDX-License-Identifier: Apache-2.0\n\n\n\"\"\"\nModifed from Timm. https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n\n\"\"\"\nfrom functools import partial\nfrom typing import List\nfrom typing import Tuple\n\nimport torch\nimport torch.hub\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import DropPath, to_2tuple, trunc_normal_, _assert\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_function\nfrom ._registry import register_model, generate_default_cfgs\nfrom .vision_transformer import Block\n\n__all__ = ['CrossVit']  # model_registry will add each entrypoint fn to this\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, multi_conv=False):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n        if multi_conv:\n            if patch_size[0] == 12:\n                self.proj = nn.Sequential(\n                    nn.Conv2d(in_chans, embed_dim // 4, kernel_size=7, stride=4, padding=3),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=3, padding=0),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n                )\n            elif patch_size[0] == 16:\n                self.proj = nn.Sequential(\n                    nn.Conv2d(in_chans, embed_dim // 4, kernel_size=7, stride=4, padding=3),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=2, padding=1),\n                )\n        else:\n            self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        _assert(H == self.img_size[0],\n                f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n        _assert(W == self.img_size[1],\n                f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass CrossAttention(nn.Module):\n    def __init__(\n            self,\n            dim,\n            num_heads=8,\n            qkv_bias=False,\n            attn_drop=0.,\n            proj_drop=0.,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = head_dim ** -0.5\n\n        self.wq = nn.Linear(dim, dim, bias=qkv_bias)\n        self.wk = nn.Linear(dim, dim, bias=qkv_bias)\n        self.wv = nn.Linear(dim, dim, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        # B1C -> B1H(C/H) -> BH1(C/H)\n        q = self.wq(x[:, 0:1, ...]).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        # BNC -> BNH(C/H) -> BHN(C/H)\n        k = self.wk(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        # BNC -> BNH(C/H) -> BHN(C/H)\n        v = self.wv(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale  # BH1(C/H) @ BH(C/H)N -> BH1N\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, 1, C)  # (BH1N @ BHN(C/H)) -> BH1(C/H) -> B1H(C/H) -> B1C\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass CrossAttentionBlock(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = CrossAttention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n        )\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        x = x[:, 0:1, ...] + self.drop_path(self.attn(self.norm1(x)))\n        return x\n\n\nclass MultiScaleBlock(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            patches,\n            depth,\n            num_heads,\n            mlp_ratio,\n            qkv_bias=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n\n        num_branches = len(dim)\n        self.num_branches = num_branches\n        # different branch could have different embedding size, the first one is the base\n        self.blocks = nn.ModuleList()\n        for d in range(num_branches):\n            tmp = []\n            for i in range(depth[d]):\n                tmp.append(Block(\n                    dim=dim[d],\n                    num_heads=num_heads[d],\n                    mlp_ratio=mlp_ratio[d],\n                    qkv_bias=qkv_bias,\n                    proj_drop=proj_drop,\n                    attn_drop=attn_drop,\n                    drop_path=drop_path[i],\n                    norm_layer=norm_layer,\n                ))\n            if len(tmp) != 0:\n                self.blocks.append(nn.Sequential(*tmp))\n\n        if len(self.blocks) == 0:\n            self.blocks = None\n\n        self.projs = nn.ModuleList()\n        for d in range(num_branches):\n            if dim[d] == dim[(d + 1) % num_branches] and False:\n                tmp = [nn.Identity()]\n            else:\n                tmp = [norm_layer(dim[d]), act_layer(), nn.Linear(dim[d], dim[(d + 1) % num_branches])]\n            self.projs.append(nn.Sequential(*tmp))\n\n        self.fusion = nn.ModuleList()\n        for d in range(num_branches):\n            d_ = (d + 1) % num_branches\n            nh = num_heads[d_]\n            if depth[-1] == 0:  # backward capability:\n                self.fusion.append(\n                    CrossAttentionBlock(\n                        dim=dim[d_],\n                        num_heads=nh,\n                        mlp_ratio=mlp_ratio[d],\n                        qkv_bias=qkv_bias,\n                        proj_drop=proj_drop,\n                        attn_drop=attn_drop,\n                        drop_path=drop_path[-1],\n                        norm_layer=norm_layer,\n                    ))\n            else:\n                tmp = []\n                for _ in range(depth[-1]):\n                    tmp.append(CrossAttentionBlock(\n                        dim=dim[d_],\n                        num_heads=nh,\n                        mlp_ratio=mlp_ratio[d],\n                        qkv_bias=qkv_bias,\n                        proj_drop=proj_drop,\n                        attn_drop=attn_drop,\n                        drop_path=drop_path[-1],\n                        norm_layer=norm_layer,\n                    ))\n                self.fusion.append(nn.Sequential(*tmp))\n\n        self.revert_projs = nn.ModuleList()\n        for d in range(num_branches):\n            if dim[(d + 1) % num_branches] == dim[d] and False:\n                tmp = [nn.Identity()]\n            else:\n                tmp = [norm_layer(dim[(d + 1) % num_branches]), act_layer(),\n                       nn.Linear(dim[(d + 1) % num_branches], dim[d])]\n            self.revert_projs.append(nn.Sequential(*tmp))\n\n    def forward(self, x: List[torch.Tensor]) -> List[torch.Tensor]:\n\n        outs_b = []\n        for i, block in enumerate(self.blocks):\n            outs_b.append(block(x[i]))\n\n        # only take the cls token out\n        proj_cls_token = torch.jit.annotate(List[torch.Tensor], [])\n        for i, proj in enumerate(self.projs):\n            proj_cls_token.append(proj(outs_b[i][:, 0:1, ...]))\n\n        # cross attention\n        outs = []\n        for i, (fusion, revert_proj) in enumerate(zip(self.fusion, self.revert_projs)):\n            tmp = torch.cat((proj_cls_token[i], outs_b[(i + 1) % self.num_branches][:, 1:, ...]), dim=1)\n            tmp = fusion(tmp)\n            reverted_proj_cls_token = revert_proj(tmp[:, 0:1, ...])\n            tmp = torch.cat((reverted_proj_cls_token, outs_b[i][:, 1:, ...]), dim=1)\n            outs.append(tmp)\n        return outs\n\n\ndef _compute_num_patches(img_size, patches):\n    return [i[0] // p * i[1] // p for i, p in zip(img_size, patches)]\n\n\n@register_notrace_function\ndef scale_image(x, ss: Tuple[int, int], crop_scale: bool = False):  # annotations for torchscript\n    \"\"\"\n    Pulled out of CrossViT.forward_features to bury conditional logic in a leaf node for FX tracing.\n    Args:\n        x (Tensor): input image\n        ss (tuple[int, int]): height and width to scale to\n        crop_scale (bool): whether to crop instead of interpolate to achieve the desired scale. Defaults to False\n    Returns:\n        Tensor: the \"scaled\" image batch tensor\n    \"\"\"\n    H, W = x.shape[-2:]\n    if H != ss[0] or W != ss[1]:\n        if crop_scale and ss[0] <= H and ss[1] <= W:\n            cu, cl = int(round((H - ss[0]) / 2.)), int(round((W - ss[1]) / 2.))\n            x = x[:, :, cu:cu + ss[0], cl:cl + ss[1]]\n        else:\n            x = torch.nn.functional.interpolate(x, size=ss, mode='bicubic', align_corners=False)\n    return x\n\n\nclass CrossVit(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(\n            self,\n            img_size=224,\n            img_scale=(1.0, 1.0),\n            patch_size=(8, 16),\n            in_chans=3,\n            num_classes=1000,\n            embed_dim=(192, 384),\n            depth=((1, 3, 1), (1, 3, 1), (1, 3, 1)),\n            num_heads=(6, 12),\n            mlp_ratio=(2., 2., 4.),\n            multi_conv=False,\n            crop_scale=False,\n            qkv_bias=True,\n            drop_rate=0.,\n            pos_drop_rate=0.,\n            proj_drop_rate=0.,\n            attn_drop_rate=0.,\n            drop_path_rate=0.,\n            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n            global_pool='token',\n    ):\n        super().__init__()\n        assert global_pool in ('token', 'avg')\n\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.img_size = to_2tuple(img_size)\n        img_scale = to_2tuple(img_scale)\n        self.img_size_scaled = [tuple([int(sj * si) for sj in self.img_size]) for si in img_scale]\n        self.crop_scale = crop_scale  # crop instead of interpolate for scale\n        num_patches = _compute_num_patches(self.img_size_scaled, patch_size)\n        self.num_branches = len(patch_size)\n        self.embed_dim = embed_dim\n        self.num_features = sum(embed_dim)\n        self.patch_embed = nn.ModuleList()\n\n        # hard-coded for torch jit script\n        for i in range(self.num_branches):\n            setattr(self, f'pos_embed_{i}', nn.Parameter(torch.zeros(1, 1 + num_patches[i], embed_dim[i])))\n            setattr(self, f'cls_token_{i}', nn.Parameter(torch.zeros(1, 1, embed_dim[i])))\n\n        for im_s, p, d in zip(self.img_size_scaled, patch_size, embed_dim):\n            self.patch_embed.append(\n                PatchEmbed(\n                    img_size=im_s,\n                    patch_size=p,\n                    in_chans=in_chans,\n                    embed_dim=d,\n                    multi_conv=multi_conv,\n                ))\n\n        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n\n        total_depth = sum([sum(x[-2:]) for x in depth])\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, total_depth)]  # stochastic depth decay rule\n        dpr_ptr = 0\n        self.blocks = nn.ModuleList()\n        for idx, block_cfg in enumerate(depth):\n            curr_depth = max(block_cfg[:-1]) + block_cfg[-1]\n            dpr_ = dpr[dpr_ptr:dpr_ptr + curr_depth]\n            blk = MultiScaleBlock(\n                embed_dim,\n                num_patches,\n                block_cfg,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr_,\n                norm_layer=norm_layer,\n            )\n            dpr_ptr += curr_depth\n            self.blocks.append(blk)\n\n        self.norm = nn.ModuleList([norm_layer(embed_dim[i]) for i in range(self.num_branches)])\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.ModuleList([\n            nn.Linear(embed_dim[i], num_classes) if num_classes > 0 else nn.Identity()\n            for i in range(self.num_branches)])\n\n        for i in range(self.num_branches):\n            trunc_normal_(getattr(self, f'pos_embed_{i}'), std=.02)\n            trunc_normal_(getattr(self, f'cls_token_{i}'), std=.02)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        out = set()\n        for i in range(self.num_branches):\n            out.add(f'cls_token_{i}')\n            pe = getattr(self, f'pos_embed_{i}', None)\n            if pe is not None and pe.requires_grad:\n                out.add(f'pos_embed_{i}')\n        return out\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^cls_token|pos_embed|patch_embed',  # stem and embed\n            blocks=[(r'^blocks\\.(\\d+)', None), (r'^norm', (99999,))]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, 'gradient checkpointing not supported'\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('token', 'avg')\n            self.global_pool = global_pool\n        self.head = nn.ModuleList(\n            [nn.Linear(self.embed_dim[i], num_classes) if num_classes > 0 else nn.Identity() for i in\n             range(self.num_branches)])\n\n    def forward_features(self, x) -> List[torch.Tensor]:\n        B = x.shape[0]\n        xs = []\n        for i, patch_embed in enumerate(self.patch_embed):\n            x_ = x\n            ss = self.img_size_scaled[i]\n            x_ = scale_image(x_, ss, self.crop_scale)\n            x_ = patch_embed(x_)\n            cls_tokens = self.cls_token_0 if i == 0 else self.cls_token_1  # hard-coded for torch jit script\n            cls_tokens = cls_tokens.expand(B, -1, -1)\n            x_ = torch.cat((cls_tokens, x_), dim=1)\n            pos_embed = self.pos_embed_0 if i == 0 else self.pos_embed_1  # hard-coded for torch jit script\n            x_ = x_ + pos_embed\n            x_ = self.pos_drop(x_)\n            xs.append(x_)\n\n        for i, blk in enumerate(self.blocks):\n            xs = blk(xs)\n\n        # NOTE: was before branch token section, move to here to assure all branch token are before layer norm\n        xs = [norm(xs[i]) for i, norm in enumerate(self.norm)]\n        return xs\n\n    def forward_head(self, xs: List[torch.Tensor], pre_logits: bool = False) -> torch.Tensor:\n        xs = [x[:, 1:].mean(dim=1) for x in xs] if self.global_pool == 'avg' else [x[:, 0] for x in xs]\n        xs = [self.head_drop(x) for x in xs]\n        if pre_logits or isinstance(self.head[0], nn.Identity):\n            return torch.cat([x for x in xs], dim=1)\n        return torch.mean(torch.stack([head(xs[i]) for i, head in enumerate(self.head)], dim=0), dim=0)\n\n    def forward(self, x):\n        xs = self.forward_features(x)\n        x = self.forward_head(xs)\n        return x\n\n\ndef _create_crossvit(variant, pretrained=False, **kwargs):\n    if kwargs.get('features_only', None):\n        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n\n    def pretrained_filter_fn(state_dict):\n        new_state_dict = {}\n        for key in state_dict.keys():\n            if 'pos_embed' in key or 'cls_token' in key:\n                new_key = key.replace(\".\", \"_\")\n            else:\n                new_key = key\n            new_state_dict[new_key] = state_dict[key]\n        return new_state_dict\n\n    return build_model_with_cfg(\n        CrossVit,\n        variant,\n        pretrained,\n        pretrained_filter_fn=pretrained_filter_fn,\n        **kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 240, 240), 'pool_size': None, 'crop_pct': 0.875,\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD, 'fixed_input_size': True,\n        'first_conv': ('patch_embed.0.proj', 'patch_embed.1.proj'),\n        'classifier': ('head.0', 'head.1'),\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'crossvit_15_240.in1k': _cfg(hf_hub_id='timm/'),\n    'crossvit_15_dagger_240.in1k': _cfg(\n        hf_hub_id='timm/',\n        first_conv=('patch_embed.0.proj.0', 'patch_embed.1.proj.0'),\n    ),\n    'crossvit_15_dagger_408.in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 408, 408), first_conv=('patch_embed.0.proj.0', 'patch_embed.1.proj.0'), crop_pct=1.0,\n    ),\n    'crossvit_18_240.in1k': _cfg(hf_hub_id='timm/'),\n    'crossvit_18_dagger_240.in1k': _cfg(\n        hf_hub_id='timm/',\n        first_conv=('patch_embed.0.proj.0', 'patch_embed.1.proj.0'),\n    ),\n    'crossvit_18_dagger_408.in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 408, 408), first_conv=('patch_embed.0.proj.0', 'patch_embed.1.proj.0'), crop_pct=1.0,\n    ),\n    'crossvit_9_240.in1k': _cfg(hf_hub_id='timm/'),\n    'crossvit_9_dagger_240.in1k': _cfg(\n        hf_hub_id='timm/',\n        first_conv=('patch_embed.0.proj.0', 'patch_embed.1.proj.0'),\n    ),\n    'crossvit_base_240.in1k': _cfg(hf_hub_id='timm/'),\n    'crossvit_small_240.in1k': _cfg(hf_hub_id='timm/'),\n    'crossvit_tiny_240.in1k': _cfg(hf_hub_id='timm/'),\n})\n\n\n@register_model\ndef crossvit_tiny_240(pretrained=False, **kwargs) -> CrossVit:\n    model_args = dict(\n        img_scale=(1.0, 224/240), patch_size=[12, 16], embed_dim=[96, 192], depth=[[1, 4, 0], [1, 4, 0], [1, 4, 0]],\n        num_heads=[3, 3], mlp_ratio=[4, 4, 1])\n    model = _create_crossvit(variant='crossvit_tiny_240', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef crossvit_small_240(pretrained=False, **kwargs) -> CrossVit:\n    model_args = dict(\n        img_scale=(1.0, 224/240), patch_size=[12, 16], embed_dim=[192, 384], depth=[[1, 4, 0], [1, 4, 0], [1, 4, 0]],\n        num_heads=[6, 6], mlp_ratio=[4, 4, 1])\n    model = _create_crossvit(variant='crossvit_small_240', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef crossvit_base_240(pretrained=False, **kwargs) -> CrossVit:\n    model_args = dict(\n        img_scale=(1.0, 224/240), patch_size=[12, 16], embed_dim=[384, 768], depth=[[1, 4, 0], [1, 4, 0], [1, 4, 0]],\n        num_heads=[12, 12], mlp_ratio=[4, 4, 1])\n    model = _create_crossvit(variant='crossvit_base_240', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef crossvit_9_240(pretrained=False, **kwargs) -> CrossVit:\n    model_args = dict(\n        img_scale=(1.0, 224/240), patch_size=[12, 16], embed_dim=[128, 256], depth=[[1, 3, 0], [1, 3, 0], [1, 3, 0]],\n        num_heads=[4, 4], mlp_ratio=[3, 3, 1])\n    model = _create_crossvit(variant='crossvit_9_240', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef crossvit_15_240(pretrained=False, **kwargs) -> CrossVit:\n    model_args = dict(\n        img_scale=(1.0, 224/240), patch_size=[12, 16], embed_dim=[192, 384], depth=[[1, 5, 0], [1, 5, 0], [1, 5, 0]],\n        num_heads=[6, 6], mlp_ratio=[3, 3, 1])\n    model = _create_crossvit(variant='crossvit_15_240', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef crossvit_18_240(pretrained=False, **kwargs) -> CrossVit:\n    model_args = dict(\n        img_scale=(1.0, 224 / 240), patch_size=[12, 16], embed_dim=[224, 448], depth=[[1, 6, 0], [1, 6, 0], [1, 6, 0]],\n        num_heads=[7, 7], mlp_ratio=[3, 3, 1], **kwargs)\n    model = _create_crossvit(variant='crossvit_18_240', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef crossvit_9_dagger_240(pretrained=False, **kwargs) -> CrossVit:\n    model_args = dict(\n        img_scale=(1.0, 224 / 240), patch_size=[12, 16], embed_dim=[128, 256], depth=[[1, 3, 0], [1, 3, 0], [1, 3, 0]],\n        num_heads=[4, 4], mlp_ratio=[3, 3, 1], multi_conv=True)\n    model = _create_crossvit(variant='crossvit_9_dagger_240', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef crossvit_15_dagger_240(pretrained=False, **kwargs) -> CrossVit:\n    model_args = dict(\n        img_scale=(1.0, 224/240), patch_size=[12, 16], embed_dim=[192, 384], depth=[[1, 5, 0], [1, 5, 0], [1, 5, 0]],\n        num_heads=[6, 6], mlp_ratio=[3, 3, 1], multi_conv=True)\n    model = _create_crossvit(variant='crossvit_15_dagger_240', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef crossvit_15_dagger_408(pretrained=False, **kwargs) -> CrossVit:\n    model_args = dict(\n        img_scale=(1.0, 384/408), patch_size=[12, 16], embed_dim=[192, 384], depth=[[1, 5, 0], [1, 5, 0], [1, 5, 0]],\n        num_heads=[6, 6], mlp_ratio=[3, 3, 1], multi_conv=True)\n    model = _create_crossvit(variant='crossvit_15_dagger_408', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef crossvit_18_dagger_240(pretrained=False, **kwargs) -> CrossVit:\n    model_args = dict(\n        img_scale=(1.0, 224/240), patch_size=[12, 16], embed_dim=[224, 448], depth=[[1, 6, 0], [1, 6, 0], [1, 6, 0]],\n        num_heads=[7, 7], mlp_ratio=[3, 3, 1], multi_conv=True)\n    model = _create_crossvit(variant='crossvit_18_dagger_240', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef crossvit_18_dagger_408(pretrained=False, **kwargs) -> CrossVit:\n    model_args = dict(\n        img_scale=(1.0, 384/408), patch_size=[12, 16], embed_dim=[224, 448], depth=[[1, 6, 0], [1, 6, 0], [1, 6, 0]],\n        num_heads=[7, 7], mlp_ratio=[3, 3, 1], multi_conv=True)\n    model = _create_crossvit(variant='crossvit_18_dagger_408', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n",
  "\"\"\" ConvMixer\n\n\"\"\"\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import SelectAdaptivePool2d\nfrom ._registry import register_model, generate_default_cfgs\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\n\n__all__ = ['ConvMixer']\n\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x):\n        return self.fn(x) + x\n\n\nclass ConvMixer(nn.Module):\n    def __init__(\n            self,\n            dim,\n            depth,\n            kernel_size=9,\n            patch_size=7,\n            in_chans=3,\n            num_classes=1000,\n            global_pool='avg',\n            drop_rate=0.,\n            act_layer=nn.GELU,\n            **kwargs,\n    ):\n        super().__init__()\n        self.num_classes = num_classes\n        self.num_features = dim\n        self.grad_checkpointing = False\n\n        self.stem = nn.Sequential(\n            nn.Conv2d(in_chans, dim, kernel_size=patch_size, stride=patch_size),\n            act_layer(),\n            nn.BatchNorm2d(dim)\n        )\n        self.blocks = nn.Sequential(\n            *[nn.Sequential(\n                    Residual(nn.Sequential(\n                        nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"),\n                        act_layer(),\n                        nn.BatchNorm2d(dim)\n                    )),\n                    nn.Conv2d(dim, dim, kernel_size=1),\n                    act_layer(),\n                    nn.BatchNorm2d(dim)\n            ) for i in range(depth)]\n        )\n        self.pooling = SelectAdaptivePool2d(pool_type=global_pool, flatten=True)\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(stem=r'^stem', blocks=r'^blocks\\.(\\d+)')\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.pooling = SelectAdaptivePool2d(pool_type=global_pool, flatten=True)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n          \n    def forward_features(self, x):\n        x = self.stem(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        x = self.pooling(x)\n        x = self.head_drop(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_convmixer(variant, pretrained=False, **kwargs):\n    return build_model_with_cfg(ConvMixer, variant, pretrained, **kwargs)\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .96, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD, 'classifier': 'head',\n        'first_conv': 'stem.0',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'convmixer_1536_20.in1k': _cfg(hf_hub_id='timm/'),\n    'convmixer_768_32.in1k': _cfg(hf_hub_id='timm/'),\n    'convmixer_1024_20_ks9_p14.in1k': _cfg(hf_hub_id='timm/')\n})\n\n\n\n@register_model\ndef convmixer_1536_20(pretrained=False, **kwargs) -> ConvMixer:\n    model_args = dict(dim=1536, depth=20, kernel_size=9, patch_size=7, **kwargs)\n    return _create_convmixer('convmixer_1536_20', pretrained, **model_args)\n\n\n@register_model\ndef convmixer_768_32(pretrained=False, **kwargs) -> ConvMixer:\n    model_args = dict(dim=768, depth=32, kernel_size=7, patch_size=7, act_layer=nn.ReLU, **kwargs)\n    return _create_convmixer('convmixer_768_32', pretrained, **model_args)\n\n\n@register_model\ndef convmixer_1024_20_ks9_p14(pretrained=False, **kwargs) -> ConvMixer:\n    model_args = dict(dim=1024, depth=20, kernel_size=9, patch_size=14, **kwargs)\n    return _create_convmixer('convmixer_1024_20_ks9_p14', pretrained, **model_args)",
  "\"\"\"\n pnasnet5large implementation grabbed from Cadene's pretrained models\n Additional credit to https://github.com/creafz\n\n https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/pnasnet.py\n\n\"\"\"\nfrom collections import OrderedDict\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.layers import ConvNormAct, create_conv2d, create_pool2d, create_classifier\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['PNASNet5Large']\n\n\nclass SeparableConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=''):\n        super(SeparableConv2d, self).__init__()\n        self.depthwise_conv2d = create_conv2d(\n            in_channels, in_channels, kernel_size=kernel_size,\n            stride=stride, padding=padding, groups=in_channels)\n        self.pointwise_conv2d = create_conv2d(\n            in_channels, out_channels, kernel_size=1, padding=padding)\n\n    def forward(self, x):\n        x = self.depthwise_conv2d(x)\n        x = self.pointwise_conv2d(x)\n        return x\n\n\nclass BranchSeparables(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, stem_cell=False, padding=''):\n        super(BranchSeparables, self).__init__()\n        middle_channels = out_channels if stem_cell else in_channels\n        self.act_1 = nn.ReLU()\n        self.separable_1 = SeparableConv2d(\n            in_channels, middle_channels, kernel_size, stride=stride, padding=padding)\n        self.bn_sep_1 = nn.BatchNorm2d(middle_channels, eps=0.001)\n        self.act_2 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(\n            middle_channels, out_channels, kernel_size, stride=1, padding=padding)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.act_1(x)\n        x = self.separable_1(x)\n        x = self.bn_sep_1(x)\n        x = self.act_2(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass ActConvBn(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=''):\n        super(ActConvBn, self).__init__()\n        self.act = nn.ReLU()\n        self.conv = create_conv2d(\n            in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.act(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass FactorizedReduction(nn.Module):\n\n    def __init__(self, in_channels, out_channels, padding=''):\n        super(FactorizedReduction, self).__init__()\n        self.act = nn.ReLU()\n        self.path_1 = nn.Sequential(OrderedDict([\n            ('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False)),\n            ('conv', create_conv2d(in_channels, out_channels // 2, kernel_size=1, padding=padding)),\n        ]))\n        self.path_2 = nn.Sequential(OrderedDict([\n            ('pad', nn.ZeroPad2d((-1, 1, -1, 1))),  # shift\n            ('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False)),\n            ('conv', create_conv2d(in_channels, out_channels // 2, kernel_size=1, padding=padding)),\n        ]))\n        self.final_path_bn = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.act(x)\n        x_path1 = self.path_1(x)\n        x_path2 = self.path_2(x)\n        out = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n        return out\n\n\nclass CellBase(nn.Module):\n\n    def cell_forward(self, x_left, x_right):\n        x_comb_iter_0_left = self.comb_iter_0_left(x_left)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_right)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_comb_iter_2)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_right)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_left)\n        if self.comb_iter_4_right is not None:\n            x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        else:\n            x_comb_iter_4_right = x_right\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass CellStem0(CellBase):\n\n    def __init__(self, in_chs_left, out_chs_left, in_chs_right, out_chs_right, pad_type=''):\n        super(CellStem0, self).__init__()\n        self.conv_1x1 = ActConvBn(in_chs_right, out_chs_right, kernel_size=1, padding=pad_type)\n\n        self.comb_iter_0_left = BranchSeparables(\n            in_chs_left, out_chs_left, kernel_size=5, stride=2, stem_cell=True, padding=pad_type)\n        self.comb_iter_0_right = nn.Sequential(OrderedDict([\n            ('max_pool', create_pool2d('max', 3, stride=2, padding=pad_type)),\n            ('conv', create_conv2d(in_chs_left, out_chs_left, kernel_size=1, padding=pad_type)),\n            ('bn', nn.BatchNorm2d(out_chs_left, eps=0.001)),\n        ]))\n\n        self.comb_iter_1_left = BranchSeparables(\n            out_chs_right, out_chs_right, kernel_size=7, stride=2, padding=pad_type)\n        self.comb_iter_1_right = create_pool2d('max', 3, stride=2, padding=pad_type)\n\n        self.comb_iter_2_left = BranchSeparables(\n            out_chs_right, out_chs_right, kernel_size=5, stride=2, padding=pad_type)\n        self.comb_iter_2_right = BranchSeparables(\n            out_chs_right, out_chs_right, kernel_size=3, stride=2, padding=pad_type)\n\n        self.comb_iter_3_left = BranchSeparables(\n            out_chs_right, out_chs_right, kernel_size=3, padding=pad_type)\n        self.comb_iter_3_right = create_pool2d('max', 3, stride=2, padding=pad_type)\n\n        self.comb_iter_4_left = BranchSeparables(\n            in_chs_right, out_chs_right, kernel_size=3, stride=2, stem_cell=True, padding=pad_type)\n        self.comb_iter_4_right = ActConvBn(\n            out_chs_right, out_chs_right, kernel_size=1, stride=2, padding=pad_type)\n\n    def forward(self, x_left):\n        x_right = self.conv_1x1(x_left)\n        x_out = self.cell_forward(x_left, x_right)\n        return x_out\n\n\nclass Cell(CellBase):\n\n    def __init__(\n            self,\n            in_chs_left,\n            out_chs_left,\n            in_chs_right,\n            out_chs_right,\n            pad_type='',\n            is_reduction=False,\n            match_prev_layer_dims=False,\n    ):\n        super(Cell, self).__init__()\n\n        # If `is_reduction` is set to `True` stride 2 is used for\n        # convolution and pooling layers to reduce the spatial size of\n        # the output of a cell approximately by a factor of 2.\n        stride = 2 if is_reduction else 1\n\n        # If `match_prev_layer_dimensions` is set to `True`\n        # `FactorizedReduction` is used to reduce the spatial size\n        # of the left input of a cell approximately by a factor of 2.\n        self.match_prev_layer_dimensions = match_prev_layer_dims\n        if match_prev_layer_dims:\n            self.conv_prev_1x1 = FactorizedReduction(in_chs_left, out_chs_left, padding=pad_type)\n        else:\n            self.conv_prev_1x1 = ActConvBn(in_chs_left, out_chs_left, kernel_size=1, padding=pad_type)\n        self.conv_1x1 = ActConvBn(in_chs_right, out_chs_right, kernel_size=1, padding=pad_type)\n\n        self.comb_iter_0_left = BranchSeparables(\n            out_chs_left, out_chs_left, kernel_size=5, stride=stride, padding=pad_type)\n        self.comb_iter_0_right = create_pool2d('max', 3, stride=stride, padding=pad_type)\n\n        self.comb_iter_1_left = BranchSeparables(\n            out_chs_right, out_chs_right, kernel_size=7, stride=stride, padding=pad_type)\n        self.comb_iter_1_right = create_pool2d('max', 3, stride=stride, padding=pad_type)\n\n        self.comb_iter_2_left = BranchSeparables(\n            out_chs_right, out_chs_right, kernel_size=5, stride=stride, padding=pad_type)\n        self.comb_iter_2_right = BranchSeparables(\n            out_chs_right, out_chs_right, kernel_size=3, stride=stride, padding=pad_type)\n\n        self.comb_iter_3_left = BranchSeparables(out_chs_right, out_chs_right, kernel_size=3)\n        self.comb_iter_3_right = create_pool2d('max', 3, stride=stride, padding=pad_type)\n\n        self.comb_iter_4_left = BranchSeparables(\n            out_chs_left, out_chs_left, kernel_size=3, stride=stride, padding=pad_type)\n        if is_reduction:\n            self.comb_iter_4_right = ActConvBn(\n                out_chs_right, out_chs_right, kernel_size=1, stride=stride, padding=pad_type)\n        else:\n            self.comb_iter_4_right = None\n\n    def forward(self, x_left, x_right):\n        x_left = self.conv_prev_1x1(x_left)\n        x_right = self.conv_1x1(x_right)\n        x_out = self.cell_forward(x_left, x_right)\n        return x_out\n\n\nclass PNASNet5Large(nn.Module):\n    def __init__(\n            self,\n            num_classes=1000,\n            in_chans=3,\n            output_stride=32,\n            drop_rate=0.,\n            global_pool='avg',\n            pad_type='',\n    ):\n        super(PNASNet5Large, self).__init__()\n        self.num_classes = num_classes\n        self.num_features = 4320\n        assert output_stride == 32\n\n        self.conv_0 = ConvNormAct(\n            in_chans, 96, kernel_size=3, stride=2, padding=0,\n            norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.1), apply_act=False)\n\n        self.cell_stem_0 = CellStem0(\n            in_chs_left=96, out_chs_left=54, in_chs_right=96, out_chs_right=54, pad_type=pad_type)\n\n        self.cell_stem_1 = Cell(\n            in_chs_left=96, out_chs_left=108, in_chs_right=270, out_chs_right=108, pad_type=pad_type,\n            match_prev_layer_dims=True, is_reduction=True)\n        self.cell_0 = Cell(\n            in_chs_left=270, out_chs_left=216, in_chs_right=540, out_chs_right=216, pad_type=pad_type,\n            match_prev_layer_dims=True)\n        self.cell_1 = Cell(\n            in_chs_left=540, out_chs_left=216, in_chs_right=1080, out_chs_right=216, pad_type=pad_type)\n        self.cell_2 = Cell(\n            in_chs_left=1080, out_chs_left=216, in_chs_right=1080, out_chs_right=216, pad_type=pad_type)\n        self.cell_3 = Cell(\n            in_chs_left=1080, out_chs_left=216, in_chs_right=1080, out_chs_right=216, pad_type=pad_type)\n\n        self.cell_4 = Cell(\n            in_chs_left=1080, out_chs_left=432, in_chs_right=1080, out_chs_right=432, pad_type=pad_type,\n            is_reduction=True)\n        self.cell_5 = Cell(\n            in_chs_left=1080, out_chs_left=432, in_chs_right=2160, out_chs_right=432, pad_type=pad_type,\n            match_prev_layer_dims=True)\n        self.cell_6 = Cell(\n            in_chs_left=2160, out_chs_left=432, in_chs_right=2160, out_chs_right=432, pad_type=pad_type)\n        self.cell_7 = Cell(\n            in_chs_left=2160, out_chs_left=432, in_chs_right=2160, out_chs_right=432, pad_type=pad_type)\n\n        self.cell_8 = Cell(\n            in_chs_left=2160, out_chs_left=864, in_chs_right=2160, out_chs_right=864, pad_type=pad_type,\n            is_reduction=True)\n        self.cell_9 = Cell(\n            in_chs_left=2160, out_chs_left=864, in_chs_right=4320, out_chs_right=864, pad_type=pad_type,\n            match_prev_layer_dims=True)\n        self.cell_10 = Cell(\n            in_chs_left=4320, out_chs_left=864, in_chs_right=4320, out_chs_right=864, pad_type=pad_type)\n        self.cell_11 = Cell(\n            in_chs_left=4320, out_chs_left=864, in_chs_right=4320, out_chs_right=864, pad_type=pad_type)\n        self.act = nn.ReLU()\n        self.feature_info = [\n            dict(num_chs=96, reduction=2, module='conv_0'),\n            dict(num_chs=270, reduction=4, module='cell_stem_1.conv_1x1.act'),\n            dict(num_chs=1080, reduction=8, module='cell_4.conv_1x1.act'),\n            dict(num_chs=2160, reduction=16, module='cell_8.conv_1x1.act'),\n            dict(num_chs=4320, reduction=32, module='act'),\n        ]\n\n        self.global_pool, self.head_drop, self.last_linear = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool, drop_rate=drop_rate)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(stem=r'^conv_0|cell_stem_[01]', blocks=r'^cell_(\\d+)')\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, 'gradient checkpointing not supported'\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.last_linear\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.global_pool, self.last_linear = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        x_conv_0 = self.conv_0(x)\n        x_stem_0 = self.cell_stem_0(x_conv_0)\n        x_stem_1 = self.cell_stem_1(x_conv_0, x_stem_0)\n        x_cell_0 = self.cell_0(x_stem_0, x_stem_1)\n        x_cell_1 = self.cell_1(x_stem_1, x_cell_0)\n        x_cell_2 = self.cell_2(x_cell_0, x_cell_1)\n        x_cell_3 = self.cell_3(x_cell_1, x_cell_2)\n        x_cell_4 = self.cell_4(x_cell_2, x_cell_3)\n        x_cell_5 = self.cell_5(x_cell_3, x_cell_4)\n        x_cell_6 = self.cell_6(x_cell_4, x_cell_5)\n        x_cell_7 = self.cell_7(x_cell_5, x_cell_6)\n        x_cell_8 = self.cell_8(x_cell_6, x_cell_7)\n        x_cell_9 = self.cell_9(x_cell_7, x_cell_8)\n        x_cell_10 = self.cell_10(x_cell_8, x_cell_9)\n        x_cell_11 = self.cell_11(x_cell_9, x_cell_10)\n        x = self.act(x_cell_11)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        x = self.global_pool(x)\n        x = self.head_drop(x)\n        return x if pre_logits else self.last_linear(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_pnasnet(variant, pretrained=False, **kwargs):\n    return build_model_with_cfg(\n        PNASNet5Large,\n        variant,\n        pretrained,\n        feature_cfg=dict(feature_cls='hook', no_rewrite=True),  # not possible to re-write this model\n        **kwargs,\n    )\n\n\ndefault_cfgs = generate_default_cfgs({\n    'pnasnet5large.tf_in1k': {\n        'hf_hub_id': 'timm/',\n        'input_size': (3, 331, 331),\n        'pool_size': (11, 11),\n        'crop_pct': 0.911,\n        'interpolation': 'bicubic',\n        'mean': (0.5, 0.5, 0.5),\n        'std': (0.5, 0.5, 0.5),\n        'num_classes': 1000,\n        'first_conv': 'conv_0.conv',\n        'classifier': 'last_linear',\n    },\n})\n\n\n@register_model\ndef pnasnet5large(pretrained=False, **kwargs) -> PNASNet5Large:\n    r\"\"\"PNASNet-5 model architecture from the\n    `\"Progressive Neural Architecture Search\"\n    <https://arxiv.org/abs/1712.00559>`_ paper.\n    \"\"\"\n    model_kwargs = dict(pad_type='same', **kwargs)\n    return _create_pnasnet('pnasnet5large', pretrained, **model_kwargs)\n",
  "\"\"\" Deep Layer Aggregation and DLA w/ Res2Net\nDLA original adapted from Official Pytorch impl at: https://github.com/ucbdrive/dla\nDLA Paper: `Deep Layer Aggregation` - https://arxiv.org/abs/1707.06484\n\nRes2Net additions from: https://github.com/gasvn/Res2Net/\nRes2Net Paper: `Res2Net: A New Multi-scale Backbone Architecture` - https://arxiv.org/abs/1904.01169\n\"\"\"\nimport math\nfrom typing import List, Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import create_classifier\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['DLA']\n\n\nclass DlaBasic(nn.Module):\n    \"\"\"DLA Basic\"\"\"\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, **_):\n        super(DlaBasic, self).__init__()\n        self.conv1 = nn.Conv2d(\n            inplanes, planes, kernel_size=3,\n            stride=stride, padding=dilation, bias=False, dilation=dilation)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3,\n            stride=1, padding=dilation, bias=False, dilation=dilation)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.stride = stride\n\n    def forward(self, x, shortcut: Optional[torch.Tensor] = None, children: Optional[List[torch.Tensor]] = None):\n        if shortcut is None:\n            shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out += shortcut\n        out = self.relu(out)\n\n        return out\n\n\nclass DlaBottleneck(nn.Module):\n    \"\"\"DLA/DLA-X Bottleneck\"\"\"\n    expansion = 2\n\n    def __init__(self, inplanes, outplanes, stride=1, dilation=1, cardinality=1, base_width=64):\n        super(DlaBottleneck, self).__init__()\n        self.stride = stride\n        mid_planes = int(math.floor(outplanes * (base_width / 64)) * cardinality)\n        mid_planes = mid_planes // self.expansion\n\n        self.conv1 = nn.Conv2d(inplanes, mid_planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_planes)\n        self.conv2 = nn.Conv2d(\n            mid_planes, mid_planes, kernel_size=3,\n            stride=stride, padding=dilation, bias=False, dilation=dilation, groups=cardinality)\n        self.bn2 = nn.BatchNorm2d(mid_planes)\n        self.conv3 = nn.Conv2d(mid_planes, outplanes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(outplanes)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x, shortcut: Optional[torch.Tensor] = None, children: Optional[List[torch.Tensor]] = None):\n        if shortcut is None:\n            shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out += shortcut\n        out = self.relu(out)\n\n        return out\n\n\nclass DlaBottle2neck(nn.Module):\n    \"\"\" Res2Net/Res2NeXT DLA Bottleneck\n    Adapted from https://github.com/gasvn/Res2Net/blob/master/dla.py\n    \"\"\"\n    expansion = 2\n\n    def __init__(self, inplanes, outplanes, stride=1, dilation=1, scale=4, cardinality=8, base_width=4):\n        super(DlaBottle2neck, self).__init__()\n        self.is_first = stride > 1\n        self.scale = scale\n        mid_planes = int(math.floor(outplanes * (base_width / 64)) * cardinality)\n        mid_planes = mid_planes // self.expansion\n        self.width = mid_planes\n\n        self.conv1 = nn.Conv2d(inplanes, mid_planes * scale, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_planes * scale)\n\n        num_scale_convs = max(1, scale - 1)\n        convs = []\n        bns = []\n        for _ in range(num_scale_convs):\n            convs.append(nn.Conv2d(\n                mid_planes, mid_planes, kernel_size=3,\n                stride=stride, padding=dilation, dilation=dilation, groups=cardinality, bias=False))\n            bns.append(nn.BatchNorm2d(mid_planes))\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList(bns)\n        self.pool = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1) if self.is_first else None\n\n        self.conv3 = nn.Conv2d(mid_planes * scale, outplanes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(outplanes)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x, shortcut: Optional[torch.Tensor] = None, children: Optional[List[torch.Tensor]] = None):\n        if shortcut is None:\n            shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        spx = torch.split(out, self.width, 1)\n        spo = []\n        sp = spx[0]  # redundant, for torchscript\n        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):\n            if i == 0 or self.is_first:\n                sp = spx[i]\n            else:\n                sp = sp + spx[i]\n            sp = conv(sp)\n            sp = bn(sp)\n            sp = self.relu(sp)\n            spo.append(sp)\n        if self.scale > 1:\n            if self.pool is not None:  # self.is_first == True, None check for torchscript\n                spo.append(self.pool(spx[-1]))\n            else:\n                spo.append(spx[-1])\n        out = torch.cat(spo, 1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out += shortcut\n        out = self.relu(out)\n\n        return out\n\n\nclass DlaRoot(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, shortcut):\n        super(DlaRoot, self).__init__()\n        self.conv = nn.Conv2d(\n            in_channels, out_channels, 1, stride=1, bias=False, padding=(kernel_size - 1) // 2)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.shortcut = shortcut\n\n    def forward(self, x_children: List[torch.Tensor]):\n        x = self.conv(torch.cat(x_children, 1))\n        x = self.bn(x)\n        if self.shortcut:\n            x += x_children[0]\n        x = self.relu(x)\n\n        return x\n\n\nclass DlaTree(nn.Module):\n    def __init__(\n            self,\n            levels,\n            block,\n            in_channels,\n            out_channels,\n            stride=1,\n            dilation=1,\n            cardinality=1,\n            base_width=64,\n            level_root=False,\n            root_dim=0,\n            root_kernel_size=1,\n            root_shortcut=False,\n    ):\n        super(DlaTree, self).__init__()\n        if root_dim == 0:\n            root_dim = 2 * out_channels\n        if level_root:\n            root_dim += in_channels\n        self.downsample = nn.MaxPool2d(stride, stride=stride) if stride > 1 else nn.Identity()\n        self.project = nn.Identity()\n        cargs = dict(dilation=dilation, cardinality=cardinality, base_width=base_width)\n        if levels == 1:\n            self.tree1 = block(in_channels, out_channels, stride, **cargs)\n            self.tree2 = block(out_channels, out_channels, 1, **cargs)\n            if in_channels != out_channels:\n                # NOTE the official impl/weights have  project layers in levels > 1 case that are never\n                # used, I've moved the project layer here to avoid wasted params but old checkpoints will\n                # need strict=False while loading.\n                self.project = nn.Sequential(\n                    nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n                    nn.BatchNorm2d(out_channels))\n            self.root = DlaRoot(root_dim, out_channels, root_kernel_size, root_shortcut)\n        else:\n            cargs.update(dict(root_kernel_size=root_kernel_size, root_shortcut=root_shortcut))\n            self.tree1 = DlaTree(\n                levels - 1,\n                block,\n                in_channels,\n                out_channels,\n                stride,\n                root_dim=0,\n                **cargs,\n            )\n            self.tree2 = DlaTree(\n                levels - 1,\n                block,\n                out_channels,\n                out_channels,\n                root_dim=root_dim + out_channels,\n                **cargs,\n            )\n            self.root = None\n        self.level_root = level_root\n        self.root_dim = root_dim\n        self.levels = levels\n\n    def forward(self, x, shortcut: Optional[torch.Tensor] = None, children: Optional[List[torch.Tensor]] = None):\n        if children is None:\n            children = []\n        bottom = self.downsample(x)\n        shortcut = self.project(bottom)\n        if self.level_root:\n            children.append(bottom)\n        x1 = self.tree1(x, shortcut)\n        if self.root is not None:  # levels == 1\n            x2 = self.tree2(x1)\n            x = self.root([x2, x1] + children)\n        else:\n            children.append(x1)\n            x = self.tree2(x1, None, children)\n        return x\n\n\nclass DLA(nn.Module):\n    def __init__(\n            self,\n            levels,\n            channels,\n            output_stride=32,\n            num_classes=1000,\n            in_chans=3,\n            global_pool='avg',\n            cardinality=1,\n            base_width=64,\n            block=DlaBottle2neck,\n            shortcut_root=False,\n            drop_rate=0.0,\n    ):\n        super(DLA, self).__init__()\n        self.channels = channels\n        self.num_classes = num_classes\n        self.cardinality = cardinality\n        self.base_width = base_width\n        assert output_stride == 32  # FIXME support dilation\n\n        self.base_layer = nn.Sequential(\n            nn.Conv2d(in_chans, channels[0], kernel_size=7, stride=1, padding=3, bias=False),\n            nn.BatchNorm2d(channels[0]),\n            nn.ReLU(inplace=True),\n        )\n        self.level0 = self._make_conv_level(channels[0], channels[0], levels[0])\n        self.level1 = self._make_conv_level(channels[0], channels[1], levels[1], stride=2)\n        cargs = dict(cardinality=cardinality, base_width=base_width, root_shortcut=shortcut_root)\n        self.level2 = DlaTree(levels[2], block, channels[1], channels[2], 2, level_root=False, **cargs)\n        self.level3 = DlaTree(levels[3], block, channels[2], channels[3], 2, level_root=True, **cargs)\n        self.level4 = DlaTree(levels[4], block, channels[3], channels[4], 2, level_root=True, **cargs)\n        self.level5 = DlaTree(levels[5], block, channels[4], channels[5], 2, level_root=True, **cargs)\n        self.feature_info = [\n            dict(num_chs=channels[0], reduction=1, module='level0'),  # rare to have a meaningful stride 1 level\n            dict(num_chs=channels[1], reduction=2, module='level1'),\n            dict(num_chs=channels[2], reduction=4, module='level2'),\n            dict(num_chs=channels[3], reduction=8, module='level3'),\n            dict(num_chs=channels[4], reduction=16, module='level4'),\n            dict(num_chs=channels[5], reduction=32, module='level5'),\n        ]\n\n        self.num_features = channels[-1]\n        self.global_pool, self.head_drop, self.fc = create_classifier(\n            self.num_features,\n            self.num_classes,\n            pool_type=global_pool,\n            use_conv=True,\n            drop_rate=drop_rate,\n        )\n        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_conv_level(self, inplanes, planes, convs, stride=1, dilation=1):\n        modules = []\n        for i in range(convs):\n            modules.extend([\n                nn.Conv2d(\n                    inplanes, planes, kernel_size=3,\n                    stride=stride if i == 0 else 1,\n                    padding=dilation, bias=False, dilation=dilation),\n                nn.BatchNorm2d(planes),\n                nn.ReLU(inplace=True)])\n            inplanes = planes\n        return nn.Sequential(*modules)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^base_layer',\n            blocks=r'^level(\\d+)' if coarse else [\n                # an unusual arch, this achieves somewhat more granularity without getting super messy\n                (r'^level(\\d+)\\.tree(\\d+)', None),\n                (r'^level(\\d+)\\.root', (2,)),\n                (r'^level(\\d+)', (1,))\n            ]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, 'gradient checkpointing not supported'\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.fc\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.global_pool, self.fc = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool, use_conv=True)\n        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.base_layer(x)\n        x = self.level0(x)\n        x = self.level1(x)\n        x = self.level2(x)\n        x = self.level3(x)\n        x = self.level4(x)\n        x = self.level5(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        x = self.global_pool(x)\n        x = self.head_drop(x)\n        if pre_logits:\n            return self.flatten(x)\n        x = self.fc(x)\n        return self.flatten(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_dla(variant, pretrained=False, **kwargs):\n    return build_model_with_cfg(\n        DLA,\n        variant,\n        pretrained,\n        pretrained_strict=False,\n        feature_cfg=dict(out_indices=(1, 2, 3, 4, 5)),\n        **kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bilinear',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'base_layer.0', 'classifier': 'fc',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'dla34.in1k': _cfg(hf_hub_id='timm/'),\n    'dla46_c.in1k': _cfg(hf_hub_id='timm/'),\n    'dla46x_c.in1k': _cfg(hf_hub_id='timm/'),\n    'dla60x_c.in1k': _cfg(hf_hub_id='timm/'),\n    'dla60.in1k': _cfg(hf_hub_id='timm/'),\n    'dla60x.in1k': _cfg(hf_hub_id='timm/'),\n    'dla102.in1k': _cfg(hf_hub_id='timm/'),\n    'dla102x.in1k': _cfg(hf_hub_id='timm/'),\n    'dla102x2.in1k': _cfg(hf_hub_id='timm/'),\n    'dla169.in1k': _cfg(hf_hub_id='timm/'),\n    'dla60_res2net.in1k': _cfg(hf_hub_id='timm/'),\n    'dla60_res2next.in1k': _cfg(hf_hub_id='timm/'),\n})\n\n\n@register_model\ndef dla60_res2net(pretrained=False, **kwargs) -> DLA:\n    model_args = dict(\n        levels=(1, 1, 1, 2, 3, 1), channels=(16, 32, 128, 256, 512, 1024),\n        block=DlaBottle2neck, cardinality=1, base_width=28)\n    return _create_dla('dla60_res2net', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef dla60_res2next(pretrained=False,**kwargs):\n    model_args = dict(\n        levels=(1, 1, 1, 2, 3, 1), channels=(16, 32, 128, 256, 512, 1024),\n        block=DlaBottle2neck, cardinality=8, base_width=4)\n    return _create_dla('dla60_res2next', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef dla34(pretrained=False, **kwargs) -> DLA:  # DLA-34\n    model_args = dict(\n        levels=[1, 1, 1, 2, 2, 1], channels=[16, 32, 64, 128, 256, 512], block=DlaBasic)\n    return _create_dla('dla34', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef dla46_c(pretrained=False, **kwargs) -> DLA:  # DLA-46-C\n    model_args = dict(\n        levels=[1, 1, 1, 2, 2, 1], channels=[16, 32, 64, 64, 128, 256], block=DlaBottleneck)\n    return _create_dla('dla46_c', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef dla46x_c(pretrained=False, **kwargs) -> DLA:  # DLA-X-46-C\n    model_args = dict(\n        levels=[1, 1, 1, 2, 2, 1], channels=[16, 32, 64, 64, 128, 256],\n        block=DlaBottleneck, cardinality=32, base_width=4)\n    return _create_dla('dla46x_c', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef dla60x_c(pretrained=False, **kwargs) -> DLA:  # DLA-X-60-C\n    model_args = dict(\n        levels=[1, 1, 1, 2, 3, 1], channels=[16, 32, 64, 64, 128, 256],\n        block=DlaBottleneck, cardinality=32, base_width=4)\n    return _create_dla('dla60x_c', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef dla60(pretrained=False, **kwargs) -> DLA:  # DLA-60\n    model_args = dict(\n        levels=[1, 1, 1, 2, 3, 1], channels=[16, 32, 128, 256, 512, 1024],\n        block=DlaBottleneck)\n    return _create_dla('dla60', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef dla60x(pretrained=False, **kwargs) -> DLA:  # DLA-X-60\n    model_args = dict(\n        levels=[1, 1, 1, 2, 3, 1], channels=[16, 32, 128, 256, 512, 1024],\n        block=DlaBottleneck, cardinality=32, base_width=4)\n    return _create_dla('dla60x', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef dla102(pretrained=False, **kwargs) -> DLA:  # DLA-102\n    model_args = dict(\n        levels=[1, 1, 1, 3, 4, 1], channels=[16, 32, 128, 256, 512, 1024],\n        block=DlaBottleneck, shortcut_root=True)\n    return _create_dla('dla102', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef dla102x(pretrained=False, **kwargs) -> DLA:  # DLA-X-102\n    model_args = dict(\n        levels=[1, 1, 1, 3, 4, 1], channels=[16, 32, 128, 256, 512, 1024],\n        block=DlaBottleneck, cardinality=32, base_width=4, shortcut_root=True)\n    return _create_dla('dla102x', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef dla102x2(pretrained=False, **kwargs) -> DLA:  # DLA-X-102 64\n    model_args = dict(\n        levels=[1, 1, 1, 3, 4, 1], channels=[16, 32, 128, 256, 512, 1024],\n        block=DlaBottleneck, cardinality=64, base_width=4, shortcut_root=True)\n    return _create_dla('dla102x2', pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef dla169(pretrained=False, **kwargs) -> DLA:  # DLA-169\n    model_args = dict(\n        levels=[1, 1, 2, 3, 5, 1], channels=[16, 32, 128, 256, 512, 1024],\n        block=DlaBottleneck, shortcut_root=True)\n    return _create_dla('dla169', pretrained, **dict(model_args, **kwargs))\n",
  "\"\"\"\nPoolformer from MetaFormer is Actually What You Need for Vision https://arxiv.org/abs/2111.11418\n\nIdentityFormer, RandFormer, PoolFormerV2, ConvFormer, and CAFormer\nfrom MetaFormer Baselines for Vision https://arxiv.org/abs/2210.13452\n\nAll implemented models support feature extraction and variable input resolution.\n\nOriginal implementation by Weihao Yu et al.,\nadapted for timm by Fredo Guan and Ross Wightman.\n\nAdapted from https://github.com/sail-sg/metaformer, original copyright below\n\"\"\"\n\n# Copyright 2022 Garena Online Private Limited\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom collections import OrderedDict\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.jit import Final\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import trunc_normal_, DropPath, SelectAdaptivePool2d, GroupNorm1, LayerNorm, LayerNorm2d, Mlp, \\\n    use_fused_attn\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import generate_default_cfgs, register_model\n\n__all__ = ['MetaFormer']\n\n\nclass Stem(nn.Module):\n    \"\"\"\n    Stem implemented by a layer of convolution.\n    Conv2d params constant across all models.\n    \"\"\"\n\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            norm_layer=None,\n    ):\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=7,\n            stride=4,\n            padding=2\n        )\n        self.norm = norm_layer(out_channels) if norm_layer else nn.Identity()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.norm(x)\n        return x\n\n\nclass Downsampling(nn.Module):\n    \"\"\"\n    Downsampling implemented by a layer of convolution.\n    \"\"\"\n\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=1,\n            padding=0,\n            norm_layer=None,\n    ):\n        super().__init__()\n        self.norm = norm_layer(in_channels) if norm_layer else nn.Identity()\n        self.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding\n        )\n\n    def forward(self, x):\n        x = self.norm(x)\n        x = self.conv(x)\n        return x\n\n\nclass Scale(nn.Module):\n    \"\"\"\n    Scale vector by element multiplications.\n    \"\"\"\n\n    def __init__(self, dim, init_value=1.0, trainable=True, use_nchw=True):\n        super().__init__()\n        self.shape = (dim, 1, 1) if use_nchw else (dim,)\n        self.scale = nn.Parameter(init_value * torch.ones(dim), requires_grad=trainable)\n\n    def forward(self, x):\n        return x * self.scale.view(self.shape)\n\n\nclass SquaredReLU(nn.Module):\n    \"\"\"\n        Squared ReLU: https://arxiv.org/abs/2109.08668\n    \"\"\"\n\n    def __init__(self, inplace=False):\n        super().__init__()\n        self.relu = nn.ReLU(inplace=inplace)\n\n    def forward(self, x):\n        return torch.square(self.relu(x))\n\n\nclass StarReLU(nn.Module):\n    \"\"\"\n    StarReLU: s * relu(x) ** 2 + b\n    \"\"\"\n\n    def __init__(\n            self,\n            scale_value=1.0,\n            bias_value=0.0,\n            scale_learnable=True,\n            bias_learnable=True,\n            mode=None,\n            inplace=False\n    ):\n        super().__init__()\n        self.inplace = inplace\n        self.relu = nn.ReLU(inplace=inplace)\n        self.scale = nn.Parameter(scale_value * torch.ones(1), requires_grad=scale_learnable)\n        self.bias = nn.Parameter(bias_value * torch.ones(1), requires_grad=bias_learnable)\n\n    def forward(self, x):\n        return self.scale * self.relu(x) ** 2 + self.bias\n\n\nclass Attention(nn.Module):\n    \"\"\"\n    Vanilla self-attention from Transformer: https://arxiv.org/abs/1706.03762.\n    Modified from timm.\n    \"\"\"\n    fused_attn: Final[bool]\n\n    def __init__(\n            self,\n            dim,\n            head_dim=32,\n            num_heads=None,\n            qkv_bias=False,\n            attn_drop=0.,\n            proj_drop=0.,\n            proj_bias=False,\n            **kwargs\n    ):\n        super().__init__()\n\n        self.head_dim = head_dim\n        self.scale = head_dim ** -0.5\n        self.fused_attn = use_fused_attn()\n\n        self.num_heads = num_heads if num_heads else dim // head_dim\n        if self.num_heads == 0:\n            self.num_heads = 1\n\n        self.attention_dim = self.num_heads * self.head_dim\n\n        self.qkv = nn.Linear(dim, self.attention_dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(self.attention_dim, dim, bias=proj_bias)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)\n\n        if self.fused_attn:\n            x = F.scaled_dot_product_attention(\n                q, k, v,\n                dropout_p=self.attn_drop.p,\n            )\n        else:\n            attn = (q @ k.transpose(-2, -1)) * self.scale\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            x = attn @ v\n\n        x = x.transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\n# custom norm modules that disable the bias term, since the original models defs\n# used a custom norm with a weight term but no bias term.\n\nclass GroupNorm1NoBias(GroupNorm1):\n    def __init__(self, num_channels, **kwargs):\n        super().__init__(num_channels, **kwargs)\n        self.eps = kwargs.get('eps', 1e-6)\n        self.bias = None\n\n\nclass LayerNorm2dNoBias(LayerNorm2d):\n    def __init__(self, num_channels, **kwargs):\n        super().__init__(num_channels, **kwargs)\n        self.eps = kwargs.get('eps', 1e-6)\n        self.bias = None\n\n\nclass LayerNormNoBias(nn.LayerNorm):\n    def __init__(self, num_channels, **kwargs):\n        super().__init__(num_channels, **kwargs)\n        self.eps = kwargs.get('eps', 1e-6)\n        self.bias = None\n\n\nclass SepConv(nn.Module):\n    r\"\"\"\n    Inverted separable convolution from MobileNetV2: https://arxiv.org/abs/1801.04381.\n    \"\"\"\n\n    def __init__(\n            self,\n            dim,\n            expansion_ratio=2,\n            act1_layer=StarReLU,\n            act2_layer=nn.Identity,\n            bias=False,\n            kernel_size=7,\n            padding=3,\n            **kwargs\n    ):\n        super().__init__()\n        mid_channels = int(expansion_ratio * dim)\n        self.pwconv1 = nn.Conv2d(dim, mid_channels, kernel_size=1, bias=bias)\n        self.act1 = act1_layer()\n        self.dwconv = nn.Conv2d(\n            mid_channels, mid_channels, kernel_size=kernel_size,\n            padding=padding, groups=mid_channels, bias=bias)  # depthwise conv\n        self.act2 = act2_layer()\n        self.pwconv2 = nn.Conv2d(mid_channels, dim, kernel_size=1, bias=bias)\n\n    def forward(self, x):\n        x = self.pwconv1(x)\n        x = self.act1(x)\n        x = self.dwconv(x)\n        x = self.act2(x)\n        x = self.pwconv2(x)\n        return x\n\n\nclass Pooling(nn.Module):\n    \"\"\"\n    Implementation of pooling for PoolFormer: https://arxiv.org/abs/2111.11418\n    \"\"\"\n\n    def __init__(self, pool_size=3, **kwargs):\n        super().__init__()\n        self.pool = nn.AvgPool2d(\n            pool_size, stride=1, padding=pool_size // 2, count_include_pad=False)\n\n    def forward(self, x):\n        y = self.pool(x)\n        return y - x\n\n\nclass MlpHead(nn.Module):\n    \"\"\" MLP classification head\n    \"\"\"\n\n    def __init__(\n            self,\n            dim,\n            num_classes=1000,\n            mlp_ratio=4,\n            act_layer=SquaredReLU,\n            norm_layer=LayerNorm,\n            drop_rate=0.,\n            bias=True\n    ):\n        super().__init__()\n        hidden_features = int(mlp_ratio * dim)\n        self.fc1 = nn.Linear(dim, hidden_features, bias=bias)\n        self.act = act_layer()\n        self.norm = norm_layer(hidden_features)\n        self.fc2 = nn.Linear(hidden_features, num_classes, bias=bias)\n        self.head_drop = nn.Dropout(drop_rate)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.norm(x)\n        x = self.head_drop(x)\n        x = self.fc2(x)\n        return x\n\n\nclass MetaFormerBlock(nn.Module):\n    \"\"\"\n    Implementation of one MetaFormer block.\n    \"\"\"\n\n    def __init__(\n            self,\n            dim,\n            token_mixer=Pooling,\n            mlp_act=StarReLU,\n            mlp_bias=False,\n            norm_layer=LayerNorm2d,\n            proj_drop=0.,\n            drop_path=0.,\n            use_nchw=True,\n            layer_scale_init_value=None,\n            res_scale_init_value=None,\n            **kwargs\n    ):\n        super().__init__()\n        ls_layer = partial(Scale, dim=dim, init_value=layer_scale_init_value, use_nchw=use_nchw)\n        rs_layer = partial(Scale, dim=dim, init_value=res_scale_init_value, use_nchw=use_nchw)\n\n        self.norm1 = norm_layer(dim)\n        self.token_mixer = token_mixer(dim=dim, proj_drop=proj_drop, **kwargs)\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.layer_scale1 = ls_layer() if layer_scale_init_value is not None else nn.Identity()\n        self.res_scale1 = rs_layer() if res_scale_init_value is not None else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        self.mlp = Mlp(\n            dim,\n            int(4 * dim),\n            act_layer=mlp_act,\n            bias=mlp_bias,\n            drop=proj_drop,\n            use_conv=use_nchw,\n        )\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.layer_scale2 = ls_layer() if layer_scale_init_value is not None else nn.Identity()\n        self.res_scale2 = rs_layer() if res_scale_init_value is not None else nn.Identity()\n\n    def forward(self, x):\n        x = self.res_scale1(x) + \\\n            self.layer_scale1(\n                self.drop_path1(\n                    self.token_mixer(self.norm1(x))\n                )\n            )\n        x = self.res_scale2(x) + \\\n            self.layer_scale2(\n                self.drop_path2(\n                    self.mlp(self.norm2(x))\n                )\n            )\n        return x\n\n\nclass MetaFormerStage(nn.Module):\n\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            depth=2,\n            token_mixer=nn.Identity,\n            mlp_act=StarReLU,\n            mlp_bias=False,\n            downsample_norm=LayerNorm2d,\n            norm_layer=LayerNorm2d,\n            proj_drop=0.,\n            dp_rates=[0.] * 2,\n            layer_scale_init_value=None,\n            res_scale_init_value=None,\n            **kwargs,\n    ):\n        super().__init__()\n\n        self.grad_checkpointing = False\n        self.use_nchw = not issubclass(token_mixer, Attention)\n\n        # don't downsample if in_chs and out_chs are the same\n        self.downsample = nn.Identity() if in_chs == out_chs else Downsampling(\n            in_chs,\n            out_chs,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            norm_layer=downsample_norm,\n        )\n\n        self.blocks = nn.Sequential(*[MetaFormerBlock(\n            dim=out_chs,\n            token_mixer=token_mixer,\n            mlp_act=mlp_act,\n            mlp_bias=mlp_bias,\n            norm_layer=norm_layer,\n            proj_drop=proj_drop,\n            drop_path=dp_rates[i],\n            layer_scale_init_value=layer_scale_init_value,\n            res_scale_init_value=res_scale_init_value,\n            use_nchw=self.use_nchw,\n            **kwargs,\n        ) for i in range(depth)])\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    def forward(self, x: Tensor):\n        x = self.downsample(x)\n        B, C, H, W = x.shape\n\n        if not self.use_nchw:\n            x = x.reshape(B, C, -1).transpose(1, 2)\n\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n\n        if not self.use_nchw:\n            x = x.transpose(1, 2).reshape(B, C, H, W)\n\n        return x\n\n\nclass MetaFormer(nn.Module):\n    r\"\"\" MetaFormer\n        A PyTorch impl of : `MetaFormer Baselines for Vision`  -\n          https://arxiv.org/abs/2210.13452\n\n    Args:\n        in_chans (int): Number of input image channels.\n        num_classes (int): Number of classes for classification head.\n        global_pool: Pooling for classifier head.\n        depths (list or tuple): Number of blocks at each stage.\n        dims (list or tuple): Feature dimension at each stage.\n        token_mixers (list, tuple or token_fcn): Token mixer for each stage.\n        mlp_act: Activation layer for MLP.\n        mlp_bias (boolean): Enable or disable mlp bias term.\n        drop_path_rate (float): Stochastic depth rate.\n        drop_rate (float): Dropout rate.\n        layer_scale_init_values (list, tuple, float or None): Init value for Layer Scale.\n            None means not use the layer scale. Form: https://arxiv.org/abs/2103.17239.\n        res_scale_init_values (list, tuple, float or None): Init value for res Scale on residual connections.\n            None means not use the res scale. From: https://arxiv.org/abs/2110.09456.\n        downsample_norm (nn.Module): Norm layer used in stem and downsampling layers.\n        norm_layers (list, tuple or norm_fcn): Norm layers for each stage.\n        output_norm: Norm layer before classifier head.\n        use_mlp_head: Use MLP classification head.\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chans=3,\n            num_classes=1000,\n            global_pool='avg',\n            depths=(2, 2, 6, 2),\n            dims=(64, 128, 320, 512),\n            token_mixers=Pooling,\n            mlp_act=StarReLU,\n            mlp_bias=False,\n            drop_path_rate=0.,\n            proj_drop_rate=0.,\n            drop_rate=0.0,\n            layer_scale_init_values=None,\n            res_scale_init_values=(None, None, 1.0, 1.0),\n            downsample_norm=LayerNorm2dNoBias,\n            norm_layers=LayerNorm2dNoBias,\n            output_norm=LayerNorm2d,\n            use_mlp_head=True,\n            **kwargs,\n    ):\n        super().__init__()\n        self.num_classes = num_classes\n        self.num_features = dims[-1]\n        self.drop_rate = drop_rate\n        self.use_mlp_head = use_mlp_head\n        self.num_stages = len(depths)\n\n        # convert everything to lists if they aren't indexable\n        if not isinstance(depths, (list, tuple)):\n            depths = [depths]  # it means the model has only one stage\n        if not isinstance(dims, (list, tuple)):\n            dims = [dims]\n        if not isinstance(token_mixers, (list, tuple)):\n            token_mixers = [token_mixers] * self.num_stages\n        if not isinstance(norm_layers, (list, tuple)):\n            norm_layers = [norm_layers] * self.num_stages\n        if not isinstance(layer_scale_init_values, (list, tuple)):\n            layer_scale_init_values = [layer_scale_init_values] * self.num_stages\n        if not isinstance(res_scale_init_values, (list, tuple)):\n            res_scale_init_values = [res_scale_init_values] * self.num_stages\n\n        self.grad_checkpointing = False\n        self.feature_info = []\n\n        self.stem = Stem(\n            in_chans,\n            dims[0],\n            norm_layer=downsample_norm\n        )\n\n        stages = []\n        prev_dim = dims[0]\n        dp_rates = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n        for i in range(self.num_stages):\n            stages += [MetaFormerStage(\n                prev_dim,\n                dims[i],\n                depth=depths[i],\n                token_mixer=token_mixers[i],\n                mlp_act=mlp_act,\n                mlp_bias=mlp_bias,\n                proj_drop=proj_drop_rate,\n                dp_rates=dp_rates[i],\n                layer_scale_init_value=layer_scale_init_values[i],\n                res_scale_init_value=res_scale_init_values[i],\n                downsample_norm=downsample_norm,\n                norm_layer=norm_layers[i],\n                **kwargs,\n            )]\n            prev_dim = dims[i]\n            self.feature_info += [dict(num_chs=dims[i], reduction=2, module=f'stages.{i}')]\n\n        self.stages = nn.Sequential(*stages)\n\n        # if using MlpHead, dropout is handled by MlpHead\n        if num_classes > 0:\n            if self.use_mlp_head:\n                final = MlpHead(self.num_features, num_classes, drop_rate=self.drop_rate)\n            else:\n                final = nn.Linear(self.num_features, num_classes)\n        else:\n            final = nn.Identity()\n\n        self.head = nn.Sequential(OrderedDict([\n            ('global_pool', SelectAdaptivePool2d(pool_type=global_pool)),\n            ('norm', output_norm(self.num_features)),\n            ('flatten', nn.Flatten(1) if global_pool else nn.Identity()),\n            ('drop', nn.Dropout(drop_rate) if self.use_mlp_head else nn.Identity()),\n            ('fc', final)\n        ]))\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n        for stage in self.stages:\n            stage.set_grad_checkpointing(enable=enable)\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes=0, global_pool=None):\n        if global_pool is not None:\n            self.head.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n            self.head.flatten = nn.Flatten(1) if global_pool else nn.Identity()\n        if num_classes > 0:\n            if self.use_mlp_head:\n                final = MlpHead(self.num_features, num_classes, drop_rate=self.drop_rate)\n            else:\n                final = nn.Linear(self.num_features, num_classes)\n        else:\n            final = nn.Identity()\n        self.head.fc = final\n\n    def forward_head(self, x: Tensor, pre_logits: bool = False):\n        # NOTE nn.Sequential in head broken down since can't call head[:-1](x) in torchscript :(\n        x = self.head.global_pool(x)\n        x = self.head.norm(x)\n        x = self.head.flatten(x)\n        x = self.head.drop(x)\n        return x if pre_logits else self.head.fc(x)\n\n    def forward_features(self, x: Tensor):\n        x = self.stem(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.stages, x)\n        else:\n            x = self.stages(x)\n        return x\n\n    def forward(self, x: Tensor):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\n# this works but it's long and breaks backwards compatability with weights from the poolformer-only impl\ndef checkpoint_filter_fn(state_dict, model):\n    if 'stem.conv.weight' in state_dict:\n        return state_dict\n\n    import re\n    out_dict = {}\n    is_poolformerv1 = 'network.0.0.mlp.fc1.weight' in state_dict\n    model_state_dict = model.state_dict()\n    for k, v in state_dict.items():\n        if is_poolformerv1:\n            k = re.sub(r'layer_scale_([0-9]+)', r'layer_scale\\1.scale', k)\n            k = k.replace('network.1', 'downsample_layers.1')\n            k = k.replace('network.3', 'downsample_layers.2')\n            k = k.replace('network.5', 'downsample_layers.3')\n            k = k.replace('network.2', 'network.1')\n            k = k.replace('network.4', 'network.2')\n            k = k.replace('network.6', 'network.3')\n            k = k.replace('network', 'stages')\n\n        k = re.sub(r'downsample_layers.([0-9]+)', r'stages.\\1.downsample', k)\n        k = k.replace('downsample.proj', 'downsample.conv')\n        k = k.replace('patch_embed.proj', 'patch_embed.conv')\n        k = re.sub(r'([0-9]+).([0-9]+)', r'\\1.blocks.\\2', k)\n        k = k.replace('stages.0.downsample', 'patch_embed')\n        k = k.replace('patch_embed', 'stem')\n        k = k.replace('post_norm', 'norm')\n        k = k.replace('pre_norm', 'norm')\n        k = re.sub(r'^head', 'head.fc', k)\n        k = re.sub(r'^norm', 'head.norm', k)\n\n        if v.shape != model_state_dict[k] and v.numel() == model_state_dict[k].numel():\n            v = v.reshape(model_state_dict[k].shape)\n\n        out_dict[k] = v\n    return out_dict\n\n\ndef _create_metaformer(variant, pretrained=False, **kwargs):\n    default_out_indices = tuple(i for i, _ in enumerate(kwargs.get('depths', (2, 2, 6, 2))))\n    out_indices = kwargs.pop('out_indices', default_out_indices)\n\n    model = build_model_with_cfg(\n        MetaFormer,\n        variant,\n        pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),\n        **kwargs,\n    )\n\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 1.0, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'classifier': 'head.fc', 'first_conv': 'stem.conv',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'poolformer_s12.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.9),\n    'poolformer_s24.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.9),\n    'poolformer_s36.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.9),\n    'poolformer_m36.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.95),\n    'poolformer_m48.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.95),\n\n    'poolformerv2_s12.sail_in1k': _cfg(hf_hub_id='timm/'),\n    'poolformerv2_s24.sail_in1k': _cfg(hf_hub_id='timm/'),\n    'poolformerv2_s36.sail_in1k': _cfg(hf_hub_id='timm/'),\n    'poolformerv2_m36.sail_in1k': _cfg(hf_hub_id='timm/'),\n    'poolformerv2_m48.sail_in1k': _cfg(hf_hub_id='timm/'),\n\n    'convformer_s18.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2'),\n    'convformer_s18.sail_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', input_size=(3, 384, 384), pool_size=(12, 12)),\n    'convformer_s18.sail_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2'),\n    'convformer_s18.sail_in22k_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', input_size=(3, 384, 384), pool_size=(12, 12)),\n    'convformer_s18.sail_in22k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', num_classes=21841),\n\n    'convformer_s36.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2'),\n    'convformer_s36.sail_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', input_size=(3, 384, 384), pool_size=(12, 12)),\n    'convformer_s36.sail_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2'),\n    'convformer_s36.sail_in22k_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', input_size=(3, 384, 384), pool_size=(12, 12)),\n    'convformer_s36.sail_in22k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', num_classes=21841),\n\n    'convformer_m36.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2'),\n    'convformer_m36.sail_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', input_size=(3, 384, 384), pool_size=(12, 12)),\n    'convformer_m36.sail_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2'),\n    'convformer_m36.sail_in22k_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', input_size=(3, 384, 384), pool_size=(12, 12)),\n    'convformer_m36.sail_in22k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', num_classes=21841),\n\n    'convformer_b36.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2'),\n    'convformer_b36.sail_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', input_size=(3, 384, 384), pool_size=(12, 12)),\n    'convformer_b36.sail_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2'),\n    'convformer_b36.sail_in22k_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', input_size=(3, 384, 384), pool_size=(12, 12)),\n    'convformer_b36.sail_in22k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', num_classes=21841),\n\n    'caformer_s18.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2'),\n    'caformer_s18.sail_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', input_size=(3, 384, 384), pool_size=(12, 12)),\n    'caformer_s18.sail_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2'),\n    'caformer_s18.sail_in22k_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', input_size=(3, 384, 384), pool_size=(12, 12)),\n    'caformer_s18.sail_in22k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', num_classes=21841),\n\n    'caformer_s36.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2'),\n    'caformer_s36.sail_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', input_size=(3, 384, 384), pool_size=(12, 12)),\n    'caformer_s36.sail_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2'),\n    'caformer_s36.sail_in22k_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', input_size=(3, 384, 384), pool_size=(12, 12)),\n    'caformer_s36.sail_in22k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', num_classes=21841),\n\n    'caformer_m36.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2'),\n    'caformer_m36.sail_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', input_size=(3, 384, 384), pool_size=(12, 12)),\n    'caformer_m36.sail_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2'),\n    'caformer_m36.sail_in22k_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', input_size=(3, 384, 384), pool_size=(12, 12)),\n    'caformer_m36.sail_in22k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', num_classes=21841),\n\n    'caformer_b36.sail_in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2'),\n    'caformer_b36.sail_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', input_size=(3, 384, 384), pool_size=(12, 12)),\n    'caformer_b36.sail_in22k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2'),\n    'caformer_b36.sail_in22k_ft_in1k_384': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', input_size=(3, 384, 384), pool_size=(12, 12)),\n    'caformer_b36.sail_in22k': _cfg(\n        hf_hub_id='timm/',\n        classifier='head.fc.fc2', num_classes=21841),\n})\n\n\n@register_model\ndef poolformer_s12(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[2, 2, 6, 2],\n        dims=[64, 128, 320, 512],\n        downsample_norm=None,\n        mlp_act=nn.GELU,\n        mlp_bias=True,\n        norm_layers=GroupNorm1,\n        layer_scale_init_values=1e-5,\n        res_scale_init_values=None,\n        use_mlp_head=False,\n        **kwargs)\n    return _create_metaformer('poolformer_s12', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef poolformer_s24(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[4, 4, 12, 4],\n        dims=[64, 128, 320, 512],\n        downsample_norm=None,\n        mlp_act=nn.GELU,\n        mlp_bias=True,\n        norm_layers=GroupNorm1,\n        layer_scale_init_values=1e-5,\n        res_scale_init_values=None,\n        use_mlp_head=False,\n        **kwargs)\n    return _create_metaformer('poolformer_s24', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef poolformer_s36(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[6, 6, 18, 6],\n        dims=[64, 128, 320, 512],\n        downsample_norm=None,\n        mlp_act=nn.GELU,\n        mlp_bias=True,\n        norm_layers=GroupNorm1,\n        layer_scale_init_values=1e-6,\n        res_scale_init_values=None,\n        use_mlp_head=False,\n        **kwargs)\n    return _create_metaformer('poolformer_s36', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef poolformer_m36(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[6, 6, 18, 6],\n        dims=[96, 192, 384, 768],\n        downsample_norm=None,\n        mlp_act=nn.GELU,\n        mlp_bias=True,\n        norm_layers=GroupNorm1,\n        layer_scale_init_values=1e-6,\n        res_scale_init_values=None,\n        use_mlp_head=False,\n        **kwargs)\n    return _create_metaformer('poolformer_m36', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef poolformer_m48(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[8, 8, 24, 8],\n        dims=[96, 192, 384, 768],\n        downsample_norm=None,\n        mlp_act=nn.GELU,\n        mlp_bias=True,\n        norm_layers=GroupNorm1,\n        layer_scale_init_values=1e-6,\n        res_scale_init_values=None,\n        use_mlp_head=False,\n        **kwargs)\n    return _create_metaformer('poolformer_m48', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef poolformerv2_s12(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[2, 2, 6, 2],\n        dims=[64, 128, 320, 512],\n        norm_layers=GroupNorm1NoBias,\n        use_mlp_head=False,\n        **kwargs)\n    return _create_metaformer('poolformerv2_s12', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef poolformerv2_s24(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[4, 4, 12, 4],\n        dims=[64, 128, 320, 512],\n        norm_layers=GroupNorm1NoBias,\n        use_mlp_head=False,\n        **kwargs)\n    return _create_metaformer('poolformerv2_s24', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef poolformerv2_s36(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[6, 6, 18, 6],\n        dims=[64, 128, 320, 512],\n        norm_layers=GroupNorm1NoBias,\n        use_mlp_head=False,\n        **kwargs)\n    return _create_metaformer('poolformerv2_s36', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef poolformerv2_m36(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[6, 6, 18, 6],\n        dims=[96, 192, 384, 768],\n        norm_layers=GroupNorm1NoBias,\n        use_mlp_head=False,\n        **kwargs)\n    return _create_metaformer('poolformerv2_m36', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef poolformerv2_m48(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[8, 8, 24, 8],\n        dims=[96, 192, 384, 768],\n        norm_layers=GroupNorm1NoBias,\n        use_mlp_head=False,\n        **kwargs)\n    return _create_metaformer('poolformerv2_m48', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef convformer_s18(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[3, 3, 9, 3],\n        dims=[64, 128, 320, 512],\n        token_mixers=SepConv,\n        norm_layers=LayerNorm2dNoBias,\n        **kwargs)\n    return _create_metaformer('convformer_s18', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef convformer_s36(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[3, 12, 18, 3],\n        dims=[64, 128, 320, 512],\n        token_mixers=SepConv,\n        norm_layers=LayerNorm2dNoBias,\n        **kwargs)\n    return _create_metaformer('convformer_s36', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef convformer_m36(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[3, 12, 18, 3],\n        dims=[96, 192, 384, 576],\n        token_mixers=SepConv,\n        norm_layers=LayerNorm2dNoBias,\n        **kwargs)\n    return _create_metaformer('convformer_m36', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef convformer_b36(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[3, 12, 18, 3],\n        dims=[128, 256, 512, 768],\n        token_mixers=SepConv,\n        norm_layers=LayerNorm2dNoBias,\n        **kwargs)\n    return _create_metaformer('convformer_b36', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef caformer_s18(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[3, 3, 9, 3],\n        dims=[64, 128, 320, 512],\n        token_mixers=[SepConv, SepConv, Attention, Attention],\n        norm_layers=[LayerNorm2dNoBias] * 2 + [LayerNormNoBias] * 2,\n        **kwargs)\n    return _create_metaformer('caformer_s18', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef caformer_s36(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[3, 12, 18, 3],\n        dims=[64, 128, 320, 512],\n        token_mixers=[SepConv, SepConv, Attention, Attention],\n        norm_layers=[LayerNorm2dNoBias] * 2 + [LayerNormNoBias] * 2,\n        **kwargs)\n    return _create_metaformer('caformer_s36', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef caformer_m36(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[3, 12, 18, 3],\n        dims=[96, 192, 384, 576],\n        token_mixers=[SepConv, SepConv, Attention, Attention],\n        norm_layers=[LayerNorm2dNoBias] * 2 + [LayerNormNoBias] * 2,\n        **kwargs)\n    return _create_metaformer('caformer_m36', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef caformer_b36(pretrained=False, **kwargs) -> MetaFormer:\n    model_kwargs = dict(\n        depths=[3, 12, 18, 3],\n        dims=[128, 256, 512, 768],\n        token_mixers=[SepConv, SepConv, Attention, Attention],\n        norm_layers=[LayerNorm2dNoBias] * 2 + [LayerNormNoBias] * 2,\n        **kwargs)\n    return _create_metaformer('caformer_b36', pretrained=pretrained, **model_kwargs)\n",
  "from ._registry import *\n\nimport warnings\nwarnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", DeprecationWarning)\n",
  "from ._features_fx import *\n\nimport warnings\nwarnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", DeprecationWarning)\n",
  "\"\"\" Pytorch Inception-V4 implementation\nSourced from https://github.com/Cadene/tensorflow-model-zoo.torch (MIT License) which is\nbased upon Google's Tensorflow implementation and pretrained weights (Apache 2.0 License)\n\"\"\"\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import create_classifier, ConvNormAct\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['InceptionV4']\n\n\nclass Mixed3a(nn.Module):\n    def __init__(self, conv_block=ConvNormAct):\n        super(Mixed3a, self).__init__()\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n        self.conv = conv_block(64, 96, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        x0 = self.maxpool(x)\n        x1 = self.conv(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Mixed4a(nn.Module):\n    def __init__(self, conv_block=ConvNormAct):\n        super(Mixed4a, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            conv_block(160, 64, kernel_size=1, stride=1),\n            conv_block(64, 96, kernel_size=3, stride=1)\n        )\n\n        self.branch1 = nn.Sequential(\n            conv_block(160, 64, kernel_size=1, stride=1),\n            conv_block(64, 64, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            conv_block(64, 64, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            conv_block(64, 96, kernel_size=(3, 3), stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Mixed5a(nn.Module):\n    def __init__(self, conv_block=ConvNormAct):\n        super(Mixed5a, self).__init__()\n        self.conv = conv_block(192, 192, kernel_size=3, stride=2)\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.conv(x)\n        x1 = self.maxpool(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass InceptionA(nn.Module):\n    def __init__(self, conv_block=ConvNormAct):\n        super(InceptionA, self).__init__()\n        self.branch0 = conv_block(384, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            conv_block(384, 64, kernel_size=1, stride=1),\n            conv_block(64, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            conv_block(384, 64, kernel_size=1, stride=1),\n            conv_block(64, 96, kernel_size=3, stride=1, padding=1),\n            conv_block(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            conv_block(384, 96, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass ReductionA(nn.Module):\n    def __init__(self, conv_block=ConvNormAct):\n        super(ReductionA, self).__init__()\n        self.branch0 = conv_block(384, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            conv_block(384, 192, kernel_size=1, stride=1),\n            conv_block(192, 224, kernel_size=3, stride=1, padding=1),\n            conv_block(224, 256, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass InceptionB(nn.Module):\n    def __init__(self, conv_block=ConvNormAct):\n        super(InceptionB, self).__init__()\n        self.branch0 = conv_block(1024, 384, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            conv_block(1024, 192, kernel_size=1, stride=1),\n            conv_block(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            conv_block(224, 256, kernel_size=(7, 1), stride=1, padding=(3, 0))\n        )\n\n        self.branch2 = nn.Sequential(\n            conv_block(1024, 192, kernel_size=1, stride=1),\n            conv_block(192, 192, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            conv_block(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            conv_block(224, 224, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            conv_block(224, 256, kernel_size=(1, 7), stride=1, padding=(0, 3))\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            conv_block(1024, 128, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass ReductionB(nn.Module):\n    def __init__(self, conv_block=ConvNormAct):\n        super(ReductionB, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            conv_block(1024, 192, kernel_size=1, stride=1),\n            conv_block(192, 192, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            conv_block(1024, 256, kernel_size=1, stride=1),\n            conv_block(256, 256, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            conv_block(256, 320, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            conv_block(320, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass InceptionC(nn.Module):\n    def __init__(self, conv_block=ConvNormAct):\n        super(InceptionC, self).__init__()\n\n        self.branch0 = conv_block(1536, 256, kernel_size=1, stride=1)\n\n        self.branch1_0 = conv_block(1536, 384, kernel_size=1, stride=1)\n        self.branch1_1a = conv_block(384, 256, kernel_size=(1, 3), stride=1, padding=(0, 1))\n        self.branch1_1b = conv_block(384, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))\n\n        self.branch2_0 = conv_block(1536, 384, kernel_size=1, stride=1)\n        self.branch2_1 = conv_block(384, 448, kernel_size=(3, 1), stride=1, padding=(1, 0))\n        self.branch2_2 = conv_block(448, 512, kernel_size=(1, 3), stride=1, padding=(0, 1))\n        self.branch2_3a = conv_block(512, 256, kernel_size=(1, 3), stride=1, padding=(0, 1))\n        self.branch2_3b = conv_block(512, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            conv_block(1536, 256, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n\n        x1_0 = self.branch1_0(x)\n        x1_1a = self.branch1_1a(x1_0)\n        x1_1b = self.branch1_1b(x1_0)\n        x1 = torch.cat((x1_1a, x1_1b), 1)\n\n        x2_0 = self.branch2_0(x)\n        x2_1 = self.branch2_1(x2_0)\n        x2_2 = self.branch2_2(x2_1)\n        x2_3a = self.branch2_3a(x2_2)\n        x2_3b = self.branch2_3b(x2_2)\n        x2 = torch.cat((x2_3a, x2_3b), 1)\n\n        x3 = self.branch3(x)\n\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass InceptionV4(nn.Module):\n    def __init__(\n            self,\n            num_classes=1000,\n            in_chans=3,\n            output_stride=32,\n            drop_rate=0.,\n            global_pool='avg',\n            norm_layer='batchnorm2d',\n            norm_eps=1e-3,\n            act_layer='relu',\n    ):\n        super(InceptionV4, self).__init__()\n        assert output_stride == 32\n        self.num_classes = num_classes\n        self.num_features = 1536\n        conv_block = partial(\n            ConvNormAct,\n            padding=0,\n            norm_layer=norm_layer,\n            act_layer=act_layer,\n            norm_kwargs=dict(eps=norm_eps),\n            act_kwargs=dict(inplace=True),\n        )\n\n        features = [\n            conv_block(in_chans, 32, kernel_size=3, stride=2),\n            conv_block(32, 32, kernel_size=3, stride=1),\n            conv_block(32, 64, kernel_size=3, stride=1, padding=1),\n            Mixed3a(conv_block),\n            Mixed4a(conv_block),\n            Mixed5a(conv_block),\n        ]\n        features += [InceptionA(conv_block) for _ in range(4)]\n        features += [ReductionA(conv_block)]  # Mixed6a\n        features += [InceptionB(conv_block) for _ in range(7)]\n        features += [ReductionB(conv_block)]  # Mixed7a\n        features += [InceptionC(conv_block) for _ in range(3)]\n        self.features = nn.Sequential(*features)\n        self.feature_info = [\n            dict(num_chs=64, reduction=2, module='features.2'),\n            dict(num_chs=160, reduction=4, module='features.3'),\n            dict(num_chs=384, reduction=8, module='features.9'),\n            dict(num_chs=1024, reduction=16, module='features.17'),\n            dict(num_chs=1536, reduction=32, module='features.21'),\n        ]\n        self.global_pool, self.head_drop, self.last_linear = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool, drop_rate=drop_rate)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^features\\.[012]\\.',\n            blocks=r'^features\\.(\\d+)'\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, 'gradient checkpointing not supported'\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.last_linear\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.global_pool, self.last_linear = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        return self.features(x)\n\n    def forward_head(self, x, pre_logits: bool = False):\n        x = self.global_pool(x)\n        x = self.head_drop(x)\n        return x if pre_logits else self.last_linear(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_inception_v4(variant, pretrained=False, **kwargs) -> InceptionV4:\n    return build_model_with_cfg(\n        InceptionV4,\n        variant,\n        pretrained,\n        feature_cfg=dict(flatten_sequential=True),\n        **kwargs,\n    )\n\n\ndefault_cfgs = generate_default_cfgs({\n    'inception_v4.tf_in1k': {\n        'hf_hub_id': 'timm/',\n        'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (8, 8),\n        'crop_pct': 0.875, 'interpolation': 'bicubic',\n        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,\n        'first_conv': 'features.0.conv', 'classifier': 'last_linear',\n    }\n})\n\n\n@register_model\ndef inception_v4(pretrained=False, **kwargs):\n    return _create_inception_v4('inception_v4', pretrained, **kwargs)\n",
  "from ._hub import *\n\nimport warnings\nwarnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", DeprecationWarning)\n",
  "\"\"\" PyTorch Feature Extraction Helpers\n\nA collection of classes, functions, modules to help extract features from models\nand provide a common interface for describing them.\n\nThe return_layers, module re-writing idea inspired by torchvision IntermediateLayerGetter\nhttps://github.com/pytorch/vision/blob/d88d8961ae51507d0cb680329d985b1488b1b76b/torchvision/models/_utils.py\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom collections import OrderedDict, defaultdict\nfrom copy import deepcopy\nfrom functools import partial\nfrom typing import Dict, List, Sequence, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nfrom timm.layers import Format\n\n\n__all__ = ['FeatureInfo', 'FeatureHooks', 'FeatureDictNet', 'FeatureListNet', 'FeatureHookNet']\n\n\nclass FeatureInfo:\n\n    def __init__(self, feature_info: List[Dict], out_indices: Tuple[int]):\n        prev_reduction = 1\n        for i, fi in enumerate(feature_info):\n            # sanity check the mandatory fields, there may be additional fields depending on the model\n            assert 'num_chs' in fi and fi['num_chs'] > 0\n            assert 'reduction' in fi and fi['reduction'] >= prev_reduction\n            prev_reduction = fi['reduction']\n            assert 'module' in fi\n            fi.setdefault('index', i)\n        self.out_indices = out_indices\n        self.info = feature_info\n\n    def from_other(self, out_indices: Tuple[int]):\n        return FeatureInfo(deepcopy(self.info), out_indices)\n\n    def get(self, key, idx=None):\n        \"\"\" Get value by key at specified index (indices)\n        if idx == None, returns value for key at each output index\n        if idx is an integer, return value for that feature module index (ignoring output indices)\n        if idx is a list/tupple, return value for each module index (ignoring output indices)\n        \"\"\"\n        if idx is None:\n            return [self.info[i][key] for i in self.out_indices]\n        if isinstance(idx, (tuple, list)):\n            return [self.info[i][key] for i in idx]\n        else:\n            return self.info[idx][key]\n\n    def get_dicts(self, keys=None, idx=None):\n        \"\"\" return info dicts for specified keys (or all if None) at specified indices (or out_indices if None)\n        \"\"\"\n        if idx is None:\n            if keys is None:\n                return [self.info[i] for i in self.out_indices]\n            else:\n                return [{k: self.info[i][k] for k in keys} for i in self.out_indices]\n        if isinstance(idx, (tuple, list)):\n            return [self.info[i] if keys is None else {k: self.info[i][k] for k in keys} for i in idx]\n        else:\n            return self.info[idx] if keys is None else {k: self.info[idx][k] for k in keys}\n\n    def channels(self, idx=None):\n        \"\"\" feature channels accessor\n        \"\"\"\n        return self.get('num_chs', idx)\n\n    def reduction(self, idx=None):\n        \"\"\" feature reduction (output stride) accessor\n        \"\"\"\n        return self.get('reduction', idx)\n\n    def module_name(self, idx=None):\n        \"\"\" feature module name accessor\n        \"\"\"\n        return self.get('module', idx)\n\n    def __getitem__(self, item):\n        return self.info[item]\n\n    def __len__(self):\n        return len(self.info)\n\n\nclass FeatureHooks:\n    \"\"\" Feature Hook Helper\n\n    This module helps with the setup and extraction of hooks for extracting features from\n    internal nodes in a model by node name.\n\n    FIXME This works well in eager Python but needs redesign for torchscript.\n    \"\"\"\n\n    def __init__(\n            self,\n            hooks: Sequence[str],\n            named_modules: dict,\n            out_map: Sequence[Union[int, str]] = None,\n            default_hook_type: str = 'forward',\n    ):\n        # setup feature hooks\n        self._feature_outputs = defaultdict(OrderedDict)\n        modules = {k: v for k, v in named_modules}\n        for i, h in enumerate(hooks):\n            hook_name = h['module']\n            m = modules[hook_name]\n            hook_id = out_map[i] if out_map else hook_name\n            hook_fn = partial(self._collect_output_hook, hook_id)\n            hook_type = h.get('hook_type', default_hook_type)\n            if hook_type == 'forward_pre':\n                m.register_forward_pre_hook(hook_fn)\n            elif hook_type == 'forward':\n                m.register_forward_hook(hook_fn)\n            else:\n                assert False, \"Unsupported hook type\"\n\n    def _collect_output_hook(self, hook_id, *args):\n        x = args[-1]  # tensor we want is last argument, output for fwd, input for fwd_pre\n        if isinstance(x, tuple):\n            x = x[0]  # unwrap input tuple\n        self._feature_outputs[x.device][hook_id] = x\n\n    def get_output(self, device) -> Dict[str, torch.tensor]:\n        output = self._feature_outputs[device]\n        self._feature_outputs[device] = OrderedDict()  # clear after reading\n        return output\n\n\ndef _module_list(module, flatten_sequential=False):\n    # a yield/iter would be better for this but wouldn't be compatible with torchscript\n    ml = []\n    for name, module in module.named_children():\n        if flatten_sequential and isinstance(module, nn.Sequential):\n            # first level of Sequential containers is flattened into containing model\n            for child_name, child_module in module.named_children():\n                combined = [name, child_name]\n                ml.append(('_'.join(combined), '.'.join(combined), child_module))\n        else:\n            ml.append((name, name, module))\n    return ml\n\n\ndef _get_feature_info(net, out_indices):\n    feature_info = getattr(net, 'feature_info')\n    if isinstance(feature_info, FeatureInfo):\n        return feature_info.from_other(out_indices)\n    elif isinstance(feature_info, (list, tuple)):\n        return FeatureInfo(net.feature_info, out_indices)\n    else:\n        assert False, \"Provided feature_info is not valid\"\n\n\ndef _get_return_layers(feature_info, out_map):\n    module_names = feature_info.module_name()\n    return_layers = {}\n    for i, name in enumerate(module_names):\n        return_layers[name] = out_map[i] if out_map is not None else feature_info.out_indices[i]\n    return return_layers\n\n\nclass FeatureDictNet(nn.ModuleDict):\n    \"\"\" Feature extractor with OrderedDict return\n\n    Wrap a model and extract features as specified by the out indices, the network is\n    partially re-built from contained modules.\n\n    There is a strong assumption that the modules have been registered into the model in the same\n    order as they are used. There should be no reuse of the same nn.Module more than once, including\n    trivial modules like `self.relu = nn.ReLU`.\n\n    Only submodules that are directly assigned to the model class (`model.feature1`) or at most\n    one Sequential container deep (`model.features.1`, with flatten_sequent=True) can be captured.\n    All Sequential containers that are directly assigned to the original model will have their\n    modules assigned to this module with the name `model.features.1` being changed to `model.features_1`\n    \"\"\"\n    def __init__(\n            self,\n            model: nn.Module,\n            out_indices: Tuple[int, ...] = (0, 1, 2, 3, 4),\n            out_map: Sequence[Union[int, str]] = None,\n            output_fmt: str = 'NCHW',\n            feature_concat: bool = False,\n            flatten_sequential: bool = False,\n    ):\n        \"\"\"\n        Args:\n            model: Model from which to extract features.\n            out_indices: Output indices of the model features to extract.\n            out_map: Return id mapping for each output index, otherwise str(index) is used.\n            feature_concat: Concatenate intermediate features that are lists or tuples instead of selecting\n                first element e.g. `x[0]`\n            flatten_sequential: Flatten first two-levels of sequential modules in model (re-writes model modules)\n        \"\"\"\n        super(FeatureDictNet, self).__init__()\n        self.feature_info = _get_feature_info(model, out_indices)\n        self.output_fmt = Format(output_fmt)\n        self.concat = feature_concat\n        self.grad_checkpointing = False\n        self.return_layers = {}\n\n        return_layers = _get_return_layers(self.feature_info, out_map)\n        modules = _module_list(model, flatten_sequential=flatten_sequential)\n        remaining = set(return_layers.keys())\n        layers = OrderedDict()\n        for new_name, old_name, module in modules:\n            layers[new_name] = module\n            if old_name in remaining:\n                # return id has to be consistently str type for torchscript\n                self.return_layers[new_name] = str(return_layers[old_name])\n                remaining.remove(old_name)\n            if not remaining:\n                break\n        assert not remaining and len(self.return_layers) == len(return_layers), \\\n            f'Return layers ({remaining}) are not present in model'\n        self.update(layers)\n\n    def set_grad_checkpointing(self, enable: bool = True):\n        self.grad_checkpointing = enable\n\n    def _collect(self, x) -> (Dict[str, torch.Tensor]):\n        out = OrderedDict()\n        for i, (name, module) in enumerate(self.items()):\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                # Skipping checkpoint of first module because need a gradient at input\n                # Skipping last because networks with in-place ops might fail w/ checkpointing enabled\n                # NOTE: first_or_last module could be static, but recalc in is_scripting guard to avoid jit issues\n                first_or_last_module = i == 0 or i == max(len(self) - 1, 0)\n                x = module(x) if first_or_last_module else checkpoint(module, x)\n            else:\n                x = module(x)\n\n            if name in self.return_layers:\n                out_id = self.return_layers[name]\n                if isinstance(x, (tuple, list)):\n                    # If model tap is a tuple or list, concat or select first element\n                    # FIXME this may need to be more generic / flexible for some nets\n                    out[out_id] = torch.cat(x, 1) if self.concat else x[0]\n                else:\n                    out[out_id] = x\n        return out\n\n    def forward(self, x) -> Dict[str, torch.Tensor]:\n        return self._collect(x)\n\n\nclass FeatureListNet(FeatureDictNet):\n    \"\"\" Feature extractor with list return\n\n    A specialization of FeatureDictNet that always returns features as a list (values() of dict).\n    \"\"\"\n    def __init__(\n            self,\n            model: nn.Module,\n            out_indices: Tuple[int, ...] = (0, 1, 2, 3, 4),\n            output_fmt: str = 'NCHW',\n            feature_concat: bool = False,\n            flatten_sequential: bool = False,\n    ):\n        \"\"\"\n        Args:\n            model: Model from which to extract features.\n            out_indices: Output indices of the model features to extract.\n            feature_concat: Concatenate intermediate features that are lists or tuples instead of selecting\n                first element e.g. `x[0]`\n            flatten_sequential: Flatten first two-levels of sequential modules in model (re-writes model modules)\n        \"\"\"\n        super().__init__(\n            model,\n            out_indices=out_indices,\n            output_fmt=output_fmt,\n            feature_concat=feature_concat,\n            flatten_sequential=flatten_sequential,\n        )\n\n    def forward(self, x) -> (List[torch.Tensor]):\n        return list(self._collect(x).values())\n\n\nclass FeatureHookNet(nn.ModuleDict):\n    \"\"\" FeatureHookNet\n\n    Wrap a model and extract features specified by the out indices using forward/forward-pre hooks.\n\n    If `no_rewrite` is True, features are extracted via hooks without modifying the underlying\n    network in any way.\n\n    If `no_rewrite` is False, the model will be re-written as in the\n    FeatureList/FeatureDict case by folding first to second (Sequential only) level modules into this one.\n\n    FIXME this does not currently work with Torchscript, see FeatureHooks class\n    \"\"\"\n    def __init__(\n            self,\n            model: nn.Module,\n            out_indices: Tuple[int, ...] = (0, 1, 2, 3, 4),\n            out_map: Sequence[Union[int, str]] = None,\n            return_dict: bool = False,\n            output_fmt: str = 'NCHW',\n            no_rewrite: bool = False,\n            flatten_sequential: bool = False,\n            default_hook_type: str = 'forward',\n    ):\n        \"\"\"\n\n        Args:\n            model: Model from which to extract features.\n            out_indices: Output indices of the model features to extract.\n            out_map: Return id mapping for each output index, otherwise str(index) is used.\n            return_dict: Output features as a dict.\n            no_rewrite: Enforce that model is not re-written if True, ie no modules are removed / changed.\n                flatten_sequential arg must also be False if this is set True.\n            flatten_sequential: Re-write modules by flattening first two levels of nn.Sequential containers.\n            default_hook_type: The default hook type to use if not specified in model.feature_info.\n        \"\"\"\n        super().__init__()\n        assert not torch.jit.is_scripting()\n        self.feature_info = _get_feature_info(model, out_indices)\n        self.return_dict = return_dict\n        self.output_fmt = Format(output_fmt)\n        self.grad_checkpointing = False\n\n        layers = OrderedDict()\n        hooks = []\n        if no_rewrite:\n            assert not flatten_sequential\n            if hasattr(model, 'reset_classifier'):  # make sure classifier is removed?\n                model.reset_classifier(0)\n            layers['body'] = model\n            hooks.extend(self.feature_info.get_dicts())\n        else:\n            modules = _module_list(model, flatten_sequential=flatten_sequential)\n            remaining = {\n                f['module']: f['hook_type'] if 'hook_type' in f else default_hook_type\n                for f in self.feature_info.get_dicts()\n            }\n            for new_name, old_name, module in modules:\n                layers[new_name] = module\n                for fn, fm in module.named_modules(prefix=old_name):\n                    if fn in remaining:\n                        hooks.append(dict(module=fn, hook_type=remaining[fn]))\n                        del remaining[fn]\n                if not remaining:\n                    break\n            assert not remaining, f'Return layers ({remaining}) are not present in model'\n        self.update(layers)\n        self.hooks = FeatureHooks(hooks, model.named_modules(), out_map=out_map)\n\n    def set_grad_checkpointing(self, enable: bool = True):\n        self.grad_checkpointing = enable\n\n    def forward(self, x):\n        for i, (name, module) in enumerate(self.items()):\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                # Skipping checkpoint of first module because need a gradient at input\n                # Skipping last because networks with in-place ops might fail w/ checkpointing enabled\n                # NOTE: first_or_last module could be static, but recalc in is_scripting guard to avoid jit issues\n                first_or_last_module = i == 0 or i == max(len(self) - 1, 0)\n                x = module(x) if first_or_last_module else checkpoint(module, x)\n            else:\n                x = module(x)\n        out = self.hooks.get_output(x.device)\n        return out if self.return_dict else list(out.values())\n",
  "\"\"\" Global Context ViT\n\nFrom scratch implementation of GCViT in the style of timm swin_transformer_v2_cr.py\n\nGlobal Context Vision Transformers -https://arxiv.org/abs/2206.09959\n\n@article{hatamizadeh2022global,\n  title={Global Context Vision Transformers},\n  author={Hatamizadeh, Ali and Yin, Hongxu and Kautz, Jan and Molchanov, Pavlo},\n  journal={arXiv preprint arXiv:2206.09959},\n  year={2022}\n}\n\nFree of any code related to NVIDIA GCVit impl at https://github.com/NVlabs/GCVit.\nThe license for this code release is Apache 2.0 with no commercial restrictions.\n\nHowever, weight files adapted from NVIDIA GCVit impl ARE under a non-commercial share-alike license\n(https://creativecommons.org/licenses/by-nc-sa/4.0/) until I have a chance to train new ones...\n\nHacked together by / Copyright 2022, Ross Wightman\n\"\"\"\nimport math\nfrom functools import partial\nfrom typing import Callable, List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import DropPath, to_2tuple, to_ntuple, Mlp, ClassifierHead, LayerNorm2d, \\\n    get_attn, get_act_layer, get_norm_layer, RelPosBias, _assert\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_function\nfrom ._manipulate import named_apply\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['GlobalContextVit']\n\n\nclass MbConvBlock(nn.Module):\n    \"\"\" A depthwise separable / fused mbconv style residual block with SE, `no norm.\n    \"\"\"\n    def __init__(\n            self,\n            in_chs,\n            out_chs=None,\n            expand_ratio=1.0,\n            attn_layer='se',\n            bias=False,\n            act_layer=nn.GELU,\n    ):\n        super().__init__()\n        attn_kwargs = dict(act_layer=act_layer)\n        if isinstance(attn_layer, str) and attn_layer == 'se' or attn_layer == 'eca':\n            attn_kwargs['rd_ratio'] = 0.25\n            attn_kwargs['bias'] = False\n        attn_layer = get_attn(attn_layer)\n        out_chs = out_chs or in_chs\n        mid_chs = int(expand_ratio * in_chs)\n\n        self.conv_dw = nn.Conv2d(in_chs, mid_chs, 3, 1, 1, groups=in_chs, bias=bias)\n        self.act = act_layer()\n        self.se = attn_layer(mid_chs, **attn_kwargs)\n        self.conv_pw = nn.Conv2d(mid_chs, out_chs, 1, 1, 0, bias=bias)\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv_dw(x)\n        x = self.act(x)\n        x = self.se(x)\n        x = self.conv_pw(x)\n        x = x + shortcut\n        return x\n\n\nclass Downsample2d(nn.Module):\n    def __init__(\n            self,\n            dim,\n            dim_out=None,\n            reduction='conv',\n            act_layer=nn.GELU,\n            norm_layer=LayerNorm2d,  # NOTE in NCHW\n    ):\n        super().__init__()\n        dim_out = dim_out or dim\n\n        self.norm1 = norm_layer(dim) if norm_layer is not None else nn.Identity()\n        self.conv_block = MbConvBlock(dim, act_layer=act_layer)\n        assert reduction in ('conv', 'max', 'avg')\n        if reduction == 'conv':\n            self.reduction = nn.Conv2d(dim, dim_out, 3, 2, 1, bias=False)\n        elif reduction == 'max':\n            assert dim == dim_out\n            self.reduction = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        else:\n            assert dim == dim_out\n            self.reduction = nn.AvgPool2d(kernel_size=2)\n        self.norm2 = norm_layer(dim_out) if norm_layer is not None else nn.Identity()\n\n    def forward(self, x):\n        x = self.norm1(x)\n        x = self.conv_block(x)\n        x = self.reduction(x)\n        x = self.norm2(x)\n        return x\n\n\nclass FeatureBlock(nn.Module):\n    def __init__(\n            self,\n            dim,\n            levels=0,\n            reduction='max',\n            act_layer=nn.GELU,\n    ):\n        super().__init__()\n        reductions = levels\n        levels = max(1, levels)\n        if reduction == 'avg':\n            pool_fn = partial(nn.AvgPool2d, kernel_size=2)\n        else:\n            pool_fn = partial(nn.MaxPool2d, kernel_size=3, stride=2, padding=1)\n        self.blocks = nn.Sequential()\n        for i in range(levels):\n            self.blocks.add_module(f'conv{i+1}', MbConvBlock(dim, act_layer=act_layer))\n            if reductions:\n                self.blocks.add_module(f'pool{i+1}', pool_fn())\n                reductions -= 1\n\n    def forward(self, x):\n        return self.blocks(x)\n\n\nclass Stem(nn.Module):\n    def __init__(\n            self,\n            in_chs: int = 3,\n            out_chs: int = 96,\n            act_layer: Callable = nn.GELU,\n            norm_layer: Callable = LayerNorm2d,  # NOTE stem in NCHW\n    ):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_chs, out_chs, kernel_size=3, stride=2, padding=1)\n        self.down = Downsample2d(out_chs, act_layer=act_layer, norm_layer=norm_layer)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.down(x)\n        return x\n\n\nclass WindowAttentionGlobal(nn.Module):\n\n    def __init__(\n            self,\n            dim: int,\n            num_heads: int,\n            window_size: Tuple[int, int],\n            use_global: bool = True,\n            qkv_bias: bool = True,\n            attn_drop: float = 0.,\n            proj_drop: float = 0.,\n    ):\n        super().__init__()\n        window_size = to_2tuple(window_size)\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.use_global = use_global\n\n        self.rel_pos = RelPosBias(window_size=window_size, num_heads=num_heads)\n        if self.use_global:\n            self.qkv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n        else:\n            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, q_global: Optional[torch.Tensor] = None):\n        B, N, C = x.shape\n        if self.use_global and q_global is not None:\n            _assert(x.shape[-1] == q_global.shape[-1], 'x and q_global seq lengths should be equal')\n\n            kv = self.qkv(x)\n            kv = kv.reshape(B, N, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n            k, v = kv.unbind(0)\n\n            q = q_global.repeat(B // q_global.shape[0], 1, 1, 1)\n            q = q.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        else:\n            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n            q, k, v = qkv.unbind(0)\n        q = q * self.scale\n\n        attn = q @ k.transpose(-2, -1).contiguous()  # NOTE contiguous() fixes an odd jit bug in PyTorch 2.0\n        attn = self.rel_pos(attn)\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\ndef window_partition(x, window_size: Tuple[int, int]):\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size[0], window_size[0], W // window_size[1], window_size[1], C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0], window_size[1], C)\n    return windows\n\n\n@register_notrace_function  # reason: int argument is a Proxy\ndef window_reverse(windows, window_size: Tuple[int, int], img_size: Tuple[int, int]):\n    H, W = img_size\n    C = windows.shape[-1]\n    x = windows.view(-1, H // window_size[0], W // window_size[1], window_size[0], window_size[1], C)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, C)\n    return x\n\n\nclass LayerScale(nn.Module):\n    def __init__(self, dim, init_values=1e-5, inplace=False):\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n\n\nclass GlobalContextVitBlock(nn.Module):\n    def __init__(\n            self,\n            dim: int,\n            feat_size: Tuple[int, int],\n            num_heads: int,\n            window_size: int = 7,\n            mlp_ratio: float = 4.,\n            use_global: bool = True,\n            qkv_bias: bool = True,\n            layer_scale: Optional[float] = None,\n            proj_drop: float = 0.,\n            attn_drop: float = 0.,\n            drop_path: float = 0.,\n            attn_layer: Callable = WindowAttentionGlobal,\n            act_layer: Callable = nn.GELU,\n            norm_layer: Callable = nn.LayerNorm,\n    ):\n        super().__init__()\n        feat_size = to_2tuple(feat_size)\n        window_size = to_2tuple(window_size)\n        self.window_size = window_size\n        self.num_windows = int((feat_size[0] // window_size[0]) * (feat_size[1] // window_size[1]))\n\n        self.norm1 = norm_layer(dim)\n        self.attn = attn_layer(\n            dim,\n            num_heads=num_heads,\n            window_size=window_size,\n            use_global=use_global,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n        )\n        self.ls1 = LayerScale(dim, layer_scale) if layer_scale is not None else nn.Identity()\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=proj_drop)\n        self.ls2 = LayerScale(dim, layer_scale) if layer_scale is not None else nn.Identity()\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def _window_attn(self, x, q_global: Optional[torch.Tensor] = None):\n        B, H, W, C = x.shape\n        x_win = window_partition(x, self.window_size)\n        x_win = x_win.view(-1, self.window_size[0] * self.window_size[1], C)\n        attn_win = self.attn(x_win, q_global)\n        x = window_reverse(attn_win, self.window_size, (H, W))\n        return x\n\n    def forward(self, x, q_global: Optional[torch.Tensor] = None):\n        x = x + self.drop_path1(self.ls1(self._window_attn(self.norm1(x), q_global)))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x\n\n\nclass GlobalContextVitStage(nn.Module):\n    def __init__(\n            self,\n            dim,\n            depth: int,\n            num_heads: int,\n            feat_size: Tuple[int, int],\n            window_size: Tuple[int, int],\n            downsample: bool = True,\n            global_norm: bool = False,\n            stage_norm: bool = False,\n            mlp_ratio: float = 4.,\n            qkv_bias: bool = True,\n            layer_scale: Optional[float] = None,\n            proj_drop: float = 0.,\n            attn_drop: float = 0.,\n            drop_path: Union[List[float], float] = 0.0,\n            act_layer: Callable = nn.GELU,\n            norm_layer: Callable = nn.LayerNorm,\n            norm_layer_cl: Callable = LayerNorm2d,\n    ):\n        super().__init__()\n        if downsample:\n            self.downsample = Downsample2d(\n                dim=dim,\n                dim_out=dim * 2,\n                norm_layer=norm_layer,\n            )\n            dim = dim * 2\n            feat_size = (feat_size[0] // 2, feat_size[1] // 2)\n        else:\n            self.downsample = nn.Identity()\n        self.feat_size = feat_size\n        window_size = to_2tuple(window_size)\n\n        feat_levels = int(math.log2(min(feat_size) / min(window_size)))\n        self.global_block = FeatureBlock(dim, feat_levels)\n        self.global_norm = norm_layer_cl(dim) if global_norm else nn.Identity()\n\n        self.blocks = nn.ModuleList([\n            GlobalContextVitBlock(\n                dim=dim,\n                num_heads=num_heads,\n                feat_size=feat_size,\n                window_size=window_size,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                use_global=(i % 2 != 0),\n                layer_scale=layer_scale,\n                proj_drop=proj_drop,\n                attn_drop=attn_drop,\n                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                act_layer=act_layer,\n                norm_layer=norm_layer_cl,\n            )\n            for i in range(depth)\n        ])\n        self.norm = norm_layer_cl(dim) if stage_norm else nn.Identity()\n        self.dim = dim\n        self.feat_size = feat_size\n        self.grad_checkpointing = False\n\n    def forward(self, x):\n        # input NCHW, downsample & global block are 2d conv + pooling\n        x = self.downsample(x)\n        global_query = self.global_block(x)\n\n        # reshape NCHW --> NHWC for transformer blocks\n        x = x.permute(0, 2, 3, 1)\n        global_query = self.global_norm(global_query.permute(0, 2, 3, 1))\n        for blk in self.blocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x, global_query)\n        x = self.norm(x)\n        x = x.permute(0, 3, 1, 2).contiguous()  # back to NCHW\n        return x\n\n\nclass GlobalContextVit(nn.Module):\n    def __init__(\n            self,\n            in_chans: int = 3,\n            num_classes: int = 1000,\n            global_pool: str = 'avg',\n            img_size: Tuple[int, int] = 224,\n            window_ratio: Tuple[int, ...] = (32, 32, 16, 32),\n            window_size: Tuple[int, ...] = None,\n            embed_dim: int = 64,\n            depths: Tuple[int, ...] = (3, 4, 19, 5),\n            num_heads: Tuple[int, ...] = (2, 4, 8, 16),\n            mlp_ratio: float = 3.0,\n            qkv_bias: bool = True,\n            layer_scale: Optional[float] = None,\n            drop_rate: float = 0.,\n            proj_drop_rate: float = 0.,\n            attn_drop_rate: float = 0.,\n            drop_path_rate: float = 0.,\n            weight_init='',\n            act_layer: str = 'gelu',\n            norm_layer: str = 'layernorm2d',\n            norm_layer_cl: str = 'layernorm',\n            norm_eps: float = 1e-5,\n    ):\n        super().__init__()\n        act_layer = get_act_layer(act_layer)\n        norm_layer = partial(get_norm_layer(norm_layer), eps=norm_eps)\n        norm_layer_cl = partial(get_norm_layer(norm_layer_cl), eps=norm_eps)\n\n        img_size = to_2tuple(img_size)\n        feat_size = tuple(d // 4 for d in img_size)  # stem reduction by 4\n        self.global_pool = global_pool\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        num_stages = len(depths)\n        self.num_features = int(embed_dim * 2 ** (num_stages - 1))\n        if window_size is not None:\n            window_size = to_ntuple(num_stages)(window_size)\n        else:\n            assert window_ratio is not None\n            window_size = tuple([(img_size[0] // r, img_size[1] // r) for r in to_ntuple(num_stages)(window_ratio)])\n\n        self.stem = Stem(\n            in_chs=in_chans,\n            out_chs=embed_dim,\n            act_layer=act_layer,\n            norm_layer=norm_layer\n        )\n\n        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n        stages = []\n        for i in range(num_stages):\n            last_stage = i == num_stages - 1\n            stage_scale = 2 ** max(i - 1, 0)\n            stages.append(GlobalContextVitStage(\n                dim=embed_dim * stage_scale,\n                depth=depths[i],\n                num_heads=num_heads[i],\n                feat_size=(feat_size[0] // stage_scale, feat_size[1] // stage_scale),\n                window_size=window_size[i],\n                downsample=i != 0,\n                stage_norm=last_stage,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                layer_scale=layer_scale,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n                norm_layer_cl=norm_layer_cl,\n            ))\n        self.stages = nn.Sequential(*stages)\n\n        # Classifier head\n        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=drop_rate)\n\n        if weight_init:\n            named_apply(partial(self._init_weights, scheme=weight_init), self)\n\n    def _init_weights(self, module, name, scheme='vit'):\n        # note Conv2d left as default init\n        if scheme == 'vit':\n            if isinstance(module, nn.Linear):\n                nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    if 'mlp' in name:\n                        nn.init.normal_(module.bias, std=1e-6)\n                    else:\n                        nn.init.zeros_(module.bias)\n        else:\n            if isinstance(module, nn.Linear):\n                nn.init.normal_(module.weight, std=.02)\n                if module.bias is not None:\n                    nn.init.zeros_(module.bias)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\n            k for k, _ in self.named_parameters()\n            if any(n in k for n in [\"relative_position_bias_table\", \"rel_pos.mlp\"])}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^stem',  # stem and embed\n            blocks=r'^stages\\.(\\d+)'\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        for s in self.stages:\n            s.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is None:\n            global_pool = self.head.global_pool.pool_type\n        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate)\n\n    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.stem(x)\n        x = self.stages(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=pre_logits)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_gcvit(variant, pretrained=False, **kwargs):\n    if kwargs.get('features_only', None):\n        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n    model = build_model_with_cfg(GlobalContextVit, variant, pretrained, **kwargs)\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.conv1', 'classifier': 'head.fc',\n        'fixed_input_size': True,\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'gcvit_xxtiny.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_xxtiny_224_nvidia-d1d86009.pth'),\n    'gcvit_xtiny.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_xtiny_224_nvidia-274b92b7.pth'),\n    'gcvit_tiny.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_tiny_224_nvidia-ac783954.pth'),\n    'gcvit_small.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_small_224_nvidia-4e98afa2.pth'),\n    'gcvit_base.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_base_224_nvidia-f009139b.pth'),\n})\n\n\n@register_model\ndef gcvit_xxtiny(pretrained=False, **kwargs) -> GlobalContextVit:\n    model_kwargs = dict(\n        depths=(2, 2, 6, 2),\n        num_heads=(2, 4, 8, 16),\n        **kwargs)\n    return _create_gcvit('gcvit_xxtiny', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef gcvit_xtiny(pretrained=False, **kwargs) -> GlobalContextVit:\n    model_kwargs = dict(\n        depths=(3, 4, 6, 5),\n        num_heads=(2, 4, 8, 16),\n        **kwargs)\n    return _create_gcvit('gcvit_xtiny', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef gcvit_tiny(pretrained=False, **kwargs) -> GlobalContextVit:\n    model_kwargs = dict(\n        depths=(3, 4, 19, 5),\n        num_heads=(2, 4, 8, 16),\n        **kwargs)\n    return _create_gcvit('gcvit_tiny', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef gcvit_small(pretrained=False, **kwargs) -> GlobalContextVit:\n    model_kwargs = dict(\n        depths=(3, 4, 19, 5),\n        num_heads=(3, 6, 12, 24),\n        embed_dim=96,\n        mlp_ratio=2,\n        layer_scale=1e-5,\n        **kwargs)\n    return _create_gcvit('gcvit_small', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef gcvit_base(pretrained=False, **kwargs) -> GlobalContextVit:\n    model_kwargs = dict(\n        depths=(3, 4, 19, 5),\n        num_heads=(4, 8, 16, 32),\n        embed_dim=128,\n        mlp_ratio=2,\n        layer_scale=1e-5,\n        **kwargs)\n    return _create_gcvit('gcvit_base', pretrained=pretrained, **model_kwargs)\n",
  "\"\"\"Pytorch Densenet implementation w/ tweaks\nThis file is a copy of https://github.com/pytorch/vision 'densenet.py' (BSD-3-Clause) with\nfixed kwargs passthrough and addition of dynamic global avg/max pool.\n\"\"\"\nimport re\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as cp\nfrom torch.jit.annotations import List\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import BatchNormAct2d, get_norm_act_layer, BlurPool2d, create_classifier\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import MATCH_PREV_GROUP\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['DenseNet']\n\n\nclass DenseLayer(nn.Module):\n    def __init__(\n            self,\n            num_input_features,\n            growth_rate,\n            bn_size,\n            norm_layer=BatchNormAct2d,\n            drop_rate=0.,\n            grad_checkpointing=False,\n    ):\n        super(DenseLayer, self).__init__()\n        self.add_module('norm1', norm_layer(num_input_features)),\n        self.add_module('conv1', nn.Conv2d(\n            num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module('norm2', norm_layer(bn_size * growth_rate)),\n        self.add_module('conv2', nn.Conv2d(\n            bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = float(drop_rate)\n        self.grad_checkpointing = grad_checkpointing\n\n    def bottleneck_fn(self, xs):\n        # type: (List[torch.Tensor]) -> torch.Tensor\n        concated_features = torch.cat(xs, 1)\n        bottleneck_output = self.conv1(self.norm1(concated_features))  # noqa: T484\n        return bottleneck_output\n\n    # todo: rewrite when torchscript supports any\n    def any_requires_grad(self, x):\n        # type: (List[torch.Tensor]) -> bool\n        for tensor in x:\n            if tensor.requires_grad:\n                return True\n        return False\n\n    @torch.jit.unused  # noqa: T484\n    def call_checkpoint_bottleneck(self, x):\n        # type: (List[torch.Tensor]) -> torch.Tensor\n        def closure(*xs):\n            return self.bottleneck_fn(xs)\n\n        return cp.checkpoint(closure, *x)\n\n    @torch.jit._overload_method  # noqa: F811\n    def forward(self, x):\n        # type: (List[torch.Tensor]) -> (torch.Tensor)\n        pass\n\n    @torch.jit._overload_method  # noqa: F811\n    def forward(self, x):\n        # type: (torch.Tensor) -> (torch.Tensor)\n        pass\n\n    # torchscript does not yet support *args, so we overload method\n    # allowing it to take either a List[Tensor] or single Tensor\n    def forward(self, x):  # noqa: F811\n        if isinstance(x, torch.Tensor):\n            prev_features = [x]\n        else:\n            prev_features = x\n\n        if self.grad_checkpointing and self.any_requires_grad(prev_features):\n            if torch.jit.is_scripting():\n                raise Exception(\"Memory Efficient not supported in JIT\")\n            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\n        else:\n            bottleneck_output = self.bottleneck_fn(prev_features)\n\n        new_features = self.conv2(self.norm2(bottleneck_output))\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return new_features\n\n\nclass DenseBlock(nn.ModuleDict):\n    _version = 2\n\n    def __init__(\n            self,\n            num_layers,\n            num_input_features,\n            bn_size,\n            growth_rate,\n            norm_layer=BatchNormAct2d,\n            drop_rate=0.,\n            grad_checkpointing=False,\n    ):\n        super(DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = DenseLayer(\n                num_input_features + i * growth_rate,\n                growth_rate=growth_rate,\n                bn_size=bn_size,\n                norm_layer=norm_layer,\n                drop_rate=drop_rate,\n                grad_checkpointing=grad_checkpointing,\n            )\n            self.add_module('denselayer%d' % (i + 1), layer)\n\n    def forward(self, init_features):\n        features = [init_features]\n        for name, layer in self.items():\n            new_features = layer(features)\n            features.append(new_features)\n        return torch.cat(features, 1)\n\n\nclass DenseTransition(nn.Sequential):\n    def __init__(\n            self,\n            num_input_features,\n            num_output_features,\n            norm_layer=BatchNormAct2d,\n            aa_layer=None,\n    ):\n        super(DenseTransition, self).__init__()\n        self.add_module('norm', norm_layer(num_input_features))\n        self.add_module('conv', nn.Conv2d(\n            num_input_features, num_output_features, kernel_size=1, stride=1, bias=False))\n        if aa_layer is not None:\n            self.add_module('pool', aa_layer(num_output_features, stride=2))\n        else:\n            self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n\n\nclass DenseNet(nn.Module):\n    r\"\"\"Densenet-BC model class, based on\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:\n        growth_rate (int) - how many filters to add each layer (`k` in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate before classifier layer\n        proj_drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n    \"\"\"\n\n    def __init__(\n            self,\n            growth_rate=32,\n            block_config=(6, 12, 24, 16),\n            num_classes=1000,\n            in_chans=3,\n            global_pool='avg',\n            bn_size=4,\n            stem_type='',\n            act_layer='relu',\n            norm_layer='batchnorm2d',\n            aa_layer=None,\n            drop_rate=0.,\n            proj_drop_rate=0.,\n            memory_efficient=False,\n            aa_stem_only=True,\n    ):\n        self.num_classes = num_classes\n        super(DenseNet, self).__init__()\n        norm_layer = get_norm_act_layer(norm_layer, act_layer=act_layer)\n\n        # Stem\n        deep_stem = 'deep' in stem_type  # 3x3 deep stem\n        num_init_features = growth_rate * 2\n        if aa_layer is None:\n            stem_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        else:\n            stem_pool = nn.Sequential(*[\n                nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n                aa_layer(channels=num_init_features, stride=2)])\n        if deep_stem:\n            stem_chs_1 = stem_chs_2 = growth_rate\n            if 'tiered' in stem_type:\n                stem_chs_1 = 3 * (growth_rate // 4)\n                stem_chs_2 = num_init_features if 'narrow' in stem_type else 6 * (growth_rate // 4)\n            self.features = nn.Sequential(OrderedDict([\n                ('conv0', nn.Conv2d(in_chans, stem_chs_1, 3, stride=2, padding=1, bias=False)),\n                ('norm0', norm_layer(stem_chs_1)),\n                ('conv1', nn.Conv2d(stem_chs_1, stem_chs_2, 3, stride=1, padding=1, bias=False)),\n                ('norm1', norm_layer(stem_chs_2)),\n                ('conv2', nn.Conv2d(stem_chs_2, num_init_features, 3, stride=1, padding=1, bias=False)),\n                ('norm2', norm_layer(num_init_features)),\n                ('pool0', stem_pool),\n            ]))\n        else:\n            self.features = nn.Sequential(OrderedDict([\n                ('conv0', nn.Conv2d(in_chans, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n                ('norm0', norm_layer(num_init_features)),\n                ('pool0', stem_pool),\n            ]))\n        self.feature_info = [\n            dict(num_chs=num_init_features, reduction=2, module=f'features.norm{2 if deep_stem else 0}')]\n        current_stride = 4\n\n        # DenseBlocks\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = DenseBlock(\n                num_layers=num_layers,\n                num_input_features=num_features,\n                bn_size=bn_size,\n                growth_rate=growth_rate,\n                norm_layer=norm_layer,\n                drop_rate=proj_drop_rate,\n                grad_checkpointing=memory_efficient,\n            )\n            module_name = f'denseblock{(i + 1)}'\n            self.features.add_module(module_name, block)\n            num_features = num_features + num_layers * growth_rate\n            transition_aa_layer = None if aa_stem_only else aa_layer\n            if i != len(block_config) - 1:\n                self.feature_info += [\n                    dict(num_chs=num_features, reduction=current_stride, module='features.' + module_name)]\n                current_stride *= 2\n                trans = DenseTransition(\n                    num_input_features=num_features,\n                    num_output_features=num_features // 2,\n                    norm_layer=norm_layer,\n                    aa_layer=transition_aa_layer,\n                )\n                self.features.add_module(f'transition{i + 1}', trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module('norm5', norm_layer(num_features))\n\n        self.feature_info += [dict(num_chs=num_features, reduction=current_stride, module='features.norm5')]\n        self.num_features = num_features\n\n        # Linear layer\n        global_pool, classifier = create_classifier(\n            self.num_features,\n            self.num_classes,\n            pool_type=global_pool,\n        )\n        self.global_pool = global_pool\n        self.head_drop = nn.Dropout(drop_rate)\n        self.classifier = classifier\n\n        # Official init from torch repo.\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^features\\.conv[012]|features\\.norm[012]|features\\.pool[012]',\n            blocks=r'^features\\.(?:denseblock|transition)(\\d+)' if coarse else [\n                (r'^features\\.denseblock(\\d+)\\.denselayer(\\d+)', None),\n                (r'^features\\.transition(\\d+)', MATCH_PREV_GROUP)  # FIXME combine with previous denselayer\n            ]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        for b in self.features.modules():\n            if isinstance(b, DenseLayer):\n                b.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.classifier\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.global_pool, self.classifier = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        return self.features(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x)\n        x = self.head_drop(x)\n        x = self.classifier(x)\n        return x\n\n\ndef _filter_torchvision_pretrained(state_dict):\n    pattern = re.compile(\n        r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n\n    for key in list(state_dict.keys()):\n        res = pattern.match(key)\n        if res:\n            new_key = res.group(1) + res.group(2)\n            state_dict[new_key] = state_dict[key]\n            del state_dict[key]\n    return state_dict\n\n\ndef _create_densenet(variant, growth_rate, block_config, pretrained, **kwargs):\n    kwargs['growth_rate'] = growth_rate\n    kwargs['block_config'] = block_config\n    return build_model_with_cfg(\n        DenseNet,\n        variant,\n        pretrained,\n        feature_cfg=dict(flatten_sequential=True),\n        pretrained_filter_fn=_filter_torchvision_pretrained,\n        **kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'features.conv0', 'classifier': 'classifier', **kwargs,\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'densenet121.ra_in1k': _cfg(\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=0.95),\n    'densenetblur121d.ra_in1k': _cfg(\n        hf_hub_id='timm/',\n        test_input_size=(3, 288, 288), test_crop_pct=0.95),\n    'densenet264d.untrained': _cfg(),\n    'densenet121.tv_in1k': _cfg(hf_hub_id='timm/'),\n    'densenet169.tv_in1k': _cfg(hf_hub_id='timm/'),\n    'densenet201.tv_in1k': _cfg(hf_hub_id='timm/'),\n    'densenet161.tv_in1k': _cfg(hf_hub_id='timm/'),\n})\n\n\n@register_model\ndef densenet121(pretrained=False, **kwargs) -> DenseNet:\n    r\"\"\"Densenet-121 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"\n    model = _create_densenet(\n        'densenet121', growth_rate=32, block_config=(6, 12, 24, 16), pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef densenetblur121d(pretrained=False, **kwargs) -> DenseNet:\n    r\"\"\"Densenet-121 w/ blur-pooling & 3-layer 3x3 stem\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"\n    model = _create_densenet(\n        'densenetblur121d', growth_rate=32, block_config=(6, 12, 24, 16), pretrained=pretrained,\n        stem_type='deep', aa_layer=BlurPool2d, **kwargs)\n    return model\n\n\n@register_model\ndef densenet169(pretrained=False, **kwargs) -> DenseNet:\n    r\"\"\"Densenet-169 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"\n    model = _create_densenet(\n        'densenet169', growth_rate=32, block_config=(6, 12, 32, 32), pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef densenet201(pretrained=False, **kwargs) -> DenseNet:\n    r\"\"\"Densenet-201 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"\n    model = _create_densenet(\n        'densenet201', growth_rate=32, block_config=(6, 12, 48, 32), pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef densenet161(pretrained=False, **kwargs) -> DenseNet:\n    r\"\"\"Densenet-161 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"\n    model = _create_densenet(\n        'densenet161', growth_rate=48, block_config=(6, 12, 36, 24), pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef densenet264d(pretrained=False, **kwargs) -> DenseNet:\n    r\"\"\"Densenet-264 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"\n    model = _create_densenet(\n        'densenet264d', growth_rate=48, block_config=(6, 12, 64, 48), stem_type='deep', pretrained=pretrained, **kwargs)\n    return model\n\n",
  "import os\nfrom typing import Any, Dict, Optional, Union\nfrom urllib.parse import urlsplit\n\nfrom timm.layers import set_layer_config\nfrom ._helpers import load_checkpoint\nfrom ._hub import load_model_config_from_hf\nfrom ._pretrained import PretrainedCfg\nfrom ._registry import is_model, model_entrypoint, split_model_name_tag\n\n\n__all__ = ['parse_model_name', 'safe_model_name', 'create_model']\n\n\ndef parse_model_name(model_name: str):\n    if model_name.startswith('hf_hub'):\n        # NOTE for backwards compat, deprecate hf_hub use\n        model_name = model_name.replace('hf_hub', 'hf-hub')\n    parsed = urlsplit(model_name)\n    assert parsed.scheme in ('', 'timm', 'hf-hub')\n    if parsed.scheme == 'hf-hub':\n        # FIXME may use fragment as revision, currently `@` in URI path\n        return parsed.scheme, parsed.path\n    else:\n        model_name = os.path.split(parsed.path)[-1]\n        return 'timm', model_name\n\n\ndef safe_model_name(model_name: str, remove_source: bool = True):\n    # return a filename / path safe model name\n    def make_safe(name):\n        return ''.join(c if c.isalnum() else '_' for c in name).rstrip('_')\n    if remove_source:\n        model_name = parse_model_name(model_name)[-1]\n    return make_safe(model_name)\n\n\ndef create_model(\n        model_name: str,\n        pretrained: bool = False,\n        pretrained_cfg: Optional[Union[str, Dict[str, Any], PretrainedCfg]] = None,\n        pretrained_cfg_overlay:  Optional[Dict[str, Any]] = None,\n        checkpoint_path: str = '',\n        scriptable: Optional[bool] = None,\n        exportable: Optional[bool] = None,\n        no_jit: Optional[bool] = None,\n        **kwargs,\n):\n    \"\"\"Create a model.\n\n    Lookup model's entrypoint function and pass relevant args to create a new model.\n\n    <Tip>\n        **kwargs will be passed through entrypoint fn to ``timm.models.build_model_with_cfg()``\n        and then the model class __init__(). kwargs values set to None are pruned before passing.\n    </Tip>\n\n    Args:\n        model_name: Name of model to instantiate.\n        pretrained: If set to `True`, load pretrained ImageNet-1k weights.\n        pretrained_cfg: Pass in an external pretrained_cfg for model.\n        pretrained_cfg_overlay: Replace key-values in base pretrained_cfg with these.\n        checkpoint_path: Path of checkpoint to load _after_ the model is initialized.\n        scriptable: Set layer config so that model is jit scriptable (not working for all models yet).\n        exportable: Set layer config so that model is traceable / ONNX exportable (not fully impl/obeyed yet).\n        no_jit: Set layer config so that model doesn't utilize jit scripted layers (so far activations only).\n\n    Keyword Args:\n        drop_rate (float): Classifier dropout rate for training.\n        drop_path_rate (float): Stochastic depth drop rate for training.\n        global_pool (str): Classifier global pooling type.\n\n    Example:\n\n    ```py\n    >>> from timm import create_model\n\n    >>> # Create a MobileNetV3-Large model with no pretrained weights.\n    >>> model = create_model('mobilenetv3_large_100')\n\n    >>> # Create a MobileNetV3-Large model with pretrained weights.\n    >>> model = create_model('mobilenetv3_large_100', pretrained=True)\n    >>> model.num_classes\n    1000\n\n    >>> # Create a MobileNetV3-Large model with pretrained weights and a new head with 10 classes.\n    >>> model = create_model('mobilenetv3_large_100', pretrained=True, num_classes=10)\n    >>> model.num_classes\n    10\n    ```\n    \"\"\"\n    # Parameters that aren't supported by all models or are intended to only override model defaults if set\n    # should default to None in command line args/cfg. Remove them if they are present and not set so that\n    # non-supporting models don't break and default args remain in effect.\n    kwargs = {k: v for k, v in kwargs.items() if v is not None}\n\n    model_source, model_name = parse_model_name(model_name)\n    if model_source == 'hf-hub':\n        assert not pretrained_cfg, 'pretrained_cfg should not be set when sourcing model from Hugging Face Hub.'\n        # For model names specified in the form `hf-hub:path/architecture_name@revision`,\n        # load model weights + pretrained_cfg from Hugging Face hub.\n        pretrained_cfg, model_name = load_model_config_from_hf(model_name)\n    else:\n        model_name, pretrained_tag = split_model_name_tag(model_name)\n        if pretrained_tag and not pretrained_cfg:\n            # a valid pretrained_cfg argument takes priority over tag in model name\n            pretrained_cfg = pretrained_tag\n\n    if not is_model(model_name):\n        raise RuntimeError('Unknown model (%s)' % model_name)\n\n    create_fn = model_entrypoint(model_name)\n    with set_layer_config(scriptable=scriptable, exportable=exportable, no_jit=no_jit):\n        model = create_fn(\n            pretrained=pretrained,\n            pretrained_cfg=pretrained_cfg,\n            pretrained_cfg_overlay=pretrained_cfg_overlay,\n            **kwargs,\n        )\n\n    if checkpoint_path:\n        load_checkpoint(model, checkpoint_path)\n\n    return model\n",
  "import copy\nfrom collections import deque, defaultdict\nfrom dataclasses import dataclass, field, replace, asdict\nfrom typing import Any, Deque, Dict, Tuple, Optional, Union\n\n\n__all__ = ['PretrainedCfg', 'filter_pretrained_cfg', 'DefaultCfg']\n\n\n@dataclass\nclass PretrainedCfg:\n    \"\"\"\n    \"\"\"\n    # weight source locations\n    url: Optional[Union[str, Tuple[str, str]]] = None  # remote URL\n    file: Optional[str] = None  # local / shared filesystem path\n    state_dict: Optional[Dict[str, Any]] = None  # in-memory state dict\n    hf_hub_id: Optional[str] = None  # Hugging Face Hub model id ('organization/model')\n    hf_hub_filename: Optional[str] = None  # Hugging Face Hub filename (overrides default)\n\n    source: Optional[str] = None  # source of cfg / weight location used (url, file, hf-hub)\n    architecture: Optional[str] = None  # architecture variant can be set when not implicit\n    tag: Optional[str] = None  # pretrained tag of source\n    custom_load: bool = False  # use custom model specific model.load_pretrained() (ie for npz files)\n\n    # input / data config\n    input_size: Tuple[int, int, int] = (3, 224, 224)\n    test_input_size: Optional[Tuple[int, int, int]] = None\n    min_input_size: Optional[Tuple[int, int, int]] = None\n    fixed_input_size: bool = False\n    interpolation: str = 'bicubic'\n    crop_pct: float = 0.875\n    test_crop_pct: Optional[float] = None\n    crop_mode: str = 'center'\n    mean: Tuple[float, ...] = (0.485, 0.456, 0.406)\n    std: Tuple[float, ...] = (0.229, 0.224, 0.225)\n\n    # head / classifier config and meta-data\n    num_classes: int = 1000\n    label_offset: Optional[int] = None\n    label_names: Optional[Tuple[str]] = None\n    label_descriptions: Optional[Dict[str, str]] = None\n\n    # model attributes that vary with above or required for pretrained adaptation\n    pool_size: Optional[Tuple[int, ...]] = None\n    test_pool_size: Optional[Tuple[int, ...]] = None\n    first_conv: Optional[str] = None\n    classifier: Optional[str] = None\n\n    license: Optional[str] = None\n    description: Optional[str] = None\n    origin_url: Optional[str] = None\n    paper_name: Optional[str] = None\n    paper_ids: Optional[Union[str, Tuple[str]]] = None\n    notes: Optional[Tuple[str]] = None\n\n    @property\n    def has_weights(self):\n        return self.url or self.file or self.hf_hub_id\n\n    def to_dict(self, remove_source=False, remove_null=True):\n        return filter_pretrained_cfg(\n            asdict(self),\n            remove_source=remove_source,\n            remove_null=remove_null\n        )\n\n\ndef filter_pretrained_cfg(cfg, remove_source=False, remove_null=True):\n    filtered_cfg = {}\n    keep_null = {'pool_size', 'first_conv', 'classifier'}  # always keep these keys, even if none\n    for k, v in cfg.items():\n        if remove_source and k in {'url', 'file', 'hf_hub_id', 'hf_hub_id', 'hf_hub_filename', 'source'}:\n            continue\n        if remove_null and v is None and k not in keep_null:\n            continue\n        filtered_cfg[k] = v\n    return filtered_cfg\n\n\n@dataclass\nclass DefaultCfg:\n    tags: Deque[str] = field(default_factory=deque)  # priority queue of tags (first is default)\n    cfgs: Dict[str, PretrainedCfg] = field(default_factory=dict)  # pretrained cfgs by tag\n    is_pretrained: bool = False  # at least one of the configs has a pretrained source set\n\n    @property\n    def default(self):\n        return self.cfgs[self.tags[0]]\n\n    @property\n    def default_with_tag(self):\n        tag = self.tags[0]\n        return tag, self.cfgs[tag]\n",
  "\"\"\" Model Registry\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport fnmatch\nimport re\nimport sys\nimport warnings\nfrom collections import defaultdict, deque\nfrom copy import deepcopy\nfrom dataclasses import replace\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Set, Sequence, Union, Tuple\n\nfrom ._pretrained import PretrainedCfg, DefaultCfg\n\n__all__ = [\n    'split_model_name_tag', 'get_arch_name', 'register_model', 'generate_default_cfgs',\n    'list_models', 'list_pretrained', 'is_model', 'model_entrypoint', 'list_modules', 'is_model_in_modules',\n    'get_pretrained_cfg_value', 'is_model_pretrained'\n]\n\n_module_to_models: Dict[str, Set[str]] = defaultdict(set)  # dict of sets to check membership of model in module\n_model_to_module: Dict[str, str] = {}  # mapping of model names to module names\n_model_entrypoints: Dict[str, Callable[..., Any]] = {}  # mapping of model names to architecture entrypoint fns\n_model_has_pretrained: Set[str] = set()  # set of model names that have pretrained weight url present\n_model_default_cfgs: Dict[str, PretrainedCfg] = {}  # central repo for model arch -> default cfg objects\n_model_pretrained_cfgs: Dict[str, PretrainedCfg] = {}  # central repo for model arch.tag -> pretrained cfgs\n_model_with_tags: Dict[str, List[str]] = defaultdict(list)  # shortcut to map each model arch to all model + tag names\n_module_to_deprecated_models: Dict[str, Dict[str, Optional[str]]] = defaultdict(dict)\n_deprecated_models: Dict[str, Optional[str]] = {}\n\n\ndef split_model_name_tag(model_name: str, no_tag: str = '') -> Tuple[str, str]:\n    model_name, *tag_list = model_name.split('.', 1)\n    tag = tag_list[0] if tag_list else no_tag\n    return model_name, tag\n\n\ndef get_arch_name(model_name: str) -> str:\n    return split_model_name_tag(model_name)[0]\n\n\ndef generate_default_cfgs(cfgs: Dict[str, Union[Dict[str, Any], PretrainedCfg]]):\n    out = defaultdict(DefaultCfg)\n    default_set = set()  # no tag and tags ending with * are prioritized as default\n\n    for k, v in cfgs.items():\n        if isinstance(v, dict):\n            v = PretrainedCfg(**v)\n        has_weights = v.has_weights\n\n        model, tag = split_model_name_tag(k)\n        is_default_set = model in default_set\n        priority = (has_weights and not tag) or (tag.endswith('*') and not is_default_set)\n        tag = tag.strip('*')\n\n        default_cfg = out[model]\n\n        if priority:\n            default_cfg.tags.appendleft(tag)\n            default_set.add(model)\n        elif has_weights and not default_cfg.is_pretrained:\n            default_cfg.tags.appendleft(tag)\n        else:\n            default_cfg.tags.append(tag)\n\n        if has_weights:\n            default_cfg.is_pretrained = True\n\n        default_cfg.cfgs[tag] = v\n\n    return out\n\n\ndef register_model(fn: Callable[..., Any]) -> Callable[..., Any]:\n    # lookup containing module\n    mod = sys.modules[fn.__module__]\n    module_name_split = fn.__module__.split('.')\n    module_name = module_name_split[-1] if len(module_name_split) else ''\n\n    # add model to __all__ in module\n    model_name = fn.__name__\n    if hasattr(mod, '__all__'):\n        mod.__all__.append(model_name)\n    else:\n        mod.__all__ = [model_name]  # type: ignore\n\n    # add entries to registry dict/sets\n    _model_entrypoints[model_name] = fn\n    _model_to_module[model_name] = module_name\n    _module_to_models[module_name].add(model_name)\n    if hasattr(mod, 'default_cfgs') and model_name in mod.default_cfgs:\n        # this will catch all models that have entrypoint matching cfg key, but miss any aliasing\n        # entrypoints or non-matching combos\n        default_cfg = mod.default_cfgs[model_name]\n        if not isinstance(default_cfg, DefaultCfg):\n            # new style default cfg dataclass w/ multiple entries per model-arch\n            assert isinstance(default_cfg, dict)\n            # old style cfg dict per model-arch\n            pretrained_cfg = PretrainedCfg(**default_cfg)\n            default_cfg = DefaultCfg(tags=deque(['']), cfgs={'': pretrained_cfg})\n\n        for tag_idx, tag in enumerate(default_cfg.tags):\n            is_default = tag_idx == 0\n            pretrained_cfg = default_cfg.cfgs[tag]\n            model_name_tag = '.'.join([model_name, tag]) if tag else model_name\n            replace_items = dict(architecture=model_name, tag=tag if tag else None)\n            if pretrained_cfg.hf_hub_id and pretrained_cfg.hf_hub_id == 'timm/':\n                # auto-complete hub name w/ architecture.tag\n                replace_items['hf_hub_id'] = pretrained_cfg.hf_hub_id + model_name_tag\n            pretrained_cfg = replace(pretrained_cfg, **replace_items)\n\n            if is_default:\n                _model_pretrained_cfgs[model_name] = pretrained_cfg\n                if pretrained_cfg.has_weights:\n                    # add tagless entry if it's default and has weights\n                    _model_has_pretrained.add(model_name)\n\n            if tag:\n                _model_pretrained_cfgs[model_name_tag] = pretrained_cfg\n                if pretrained_cfg.has_weights:\n                    # add model w/ tag if tag is valid\n                    _model_has_pretrained.add(model_name_tag)\n                _model_with_tags[model_name].append(model_name_tag)\n            else:\n                _model_with_tags[model_name].append(model_name)  # has empty tag (to slowly remove these instances)\n\n        _model_default_cfgs[model_name] = default_cfg\n\n    return fn\n\n\ndef _deprecated_model_shim(deprecated_name: str, current_fn: Callable = None, current_tag: str = ''):\n    def _fn(pretrained=False, **kwargs):\n        assert current_fn is not None,  f'Model {deprecated_name} has been removed with no replacement.'\n        current_name = '.'.join([current_fn.__name__, current_tag]) if current_tag else current_fn.__name__\n        warnings.warn(f'Mapping deprecated model name {deprecated_name} to current {current_name}.', stacklevel=2)\n        pretrained_cfg = kwargs.pop('pretrained_cfg', None)\n        return current_fn(pretrained=pretrained, pretrained_cfg=pretrained_cfg or current_tag, **kwargs)\n    return _fn\n\n\ndef register_model_deprecations(module_name: str, deprecation_map: Dict[str, Optional[str]]):\n    mod = sys.modules[module_name]\n    module_name_split = module_name.split('.')\n    module_name = module_name_split[-1] if len(module_name_split) else ''\n\n    for deprecated, current in deprecation_map.items():\n        if hasattr(mod, '__all__'):\n            mod.__all__.append(deprecated)\n        current_fn = None\n        current_tag = ''\n        if current:\n            current_name, current_tag = split_model_name_tag(current)\n            current_fn = getattr(mod, current_name)\n        deprecated_entrypoint_fn = _deprecated_model_shim(deprecated, current_fn, current_tag)\n        setattr(mod, deprecated, deprecated_entrypoint_fn)\n        _model_entrypoints[deprecated] = deprecated_entrypoint_fn\n        _model_to_module[deprecated] = module_name\n        _module_to_models[module_name].add(deprecated)\n        _deprecated_models[deprecated] = current\n        _module_to_deprecated_models[module_name][deprecated] = current\n\n\ndef _natural_key(string_: str) -> List[Union[int, str]]:\n    \"\"\"See https://blog.codinghorror.com/sorting-for-humans-natural-sort-order/\"\"\"\n    return [int(s) if s.isdigit() else s for s in re.split(r'(\\d+)', string_.lower())]\n\n\ndef _expand_filter(filter: str):\n    \"\"\" expand a 'base_filter' to 'base_filter.*' if no tag portion\"\"\"\n    filter_base, filter_tag = split_model_name_tag(filter)\n    if not filter_tag:\n        return ['.'.join([filter_base, '*']), filter]\n    else:\n        return [filter]\n\n\ndef list_models(\n        filter: Union[str, List[str]] = '',\n        module: str = '',\n        pretrained: bool = False,\n        exclude_filters: Union[str, List[str]] = '',\n        name_matches_cfg: bool = False,\n        include_tags: Optional[bool] = None,\n) -> List[str]:\n    \"\"\" Return list of available model names, sorted alphabetically\n\n    Args:\n        filter - Wildcard filter string that works with fnmatch\n        module - Limit model selection to a specific submodule (ie 'vision_transformer')\n        pretrained - Include only models with valid pretrained weights if True\n        exclude_filters - Wildcard filters to exclude models after including them with filter\n        name_matches_cfg - Include only models w/ model_name matching default_cfg name (excludes some aliases)\n        include_tags - Include pretrained tags in model names (model.tag). If None, defaults\n            set to True when pretrained=True else False (default: None)\n\n    Returns:\n        models - The sorted list of models\n\n    Example:\n        model_list('gluon_resnet*') -- returns all models starting with 'gluon_resnet'\n        model_list('*resnext*, 'resnet') -- returns all models with 'resnext' in 'resnet' module\n    \"\"\"\n    if filter:\n        include_filters = filter if isinstance(filter, (tuple, list)) else [filter]\n    else:\n        include_filters = []\n\n    if include_tags is None:\n        # FIXME should this be default behaviour? or default to include_tags=True?\n        include_tags = pretrained\n\n    all_models: Set[str] = _module_to_models[module] if module else set(_model_entrypoints.keys())\n    all_models = all_models - _deprecated_models.keys()  # remove deprecated models from listings\n\n    if include_tags:\n        # expand model names to include names w/ pretrained tags\n        models_with_tags: Set[str] = set()\n        for m in all_models:\n            models_with_tags.update(_model_with_tags[m])\n        all_models = models_with_tags\n        # expand include and exclude filters to include a '.*' for proper match if no tags in filter\n        include_filters = [ef for f in include_filters for ef in _expand_filter(f)]\n        exclude_filters = [ef for f in exclude_filters for ef in _expand_filter(f)]\n\n    if include_filters:\n        models: Set[str] = set()\n        for f in include_filters:\n            include_models = fnmatch.filter(all_models, f)  # include these models\n            if len(include_models):\n                models = models.union(include_models)\n    else:\n        models = all_models\n\n    if exclude_filters:\n        if not isinstance(exclude_filters, (tuple, list)):\n            exclude_filters = [exclude_filters]\n        for xf in exclude_filters:\n            exclude_models = fnmatch.filter(models, xf)  # exclude these models\n            if len(exclude_models):\n                models = models.difference(exclude_models)\n\n    if pretrained:\n        models = _model_has_pretrained.intersection(models)\n\n    if name_matches_cfg:\n        models = set(_model_pretrained_cfgs).intersection(models)\n\n    return sorted(models, key=_natural_key)\n\n\ndef list_pretrained(\n        filter: Union[str, List[str]] = '',\n        exclude_filters: str = '',\n) -> List[str]:\n    return list_models(\n        filter=filter,\n        pretrained=True,\n        exclude_filters=exclude_filters,\n        include_tags=True,\n    )\n\n\ndef get_deprecated_models(module: str = '') -> Dict[str, str]:\n    all_deprecated = _module_to_deprecated_models[module] if module else _deprecated_models\n    return deepcopy(all_deprecated)\n\n\ndef is_model(model_name: str) -> bool:\n    \"\"\" Check if a model name exists\n    \"\"\"\n    arch_name = get_arch_name(model_name)\n    return arch_name in _model_entrypoints\n\n\ndef model_entrypoint(model_name: str, module_filter: Optional[str] = None) -> Callable[..., Any]:\n    \"\"\"Fetch a model entrypoint for specified model name\n    \"\"\"\n    arch_name = get_arch_name(model_name)\n    if module_filter and arch_name not in _module_to_models.get(module_filter, {}):\n        raise RuntimeError(f'Model ({model_name} not found in module {module_filter}.')\n    return _model_entrypoints[arch_name]\n\n\ndef list_modules() -> List[str]:\n    \"\"\" Return list of module names that contain models / model entrypoints\n    \"\"\"\n    modules = _module_to_models.keys()\n    return sorted(modules)\n\n\ndef is_model_in_modules(\n        model_name: str, module_names: Union[Tuple[str, ...], List[str], Set[str]]\n) -> bool:\n    \"\"\"Check if a model exists within a subset of modules\n\n    Args:\n        model_name - name of model to check\n        module_names - names of modules to search in\n    \"\"\"\n    arch_name = get_arch_name(model_name)\n    assert isinstance(module_names, (tuple, list, set))\n    return any(arch_name in _module_to_models[n] for n in module_names)\n\n\ndef is_model_pretrained(model_name: str) -> bool:\n    return model_name in _model_has_pretrained\n\n\ndef get_pretrained_cfg(model_name: str, allow_unregistered: bool = True) -> Optional[PretrainedCfg]:\n    if model_name in _model_pretrained_cfgs:\n        return deepcopy(_model_pretrained_cfgs[model_name])\n    arch_name, tag = split_model_name_tag(model_name)\n    if arch_name in _model_default_cfgs:\n        # if model arch exists, but the tag is wrong, error out\n        raise RuntimeError(f'Invalid pretrained tag ({tag}) for {arch_name}.')\n    if allow_unregistered:\n        # if model arch doesn't exist, it has no pretrained_cfg registered, allow a default to be created\n        return None\n    raise RuntimeError(f'Model architecture ({arch_name}) has no pretrained cfg registered.')\n\n\ndef get_pretrained_cfg_value(model_name: str, cfg_key: str) -> Optional[Any]:\n    \"\"\" Get a specific model default_cfg value by key. None if key doesn't exist.\n    \"\"\"\n    cfg = get_pretrained_cfg(model_name, allow_unregistered=False)\n    return getattr(cfg, cfg_key, None)\n",
  "\"\"\" Transformer in Transformer (TNT) in PyTorch\n\nA PyTorch implement of TNT as described in\n'Transformer in Transformer' - https://arxiv.org/abs/2103.00112\n\nThe official mindspore code is released and available at\nhttps://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/TNT\n\"\"\"\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import Mlp, DropPath, trunc_normal_, _assert, to_2tuple\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model\nfrom .vision_transformer import resize_pos_embed\n\n__all__ = ['TNT']  # model_registry will add each entrypoint fn to this\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'pixel_embed.proj', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = {\n    'tnt_s_patch16_224': _cfg(\n        url='https://github.com/contrastive/pytorch-image-models/releases/download/TNT/tnt_s_patch16_224.pth.tar',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n    ),\n    'tnt_b_patch16_224': _cfg(\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n    ),\n}\n\n\nclass Attention(nn.Module):\n    \"\"\" Multi-Head Attention\n    \"\"\"\n    def __init__(self, dim, hidden_dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.num_heads = num_heads\n        head_dim = hidden_dim // num_heads\n        self.head_dim = head_dim\n        self.scale = head_dim ** -0.5\n\n        self.qk = nn.Linear(dim, hidden_dim * 2, bias=qkv_bias)\n        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop, inplace=True)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop, inplace=True)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qk = self.qk(x).reshape(B, N, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k = qk.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n        v = self.v(x).reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n    \"\"\" TNT Block\n    \"\"\"\n    def __init__(\n            self,\n            dim,\n            dim_out,\n            num_pixel,\n            num_heads_in=4,\n            num_heads_out=12,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        # Inner transformer\n        self.norm_in = norm_layer(dim)\n        self.attn_in = Attention(\n            dim,\n            dim,\n            num_heads=num_heads_in,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n        )\n        \n        self.norm_mlp_in = norm_layer(dim)\n        self.mlp_in = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * 4),\n            out_features=dim,\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n        \n        self.norm1_proj = norm_layer(dim)\n        self.proj = nn.Linear(dim * num_pixel, dim_out, bias=True)\n\n        # Outer transformer\n        self.norm_out = norm_layer(dim_out)\n        self.attn_out = Attention(\n            dim_out,\n            dim_out,\n            num_heads=num_heads_out,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n        )\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        \n        self.norm_mlp = norm_layer(dim_out)\n        self.mlp = Mlp(\n            in_features=dim_out,\n            hidden_features=int(dim_out * mlp_ratio),\n            out_features=dim_out,\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n\n    def forward(self, pixel_embed, patch_embed):\n        # inner\n        pixel_embed = pixel_embed + self.drop_path(self.attn_in(self.norm_in(pixel_embed)))\n        pixel_embed = pixel_embed + self.drop_path(self.mlp_in(self.norm_mlp_in(pixel_embed)))\n        # outer\n        B, N, C = patch_embed.size()\n        patch_embed = torch.cat(\n            [patch_embed[:, 0:1], patch_embed[:, 1:] + self.proj(self.norm1_proj(pixel_embed).reshape(B, N - 1, -1))],\n            dim=1)\n        patch_embed = patch_embed + self.drop_path(self.attn_out(self.norm_out(patch_embed)))\n        patch_embed = patch_embed + self.drop_path(self.mlp(self.norm_mlp(patch_embed)))\n        return pixel_embed, patch_embed\n\n\nclass PixelEmbed(nn.Module):\n    \"\"\" Image to Pixel Embedding\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, in_dim=48, stride=4):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        # grid_size property necessary for resizing positional embedding\n        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        num_patches = (self.grid_size[0]) * (self.grid_size[1])\n        self.img_size = img_size\n        self.num_patches = num_patches\n        self.in_dim = in_dim\n        new_patch_size = [math.ceil(ps / stride) for ps in patch_size]\n        self.new_patch_size = new_patch_size\n\n        self.proj = nn.Conv2d(in_chans, self.in_dim, kernel_size=7, padding=3, stride=stride)\n        self.unfold = nn.Unfold(kernel_size=new_patch_size, stride=new_patch_size)\n\n    def forward(self, x, pixel_pos):\n        B, C, H, W = x.shape\n        _assert(H == self.img_size[0],\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n        _assert(W == self.img_size[1],\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n        x = self.proj(x)\n        x = self.unfold(x)\n        x = x.transpose(1, 2).reshape(B * self.num_patches, self.in_dim, self.new_patch_size[0], self.new_patch_size[1])\n        x = x + pixel_pos\n        x = x.reshape(B * self.num_patches, self.in_dim, -1).transpose(1, 2)\n        return x\n\n\nclass TNT(nn.Module):\n    \"\"\" Transformer in Transformer - https://arxiv.org/abs/2103.00112\n    \"\"\"\n    def __init__(\n            self,\n            img_size=224,\n            patch_size=16,\n            in_chans=3,\n            num_classes=1000,\n            global_pool='token',\n            embed_dim=768,\n            inner_dim=48,\n            depth=12,\n            num_heads_inner=4,\n            num_heads_outer=12,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            drop_rate=0.,\n            pos_drop_rate=0.,\n            proj_drop_rate=0.,\n            attn_drop_rate=0.,\n            drop_path_rate=0.,\n            norm_layer=nn.LayerNorm,\n            first_stride=4,\n    ):\n        super().__init__()\n        assert global_pool in ('', 'token', 'avg')\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.grad_checkpointing = False\n\n        self.pixel_embed = PixelEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            in_dim=inner_dim,\n            stride=first_stride,\n        )\n        num_patches = self.pixel_embed.num_patches\n        self.num_patches = num_patches\n        new_patch_size = self.pixel_embed.new_patch_size\n        num_pixel = new_patch_size[0] * new_patch_size[1]\n        \n        self.norm1_proj = norm_layer(num_pixel * inner_dim)\n        self.proj = nn.Linear(num_pixel * inner_dim, embed_dim)\n        self.norm2_proj = norm_layer(embed_dim)\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.patch_pos = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        self.pixel_pos = nn.Parameter(torch.zeros(1, inner_dim, new_patch_size[0], new_patch_size[1]))\n        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        blocks = []\n        for i in range(depth):\n            blocks.append(Block(\n                dim=inner_dim,\n                dim_out=embed_dim,\n                num_pixel=num_pixel,\n                num_heads_in=num_heads_inner,\n                num_heads_out=num_heads_outer,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n            ))\n        self.blocks = nn.ModuleList(blocks)\n        self.norm = norm_layer(embed_dim)\n\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        trunc_normal_(self.cls_token, std=.02)\n        trunc_normal_(self.patch_pos, std=.02)\n        trunc_normal_(self.pixel_pos, std=.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'patch_pos', 'pixel_pos', 'cls_token'}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^cls_token|patch_pos|pixel_pos|pixel_embed|norm[12]_proj|proj',  # stem and embed / pos\n            blocks=[\n                (r'^blocks\\.(\\d+)', None),\n                (r'^norm', (99999,)),\n            ]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('', 'token', 'avg')\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        B = x.shape[0]\n        pixel_embed = self.pixel_embed(x, self.pixel_pos)\n        \n        patch_embed = self.norm2_proj(self.proj(self.norm1_proj(pixel_embed.reshape(B, self.num_patches, -1))))\n        patch_embed = torch.cat((self.cls_token.expand(B, -1, -1), patch_embed), dim=1)\n        patch_embed = patch_embed + self.patch_pos\n        patch_embed = self.pos_drop(patch_embed)\n\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            for blk in self.blocks:\n                pixel_embed, patch_embed = checkpoint(blk, pixel_embed, patch_embed)\n        else:\n            for blk in self.blocks:\n                pixel_embed, patch_embed = blk(pixel_embed, patch_embed)\n\n        patch_embed = self.norm(patch_embed)\n        return patch_embed\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool:\n            x = x[:, 1:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]\n        x = self.head_drop(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n    if state_dict['patch_pos'].shape != model.patch_pos.shape:\n        state_dict['patch_pos'] = resize_pos_embed(state_dict['patch_pos'],\n            model.patch_pos, getattr(model, 'num_tokens', 1), model.pixel_embed.grid_size)\n    return state_dict\n\n\ndef _create_tnt(variant, pretrained=False, **kwargs):\n    if kwargs.get('features_only', None):\n        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n\n    model = build_model_with_cfg(\n        TNT, variant, pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        **kwargs)\n    return model\n\n\n@register_model\ndef tnt_s_patch16_224(pretrained=False, **kwargs) -> TNT:\n    model_cfg = dict(\n        patch_size=16, embed_dim=384, inner_dim=24, depth=12, num_heads_outer=6,\n        qkv_bias=False)\n    model = _create_tnt('tnt_s_patch16_224', pretrained=pretrained, **dict(model_cfg, **kwargs))\n    return model\n\n\n@register_model\ndef tnt_b_patch16_224(pretrained=False, **kwargs) -> TNT:\n    model_cfg = dict(\n        patch_size=16, embed_dim=640, inner_dim=40, depth=12, num_heads_outer=10,\n        qkv_bias=False)\n    model = _create_tnt('tnt_b_patch16_224', pretrained=pretrained, **dict(model_cfg, **kwargs))\n    return model\n",
  "\"\"\" ResNeSt Models\n\nPaper: `ResNeSt: Split-Attention Networks` - https://arxiv.org/abs/2004.08955\n\nAdapted from original PyTorch impl w/ weights at https://github.com/zhanghang1989/ResNeSt by Hang Zhang\n\nModified for torchscript compat, and consistency with timm by Ross Wightman\n\"\"\"\nfrom torch import nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import SplitAttn\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\nfrom .resnet import ResNet\n\n\nclass ResNestBottleneck(nn.Module):\n    \"\"\"ResNet Bottleneck\n    \"\"\"\n    # pylint: disable=unused-argument\n    expansion = 4\n\n    def __init__(\n            self,\n            inplanes,\n            planes,\n            stride=1,\n            downsample=None,\n            radix=1,\n            cardinality=1,\n            base_width=64,\n            avd=False,\n            avd_first=False,\n            is_first=False,\n            reduce_first=1,\n            dilation=1,\n            first_dilation=None,\n            act_layer=nn.ReLU,\n            norm_layer=nn.BatchNorm2d,\n            attn_layer=None,\n            aa_layer=None,\n            drop_block=None,\n            drop_path=None,\n    ):\n        super(ResNestBottleneck, self).__init__()\n        assert reduce_first == 1  # not supported\n        assert attn_layer is None  # not supported\n        assert aa_layer is None  # TODO not yet supported\n        assert drop_path is None  # TODO not yet supported\n\n        group_width = int(planes * (base_width / 64.)) * cardinality\n        first_dilation = first_dilation or dilation\n        if avd and (stride > 1 or is_first):\n            avd_stride = stride\n            stride = 1\n        else:\n            avd_stride = 0\n        self.radix = radix\n\n        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(group_width)\n        self.act1 = act_layer(inplace=True)\n        self.avd_first = nn.AvgPool2d(3, avd_stride, padding=1) if avd_stride > 0 and avd_first else None\n\n        if self.radix >= 1:\n            self.conv2 = SplitAttn(\n                group_width, group_width, kernel_size=3, stride=stride, padding=first_dilation,\n                dilation=first_dilation, groups=cardinality, radix=radix, norm_layer=norm_layer, drop_layer=drop_block)\n            self.bn2 = nn.Identity()\n            self.drop_block = nn.Identity()\n            self.act2 = nn.Identity()\n        else:\n            self.conv2 = nn.Conv2d(\n                group_width, group_width, kernel_size=3, stride=stride, padding=first_dilation,\n                dilation=first_dilation, groups=cardinality, bias=False)\n            self.bn2 = norm_layer(group_width)\n            self.drop_block = drop_block() if drop_block is not None else nn.Identity()\n            self.act2 = act_layer(inplace=True)\n        self.avd_last = nn.AvgPool2d(3, avd_stride, padding=1) if avd_stride > 0 and not avd_first else None\n\n        self.conv3 = nn.Conv2d(group_width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes*4)\n        self.act3 = act_layer(inplace=True)\n        self.downsample = downsample\n\n    def zero_init_last(self):\n        if getattr(self.bn3, 'weight', None) is not None:\n            nn.init.zeros_(self.bn3.weight)\n\n    def forward(self, x):\n        shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.act1(out)\n\n        if self.avd_first is not None:\n            out = self.avd_first(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.drop_block(out)\n        out = self.act2(out)\n\n        if self.avd_last is not None:\n            out = self.avd_last(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out += shortcut\n        out = self.act3(out)\n        return out\n\n\ndef _create_resnest(variant, pretrained=False, **kwargs):\n    return build_model_with_cfg(\n        ResNet,\n        variant,\n        pretrained,\n        **kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bilinear',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'conv1.0', 'classifier': 'fc',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'resnest14d.gluon_in1k': _cfg(hf_hub_id='timm/'),\n    'resnest26d.gluon_in1k': _cfg(hf_hub_id='timm/'),\n    'resnest50d.in1k': _cfg(hf_hub_id='timm/'),\n    'resnest101e.in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 256, 256), pool_size=(8, 8)),\n    'resnest200e.in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=0.909, interpolation='bicubic'),\n    'resnest269e.in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 416, 416), pool_size=(13, 13), crop_pct=0.928, interpolation='bicubic'),\n    'resnest50d_4s2x40d.in1k': _cfg(\n        hf_hub_id='timm/',\n        interpolation='bicubic'),\n    'resnest50d_1s4x24d.in1k': _cfg(\n        hf_hub_id='timm/',\n        interpolation='bicubic')\n})\n\n\n@register_model\ndef resnest14d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\" ResNeSt-14d model. Weights ported from GluonCV.\n    \"\"\"\n    model_kwargs = dict(\n        block=ResNestBottleneck, layers=[1, 1, 1, 1],\n        stem_type='deep', stem_width=32, avg_down=True, base_width=64, cardinality=1,\n        block_args=dict(radix=2, avd=True, avd_first=False))\n    return _create_resnest('resnest14d', pretrained=pretrained, **dict(model_kwargs, **kwargs))\n\n\n@register_model\ndef resnest26d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\" ResNeSt-26d model. Weights ported from GluonCV.\n    \"\"\"\n    model_kwargs = dict(\n        block=ResNestBottleneck, layers=[2, 2, 2, 2],\n        stem_type='deep', stem_width=32, avg_down=True, base_width=64, cardinality=1,\n        block_args=dict(radix=2, avd=True, avd_first=False))\n    return _create_resnest('resnest26d', pretrained=pretrained, **dict(model_kwargs, **kwargs))\n\n\n@register_model\ndef resnest50d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\" ResNeSt-50d model. Matches paper ResNeSt-50 model, https://arxiv.org/abs/2004.08955\n    Since this codebase supports all possible variations, 'd' for deep stem, stem_width 32, avg in downsample.\n    \"\"\"\n    model_kwargs = dict(\n        block=ResNestBottleneck, layers=[3, 4, 6, 3],\n        stem_type='deep', stem_width=32, avg_down=True, base_width=64, cardinality=1,\n        block_args=dict(radix=2, avd=True, avd_first=False))\n    return _create_resnest('resnest50d', pretrained=pretrained, **dict(model_kwargs, **kwargs))\n\n\n@register_model\ndef resnest101e(pretrained=False, **kwargs) -> ResNet:\n    \"\"\" ResNeSt-101e model. Matches paper ResNeSt-101 model, https://arxiv.org/abs/2004.08955\n     Since this codebase supports all possible variations, 'e' for deep stem, stem_width 64, avg in downsample.\n    \"\"\"\n    model_kwargs = dict(\n        block=ResNestBottleneck, layers=[3, 4, 23, 3],\n        stem_type='deep', stem_width=64, avg_down=True, base_width=64, cardinality=1,\n        block_args=dict(radix=2, avd=True, avd_first=False))\n    return _create_resnest('resnest101e', pretrained=pretrained, **dict(model_kwargs, **kwargs))\n\n\n@register_model\ndef resnest200e(pretrained=False, **kwargs) -> ResNet:\n    \"\"\" ResNeSt-200e model. Matches paper ResNeSt-200 model, https://arxiv.org/abs/2004.08955\n    Since this codebase supports all possible variations, 'e' for deep stem, stem_width 64, avg in downsample.\n    \"\"\"\n    model_kwargs = dict(\n        block=ResNestBottleneck, layers=[3, 24, 36, 3],\n        stem_type='deep', stem_width=64, avg_down=True, base_width=64, cardinality=1,\n        block_args=dict(radix=2, avd=True, avd_first=False))\n    return _create_resnest('resnest200e', pretrained=pretrained, **dict(model_kwargs, **kwargs))\n\n\n@register_model\ndef resnest269e(pretrained=False, **kwargs) -> ResNet:\n    \"\"\" ResNeSt-269e model. Matches paper ResNeSt-269 model, https://arxiv.org/abs/2004.08955\n    Since this codebase supports all possible variations, 'e' for deep stem, stem_width 64, avg in downsample.\n    \"\"\"\n    model_kwargs = dict(\n        block=ResNestBottleneck, layers=[3, 30, 48, 8],\n        stem_type='deep', stem_width=64, avg_down=True, base_width=64, cardinality=1,\n        block_args=dict(radix=2, avd=True, avd_first=False))\n    return _create_resnest('resnest269e', pretrained=pretrained, **dict(model_kwargs, **kwargs))\n\n\n@register_model\ndef resnest50d_4s2x40d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"ResNeSt-50 4s2x40d from https://github.com/zhanghang1989/ResNeSt/blob/master/ablation.md\n    \"\"\"\n    model_kwargs = dict(\n        block=ResNestBottleneck, layers=[3, 4, 6, 3],\n        stem_type='deep', stem_width=32, avg_down=True, base_width=40, cardinality=2,\n        block_args=dict(radix=4, avd=True, avd_first=True))\n    return _create_resnest('resnest50d_4s2x40d', pretrained=pretrained, **dict(model_kwargs, **kwargs))\n\n\n@register_model\ndef resnest50d_1s4x24d(pretrained=False, **kwargs) -> ResNet:\n    \"\"\"ResNeSt-50 1s4x24d from https://github.com/zhanghang1989/ResNeSt/blob/master/ablation.md\n    \"\"\"\n    model_kwargs = dict(\n        block=ResNestBottleneck, layers=[3, 4, 6, 3],\n        stem_type='deep', stem_width=32, avg_down=True, base_width=24, cardinality=4,\n        block_args=dict(radix=1, avd=True, avd_first=True))\n    return _create_resnest('resnest50d_1s4x24d', pretrained=pretrained, **dict(model_kwargs, **kwargs))\n",
  "\"\"\" EfficientNet, MobileNetV3, etc Builder\n\nAssembles EfficieNet and related network feature blocks from string definitions.\nHandles stride, dilation calculations, and selects feature extraction points.\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\n\nimport logging\nimport math\nimport re\nfrom copy import deepcopy\nfrom functools import partial\n\nimport torch.nn as nn\n\nfrom ._efficientnet_blocks import *\nfrom timm.layers import CondConv2d, get_condconv_initializer, get_act_layer, get_attn, make_divisible\n\n__all__ = [\"EfficientNetBuilder\", \"decode_arch_def\", \"efficientnet_init_weights\",\n           'resolve_bn_args', 'resolve_act_layer', 'round_channels', 'BN_MOMENTUM_TF_DEFAULT', 'BN_EPS_TF_DEFAULT']\n\n_logger = logging.getLogger(__name__)\n\n\n_DEBUG_BUILDER = False\n\n# Defaults used for Google/Tensorflow training of mobile networks /w RMSprop as per\n# papers and TF reference implementations. PT momentum equiv for TF decay is (1 - TF decay)\n# NOTE: momentum varies btw .99 and .9997 depending on source\n# .99 in official TF TPU impl\n# .9997 (/w .999 in search space) for paper\nBN_MOMENTUM_TF_DEFAULT = 1 - 0.99\nBN_EPS_TF_DEFAULT = 1e-3\n_BN_ARGS_TF = dict(momentum=BN_MOMENTUM_TF_DEFAULT, eps=BN_EPS_TF_DEFAULT)\n\n\ndef get_bn_args_tf():\n    return _BN_ARGS_TF.copy()\n\n\ndef resolve_bn_args(kwargs):\n    bn_args = {}\n    bn_momentum = kwargs.pop('bn_momentum', None)\n    if bn_momentum is not None:\n        bn_args['momentum'] = bn_momentum\n    bn_eps = kwargs.pop('bn_eps', None)\n    if bn_eps is not None:\n        bn_args['eps'] = bn_eps\n    return bn_args\n\n\ndef resolve_act_layer(kwargs, default='relu'):\n    return get_act_layer(kwargs.pop('act_layer', default))\n\n\ndef round_channels(channels, multiplier=1.0, divisor=8, channel_min=None, round_limit=0.9):\n    \"\"\"Round number of filters based on depth multiplier.\"\"\"\n    if not multiplier:\n        return channels\n    return make_divisible(channels * multiplier, divisor, channel_min, round_limit=round_limit)\n\n\ndef _log_info_if(msg, condition):\n    if condition:\n        _logger.info(msg)\n\n\ndef _parse_ksize(ss):\n    if ss.isdigit():\n        return int(ss)\n    else:\n        return [int(k) for k in ss.split('.')]\n\n\ndef _decode_block_str(block_str):\n    \"\"\" Decode block definition string\n\n    Gets a list of block arg (dicts) through a string notation of arguments.\n    E.g. ir_r2_k3_s2_e1_i32_o16_se0.25_noskip\n\n    All args can exist in any order with the exception of the leading string which\n    is assumed to indicate the block type.\n\n    leading string - block type (\n      ir = InvertedResidual, ds = DepthwiseSep, dsa = DeptwhiseSep with pw act, cn = ConvBnAct)\n    r - number of repeat blocks,\n    k - kernel size,\n    s - strides (1-9),\n    e - expansion ratio,\n    c - output channels,\n    se - squeeze/excitation ratio\n    n - activation fn ('re', 'r6', 'hs', or 'sw')\n    Args:\n        block_str: a string representation of block arguments.\n    Returns:\n        A list of block args (dicts)\n    Raises:\n        ValueError: if the string def not properly specified (TODO)\n    \"\"\"\n    assert isinstance(block_str, str)\n    ops = block_str.split('_')\n    block_type = ops[0]  # take the block type off the front\n    ops = ops[1:]\n    options = {}\n    skip = None\n    for op in ops:\n        # string options being checked on individual basis, combine if they grow\n        if op == 'noskip':\n            skip = False  # force no skip connection\n        elif op == 'skip':\n            skip = True  # force a skip connection\n        elif op.startswith('n'):\n            # activation fn\n            key = op[0]\n            v = op[1:]\n            if v == 're':\n                value = get_act_layer('relu')\n            elif v == 'r6':\n                value = get_act_layer('relu6')\n            elif v == 'hs':\n                value = get_act_layer('hard_swish')\n            elif v == 'sw':\n                value = get_act_layer('swish')  # aka SiLU\n            elif v == 'mi':\n                value = get_act_layer('mish')\n            else:\n                continue\n            options[key] = value\n        else:\n            # all numeric options\n            splits = re.split(r'(\\d.*)', op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n    # if act_layer is None, the model default (passed to model init) will be used\n    act_layer = options['n'] if 'n' in options else None\n    exp_kernel_size = _parse_ksize(options['a']) if 'a' in options else 1\n    pw_kernel_size = _parse_ksize(options['p']) if 'p' in options else 1\n    force_in_chs = int(options['fc']) if 'fc' in options else 0  # FIXME hack to deal with in_chs issue in TPU def\n    num_repeat = int(options['r'])\n\n    # each type of block has different valid arguments, fill accordingly\n    block_args = dict(\n        block_type=block_type,\n        out_chs=int(options['c']),\n        stride=int(options['s']),\n        act_layer=act_layer,\n    )\n    if block_type == 'ir':\n        block_args.update(dict(\n            dw_kernel_size=_parse_ksize(options['k']),\n            exp_kernel_size=exp_kernel_size,\n            pw_kernel_size=pw_kernel_size,\n            exp_ratio=float(options['e']),\n            se_ratio=float(options['se']) if 'se' in options else 0.,\n            noskip=skip is False,\n        ))\n        if 'cc' in options:\n            block_args['num_experts'] = int(options['cc'])\n    elif block_type == 'ds' or block_type == 'dsa':\n        block_args.update(dict(\n            dw_kernel_size=_parse_ksize(options['k']),\n            pw_kernel_size=pw_kernel_size,\n            se_ratio=float(options['se']) if 'se' in options else 0.,\n            pw_act=block_type == 'dsa',\n            noskip=block_type == 'dsa' or skip is False,\n        ))\n    elif block_type == 'er':\n        block_args.update(dict(\n            exp_kernel_size=_parse_ksize(options['k']),\n            pw_kernel_size=pw_kernel_size,\n            exp_ratio=float(options['e']),\n            force_in_chs=force_in_chs,\n            se_ratio=float(options['se']) if 'se' in options else 0.,\n            noskip=skip is False,\n        ))\n    elif block_type == 'cn':\n        block_args.update(dict(\n            kernel_size=int(options['k']),\n            skip=skip is True,\n        ))\n    else:\n        assert False, 'Unknown block type (%s)' % block_type\n    if 'gs' in options:\n        block_args['group_size'] = options['gs']\n\n    return block_args, num_repeat\n\n\ndef _scale_stage_depth(stack_args, repeats, depth_multiplier=1.0, depth_trunc='ceil'):\n    \"\"\" Per-stage depth scaling\n    Scales the block repeats in each stage. This depth scaling impl maintains\n    compatibility with the EfficientNet scaling method, while allowing sensible\n    scaling for other models that may have multiple block arg definitions in each stage.\n    \"\"\"\n\n    # We scale the total repeat count for each stage, there may be multiple\n    # block arg defs per stage so we need to sum.\n    num_repeat = sum(repeats)\n    if depth_trunc == 'round':\n        # Truncating to int by rounding allows stages with few repeats to remain\n        # proportionally smaller for longer. This is a good choice when stage definitions\n        # include single repeat stages that we'd prefer to keep that way as long as possible\n        num_repeat_scaled = max(1, round(num_repeat * depth_multiplier))\n    else:\n        # The default for EfficientNet truncates repeats to int via 'ceil'.\n        # Any multiplier > 1.0 will result in an increased depth for every stage.\n        num_repeat_scaled = int(math.ceil(num_repeat * depth_multiplier))\n\n    # Proportionally distribute repeat count scaling to each block definition in the stage.\n    # Allocation is done in reverse as it results in the first block being less likely to be scaled.\n    # The first block makes less sense to repeat in most of the arch definitions.\n    repeats_scaled = []\n    for r in repeats[::-1]:\n        rs = max(1, round((r / num_repeat * num_repeat_scaled)))\n        repeats_scaled.append(rs)\n        num_repeat -= r\n        num_repeat_scaled -= rs\n    repeats_scaled = repeats_scaled[::-1]\n\n    # Apply the calculated scaling to each block arg in the stage\n    sa_scaled = []\n    for ba, rep in zip(stack_args, repeats_scaled):\n        sa_scaled.extend([deepcopy(ba) for _ in range(rep)])\n    return sa_scaled\n\n\ndef decode_arch_def(\n        arch_def,\n        depth_multiplier=1.0,\n        depth_trunc='ceil',\n        experts_multiplier=1,\n        fix_first_last=False,\n        group_size=None,\n):\n    \"\"\" Decode block architecture definition strings -> block kwargs\n\n    Args:\n        arch_def: architecture definition strings, list of list of strings\n        depth_multiplier: network depth multiplier\n        depth_trunc: networ depth truncation mode when applying multiplier\n        experts_multiplier: CondConv experts multiplier\n        fix_first_last: fix first and last block depths when multiplier is applied\n        group_size: group size override for all blocks that weren't explicitly set in arch string\n\n    Returns:\n        list of list of block kwargs\n    \"\"\"\n    arch_args = []\n    if isinstance(depth_multiplier, tuple):\n        assert len(depth_multiplier) == len(arch_def)\n    else:\n        depth_multiplier = (depth_multiplier,) * len(arch_def)\n    for stack_idx, (block_strings, multiplier) in enumerate(zip(arch_def, depth_multiplier)):\n        assert isinstance(block_strings, list)\n        stack_args = []\n        repeats = []\n        for block_str in block_strings:\n            assert isinstance(block_str, str)\n            ba, rep = _decode_block_str(block_str)\n            if ba.get('num_experts', 0) > 0 and experts_multiplier > 1:\n                ba['num_experts'] *= experts_multiplier\n            if group_size is not None:\n                ba.setdefault('group_size', group_size)\n            stack_args.append(ba)\n            repeats.append(rep)\n        if fix_first_last and (stack_idx == 0 or stack_idx == len(arch_def) - 1):\n            arch_args.append(_scale_stage_depth(stack_args, repeats, 1.0, depth_trunc))\n        else:\n            arch_args.append(_scale_stage_depth(stack_args, repeats, multiplier, depth_trunc))\n    return arch_args\n\n\nclass EfficientNetBuilder:\n    \"\"\" Build Trunk Blocks\n\n    This ended up being somewhat of a cross between\n    https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mnasnet_models.py\n    and\n    https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/modeling/backbone/fbnet_builder.py\n\n    \"\"\"\n    def __init__(self, output_stride=32, pad_type='', round_chs_fn=round_channels, se_from_exp=False,\n                 act_layer=None, norm_layer=None, se_layer=None, drop_path_rate=0., feature_location=''):\n        self.output_stride = output_stride\n        self.pad_type = pad_type\n        self.round_chs_fn = round_chs_fn\n        self.se_from_exp = se_from_exp  # calculate se channel reduction from expanded (mid) chs\n        self.act_layer = act_layer\n        self.norm_layer = norm_layer\n        self.se_layer = get_attn(se_layer)\n        try:\n            self.se_layer(8, rd_ratio=1.0)  # test if attn layer accepts rd_ratio arg\n            self.se_has_ratio = True\n        except TypeError:\n            self.se_has_ratio = False\n        self.drop_path_rate = drop_path_rate\n        if feature_location == 'depthwise':\n            # old 'depthwise' mode renamed 'expansion' to match TF impl, old expansion mode didn't make sense\n            _logger.warning(\"feature_location=='depthwise' is deprecated, using 'expansion'\")\n            feature_location = 'expansion'\n        self.feature_location = feature_location\n        assert feature_location in ('bottleneck', 'expansion', '')\n        self.verbose = _DEBUG_BUILDER\n\n        # state updated during build, consumed by model\n        self.in_chs = None\n        self.features = []\n\n    def _make_block(self, ba, block_idx, block_count):\n        drop_path_rate = self.drop_path_rate * block_idx / block_count\n        bt = ba.pop('block_type')\n        ba['in_chs'] = self.in_chs\n        ba['out_chs'] = self.round_chs_fn(ba['out_chs'])\n        if 'force_in_chs' in ba and ba['force_in_chs']:\n            # NOTE this is a hack to work around mismatch in TF EdgeEffNet impl\n            ba['force_in_chs'] = self.round_chs_fn(ba['force_in_chs'])\n        ba['pad_type'] = self.pad_type\n        # block act fn overrides the model default\n        ba['act_layer'] = ba['act_layer'] if ba['act_layer'] is not None else self.act_layer\n        assert ba['act_layer'] is not None\n        ba['norm_layer'] = self.norm_layer\n        ba['drop_path_rate'] = drop_path_rate\n        if bt != 'cn':\n            se_ratio = ba.pop('se_ratio')\n            if se_ratio and self.se_layer is not None:\n                if not self.se_from_exp:\n                    # adjust se_ratio by expansion ratio if calculating se channels from block input\n                    se_ratio /= ba.get('exp_ratio', 1.0)\n                if self.se_has_ratio:\n                    ba['se_layer'] = partial(self.se_layer, rd_ratio=se_ratio)\n                else:\n                    ba['se_layer'] = self.se_layer\n\n        if bt == 'ir':\n            _log_info_if('  InvertedResidual {}, Args: {}'.format(block_idx, str(ba)), self.verbose)\n            block = CondConvResidual(**ba) if ba.get('num_experts', 0) else InvertedResidual(**ba)\n        elif bt == 'ds' or bt == 'dsa':\n            _log_info_if('  DepthwiseSeparable {}, Args: {}'.format(block_idx, str(ba)), self.verbose)\n            block = DepthwiseSeparableConv(**ba)\n        elif bt == 'er':\n            _log_info_if('  EdgeResidual {}, Args: {}'.format(block_idx, str(ba)), self.verbose)\n            block = EdgeResidual(**ba)\n        elif bt == 'cn':\n            _log_info_if('  ConvBnAct {}, Args: {}'.format(block_idx, str(ba)), self.verbose)\n            block = ConvBnAct(**ba)\n        else:\n            assert False, 'Uknkown block type (%s) while building model.' % bt\n\n        self.in_chs = ba['out_chs']  # update in_chs for arg of next block\n        return block\n\n    def __call__(self, in_chs, model_block_args):\n        \"\"\" Build the blocks\n        Args:\n            in_chs: Number of input-channels passed to first block\n            model_block_args: A list of lists, outer list defines stages, inner\n                list contains strings defining block configuration(s)\n        Return:\n             List of block stacks (each stack wrapped in nn.Sequential)\n        \"\"\"\n        _log_info_if('Building model trunk with %d stages...' % len(model_block_args), self.verbose)\n        self.in_chs = in_chs\n        total_block_count = sum([len(x) for x in model_block_args])\n        total_block_idx = 0\n        current_stride = 2\n        current_dilation = 1\n        stages = []\n        if model_block_args[0][0]['stride'] > 1:\n            # if the first block starts with a stride, we need to extract first level feat from stem\n            feature_info = dict(module='bn1', num_chs=in_chs, stage=0, reduction=current_stride)\n            self.features.append(feature_info)\n\n        # outer list of block_args defines the stacks\n        for stack_idx, stack_args in enumerate(model_block_args):\n            last_stack = stack_idx + 1 == len(model_block_args)\n            _log_info_if('Stack: {}'.format(stack_idx), self.verbose)\n            assert isinstance(stack_args, list)\n\n            blocks = []\n            # each stack (stage of blocks) contains a list of block arguments\n            for block_idx, block_args in enumerate(stack_args):\n                last_block = block_idx + 1 == len(stack_args)\n                _log_info_if(' Block: {}'.format(block_idx), self.verbose)\n\n                assert block_args['stride'] in (1, 2)\n                if block_idx >= 1:   # only the first block in any stack can have a stride > 1\n                    block_args['stride'] = 1\n\n                extract_features = False\n                if last_block:\n                    next_stack_idx = stack_idx + 1\n                    extract_features = next_stack_idx >= len(model_block_args) or \\\n                        model_block_args[next_stack_idx][0]['stride'] > 1\n\n                next_dilation = current_dilation\n                if block_args['stride'] > 1:\n                    next_output_stride = current_stride * block_args['stride']\n                    if next_output_stride > self.output_stride:\n                        next_dilation = current_dilation * block_args['stride']\n                        block_args['stride'] = 1\n                        _log_info_if('  Converting stride to dilation to maintain output_stride=={}'.format(\n                            self.output_stride), self.verbose)\n                    else:\n                        current_stride = next_output_stride\n                block_args['dilation'] = current_dilation\n                if next_dilation != current_dilation:\n                    current_dilation = next_dilation\n\n                # create the block\n                block = self._make_block(block_args, total_block_idx, total_block_count)\n                blocks.append(block)\n\n                # stash feature module name and channel info for model feature extraction\n                if extract_features:\n                    feature_info = dict(\n                        stage=stack_idx + 1,\n                        reduction=current_stride,\n                        **block.feature_info(self.feature_location),\n                    )\n                    leaf_name = feature_info.get('module', '')\n                    if leaf_name:\n                        feature_info['module'] = '.'.join([f'blocks.{stack_idx}.{block_idx}', leaf_name])\n                    else:\n                        assert last_block\n                        feature_info['module'] = f'blocks.{stack_idx}'\n                    self.features.append(feature_info)\n\n                total_block_idx += 1  # incr global block idx (across all stacks)\n            stages.append(nn.Sequential(*blocks))\n        return stages\n\n\ndef _init_weight_goog(m, n='', fix_group_fanout=True):\n    \"\"\" Weight initialization as per Tensorflow official implementations.\n\n    Args:\n        m (nn.Module): module to init\n        n (str): module name\n        fix_group_fanout (bool): enable correct (matching Tensorflow TPU impl) fanout calculation w/ group convs\n\n    Handles layers in EfficientNet, EfficientNet-CondConv, MixNet, MnasNet, MobileNetV3, etc:\n    * https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mnasnet_model.py\n    * https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py\n    \"\"\"\n    if isinstance(m, CondConv2d):\n        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        if fix_group_fanout:\n            fan_out //= m.groups\n        init_weight_fn = get_condconv_initializer(\n            lambda w: nn.init.normal_(w, 0, math.sqrt(2.0 / fan_out)), m.num_experts, m.weight_shape)\n        init_weight_fn(m.weight)\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.Conv2d):\n        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        if fix_group_fanout:\n            fan_out //= m.groups\n        nn.init.normal_(m.weight, 0, math.sqrt(2.0 / fan_out))\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.BatchNorm2d):\n        nn.init.ones_(m.weight)\n        nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.Linear):\n        fan_out = m.weight.size(0)  # fan-out\n        fan_in = 0\n        if 'routing_fn' in n:\n            fan_in = m.weight.size(1)\n        init_range = 1.0 / math.sqrt(fan_in + fan_out)\n        nn.init.uniform_(m.weight, -init_range, init_range)\n        nn.init.zeros_(m.bias)\n\n\ndef efficientnet_init_weights(model: nn.Module, init_fn=None):\n    init_fn = init_fn or _init_weight_goog\n    for n, m in model.named_modules():\n        init_fn(m, n)\n\n",
  "\"\"\" \nCoaT architecture.\n\nPaper: Co-Scale Conv-Attentional Image Transformers - https://arxiv.org/abs/2104.06399\n\nOfficial CoaT code at: https://github.com/mlpc-ucsd/CoaT\n\nModified from timm/models/vision_transformer.py\n\"\"\"\nfrom functools import partial\nfrom typing import Tuple, List, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import PatchEmbed, Mlp, DropPath, to_2tuple, trunc_normal_, _assert, LayerNorm\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['CoaT']\n\n\nclass ConvRelPosEnc(nn.Module):\n    \"\"\" Convolutional relative position encoding. \"\"\"\n    def __init__(self, head_chs, num_heads, window):\n        \"\"\"\n        Initialization.\n            Ch: Channels per head.\n            h: Number of heads.\n            window: Window size(s) in convolutional relative positional encoding. It can have two forms:\n                1. An integer of window size, which assigns all attention heads with the same window s\n                    size in ConvRelPosEnc.\n                2. A dict mapping window size to #attention head splits (\n                    e.g. {window size 1: #attention head split 1, window size 2: #attention head split 2})\n                    It will apply different window size to the attention head splits.\n        \"\"\"\n        super().__init__()\n\n        if isinstance(window, int):\n            # Set the same window size for all attention heads.\n            window = {window: num_heads}\n            self.window = window\n        elif isinstance(window, dict):\n            self.window = window\n        else:\n            raise ValueError()            \n        \n        self.conv_list = nn.ModuleList()\n        self.head_splits = []\n        for cur_window, cur_head_split in window.items():\n            dilation = 1\n            # Determine padding size.\n            # Ref: https://discuss.pytorch.org/t/how-to-keep-the-shape-of-input-and-output-same-when-dilation-conv/14338\n            padding_size = (cur_window + (cur_window - 1) * (dilation - 1)) // 2\n            cur_conv = nn.Conv2d(\n                cur_head_split * head_chs,\n                cur_head_split * head_chs,\n                kernel_size=(cur_window, cur_window), \n                padding=(padding_size, padding_size),\n                dilation=(dilation, dilation),                          \n                groups=cur_head_split * head_chs,\n            )\n            self.conv_list.append(cur_conv)\n            self.head_splits.append(cur_head_split)\n        self.channel_splits = [x * head_chs for x in self.head_splits]\n\n    def forward(self, q, v, size: Tuple[int, int]):\n        B, num_heads, N, C = q.shape\n        H, W = size\n        _assert(N == 1 + H * W, '')\n\n        # Convolutional relative position encoding.\n        q_img = q[:, :, 1:, :]  # [B, h, H*W, Ch]\n        v_img = v[:, :, 1:, :]  # [B, h, H*W, Ch]\n\n        v_img = v_img.transpose(-1, -2).reshape(B, num_heads * C, H, W)\n        v_img_list = torch.split(v_img, self.channel_splits, dim=1)  # Split according to channels\n        conv_v_img_list = []\n        for i, conv in enumerate(self.conv_list):\n            conv_v_img_list.append(conv(v_img_list[i]))\n        conv_v_img = torch.cat(conv_v_img_list, dim=1)\n        conv_v_img = conv_v_img.reshape(B, num_heads, C, H * W).transpose(-1, -2)\n\n        EV_hat = q_img * conv_v_img\n        EV_hat = F.pad(EV_hat, (0, 0, 1, 0, 0, 0))  # [B, h, N, Ch].\n        return EV_hat\n\n\nclass FactorAttnConvRelPosEnc(nn.Module):\n    \"\"\" Factorized attention with convolutional relative position encoding class. \"\"\"\n    def __init__(\n            self,\n            dim,\n            num_heads=8,\n            qkv_bias=False,\n            attn_drop=0.,\n            proj_drop=0.,\n            shared_crpe=None,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)  # Note: attn_drop is actually not used.\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        # Shared convolutional relative position encoding.\n        self.crpe = shared_crpe\n\n    def forward(self, x, size: Tuple[int, int]):\n        B, N, C = x.shape\n\n        # Generate Q, K, V.\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)  # [B, h, N, Ch]\n\n        # Factorized attention.\n        k_softmax = k.softmax(dim=2)\n        factor_att = k_softmax.transpose(-1, -2) @ v\n        factor_att = q @ factor_att\n\n        # Convolutional relative position encoding.\n        crpe = self.crpe(q, v, size=size)  # [B, h, N, Ch]\n\n        # Merge and reshape.\n        x = self.scale * factor_att + crpe\n        x = x.transpose(1, 2).reshape(B, N, C)  # [B, h, N, Ch] -> [B, N, h, Ch] -> [B, N, C]\n\n        # Output projection.\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n\n\nclass ConvPosEnc(nn.Module):\n    \"\"\" Convolutional Position Encoding. \n        Note: This module is similar to the conditional position encoding in CPVT.\n    \"\"\"\n    def __init__(self, dim, k=3):\n        super(ConvPosEnc, self).__init__()\n        self.proj = nn.Conv2d(dim, dim, k, 1, k//2, groups=dim) \n    \n    def forward(self, x, size: Tuple[int, int]):\n        B, N, C = x.shape\n        H, W = size\n        _assert(N == 1 + H * W, '')\n\n        # Extract CLS token and image tokens.\n        cls_token, img_tokens = x[:, :1], x[:, 1:]  # [B, 1, C], [B, H*W, C]\n        \n        # Depthwise convolution.\n        feat = img_tokens.transpose(1, 2).view(B, C, H, W)\n        x = self.proj(feat) + feat\n        x = x.flatten(2).transpose(1, 2)\n\n        # Combine with CLS token.\n        x = torch.cat((cls_token, x), dim=1)\n\n        return x\n\n\nclass SerialBlock(nn.Module):\n    \"\"\" Serial block class.\n        Note: In this implementation, each serial block only contains a conv-attention and a FFN (MLP) module. \"\"\"\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            shared_cpe=None,\n            shared_crpe=None,\n    ):\n        super().__init__()\n\n        # Conv-Attention.\n        self.cpe = shared_cpe\n\n        self.norm1 = norm_layer(dim)\n        self.factoratt_crpe = FactorAttnConvRelPosEnc(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n            shared_crpe=shared_crpe,\n        )\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        # MLP.\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n\n    def forward(self, x, size: Tuple[int, int]):\n        # Conv-Attention.\n        x = self.cpe(x, size)\n        cur = self.norm1(x)\n        cur = self.factoratt_crpe(cur, size)\n        x = x + self.drop_path(cur) \n\n        # MLP. \n        cur = self.norm2(x)\n        cur = self.mlp(cur)\n        x = x + self.drop_path(cur)\n\n        return x\n\n\nclass ParallelBlock(nn.Module):\n    \"\"\" Parallel block class. \"\"\"\n    def __init__(\n            self,\n            dims,\n            num_heads,\n            mlp_ratios=[],\n            qkv_bias=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            shared_crpes=None,\n    ):\n        super().__init__()\n\n        # Conv-Attention.\n        self.norm12 = norm_layer(dims[1])\n        self.norm13 = norm_layer(dims[2])\n        self.norm14 = norm_layer(dims[3])\n        self.factoratt_crpe2 = FactorAttnConvRelPosEnc(\n            dims[1],\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n            shared_crpe=shared_crpes[1],\n        )\n        self.factoratt_crpe3 = FactorAttnConvRelPosEnc(\n            dims[2],\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n            shared_crpe=shared_crpes[2],\n        )\n        self.factoratt_crpe4 = FactorAttnConvRelPosEnc(\n            dims[3],\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n            shared_crpe=shared_crpes[3],\n        )\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        # MLP.\n        self.norm22 = norm_layer(dims[1])\n        self.norm23 = norm_layer(dims[2])\n        self.norm24 = norm_layer(dims[3])\n        # In parallel block, we assume dimensions are the same and share the linear transformation.\n        assert dims[1] == dims[2] == dims[3]\n        assert mlp_ratios[1] == mlp_ratios[2] == mlp_ratios[3]\n        mlp_hidden_dim = int(dims[1] * mlp_ratios[1])\n        self.mlp2 = self.mlp3 = self.mlp4 = Mlp(\n            in_features=dims[1],\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n\n    def upsample(self, x, factor: float, size: Tuple[int, int]):\n        \"\"\" Feature map up-sampling. \"\"\"\n        return self.interpolate(x, scale_factor=factor, size=size)\n\n    def downsample(self, x, factor: float, size: Tuple[int, int]):\n        \"\"\" Feature map down-sampling. \"\"\"\n        return self.interpolate(x, scale_factor=1.0/factor, size=size)\n\n    def interpolate(self, x, scale_factor: float, size: Tuple[int, int]):\n        \"\"\" Feature map interpolation. \"\"\"\n        B, N, C = x.shape\n        H, W = size\n        _assert(N == 1 + H * W, '')\n\n        cls_token = x[:, :1, :]\n        img_tokens = x[:, 1:, :]\n        \n        img_tokens = img_tokens.transpose(1, 2).reshape(B, C, H, W)\n        img_tokens = F.interpolate(\n            img_tokens,\n            scale_factor=scale_factor,\n            recompute_scale_factor=False,\n            mode='bilinear',\n            align_corners=False,\n        )\n        img_tokens = img_tokens.reshape(B, C, -1).transpose(1, 2)\n        \n        out = torch.cat((cls_token, img_tokens), dim=1)\n\n        return out\n\n    def forward(self, x1, x2, x3, x4, sizes: List[Tuple[int, int]]):\n        _, S2, S3, S4 = sizes\n        cur2 = self.norm12(x2)\n        cur3 = self.norm13(x3)\n        cur4 = self.norm14(x4)\n        cur2 = self.factoratt_crpe2(cur2, size=S2)\n        cur3 = self.factoratt_crpe3(cur3, size=S3)\n        cur4 = self.factoratt_crpe4(cur4, size=S4)\n        upsample3_2 = self.upsample(cur3, factor=2., size=S3)\n        upsample4_3 = self.upsample(cur4, factor=2., size=S4)\n        upsample4_2 = self.upsample(cur4, factor=4., size=S4)\n        downsample2_3 = self.downsample(cur2, factor=2., size=S2)\n        downsample3_4 = self.downsample(cur3, factor=2., size=S3)\n        downsample2_4 = self.downsample(cur2, factor=4., size=S2)\n        cur2 = cur2 + upsample3_2 + upsample4_2\n        cur3 = cur3 + upsample4_3 + downsample2_3\n        cur4 = cur4 + downsample3_4 + downsample2_4\n        x2 = x2 + self.drop_path(cur2) \n        x3 = x3 + self.drop_path(cur3) \n        x4 = x4 + self.drop_path(cur4) \n\n        # MLP. \n        cur2 = self.norm22(x2)\n        cur3 = self.norm23(x3)\n        cur4 = self.norm24(x4)\n        cur2 = self.mlp2(cur2)\n        cur3 = self.mlp3(cur3)\n        cur4 = self.mlp4(cur4)\n        x2 = x2 + self.drop_path(cur2)\n        x3 = x3 + self.drop_path(cur3)\n        x4 = x4 + self.drop_path(cur4) \n\n        return x1, x2, x3, x4\n\n\nclass CoaT(nn.Module):\n    \"\"\" CoaT class. \"\"\"\n    def __init__(\n            self,\n            img_size=224,\n            patch_size=16,\n            in_chans=3,\n            num_classes=1000,\n            embed_dims=(64, 128, 320, 512),\n            serial_depths=(3, 4, 6, 3),\n            parallel_depth=0,\n            num_heads=8,\n            mlp_ratios=(4, 4, 4, 4),\n            qkv_bias=True,\n            drop_rate=0.,\n            proj_drop_rate=0.,\n            attn_drop_rate=0.,\n            drop_path_rate=0.,\n            norm_layer=LayerNorm,\n            return_interm_layers=False,\n            out_features=None,\n            crpe_window=None,\n            global_pool='token',\n    ):\n        super().__init__()\n        assert global_pool in ('token', 'avg')\n        crpe_window = crpe_window or {3: 2, 5: 3, 7: 3}\n        self.return_interm_layers = return_interm_layers\n        self.out_features = out_features\n        self.embed_dims = embed_dims\n        self.num_features = embed_dims[-1]\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n\n        # Patch embeddings.\n        img_size = to_2tuple(img_size)\n        self.patch_embed1 = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans,\n            embed_dim=embed_dims[0], norm_layer=nn.LayerNorm)\n        self.patch_embed2 = PatchEmbed(\n            img_size=[x // 4 for x in img_size], patch_size=2, in_chans=embed_dims[0],\n            embed_dim=embed_dims[1], norm_layer=nn.LayerNorm)\n        self.patch_embed3 = PatchEmbed(\n            img_size=[x // 8 for x in img_size], patch_size=2, in_chans=embed_dims[1],\n            embed_dim=embed_dims[2], norm_layer=nn.LayerNorm)\n        self.patch_embed4 = PatchEmbed(\n            img_size=[x // 16 for x in img_size], patch_size=2, in_chans=embed_dims[2],\n            embed_dim=embed_dims[3], norm_layer=nn.LayerNorm)\n\n        # Class tokens.\n        self.cls_token1 = nn.Parameter(torch.zeros(1, 1, embed_dims[0]))\n        self.cls_token2 = nn.Parameter(torch.zeros(1, 1, embed_dims[1]))\n        self.cls_token3 = nn.Parameter(torch.zeros(1, 1, embed_dims[2]))\n        self.cls_token4 = nn.Parameter(torch.zeros(1, 1, embed_dims[3]))\n\n        # Convolutional position encodings.\n        self.cpe1 = ConvPosEnc(dim=embed_dims[0], k=3)\n        self.cpe2 = ConvPosEnc(dim=embed_dims[1], k=3)\n        self.cpe3 = ConvPosEnc(dim=embed_dims[2], k=3)\n        self.cpe4 = ConvPosEnc(dim=embed_dims[3], k=3)\n\n        # Convolutional relative position encodings.\n        self.crpe1 = ConvRelPosEnc(head_chs=embed_dims[0] // num_heads, num_heads=num_heads, window=crpe_window)\n        self.crpe2 = ConvRelPosEnc(head_chs=embed_dims[1] // num_heads, num_heads=num_heads, window=crpe_window)\n        self.crpe3 = ConvRelPosEnc(head_chs=embed_dims[2] // num_heads, num_heads=num_heads, window=crpe_window)\n        self.crpe4 = ConvRelPosEnc(head_chs=embed_dims[3] // num_heads, num_heads=num_heads, window=crpe_window)\n\n        # Disable stochastic depth.\n        dpr = drop_path_rate\n        assert dpr == 0.0\n        skwargs = dict(\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            proj_drop=proj_drop_rate,\n            attn_drop=attn_drop_rate,\n            drop_path=dpr,\n            norm_layer=norm_layer,\n        )\n\n        # Serial blocks 1.\n        self.serial_blocks1 = nn.ModuleList([\n            SerialBlock(\n                dim=embed_dims[0],\n                mlp_ratio=mlp_ratios[0],\n                shared_cpe=self.cpe1,\n                shared_crpe=self.crpe1,\n                **skwargs,\n            )\n            for _ in range(serial_depths[0])]\n        )\n\n        # Serial blocks 2.\n        self.serial_blocks2 = nn.ModuleList([\n            SerialBlock(\n                dim=embed_dims[1],\n                mlp_ratio=mlp_ratios[1],\n                shared_cpe=self.cpe2,\n                shared_crpe=self.crpe2,\n                **skwargs,\n            )\n            for _ in range(serial_depths[1])]\n        )\n\n        # Serial blocks 3.\n        self.serial_blocks3 = nn.ModuleList([\n            SerialBlock(\n                dim=embed_dims[2],\n                mlp_ratio=mlp_ratios[2],\n                shared_cpe=self.cpe3,\n                shared_crpe=self.crpe3,\n                **skwargs,\n            )\n            for _ in range(serial_depths[2])]\n        )\n\n        # Serial blocks 4.\n        self.serial_blocks4 = nn.ModuleList([\n            SerialBlock(\n                dim=embed_dims[3],\n                mlp_ratio=mlp_ratios[3],\n                shared_cpe=self.cpe4,\n                shared_crpe=self.crpe4,\n                **skwargs,\n            )\n            for _ in range(serial_depths[3])]\n        )\n\n        # Parallel blocks.\n        self.parallel_depth = parallel_depth\n        if self.parallel_depth > 0:\n            self.parallel_blocks = nn.ModuleList([\n                ParallelBlock(\n                    dims=embed_dims,\n                    mlp_ratios=mlp_ratios,\n                    shared_crpes=(self.crpe1, self.crpe2, self.crpe3, self.crpe4),\n                    **skwargs,\n                )\n                for _ in range(parallel_depth)]\n            )\n        else:\n            self.parallel_blocks = None\n\n        # Classification head(s).\n        if not self.return_interm_layers:\n            if self.parallel_blocks is not None:\n                self.norm2 = norm_layer(embed_dims[1])\n                self.norm3 = norm_layer(embed_dims[2])\n            else:\n                self.norm2 = self.norm3 = None\n            self.norm4 = norm_layer(embed_dims[3])\n\n            if self.parallel_depth > 0:\n                # CoaT series: Aggregate features of last three scales for classification.\n                assert embed_dims[1] == embed_dims[2] == embed_dims[3]\n                self.aggregate = torch.nn.Conv1d(in_channels=3, out_channels=1, kernel_size=1)\n                self.head_drop = nn.Dropout(drop_rate)\n                self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n            else:\n                # CoaT-Lite series: Use feature of last scale for classification.\n                self.aggregate = None\n                self.head_drop = nn.Dropout(drop_rate)\n                self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n        # Initialize weights.\n        trunc_normal_(self.cls_token1, std=.02)\n        trunc_normal_(self.cls_token2, std=.02)\n        trunc_normal_(self.cls_token3, std=.02)\n        trunc_normal_(self.cls_token4, std=.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'cls_token1', 'cls_token2', 'cls_token3', 'cls_token4'}\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, 'gradient checkpointing not supported'\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem1=r'^cls_token1|patch_embed1|crpe1|cpe1',\n            serial_blocks1=r'^serial_blocks1\\.(\\d+)',\n            stem2=r'^cls_token2|patch_embed2|crpe2|cpe2',\n            serial_blocks2=r'^serial_blocks2\\.(\\d+)',\n            stem3=r'^cls_token3|patch_embed3|crpe3|cpe3',\n            serial_blocks3=r'^serial_blocks3\\.(\\d+)',\n            stem4=r'^cls_token4|patch_embed4|crpe4|cpe4',\n            serial_blocks4=r'^serial_blocks4\\.(\\d+)',\n            parallel_blocks=[  # FIXME (partially?) overlap parallel w/ serial blocks??\n                (r'^parallel_blocks\\.(\\d+)', None),\n                (r'^norm|aggregate', (99999,)),\n            ]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('token', 'avg')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x0):\n        B = x0.shape[0]\n\n        # Serial blocks 1.\n        x1 = self.patch_embed1(x0)\n        H1, W1 = self.patch_embed1.grid_size\n        x1 = insert_cls(x1, self.cls_token1)\n        for blk in self.serial_blocks1:\n            x1 = blk(x1, size=(H1, W1))\n        x1_nocls = remove_cls(x1).reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n        \n        # Serial blocks 2.\n        x2 = self.patch_embed2(x1_nocls)\n        H2, W2 = self.patch_embed2.grid_size\n        x2 = insert_cls(x2, self.cls_token2)\n        for blk in self.serial_blocks2:\n            x2 = blk(x2, size=(H2, W2))\n        x2_nocls = remove_cls(x2).reshape(B, H2, W2, -1).permute(0, 3, 1, 2).contiguous()\n\n        # Serial blocks 3.\n        x3 = self.patch_embed3(x2_nocls)\n        H3, W3 = self.patch_embed3.grid_size\n        x3 = insert_cls(x3, self.cls_token3)\n        for blk in self.serial_blocks3:\n            x3 = blk(x3, size=(H3, W3))\n        x3_nocls = remove_cls(x3).reshape(B, H3, W3, -1).permute(0, 3, 1, 2).contiguous()\n\n        # Serial blocks 4.\n        x4 = self.patch_embed4(x3_nocls)\n        H4, W4 = self.patch_embed4.grid_size\n        x4 = insert_cls(x4, self.cls_token4)\n        for blk in self.serial_blocks4:\n            x4 = blk(x4, size=(H4, W4))\n        x4_nocls = remove_cls(x4).reshape(B, H4, W4, -1).permute(0, 3, 1, 2).contiguous()\n\n        # Only serial blocks: Early return.\n        if self.parallel_blocks is None:\n            if not torch.jit.is_scripting() and self.return_interm_layers:\n                # Return intermediate features for down-stream tasks (e.g. Deformable DETR and Detectron2).\n                feat_out = {}   \n                if 'x1_nocls' in self.out_features:\n                    feat_out['x1_nocls'] = x1_nocls\n                if 'x2_nocls' in self.out_features:\n                    feat_out['x2_nocls'] = x2_nocls\n                if 'x3_nocls' in self.out_features:\n                    feat_out['x3_nocls'] = x3_nocls\n                if 'x4_nocls' in self.out_features:\n                    feat_out['x4_nocls'] = x4_nocls\n                return feat_out\n            else:\n                # Return features for classification.\n                x4 = self.norm4(x4)\n                return x4\n\n        # Parallel blocks.\n        for blk in self.parallel_blocks:\n            x2, x3, x4 = self.cpe2(x2, (H2, W2)), self.cpe3(x3, (H3, W3)), self.cpe4(x4, (H4, W4))\n            x1, x2, x3, x4 = blk(x1, x2, x3, x4, sizes=[(H1, W1), (H2, W2), (H3, W3), (H4, W4)])\n\n        if not torch.jit.is_scripting() and self.return_interm_layers:\n            # Return intermediate features for down-stream tasks (e.g. Deformable DETR and Detectron2).\n            feat_out = {}   \n            if 'x1_nocls' in self.out_features:\n                x1_nocls = remove_cls(x1).reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n                feat_out['x1_nocls'] = x1_nocls\n            if 'x2_nocls' in self.out_features:\n                x2_nocls = remove_cls(x2).reshape(B, H2, W2, -1).permute(0, 3, 1, 2).contiguous()\n                feat_out['x2_nocls'] = x2_nocls\n            if 'x3_nocls' in self.out_features:\n                x3_nocls = remove_cls(x3).reshape(B, H3, W3, -1).permute(0, 3, 1, 2).contiguous()\n                feat_out['x3_nocls'] = x3_nocls\n            if 'x4_nocls' in self.out_features:\n                x4_nocls = remove_cls(x4).reshape(B, H4, W4, -1).permute(0, 3, 1, 2).contiguous()\n                feat_out['x4_nocls'] = x4_nocls\n            return feat_out\n        else:\n            x2 = self.norm2(x2)\n            x3 = self.norm3(x3)\n            x4 = self.norm4(x4)\n            return [x2, x3, x4]\n\n    def forward_head(self, x_feat: Union[torch.Tensor, List[torch.Tensor]], pre_logits: bool = False):\n        if isinstance(x_feat, list):\n            assert self.aggregate is not None\n            if self.global_pool == 'avg':\n                x = torch.cat([xl[:, 1:].mean(dim=1, keepdim=True) for xl in x_feat], dim=1)  # [B, 3, C]\n            else:\n                x = torch.stack([xl[:, 0] for xl in x_feat], dim=1)  # [B, 3, C]\n            x = self.aggregate(x).squeeze(dim=1)  # Shape: [B, C]\n        else:\n            x = x_feat[:, 1:].mean(dim=1) if self.global_pool == 'avg' else x_feat[:, 0]\n        x = self.head_drop(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x) -> torch.Tensor:\n        if not torch.jit.is_scripting() and self.return_interm_layers:\n            # Return intermediate features (for down-stream tasks).\n            return self.forward_features(x)\n        else:\n            # Return features for classification.\n            x_feat = self.forward_features(x)\n            x = self.forward_head(x_feat)\n            return x\n\n\ndef insert_cls(x, cls_token):\n    \"\"\" Insert CLS token. \"\"\"\n    cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n    x = torch.cat((cls_tokens, x), dim=1)\n    return x\n\n\ndef remove_cls(x):\n    \"\"\" Remove CLS token. \"\"\"\n    return x[:, 1:, :]\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    out_dict = {}\n    state_dict = state_dict.get('model', state_dict)\n    for k, v in state_dict.items():\n        # original model had unused norm layers, removing them requires filtering pretrained checkpoints\n        if k.startswith('norm1') or \\\n                (model.norm2 is None and k.startswith('norm2')) or \\\n                (model.norm3 is None and k.startswith('norm3')):\n            continue\n        out_dict[k] = v\n    return out_dict\n\n\ndef _create_coat(variant, pretrained=False, default_cfg=None, **kwargs):\n    if kwargs.get('features_only', None):\n        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n\n    model = build_model_with_cfg(\n        CoaT,\n        variant,\n        pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        **kwargs,\n    )\n    return model\n\n\ndef _cfg_coat(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'patch_embed1.proj', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'coat_tiny.in1k': _cfg_coat(hf_hub_id='timm/'),\n    'coat_mini.in1k': _cfg_coat(hf_hub_id='timm/'),\n    'coat_small.in1k': _cfg_coat(hf_hub_id='timm/'),\n    'coat_lite_tiny.in1k': _cfg_coat(hf_hub_id='timm/'),\n    'coat_lite_mini.in1k': _cfg_coat(hf_hub_id='timm/'),\n    'coat_lite_small.in1k': _cfg_coat(hf_hub_id='timm/'),\n    'coat_lite_medium.in1k': _cfg_coat(hf_hub_id='timm/'),\n    'coat_lite_medium_384.in1k': _cfg_coat(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), crop_pct=1.0, crop_mode='squash',\n    ),\n})\n\n\n@register_model\ndef coat_tiny(pretrained=False, **kwargs) -> CoaT:\n    model_cfg = dict(\n        patch_size=4, embed_dims=[152, 152, 152, 152], serial_depths=[2, 2, 2, 2], parallel_depth=6)\n    model = _create_coat('coat_tiny', pretrained=pretrained, **dict(model_cfg, **kwargs))\n    return model\n\n\n@register_model\ndef coat_mini(pretrained=False, **kwargs) -> CoaT:\n    model_cfg = dict(\n        patch_size=4, embed_dims=[152, 216, 216, 216], serial_depths=[2, 2, 2, 2], parallel_depth=6)\n    model = _create_coat('coat_mini', pretrained=pretrained, **dict(model_cfg, **kwargs))\n    return model\n\n\n@register_model\ndef coat_small(pretrained=False, **kwargs) -> CoaT:\n    model_cfg = dict(\n        patch_size=4, embed_dims=[152, 320, 320, 320], serial_depths=[2, 2, 2, 2], parallel_depth=6, **kwargs)\n    model = _create_coat('coat_small', pretrained=pretrained, **dict(model_cfg, **kwargs))\n    return model\n\n\n@register_model\ndef coat_lite_tiny(pretrained=False, **kwargs) -> CoaT:\n    model_cfg = dict(\n        patch_size=4, embed_dims=[64, 128, 256, 320], serial_depths=[2, 2, 2, 2], mlp_ratios=[8, 8, 4, 4])\n    model = _create_coat('coat_lite_tiny', pretrained=pretrained, **dict(model_cfg, **kwargs))\n    return model\n\n\n@register_model\ndef coat_lite_mini(pretrained=False, **kwargs) -> CoaT:\n    model_cfg = dict(\n        patch_size=4, embed_dims=[64, 128, 320, 512], serial_depths=[2, 2, 2, 2], mlp_ratios=[8, 8, 4, 4])\n    model = _create_coat('coat_lite_mini', pretrained=pretrained, **dict(model_cfg, **kwargs))\n    return model\n\n\n@register_model\ndef coat_lite_small(pretrained=False, **kwargs) -> CoaT:\n    model_cfg = dict(\n        patch_size=4, embed_dims=[64, 128, 320, 512], serial_depths=[3, 4, 6, 3], mlp_ratios=[8, 8, 4, 4])\n    model = _create_coat('coat_lite_small', pretrained=pretrained, **dict(model_cfg, **kwargs))\n    return model\n\n\n@register_model\ndef coat_lite_medium(pretrained=False, **kwargs) -> CoaT:\n    model_cfg = dict(\n        patch_size=4, embed_dims=[128, 256, 320, 512], serial_depths=[3, 6, 10, 8])\n    model = _create_coat('coat_lite_medium', pretrained=pretrained, **dict(model_cfg, **kwargs))\n    return model\n\n\n@register_model\ndef coat_lite_medium_384(pretrained=False, **kwargs) -> CoaT:\n    model_cfg = dict(\n        img_size=384, patch_size=4, embed_dims=[128, 256, 320, 512], serial_depths=[3, 6, 10, 8])\n    model = _create_coat('coat_lite_medium_384', pretrained=pretrained, **dict(model_cfg, **kwargs))\n    return model",
  "\"\"\" Twins\nA PyTorch impl of : `Twins: Revisiting the Design of Spatial Attention in Vision Transformers`\n    - https://arxiv.org/pdf/2104.13840.pdf\n\nCode/weights from https://github.com/Meituan-AutoML/Twins, original copyright/license info below\n\n\"\"\"\n# --------------------------------------------------------\n# Twins\n# Copyright (c) 2021 Meituan\n# Licensed under The Apache 2.0 License [see LICENSE for details]\n# Written by Xinjie Li, Xiangxiang Chu\n# --------------------------------------------------------\nimport math\nfrom functools import partial\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import Mlp, DropPath, to_2tuple, trunc_normal_, use_fused_attn\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_module\nfrom ._registry import register_model, generate_default_cfgs\nfrom .vision_transformer import Attention\n\n__all__ = ['Twins']  # model_registry will add each entrypoint fn to this\n\nSize_ = Tuple[int, int]\n\n\n@register_notrace_module  # reason: FX can't symbolically trace control flow in forward method\nclass LocallyGroupedAttn(nn.Module):\n    \"\"\" LSA: self attention within a group\n    \"\"\"\n    fused_attn: torch.jit.Final[bool]\n\n    def __init__(self, dim, num_heads=8, attn_drop=0., proj_drop=0., ws=1):\n        assert ws != 1\n        super(LocallyGroupedAttn, self).__init__()\n        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n        self.fused_attn = use_fused_attn()\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.ws = ws\n\n    def forward(self, x, size: Size_):\n        # There are two implementations for this function, zero padding or mask. We don't observe obvious difference for\n        # both. You can choose any one, we recommend forward_padding because it's neat. However,\n        # the masking implementation is more reasonable and accurate.\n        B, N, C = x.shape\n        H, W = size\n        x = x.view(B, H, W, C)\n        pad_l = pad_t = 0\n        pad_r = (self.ws - W % self.ws) % self.ws\n        pad_b = (self.ws - H % self.ws) % self.ws\n        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n        _, Hp, Wp, _ = x.shape\n        _h, _w = Hp // self.ws, Wp // self.ws\n        x = x.reshape(B, _h, self.ws, _w, self.ws, C).transpose(2, 3)\n        qkv = self.qkv(x).reshape(\n            B, _h * _w, self.ws * self.ws, 3, self.num_heads, C // self.num_heads).permute(3, 0, 1, 4, 2, 5)\n        q, k, v = qkv.unbind(0)\n\n        if self.fused_attn:\n            x = F.scaled_dot_product_attention(\n                q, k, v,\n                dropout_p=self.attn_drop.p,\n            )\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            x = attn @ v\n\n        x = x.transpose(2, 3).reshape(B, _h, _w, self.ws, self.ws, C)\n        x = x.transpose(2, 3).reshape(B, _h * self.ws, _w * self.ws, C)\n        if pad_r > 0 or pad_b > 0:\n            x = x[:, :H, :W, :].contiguous()\n        x = x.reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    # def forward_mask(self, x, size: Size_):\n    #     B, N, C = x.shape\n    #     H, W = size\n    #     x = x.view(B, H, W, C)\n    #     pad_l = pad_t = 0\n    #     pad_r = (self.ws - W % self.ws) % self.ws\n    #     pad_b = (self.ws - H % self.ws) % self.ws\n    #     x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n    #     _, Hp, Wp, _ = x.shape\n    #     _h, _w = Hp // self.ws, Wp // self.ws\n    #     mask = torch.zeros((1, Hp, Wp), device=x.device)\n    #     mask[:, -pad_b:, :].fill_(1)\n    #     mask[:, :, -pad_r:].fill_(1)\n    #\n    #     x = x.reshape(B, _h, self.ws, _w, self.ws, C).transpose(2, 3)  # B, _h, _w, ws, ws, C\n    #     mask = mask.reshape(1, _h, self.ws, _w, self.ws).transpose(2, 3).reshape(1,  _h * _w, self.ws * self.ws)\n    #     attn_mask = mask.unsqueeze(2) - mask.unsqueeze(3)  # 1, _h*_w, ws*ws, ws*ws\n    #     attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-1000.0)).masked_fill(attn_mask == 0, float(0.0))\n    #     qkv = self.qkv(x).reshape(\n    #         B, _h * _w, self.ws * self.ws, 3, self.num_heads, C // self.num_heads).permute(3, 0, 1, 4, 2, 5)\n    #     # n_h, B, _w*_h, nhead, ws*ws, dim\n    #     q, k, v = qkv[0], qkv[1], qkv[2]  # B, _h*_w, n_head, ws*ws, dim_head\n    #     attn = (q @ k.transpose(-2, -1)) * self.scale  # B, _h*_w, n_head, ws*ws, ws*ws\n    #     attn = attn + attn_mask.unsqueeze(2)\n    #     attn = attn.softmax(dim=-1)\n    #     attn = self.attn_drop(attn)  # attn @v ->  B, _h*_w, n_head, ws*ws, dim_head\n    #     attn = (attn @ v).transpose(2, 3).reshape(B, _h, _w, self.ws, self.ws, C)\n    #     x = attn.transpose(2, 3).reshape(B, _h * self.ws, _w * self.ws, C)\n    #     if pad_r > 0 or pad_b > 0:\n    #         x = x[:, :H, :W, :].contiguous()\n    #     x = x.reshape(B, N, C)\n    #     x = self.proj(x)\n    #     x = self.proj_drop(x)\n    #     return x\n\n\nclass GlobalSubSampleAttn(nn.Module):\n    \"\"\" GSA: using a  key to summarize the information for a group to be efficient.\n    \"\"\"\n    fused_attn: torch.jit.Final[bool]\n\n    def __init__(self, dim, num_heads=8, attn_drop=0., proj_drop=0., sr_ratio=1):\n        super().__init__()\n        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n        self.fused_attn = use_fused_attn()\n\n        self.q = nn.Linear(dim, dim, bias=True)\n        self.kv = nn.Linear(dim, dim * 2, bias=True)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        self.sr_ratio = sr_ratio\n        if sr_ratio > 1:\n            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n            self.norm = nn.LayerNorm(dim)\n        else:\n            self.sr = None\n            self.norm = None\n\n    def forward(self, x, size: Size_):\n        B, N, C = x.shape\n        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        if self.sr is not None:\n            x = x.permute(0, 2, 1).reshape(B, C, *size)\n            x = self.sr(x).reshape(B, C, -1).permute(0, 2, 1)\n            x = self.norm(x)\n        kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        k, v = kv.unbind(0)\n\n        if self.fused_attn:\n            x = torch.nn.functional.scaled_dot_product_attention(\n                q, k, v,\n                dropout_p=self.attn_drop.p,\n            )\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            x = attn @ v\n\n        x = x.transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            sr_ratio=1,\n            ws=None,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        if ws is None:\n            self.attn = Attention(dim, num_heads, False, None, attn_drop, proj_drop)\n        elif ws == 1:\n            self.attn = GlobalSubSampleAttn(dim, num_heads, attn_drop, proj_drop, sr_ratio)\n        else:\n            self.attn = LocallyGroupedAttn(dim, num_heads, attn_drop, proj_drop, ws)\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x, size: Size_):\n        x = x + self.drop_path1(self.attn(self.norm1(x), size))\n        x = x + self.drop_path2(self.mlp(self.norm2(x)))\n        return x\n\n\nclass PosConv(nn.Module):\n    # PEG  from https://arxiv.org/abs/2102.10882\n    def __init__(self, in_chans, embed_dim=768, stride=1):\n        super(PosConv, self).__init__()\n        self.proj = nn.Sequential(\n            nn.Conv2d(in_chans, embed_dim, 3, stride, 1, bias=True, groups=embed_dim),\n        )\n        self.stride = stride\n\n    def forward(self, x, size: Size_):\n        B, N, C = x.shape\n        cnn_feat_token = x.transpose(1, 2).view(B, C, *size)\n        x = self.proj(cnn_feat_token)\n        if self.stride == 1:\n            x += cnn_feat_token\n        x = x.flatten(2).transpose(1, 2)\n        return x\n\n    def no_weight_decay(self):\n        return ['proj.%d.weight' % i for i in range(4)]\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \\\n            f\"img_size {img_size} should be divided by patch_size {patch_size}.\"\n        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n        self.num_patches = self.H * self.W\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x) -> Tuple[torch.Tensor, Size_]:\n        B, C, H, W = x.shape\n\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        x = self.norm(x)\n        out_size = (H // self.patch_size[0], W // self.patch_size[1])\n\n        return x, out_size\n\n\nclass Twins(nn.Module):\n    \"\"\" Twins Vision Transfomer (Revisiting Spatial Attention)\n\n    Adapted from PVT (PyramidVisionTransformer) class at https://github.com/whai362/PVT.git\n    \"\"\"\n    def __init__(\n            self,\n            img_size=224,\n            patch_size=4,\n            in_chans=3,\n            num_classes=1000,\n            global_pool='avg',\n            embed_dims=(64, 128, 256, 512),\n            num_heads=(1, 2, 4, 8),\n            mlp_ratios=(4, 4, 4, 4),\n            depths=(3, 4, 6, 3),\n            sr_ratios=(8, 4, 2, 1),\n            wss=None,\n            drop_rate=0.,\n            pos_drop_rate=0.,\n            proj_drop_rate=0.,\n            attn_drop_rate=0.,\n            drop_path_rate=0.,\n            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n            block_cls=Block,\n    ):\n        super().__init__()\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.depths = depths\n        self.embed_dims = embed_dims\n        self.num_features = embed_dims[-1]\n        self.grad_checkpointing = False\n\n        img_size = to_2tuple(img_size)\n        prev_chs = in_chans\n        self.patch_embeds = nn.ModuleList()\n        self.pos_drops = nn.ModuleList()\n        for i in range(len(depths)):\n            self.patch_embeds.append(PatchEmbed(img_size, patch_size, prev_chs, embed_dims[i]))\n            self.pos_drops.append(nn.Dropout(p=pos_drop_rate))\n            prev_chs = embed_dims[i]\n            img_size = tuple(t // patch_size for t in img_size)\n            patch_size = 2\n\n        self.blocks = nn.ModuleList()\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n        cur = 0\n        for k in range(len(depths)):\n            _block = nn.ModuleList([block_cls(\n                dim=embed_dims[k],\n                num_heads=num_heads[k],\n                mlp_ratio=mlp_ratios[k],\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[cur + i],\n                norm_layer=norm_layer,\n                sr_ratio=sr_ratios[k],\n                ws=1 if wss is None or i % 2 == 1 else wss[k]) for i in range(depths[k])],\n            )\n            self.blocks.append(_block)\n            cur += depths[k]\n\n        self.pos_block = nn.ModuleList([PosConv(embed_dim, embed_dim) for embed_dim in embed_dims])\n\n        self.norm = norm_layer(self.num_features)\n\n        # classification head\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n        # init weights\n        self.apply(self._init_weights)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return set(['pos_block.' + n for n, p in self.pos_block.named_parameters()])\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^patch_embeds.0',  # stem and embed\n            blocks=[\n                (r'^(?:blocks|patch_embeds|pos_block)\\.(\\d+)', None),\n                ('^norm', (99999,))\n            ] if coarse else [\n                (r'^blocks\\.(\\d+)\\.(\\d+)', None),\n                (r'^(?:patch_embeds|pos_block)\\.(\\d+)', (0,)),\n                (r'^norm', (99999,))\n            ]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, 'gradient checkpointing not supported'\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('', 'avg')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward_features(self, x):\n        B = x.shape[0]\n        for i, (embed, drop, blocks, pos_blk) in enumerate(\n                zip(self.patch_embeds, self.pos_drops, self.blocks, self.pos_block)):\n            x, size = embed(x)\n            x = drop(x)\n            for j, blk in enumerate(blocks):\n                x = blk(x, size)\n                if j == 0:\n                    x = pos_blk(x, size)  # PEG here\n            if i < len(self.depths) - 1:\n                x = x.reshape(B, *size, -1).permute(0, 3, 1, 2).contiguous()\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool == 'avg':\n            x = x.mean(dim=1)\n        x = self.head_drop(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_twins(variant, pretrained=False, **kwargs):\n    if kwargs.get('features_only', None):\n        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n\n    model = build_model_with_cfg(Twins, variant, pretrained, **kwargs)\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'patch_embeds.0.proj', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'twins_pcpvt_small.in1k': _cfg(hf_hub_id='timm/'),\n    'twins_pcpvt_base.in1k': _cfg(hf_hub_id='timm/'),\n    'twins_pcpvt_large.in1k': _cfg(hf_hub_id='timm/'),\n    'twins_svt_small.in1k': _cfg(hf_hub_id='timm/'),\n    'twins_svt_base.in1k': _cfg(hf_hub_id='timm/'),\n    'twins_svt_large.in1k': _cfg(hf_hub_id='timm/'),\n})\n\n\n@register_model\ndef twins_pcpvt_small(pretrained=False, **kwargs) -> Twins:\n    model_args = dict(\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n        depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1])\n    return _create_twins('twins_pcpvt_small', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef twins_pcpvt_base(pretrained=False, **kwargs) -> Twins:\n    model_args = dict(\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n        depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1])\n    return _create_twins('twins_pcpvt_base', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef twins_pcpvt_large(pretrained=False, **kwargs) -> Twins:\n    model_args = dict(\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n        depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1])\n    return _create_twins('twins_pcpvt_large', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef twins_svt_small(pretrained=False, **kwargs) -> Twins:\n    model_args = dict(\n        patch_size=4, embed_dims=[64, 128, 256, 512], num_heads=[2, 4, 8, 16], mlp_ratios=[4, 4, 4, 4],\n        depths=[2, 2, 10, 4], wss=[7, 7, 7, 7], sr_ratios=[8, 4, 2, 1])\n    return _create_twins('twins_svt_small', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef twins_svt_base(pretrained=False, **kwargs) -> Twins:\n    model_args = dict(\n        patch_size=4, embed_dims=[96, 192, 384, 768], num_heads=[3, 6, 12, 24], mlp_ratios=[4, 4, 4, 4],\n        depths=[2, 2, 18, 2], wss=[7, 7, 7, 7], sr_ratios=[8, 4, 2, 1])\n    return _create_twins('twins_svt_base', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef twins_svt_large(pretrained=False, **kwargs) -> Twins:\n    model_args = dict(\n        patch_size=4, embed_dims=[128, 256, 512, 1024], num_heads=[4, 8, 16, 32], mlp_ratios=[4, 4, 4, 4],\n        depths=[2, 2, 18, 2], wss=[7, 7, 7, 7], sr_ratios=[8, 4, 2, 1])\n    return _create_twins('twins_svt_large', pretrained=pretrained, **dict(model_args, **kwargs))\n",
  "import collections.abc\nimport math\nimport re\nfrom collections import defaultdict\nfrom itertools import chain\nfrom typing import Any, Callable, Dict, Iterator, Tuple, Type, Union\n\nimport torch\nfrom torch import nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\n__all__ = ['model_parameters', 'named_apply', 'named_modules', 'named_modules_with_params', 'adapt_input_conv',\n           'group_with_matcher', 'group_modules', 'group_parameters', 'flatten_modules', 'checkpoint_seq']\n\n\ndef model_parameters(model: nn.Module, exclude_head: bool = False):\n    if exclude_head:\n        # FIXME this a bit of a quick and dirty hack to skip classifier head params based on ordering\n        return [p for p in model.parameters()][:-2]\n    else:\n        return model.parameters()\n\n\ndef named_apply(\n        fn: Callable,\n        module: nn.Module, name='',\n        depth_first: bool = True,\n        include_root: bool = False,\n) -> nn.Module:\n    if not depth_first and include_root:\n        fn(module=module, name=name)\n    for child_name, child_module in module.named_children():\n        child_name = '.'.join((name, child_name)) if name else child_name\n        named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n    if depth_first and include_root:\n        fn(module=module, name=name)\n    return module\n\n\ndef named_modules(\n        module: nn.Module,\n        name: str = '',\n        depth_first: bool = True,\n        include_root: bool = False,\n):\n    if not depth_first and include_root:\n        yield name, module\n    for child_name, child_module in module.named_children():\n        child_name = '.'.join((name, child_name)) if name else child_name\n        yield from named_modules(\n            module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n    if depth_first and include_root:\n        yield name, module\n\n\ndef named_modules_with_params(\n        module: nn.Module,\n        name: str = '',\n        depth_first: bool = True,\n        include_root: bool = False,\n):\n    if module._parameters and not depth_first and include_root:\n        yield name, module\n    for child_name, child_module in module.named_children():\n        child_name = '.'.join((name, child_name)) if name else child_name\n        yield from named_modules_with_params(\n            module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n    if module._parameters and depth_first and include_root:\n        yield name, module\n\n\nMATCH_PREV_GROUP = (99999,)\n\n\ndef group_with_matcher(\n        named_objects: Iterator[Tuple[str, Any]],\n        group_matcher: Union[Dict, Callable],\n        return_values: bool = False,\n        reverse: bool = False\n):\n    if isinstance(group_matcher, dict):\n        # dictionary matcher contains a dict of raw-string regex expr that must be compiled\n        compiled = []\n        for group_ordinal, (group_name, mspec) in enumerate(group_matcher.items()):\n            if mspec is None:\n                continue\n            # map all matching specifications into 3-tuple (compiled re, prefix, suffix)\n            if isinstance(mspec, (tuple, list)):\n                # multi-entry match specifications require each sub-spec to be a 2-tuple (re, suffix)\n                for sspec in mspec:\n                    compiled += [(re.compile(sspec[0]), (group_ordinal,), sspec[1])]\n            else:\n                compiled += [(re.compile(mspec), (group_ordinal,), None)]\n        group_matcher = compiled\n\n    def _get_grouping(name):\n        if isinstance(group_matcher, (list, tuple)):\n            for match_fn, prefix, suffix in group_matcher:\n                r = match_fn.match(name)\n                if r:\n                    parts = (prefix, r.groups(), suffix)\n                    # map all tuple elem to int for numeric sort, filter out None entries\n                    return tuple(map(float, chain.from_iterable(filter(None, parts))))\n            return float('inf'),  # un-matched layers (neck, head) mapped to largest ordinal\n        else:\n            ord = group_matcher(name)\n            if not isinstance(ord, collections.abc.Iterable):\n                return ord,\n            return tuple(ord)\n\n    # map layers into groups via ordinals (ints or tuples of ints) from matcher\n    grouping = defaultdict(list)\n    for k, v in named_objects:\n        grouping[_get_grouping(k)].append(v if return_values else k)\n\n    # remap to integers\n    layer_id_to_param = defaultdict(list)\n    lid = -1\n    for k in sorted(filter(lambda x: x is not None, grouping.keys())):\n        if lid < 0 or k[-1] != MATCH_PREV_GROUP[0]:\n            lid += 1\n        layer_id_to_param[lid].extend(grouping[k])\n\n    if reverse:\n        assert not return_values, \"reverse mapping only sensible for name output\"\n        # output reverse mapping\n        param_to_layer_id = {}\n        for lid, lm in layer_id_to_param.items():\n            for n in lm:\n                param_to_layer_id[n] = lid\n        return param_to_layer_id\n\n    return layer_id_to_param\n\n\ndef group_parameters(\n        module: nn.Module,\n        group_matcher,\n        return_values: bool = False,\n        reverse: bool = False,\n):\n    return group_with_matcher(\n        module.named_parameters(), group_matcher, return_values=return_values, reverse=reverse)\n\n\ndef group_modules(\n        module: nn.Module,\n        group_matcher,\n        return_values: bool = False,\n        reverse: bool = False,\n):\n    return group_with_matcher(\n        named_modules_with_params(module), group_matcher, return_values=return_values, reverse=reverse)\n\n\ndef flatten_modules(\n        named_modules: Iterator[Tuple[str, nn.Module]],\n        depth: int = 1,\n        prefix: Union[str, Tuple[str, ...]] = '',\n        module_types: Union[str, Tuple[Type[nn.Module]]] = 'sequential',\n):\n    prefix_is_tuple = isinstance(prefix, tuple)\n    if isinstance(module_types, str):\n        if module_types == 'container':\n            module_types = (nn.Sequential, nn.ModuleList, nn.ModuleDict)\n        else:\n            module_types = (nn.Sequential,)\n    for name, module in named_modules:\n        if depth and isinstance(module, module_types):\n            yield from flatten_modules(\n                module.named_children(),\n                depth - 1,\n                prefix=(name,) if prefix_is_tuple else name,\n                module_types=module_types,\n            )\n        else:\n            if prefix_is_tuple:\n                name = prefix + (name,)\n                yield name, module\n            else:\n                if prefix:\n                    name = '.'.join([prefix, name])\n                yield name, module\n\n\ndef checkpoint_seq(\n        functions,\n        x,\n        every=1,\n        flatten=False,\n        skip_last=False,\n        preserve_rng_state=True\n):\n    r\"\"\"A helper function for checkpointing sequential models.\n\n    Sequential models execute a list of modules/functions in order\n    (sequentially). Therefore, we can divide such a sequence into segments\n    and checkpoint each segment. All segments except run in :func:`torch.no_grad`\n    manner, i.e., not storing the intermediate activations. The inputs of each\n    checkpointed segment will be saved for re-running the segment in the backward pass.\n\n    See :func:`~torch.utils.checkpoint.checkpoint` on how checkpointing works.\n\n    .. warning::\n        Checkpointing currently only supports :func:`torch.autograd.backward`\n        and only if its `inputs` argument is not passed. :func:`torch.autograd.grad`\n        is not supported.\n\n    .. warning:\n        At least one of the inputs needs to have :code:`requires_grad=True` if\n        grads are needed for model inputs, otherwise the checkpointed part of the\n        model won't have gradients.\n\n    Args:\n        functions: A :class:`torch.nn.Sequential` or the list of modules or functions to run sequentially.\n        x: A Tensor that is input to :attr:`functions`\n        every: checkpoint every-n functions (default: 1)\n        flatten (bool): flatten nn.Sequential of nn.Sequentials\n        skip_last (bool): skip checkpointing the last function in the sequence if True\n        preserve_rng_state (bool, optional, default=True):  Omit stashing and restoring\n            the RNG state during each checkpoint.\n\n    Returns:\n        Output of running :attr:`functions` sequentially on :attr:`*inputs`\n\n    Example:\n        >>> model = nn.Sequential(...)\n        >>> input_var = checkpoint_seq(model, input_var, every=2)\n    \"\"\"\n    def run_function(start, end, functions):\n        def forward(_x):\n            for j in range(start, end + 1):\n                _x = functions[j](_x)\n            return _x\n        return forward\n\n    if isinstance(functions, torch.nn.Sequential):\n        functions = functions.children()\n    if flatten:\n        functions = chain.from_iterable(functions)\n    if not isinstance(functions, (tuple, list)):\n        functions = tuple(functions)\n\n    num_checkpointed = len(functions)\n    if skip_last:\n        num_checkpointed -= 1\n    end = -1\n    for start in range(0, num_checkpointed, every):\n        end = min(start + every - 1, num_checkpointed - 1)\n        x = checkpoint(run_function(start, end, functions), x, preserve_rng_state=preserve_rng_state)\n    if skip_last:\n        return run_function(end + 1, len(functions) - 1, functions)(x)\n    return x\n\n\ndef adapt_input_conv(in_chans, conv_weight):\n    conv_type = conv_weight.dtype\n    conv_weight = conv_weight.float()  # Some weights are in torch.half, ensure it's float for sum on CPU\n    O, I, J, K = conv_weight.shape\n    if in_chans == 1:\n        if I > 3:\n            assert conv_weight.shape[1] % 3 == 0\n            # For models with space2depth stems\n            conv_weight = conv_weight.reshape(O, I // 3, 3, J, K)\n            conv_weight = conv_weight.sum(dim=2, keepdim=False)\n        else:\n            conv_weight = conv_weight.sum(dim=1, keepdim=True)\n    elif in_chans != 3:\n        if I != 3:\n            raise NotImplementedError('Weight format not supported by conversion.')\n        else:\n            # NOTE this strategy should be better than random init, but there could be other combinations of\n            # the original RGB input layer weights that'd work better for specific cases.\n            repeat = int(math.ceil(in_chans / 3))\n            conv_weight = conv_weight.repeat(1, repeat, 1, 1)[:, :in_chans, :, :]\n            conv_weight *= (3 / float(in_chans))\n    conv_weight = conv_weight.to(conv_type)\n    return conv_weight\n",
  "\"\"\"\nSEResNet implementation from Cadene's pretrained models\nhttps://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py\nAdditional credit to https://github.com/creafz\n\nOriginal model: https://github.com/hujie-frank/SENet\n\nResNet code gently borrowed from\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n\nFIXME I'm deprecating this model and moving them to ResNet as I don't want to maintain duplicate\nsupport for extras like dilation, switchable BN/activations, feature extraction, etc that don't exist here.\n\"\"\"\nimport math\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import create_classifier\nfrom ._builder import build_model_with_cfg\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['SENet']\n\n\ndef _weight_init(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n    elif isinstance(m, nn.BatchNorm2d):\n        nn.init.constant_(m.weight, 1.)\n        nn.init.constant_(m.bias, 0.)\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = x.mean((2, 3), keepdim=True)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"\n    Base class for bottlenecks that implements `forward()` method.\n    \"\"\"\n\n    def forward(self, x):\n        shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out = self.se_module(out) + shortcut\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    \"\"\"\n    Bottleneck for SENet154.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1, downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(\n            planes * 2, planes * 4, kernel_size=3, stride=stride,\n            padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    \"\"\"\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1, downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False, stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    \"\"\"\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1, downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width / 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False, stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1, downsample=None):\n        super(SEResNetBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, padding=1, stride=stride, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out = self.se_module(out) + shortcut\n        out = self.relu(out)\n\n        return out\n\n\nclass SENet(nn.Module):\n\n    def __init__(\n            self, block, layers, groups, reduction, drop_rate=0.2,\n            in_chans=3, inplanes=64, input_3x3=False, downsample_kernel_size=1,\n            downsample_padding=0, num_classes=1000, global_pool='avg'):\n        \"\"\"\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        \"\"\"\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        if input_3x3:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(in_chans, 64, 3, stride=2, padding=1, bias=False)),\n                ('bn1', nn.BatchNorm2d(64)),\n                ('relu1', nn.ReLU(inplace=True)),\n                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1, bias=False)),\n                ('bn2', nn.BatchNorm2d(64)),\n                ('relu2', nn.ReLU(inplace=True)),\n                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1, bias=False)),\n                ('bn3', nn.BatchNorm2d(inplanes)),\n                ('relu3', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(\n                    in_chans, inplanes, kernel_size=7, stride=2, padding=3, bias=False)),\n                ('bn1', nn.BatchNorm2d(inplanes)),\n                ('relu1', nn.ReLU(inplace=True)),\n            ]\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        # To preserve compatibility with Caffe weights `ceil_mode=True` is used instead of `padding=1`.\n        self.pool0 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n        self.feature_info = [dict(num_chs=inplanes, reduction=2, module='layer0')]\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.feature_info += [dict(num_chs=64 * block.expansion, reduction=4, module='layer1')]\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.feature_info += [dict(num_chs=128 * block.expansion, reduction=8, module='layer2')]\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.feature_info += [dict(num_chs=256 * block.expansion, reduction=16, module='layer3')]\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.feature_info += [dict(num_chs=512 * block.expansion, reduction=32, module='layer4')]\n        self.num_features = 512 * block.expansion\n        self.global_pool, self.last_linear = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool)\n\n        for m in self.modules():\n            _weight_init(m)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.inplanes, planes * block.expansion, kernel_size=downsample_kernel_size,\n                    stride=stride, padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = [block(self.inplanes, planes, groups, reduction, stride, downsample)]\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(stem=r'^layer0', blocks=r'^layer(\\d+)' if coarse else r'^layer(\\d+)\\.(\\d+)')\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, 'gradient checkpointing not supported'\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.last_linear\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.global_pool, self.last_linear = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        x = self.layer0(x)\n        x = self.pool0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        x = self.global_pool(x)\n        if self.drop_rate > 0.:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        return x if pre_logits else self.last_linear(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_senet(variant, pretrained=False, **kwargs):\n    return build_model_with_cfg(SENet, variant, pretrained, **kwargs)\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bilinear',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'layer0.conv1', 'classifier': 'last_linear',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'legacy_senet154.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/legacy_senet154-e9eb9fe6.pth'),\n    'legacy_seresnet18.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet18-4bb0ce65.pth',\n        interpolation='bicubic'),\n    'legacy_seresnet34.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet34-a4004e63.pth'),\n    'legacy_seresnet50.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/se_resnet50-ce0d4300.pth'),\n    'legacy_seresnet101.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/se_resnet101-7e38fcc6.pth'),\n    'legacy_seresnet152.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/se_resnet152-d17c99b7.pth'),\n    'legacy_seresnext26_32x4d.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext26_32x4d-65ebdb501.pth',\n        interpolation='bicubic'),\n    'legacy_seresnext50_32x4d.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/legacy_se_resnext50_32x4d-f3651bad.pth'),\n    'legacy_seresnext101_32x4d.in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/legacy_se_resnext101_32x4d-37725eac.pth'),\n})\n\n\n@register_model\ndef legacy_seresnet18(pretrained=False, **kwargs) -> SENet:\n    model_args = dict(\n        block=SEResNetBlock, layers=[2, 2, 2, 2], groups=1, reduction=16, **kwargs)\n    return _create_senet('legacy_seresnet18', pretrained, **model_args)\n\n\n@register_model\ndef legacy_seresnet34(pretrained=False, **kwargs) -> SENet:\n    model_args = dict(\n        block=SEResNetBlock, layers=[3, 4, 6, 3], groups=1, reduction=16, **kwargs)\n    return _create_senet('legacy_seresnet34', pretrained, **model_args)\n\n\n@register_model\ndef legacy_seresnet50(pretrained=False, **kwargs) -> SENet:\n    model_args = dict(\n        block=SEResNetBottleneck, layers=[3, 4, 6, 3], groups=1, reduction=16, **kwargs)\n    return _create_senet('legacy_seresnet50', pretrained, **model_args)\n\n\n@register_model\ndef legacy_seresnet101(pretrained=False, **kwargs) -> SENet:\n    model_args = dict(\n        block=SEResNetBottleneck, layers=[3, 4, 23, 3], groups=1, reduction=16, **kwargs)\n    return _create_senet('legacy_seresnet101', pretrained, **model_args)\n\n\n@register_model\ndef legacy_seresnet152(pretrained=False, **kwargs) -> SENet:\n    model_args = dict(\n        block=SEResNetBottleneck, layers=[3, 8, 36, 3], groups=1, reduction=16, **kwargs)\n    return _create_senet('legacy_seresnet152', pretrained, **model_args)\n\n\n@register_model\ndef legacy_senet154(pretrained=False, **kwargs) -> SENet:\n    model_args = dict(\n        block=SEBottleneck, layers=[3, 8, 36, 3], groups=64, reduction=16,\n        downsample_kernel_size=3, downsample_padding=1,  inplanes=128, input_3x3=True, **kwargs)\n    return _create_senet('legacy_senet154', pretrained, **model_args)\n\n\n@register_model\ndef legacy_seresnext26_32x4d(pretrained=False, **kwargs) -> SENet:\n    model_args = dict(\n        block=SEResNeXtBottleneck, layers=[2, 2, 2, 2], groups=32, reduction=16, **kwargs)\n    return _create_senet('legacy_seresnext26_32x4d', pretrained, **model_args)\n\n\n@register_model\ndef legacy_seresnext50_32x4d(pretrained=False, **kwargs) -> SENet:\n    model_args = dict(\n        block=SEResNeXtBottleneck, layers=[3, 4, 6, 3], groups=32, reduction=16, **kwargs)\n    return _create_senet('legacy_seresnext50_32x4d', pretrained, **model_args)\n\n\n@register_model\ndef legacy_seresnext101_32x4d(pretrained=False, **kwargs) -> SENet:\n    model_args = dict(\n        block=SEResNeXtBottleneck, layers=[3, 4, 23, 3], groups=32, reduction=16, **kwargs)\n    return _create_senet('legacy_seresnext101_32x4d', pretrained, **model_args)\n",
  "from functools import partial\n\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom ._builder import build_model_with_cfg\nfrom ._builder import pretrained_cfg_for_features\nfrom ._efficientnet_blocks import SqueezeExcite\nfrom ._efficientnet_builder import decode_arch_def, resolve_act_layer, resolve_bn_args, round_channels\nfrom ._registry import register_model, generate_default_cfgs\nfrom .mobilenetv3 import MobileNetV3, MobileNetV3Features\n\n__all__ = []  # model_registry will add each entrypoint fn to this\n\n\ndef _gen_hardcorenas(pretrained, variant, arch_def, **kwargs):\n    \"\"\"Creates a hardcorenas model\n\n    Ref impl: https://github.com/Alibaba-MIIL/HardCoReNAS\n    Paper: https://arxiv.org/abs/2102.11646\n\n    \"\"\"\n    num_features = 1280\n    se_layer = partial(SqueezeExcite, gate_layer='hard_sigmoid', force_act_layer=nn.ReLU, rd_round_fn=round_channels)\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        num_features=num_features,\n        stem_size=32,\n        norm_layer=partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)),\n        act_layer=resolve_act_layer(kwargs, 'hard_swish'),\n        se_layer=se_layer,\n        **kwargs,\n    )\n\n    features_only = False\n    model_cls = MobileNetV3\n    kwargs_filter = None\n    if model_kwargs.pop('features_only', False):\n        features_only = True\n        kwargs_filter = ('num_classes', 'num_features', 'global_pool', 'head_conv', 'head_bias', 'global_pool')\n        model_cls = MobileNetV3Features\n    model = build_model_with_cfg(\n        model_cls,\n        variant,\n        pretrained,\n        pretrained_strict=not features_only,\n        kwargs_filter=kwargs_filter,\n        **model_kwargs,\n    )\n    if features_only:\n        model.default_cfg = pretrained_cfg_for_features(model.default_cfg)\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bilinear',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'conv_stem', 'classifier': 'classifier',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'hardcorenas_a.miil_green_in1k': _cfg(hf_hub_id='timm/'),\n    'hardcorenas_b.miil_green_in1k': _cfg(hf_hub_id='timm/'),\n    'hardcorenas_c.miil_green_in1k': _cfg(hf_hub_id='timm/'),\n    'hardcorenas_d.miil_green_in1k': _cfg(hf_hub_id='timm/'),\n    'hardcorenas_e.miil_green_in1k': _cfg(hf_hub_id='timm/'),\n    'hardcorenas_f.miil_green_in1k': _cfg(hf_hub_id='timm/'),\n})\n\n\n@register_model\ndef hardcorenas_a(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" hardcorenas_A \"\"\"\n    arch_def = [['ds_r1_k3_s1_e1_c16_nre'], ['ir_r1_k5_s2_e3_c24_nre', 'ir_r1_k5_s1_e3_c24_nre_se0.25'],\n                ['ir_r1_k5_s2_e3_c40_nre', 'ir_r1_k5_s1_e6_c40_nre_se0.25'],\n                ['ir_r1_k5_s2_e6_c80_se0.25', 'ir_r1_k5_s1_e6_c80_se0.25'],\n                ['ir_r1_k5_s1_e6_c112_se0.25', 'ir_r1_k5_s1_e6_c112_se0.25'],\n                ['ir_r1_k5_s2_e6_c192_se0.25', 'ir_r1_k5_s1_e6_c192_se0.25'], ['cn_r1_k1_s1_c960']]\n    model = _gen_hardcorenas(pretrained=pretrained, variant='hardcorenas_a', arch_def=arch_def, **kwargs)\n    return model\n\n\n@register_model\ndef hardcorenas_b(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" hardcorenas_B \"\"\"\n    arch_def = [['ds_r1_k3_s1_e1_c16_nre'],\n                ['ir_r1_k5_s2_e3_c24_nre', 'ir_r1_k5_s1_e3_c24_nre_se0.25', 'ir_r1_k3_s1_e3_c24_nre'],\n                ['ir_r1_k5_s2_e3_c40_nre', 'ir_r1_k5_s1_e3_c40_nre', 'ir_r1_k5_s1_e3_c40_nre'],\n                ['ir_r1_k5_s2_e3_c80', 'ir_r1_k5_s1_e3_c80', 'ir_r1_k3_s1_e3_c80', 'ir_r1_k3_s1_e3_c80'],\n                ['ir_r1_k5_s1_e3_c112', 'ir_r1_k3_s1_e3_c112', 'ir_r1_k3_s1_e3_c112', 'ir_r1_k3_s1_e3_c112'],\n                ['ir_r1_k5_s2_e6_c192_se0.25', 'ir_r1_k5_s1_e6_c192_se0.25', 'ir_r1_k3_s1_e3_c192_se0.25'],\n                ['cn_r1_k1_s1_c960']]\n    model = _gen_hardcorenas(pretrained=pretrained, variant='hardcorenas_b', arch_def=arch_def, **kwargs)\n    return model\n\n\n@register_model\ndef hardcorenas_c(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" hardcorenas_C \"\"\"\n    arch_def = [['ds_r1_k3_s1_e1_c16_nre'], ['ir_r1_k5_s2_e3_c24_nre', 'ir_r1_k5_s1_e3_c24_nre_se0.25'],\n                ['ir_r1_k5_s2_e3_c40_nre', 'ir_r1_k5_s1_e3_c40_nre', 'ir_r1_k5_s1_e3_c40_nre',\n                 'ir_r1_k5_s1_e3_c40_nre'],\n                ['ir_r1_k5_s2_e4_c80', 'ir_r1_k5_s1_e6_c80_se0.25', 'ir_r1_k3_s1_e3_c80', 'ir_r1_k3_s1_e3_c80'],\n                ['ir_r1_k5_s1_e6_c112_se0.25', 'ir_r1_k3_s1_e3_c112', 'ir_r1_k3_s1_e3_c112', 'ir_r1_k3_s1_e3_c112'],\n                ['ir_r1_k5_s2_e6_c192_se0.25', 'ir_r1_k5_s1_e6_c192_se0.25', 'ir_r1_k3_s1_e3_c192_se0.25'],\n                ['cn_r1_k1_s1_c960']]\n    model = _gen_hardcorenas(pretrained=pretrained, variant='hardcorenas_c', arch_def=arch_def, **kwargs)\n    return model\n\n\n@register_model\ndef hardcorenas_d(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" hardcorenas_D \"\"\"\n    arch_def = [['ds_r1_k3_s1_e1_c16_nre'], ['ir_r1_k5_s2_e3_c24_nre_se0.25', 'ir_r1_k5_s1_e3_c24_nre_se0.25'],\n                ['ir_r1_k5_s2_e3_c40_nre_se0.25', 'ir_r1_k5_s1_e4_c40_nre_se0.25', 'ir_r1_k3_s1_e3_c40_nre_se0.25'],\n                ['ir_r1_k5_s2_e4_c80_se0.25', 'ir_r1_k3_s1_e3_c80_se0.25', 'ir_r1_k3_s1_e3_c80_se0.25',\n                 'ir_r1_k3_s1_e3_c80_se0.25'],\n                ['ir_r1_k3_s1_e4_c112_se0.25', 'ir_r1_k5_s1_e4_c112_se0.25', 'ir_r1_k3_s1_e3_c112_se0.25',\n                 'ir_r1_k5_s1_e3_c112_se0.25'],\n                ['ir_r1_k5_s2_e6_c192_se0.25', 'ir_r1_k5_s1_e6_c192_se0.25', 'ir_r1_k5_s1_e6_c192_se0.25',\n                 'ir_r1_k3_s1_e6_c192_se0.25'], ['cn_r1_k1_s1_c960']]\n    model = _gen_hardcorenas(pretrained=pretrained, variant='hardcorenas_d', arch_def=arch_def, **kwargs)\n    return model\n\n\n@register_model\ndef hardcorenas_e(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" hardcorenas_E \"\"\"\n    arch_def = [['ds_r1_k3_s1_e1_c16_nre'], ['ir_r1_k5_s2_e3_c24_nre_se0.25', 'ir_r1_k5_s1_e3_c24_nre_se0.25'],\n                ['ir_r1_k5_s2_e6_c40_nre_se0.25', 'ir_r1_k5_s1_e4_c40_nre_se0.25', 'ir_r1_k5_s1_e4_c40_nre_se0.25',\n                 'ir_r1_k3_s1_e3_c40_nre_se0.25'], ['ir_r1_k5_s2_e4_c80_se0.25', 'ir_r1_k3_s1_e6_c80_se0.25'],\n                ['ir_r1_k5_s1_e6_c112_se0.25', 'ir_r1_k5_s1_e6_c112_se0.25', 'ir_r1_k5_s1_e6_c112_se0.25',\n                 'ir_r1_k5_s1_e3_c112_se0.25'],\n                ['ir_r1_k5_s2_e6_c192_se0.25', 'ir_r1_k5_s1_e6_c192_se0.25', 'ir_r1_k5_s1_e6_c192_se0.25',\n                 'ir_r1_k3_s1_e6_c192_se0.25'], ['cn_r1_k1_s1_c960']]\n    model = _gen_hardcorenas(pretrained=pretrained, variant='hardcorenas_e', arch_def=arch_def, **kwargs)\n    return model\n\n\n@register_model\ndef hardcorenas_f(pretrained=False, **kwargs) -> MobileNetV3:\n    \"\"\" hardcorenas_F \"\"\"\n    arch_def = [['ds_r1_k3_s1_e1_c16_nre'], ['ir_r1_k5_s2_e3_c24_nre_se0.25', 'ir_r1_k5_s1_e3_c24_nre_se0.25'],\n                ['ir_r1_k5_s2_e6_c40_nre_se0.25', 'ir_r1_k5_s1_e6_c40_nre_se0.25'],\n                ['ir_r1_k5_s2_e6_c80_se0.25', 'ir_r1_k5_s1_e6_c80_se0.25', 'ir_r1_k3_s1_e3_c80_se0.25',\n                 'ir_r1_k3_s1_e3_c80_se0.25'],\n                ['ir_r1_k3_s1_e6_c112_se0.25', 'ir_r1_k5_s1_e6_c112_se0.25', 'ir_r1_k5_s1_e6_c112_se0.25',\n                 'ir_r1_k3_s1_e3_c112_se0.25'],\n                ['ir_r1_k5_s2_e6_c192_se0.25', 'ir_r1_k5_s1_e6_c192_se0.25', 'ir_r1_k3_s1_e6_c192_se0.25',\n                 'ir_r1_k3_s1_e6_c192_se0.25'], ['cn_r1_k1_s1_c960']]\n    model = _gen_hardcorenas(pretrained=pretrained, variant='hardcorenas_f', arch_def=arch_def, **kwargs)\n    return model\n",
  "\"\"\" Vision Transformer (ViT) in PyTorch\n\nA PyTorch implement of Vision Transformers as described in:\n\n'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale'\n    - https://arxiv.org/abs/2010.11929\n\n`How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers`\n    - https://arxiv.org/abs/2106.10270\n\n`FlexiViT: One Model for All Patch Sizes`\n    - https://arxiv.org/abs/2212.08013\n\nThe official jax code is released and available at\n  * https://github.com/google-research/vision_transformer\n  * https://github.com/google-research/big_vision\n\nAcknowledgments:\n  * The paper authors for releasing code and weights, thanks!\n  * I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch\n  * Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT\n  * Bert reference code checks against Huggingface Transformers and Tensorflow Bert\n\nHacked together by / Copyright 2020, Ross Wightman\n\"\"\"\nimport logging\nimport math\nfrom collections import OrderedDict\nfrom functools import partial\nfrom typing import Callable, List, Optional, Sequence, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.jit import Final\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD, \\\n    OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\nfrom timm.layers import PatchEmbed, Mlp, DropPath, trunc_normal_, lecun_normal_, resample_patch_embed, \\\n    resample_abs_pos_embed, RmsNorm, PatchDropout, use_fused_attn, SwiGLUPacked\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import named_apply, checkpoint_seq, adapt_input_conv\nfrom ._registry import generate_default_cfgs, register_model, register_model_deprecations\n\n__all__ = ['VisionTransformer']  # model_registry will add each entrypoint fn to this\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass Attention(nn.Module):\n    fused_attn: Final[bool]\n\n    def __init__(\n            self,\n            dim,\n            num_heads=8,\n            qkv_bias=False,\n            qk_norm=False,\n            attn_drop=0.,\n            proj_drop=0.,\n            norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.fused_attn = use_fused_attn()\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)\n        q, k = self.q_norm(q), self.k_norm(k)\n\n        if self.fused_attn:\n            x = F.scaled_dot_product_attention(\n                q, k, v,\n                dropout_p=self.attn_drop.p,\n            )\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            x = attn @ v\n\n        x = x.transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass LayerScale(nn.Module):\n    def __init__(self, dim, init_values=1e-5, inplace=False):\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n\n\nclass Block(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            qk_norm=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            init_values=None,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            mlp_layer=Mlp,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_norm=qk_norm,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n            norm_layer=norm_layer,\n        )\n        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        self.mlp = mlp_layer(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x\n\n\nclass ResPostBlock(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            qk_norm=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            init_values=None,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            mlp_layer=Mlp,\n    ):\n        super().__init__()\n        self.init_values = init_values\n\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_norm=qk_norm,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n            norm_layer=norm_layer,\n        )\n        self.norm1 = norm_layer(dim)\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.mlp = mlp_layer(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n        self.norm2 = norm_layer(dim)\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.init_weights()\n\n    def init_weights(self):\n        # NOTE this init overrides that base model init with specific changes for the block type\n        if self.init_values is not None:\n            nn.init.constant_(self.norm1.weight, self.init_values)\n            nn.init.constant_(self.norm2.weight, self.init_values)\n\n    def forward(self, x):\n        x = x + self.drop_path1(self.norm1(self.attn(x)))\n        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n        return x\n\n\nclass ParallelScalingBlock(nn.Module):\n    \"\"\" Parallel ViT block (MLP & Attention in parallel)\n    Based on:\n      'Scaling Vision Transformers to 22 Billion Parameters` - https://arxiv.org/abs/2302.05442\n    \"\"\"\n    fused_attn: Final[bool]\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            qk_norm=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            init_values=None,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            mlp_layer=None,  # NOTE: not used\n    ):\n        super().__init__()\n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.fused_attn = use_fused_attn()\n        mlp_hidden_dim = int(mlp_ratio * dim)\n        in_proj_out_dim = mlp_hidden_dim + 3 * dim\n\n        self.in_norm = norm_layer(dim)\n        self.in_proj = nn.Linear(dim, in_proj_out_dim, bias=qkv_bias)\n        self.in_split = [mlp_hidden_dim] + [dim] * 3\n        if qkv_bias:\n            self.register_buffer('qkv_bias', None)\n            self.register_parameter('mlp_bias', None)\n        else:\n            self.register_buffer('qkv_bias', torch.zeros(3 * dim), persistent=False)\n            self.mlp_bias = nn.Parameter(torch.zeros(mlp_hidden_dim))\n\n        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.attn_out_proj = nn.Linear(dim, dim)\n\n        self.mlp_drop = nn.Dropout(proj_drop)\n        self.mlp_act = act_layer()\n        self.mlp_out_proj = nn.Linear(mlp_hidden_dim, dim)\n\n        self.ls = LayerScale(dim, init_values=init_values) if init_values is not None else nn.Identity()\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        B, N, C = x.shape\n\n        # Combined MLP fc1 & qkv projections\n        y = self.in_norm(x)\n        if self.mlp_bias is not None:\n            # Concat constant zero-bias for qkv w/ trainable mlp_bias.\n            # Appears faster than adding to x_mlp separately\n            y = F.linear(y, self.in_proj.weight, torch.cat((self.qkv_bias, self.mlp_bias)))\n        else:\n            y = self.in_proj(y)\n        x_mlp, q, k, v = torch.split(y, self.in_split, dim=-1)\n\n        # Dot product attention w/ qk norm\n        q = self.q_norm(q.view(B, N, self.num_heads, self.head_dim)).transpose(1, 2)\n        k = self.k_norm(k.view(B, N, self.num_heads, self.head_dim)).transpose(1, 2)\n        v = v.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n        if self.fused_attn:\n            x_attn = F.scaled_dot_product_attention(\n                q, k, v,\n                dropout_p=self.attn_drop.p,\n            )\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            x_attn = attn @ v\n        x_attn = x_attn.transpose(1, 2).reshape(B, N, C)\n        x_attn = self.attn_out_proj(x_attn)\n\n        # MLP activation, dropout, fc2\n        x_mlp = self.mlp_act(x_mlp)\n        x_mlp = self.mlp_drop(x_mlp)\n        x_mlp = self.mlp_out_proj(x_mlp)\n\n        # Add residual w/ drop path & layer scale applied\n        y = self.drop_path(self.ls(x_attn + x_mlp))\n        x = x + y\n        return x\n\n\nclass ParallelThingsBlock(nn.Module):\n    \"\"\" Parallel ViT block (N parallel attention followed by N parallel MLP)\n    Based on:\n      `Three things everyone should know about Vision Transformers` - https://arxiv.org/abs/2203.09795\n    \"\"\"\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            num_parallel=2,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            qk_norm=False,\n            init_values=None,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            mlp_layer=Mlp,\n    ):\n        super().__init__()\n        self.num_parallel = num_parallel\n        self.attns = nn.ModuleList()\n        self.ffns = nn.ModuleList()\n        for _ in range(num_parallel):\n            self.attns.append(nn.Sequential(OrderedDict([\n                ('norm', norm_layer(dim)),\n                ('attn', Attention(\n                    dim,\n                    num_heads=num_heads,\n                    qkv_bias=qkv_bias,\n                    qk_norm=qk_norm,\n                    attn_drop=attn_drop,\n                    proj_drop=proj_drop,\n                    norm_layer=norm_layer,\n                )),\n                ('ls', LayerScale(dim, init_values=init_values) if init_values else nn.Identity()),\n                ('drop_path', DropPath(drop_path) if drop_path > 0. else nn.Identity())\n            ])))\n            self.ffns.append(nn.Sequential(OrderedDict([\n                ('norm', norm_layer(dim)),\n                ('mlp', mlp_layer(\n                    dim,\n                    hidden_features=int(dim * mlp_ratio),\n                    act_layer=act_layer,\n                    drop=proj_drop,\n                )),\n                ('ls', LayerScale(dim, init_values=init_values) if init_values else nn.Identity()),\n                ('drop_path', DropPath(drop_path) if drop_path > 0. else nn.Identity())\n            ])))\n\n    def _forward_jit(self, x):\n        x = x + torch.stack([attn(x) for attn in self.attns]).sum(dim=0)\n        x = x + torch.stack([ffn(x) for ffn in self.ffns]).sum(dim=0)\n        return x\n\n    @torch.jit.ignore\n    def _forward(self, x):\n        x = x + sum(attn(x) for attn in self.attns)\n        x = x + sum(ffn(x) for ffn in self.ffns)\n        return x\n\n    def forward(self, x):\n        if torch.jit.is_scripting() or torch.jit.is_tracing():\n            return self._forward_jit(x)\n        else:\n            return self._forward(x)\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer\n\n    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n        - https://arxiv.org/abs/2010.11929\n    \"\"\"\n\n    def __init__(\n            self,\n            img_size: Union[int, Tuple[int, int]] = 224,\n            patch_size: Union[int, Tuple[int, int]] = 16,\n            in_chans: int = 3,\n            num_classes: int = 1000,\n            global_pool: str = 'token',\n            embed_dim: int = 768,\n            depth: int = 12,\n            num_heads: int = 12,\n            mlp_ratio: float = 4.,\n            qkv_bias: bool = True,\n            qk_norm: bool = False,\n            init_values: Optional[float] = None,\n            class_token: bool = True,\n            no_embed_class: bool = False,\n            pre_norm: bool = False,\n            fc_norm: Optional[bool] = None,\n            drop_rate: float = 0.,\n            pos_drop_rate: float = 0.,\n            patch_drop_rate: float = 0.,\n            proj_drop_rate: float = 0.,\n            attn_drop_rate: float = 0.,\n            drop_path_rate: float = 0.,\n            weight_init: str = '',\n            embed_layer: Callable = PatchEmbed,\n            norm_layer: Optional[Callable] = None,\n            act_layer: Optional[Callable] = None,\n            block_fn: Callable = Block,\n            mlp_layer: Callable = Mlp,\n    ):\n        \"\"\"\n        Args:\n            img_size: Input image size.\n            patch_size: Patch size.\n            in_chans: Number of image input channels.\n            num_classes: Mumber of classes for classification head.\n            global_pool: Type of global pooling for final sequence (default: 'token').\n            embed_dim: Transformer embedding dimension.\n            depth: Depth of transformer.\n            num_heads: Number of attention heads.\n            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n            qkv_bias: Enable bias for qkv projections if True.\n            init_values: Layer-scale init values (layer-scale enabled if not None).\n            class_token: Use class token.\n            fc_norm: Pre head norm after pool (instead of before), if None, enabled when global_pool == 'avg'.\n            drop_rate: Head dropout rate.\n            pos_drop_rate: Position embedding dropout rate.\n            attn_drop_rate: Attention dropout rate.\n            drop_path_rate: Stochastic depth rate.\n            weight_init: Weight initialization scheme.\n            embed_layer: Patch embedding layer.\n            norm_layer: Normalization layer.\n            act_layer: MLP activation layer.\n            block_fn: Transformer block layer.\n        \"\"\"\n        super().__init__()\n        assert global_pool in ('', 'avg', 'token')\n        assert class_token or global_pool != 'token'\n        use_fc_norm = global_pool == 'avg' if fc_norm is None else fc_norm\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        act_layer = act_layer or nn.GELU\n\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.num_prefix_tokens = 1 if class_token else 0\n        self.no_embed_class = no_embed_class\n        self.grad_checkpointing = False\n\n        self.patch_embed = embed_layer(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            bias=not pre_norm,  # disable bias if pre-norm is used (e.g. CLIP)\n        )\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if class_token else None\n        embed_len = num_patches if no_embed_class else num_patches + self.num_prefix_tokens\n        self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n        if patch_drop_rate > 0:\n            self.patch_drop = PatchDropout(\n                patch_drop_rate,\n                num_prefix_tokens=self.num_prefix_tokens,\n            )\n        else:\n            self.patch_drop = nn.Identity()\n        self.norm_pre = norm_layer(embed_dim) if pre_norm else nn.Identity()\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.blocks = nn.Sequential(*[\n            block_fn(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_norm=qk_norm,\n                init_values=init_values,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                mlp_layer=mlp_layer,\n            )\n            for i in range(depth)])\n        self.norm = norm_layer(embed_dim) if not use_fc_norm else nn.Identity()\n\n        # Classifier Head\n        self.fc_norm = norm_layer(embed_dim) if use_fc_norm else nn.Identity()\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        if weight_init != 'skip':\n            self.init_weights(weight_init)\n\n    def init_weights(self, mode=''):\n        assert mode in ('jax', 'jax_nlhb', 'moco', '')\n        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.\n        trunc_normal_(self.pos_embed, std=.02)\n        if self.cls_token is not None:\n            nn.init.normal_(self.cls_token, std=1e-6)\n        named_apply(get_init_weights_vit(mode, head_bias), self)\n\n    def _init_weights(self, m):\n        # this fn left here for compat with downstream users\n        init_weights_vit_timm(m)\n\n    @torch.jit.ignore()\n    def load_pretrained(self, checkpoint_path, prefix=''):\n        _load_weights(self, checkpoint_path, prefix)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token', 'dist_token'}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^cls_token|pos_embed|patch_embed',  # stem and embed\n            blocks=[(r'^blocks\\.(\\d+)', None), (r'^norm', (99999,))]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes: int, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('', 'avg', 'token')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def _pos_embed(self, x):\n        if self.no_embed_class:\n            # deit-3, updated JAX (big vision)\n            # position embedding does not overlap with class token, add then concat\n            x = x + self.pos_embed\n            if self.cls_token is not None:\n                x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n        else:\n            # original timm, JAX, and deit vit impl\n            # pos_embed has entry for class token, concat then add\n            if self.cls_token is not None:\n                x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n            x = x + self.pos_embed\n        return self.pos_drop(x)\n\n    def _intermediate_layers(\n            self,\n            x: torch.Tensor,\n            n: Union[int, Sequence] = 1,\n    ):\n        outputs, num_blocks = [], len(self.blocks)\n        take_indices = set(range(num_blocks - n, num_blocks) if isinstance(n, int) else n)\n\n        # forward pass\n        x = self.patch_embed(x)\n        x = self._pos_embed(x)\n        x = self.patch_drop(x)\n        x = self.norm_pre(x)\n        for i, blk in enumerate(self.blocks):\n            x = blk(x)\n            if i in take_indices:\n                outputs.append(x)\n\n        return outputs\n\n    def get_intermediate_layers(\n            self,\n            x: torch.Tensor,\n            n: Union[int, Sequence] = 1,\n            reshape: bool = False,\n            return_class_token: bool = False,\n            norm: bool = False,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]]]:\n        \"\"\" Intermediate layer accessor (NOTE: This is a WIP experiment).\n        Inspired by DINO / DINOv2 interface\n        \"\"\"\n        # take last n blocks if n is an int, if in is a sequence, select by matching indices\n        outputs = self._intermediate_layers(x, n)\n        if norm:\n            outputs = [self.norm(out) for out in outputs]\n        class_tokens = [out[:, 0:self.num_prefix_tokens] for out in outputs]\n        outputs = [out[:, self.num_prefix_tokens:] for out in outputs]\n\n        if reshape:\n            grid_size = self.patch_embed.grid_size\n            outputs = [\n                out.reshape(x.shape[0], grid_size[0], grid_size[1], -1).permute(0, 3, 1, 2).contiguous()\n                for out in outputs\n            ]\n\n        if return_class_token:\n            return tuple(zip(outputs, class_tokens))\n        return tuple(outputs)\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = self._pos_embed(x)\n        x = self.patch_drop(x)\n        x = self.norm_pre(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool:\n            x = x[:, self.num_prefix_tokens:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]\n        x = self.fc_norm(x)\n        x = self.head_drop(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef init_weights_vit_timm(module: nn.Module, name: str = ''):\n    \"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\n    if isinstance(module, nn.Linear):\n        trunc_normal_(module.weight, std=.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif hasattr(module, 'init_weights'):\n        module.init_weights()\n\n\ndef init_weights_vit_jax(module: nn.Module, name: str = '', head_bias: float = 0.):\n    \"\"\" ViT weight initialization, matching JAX (Flax) impl \"\"\"\n    if isinstance(module, nn.Linear):\n        if name.startswith('head'):\n            nn.init.zeros_(module.weight)\n            nn.init.constant_(module.bias, head_bias)\n        else:\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.normal_(module.bias, std=1e-6) if 'mlp' in name else nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Conv2d):\n        lecun_normal_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif hasattr(module, 'init_weights'):\n        module.init_weights()\n\n\ndef init_weights_vit_moco(module: nn.Module, name: str = ''):\n    \"\"\" ViT weight initialization, matching moco-v3 impl minus fixed PatchEmbed \"\"\"\n    if isinstance(module, nn.Linear):\n        if 'qkv' in name:\n            # treat the weights of Q, K, V separately\n            val = math.sqrt(6. / float(module.weight.shape[0] // 3 + module.weight.shape[1]))\n            nn.init.uniform_(module.weight, -val, val)\n        else:\n            nn.init.xavier_uniform_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif hasattr(module, 'init_weights'):\n        module.init_weights()\n\n\ndef get_init_weights_vit(mode='jax', head_bias: float = 0.):\n    if 'jax' in mode:\n        return partial(init_weights_vit_jax, head_bias=head_bias)\n    elif 'moco' in mode:\n        return init_weights_vit_moco\n    else:\n        return init_weights_vit_timm\n\n\ndef resize_pos_embed(\n        posemb,\n        posemb_new,\n        num_prefix_tokens=1,\n        gs_new=(),\n        interpolation='bicubic',\n        antialias=False,\n):\n    \"\"\" Rescale the grid of position embeddings when loading from state_dict.\n\n    *DEPRECATED* This function is being deprecated in favour of resample_abs_pos_embed\n\n    Adapted from:\n        https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n    \"\"\"\n    ntok_new = posemb_new.shape[1]\n    if num_prefix_tokens:\n        posemb_prefix, posemb_grid = posemb[:, :num_prefix_tokens], posemb[0, num_prefix_tokens:]\n        ntok_new -= num_prefix_tokens\n    else:\n        posemb_prefix, posemb_grid = posemb[:, :0], posemb[0]\n    gs_old = int(math.sqrt(len(posemb_grid)))\n    if not len(gs_new):  # backwards compatibility\n        gs_new = [int(math.sqrt(ntok_new))] * 2\n    assert len(gs_new) >= 2\n    _logger.info(f'Resized position embedding: {posemb.shape} ({[gs_old, gs_old]}) to {posemb_new.shape} ({gs_new}).')\n    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n    posemb_grid = F.interpolate(posemb_grid, size=gs_new, mode=interpolation, antialias=antialias, align_corners=False)\n    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_new[0] * gs_new[1], -1)\n    posemb = torch.cat([posemb_prefix, posemb_grid], dim=1)\n    return posemb\n\n\n@torch.no_grad()\ndef _load_weights(model: VisionTransformer, checkpoint_path: str, prefix: str = ''):\n    \"\"\" Load weights from .npz checkpoints for official Google Brain Flax implementation\n    \"\"\"\n    import numpy as np\n\n    def _n2p(w, t=True):\n        if w.ndim == 4 and w.shape[0] == w.shape[1] == w.shape[2] == 1:\n            w = w.flatten()\n        if t:\n            if w.ndim == 4:\n                w = w.transpose([3, 2, 0, 1])\n            elif w.ndim == 3:\n                w = w.transpose([2, 0, 1])\n            elif w.ndim == 2:\n                w = w.transpose([1, 0])\n        return torch.from_numpy(w)\n\n    w = np.load(checkpoint_path)\n    interpolation = 'bilinear'\n    antialias = False\n    big_vision = False\n    if not prefix:\n        if 'opt/target/embedding/kernel' in w:\n            prefix = 'opt/target/'\n        elif 'params/embedding/kernel' in w:\n            prefix = 'params/'\n            big_vision = True\n\n    if hasattr(model.patch_embed, 'backbone'):\n        # hybrid\n        backbone = model.patch_embed.backbone\n        stem_only = not hasattr(backbone, 'stem')\n        stem = backbone if stem_only else backbone.stem\n        stem.conv.weight.copy_(adapt_input_conv(stem.conv.weight.shape[1], _n2p(w[f'{prefix}conv_root/kernel'])))\n        stem.norm.weight.copy_(_n2p(w[f'{prefix}gn_root/scale']))\n        stem.norm.bias.copy_(_n2p(w[f'{prefix}gn_root/bias']))\n        if not stem_only:\n            for i, stage in enumerate(backbone.stages):\n                for j, block in enumerate(stage.blocks):\n                    bp = f'{prefix}block{i + 1}/unit{j + 1}/'\n                    for r in range(3):\n                        getattr(block, f'conv{r + 1}').weight.copy_(_n2p(w[f'{bp}conv{r + 1}/kernel']))\n                        getattr(block, f'norm{r + 1}').weight.copy_(_n2p(w[f'{bp}gn{r + 1}/scale']))\n                        getattr(block, f'norm{r + 1}').bias.copy_(_n2p(w[f'{bp}gn{r + 1}/bias']))\n                    if block.downsample is not None:\n                        block.downsample.conv.weight.copy_(_n2p(w[f'{bp}conv_proj/kernel']))\n                        block.downsample.norm.weight.copy_(_n2p(w[f'{bp}gn_proj/scale']))\n                        block.downsample.norm.bias.copy_(_n2p(w[f'{bp}gn_proj/bias']))\n        embed_conv_w = _n2p(w[f'{prefix}embedding/kernel'])\n    else:\n        embed_conv_w = adapt_input_conv(\n            model.patch_embed.proj.weight.shape[1], _n2p(w[f'{prefix}embedding/kernel']))\n    if embed_conv_w.shape[-2:] != model.patch_embed.proj.weight.shape[-2:]:\n        embed_conv_w = resample_patch_embed(\n            embed_conv_w,\n            model.patch_embed.proj.weight.shape[-2:],\n            interpolation=interpolation,\n            antialias=antialias,\n            verbose=True,\n        )\n\n    model.patch_embed.proj.weight.copy_(embed_conv_w)\n    model.patch_embed.proj.bias.copy_(_n2p(w[f'{prefix}embedding/bias']))\n    if model.cls_token is not None:\n        model.cls_token.copy_(_n2p(w[f'{prefix}cls'], t=False))\n    if big_vision:\n        pos_embed_w = _n2p(w[f'{prefix}pos_embedding'], t=False)\n    else:\n        pos_embed_w = _n2p(w[f'{prefix}Transformer/posembed_input/pos_embedding'], t=False)\n    if pos_embed_w.shape != model.pos_embed.shape:\n        old_shape = pos_embed_w.shape\n        num_prefix_tokens = 0 if getattr(model, 'no_embed_class', False) else getattr(model, 'num_prefix_tokens', 1)\n        pos_embed_w = resample_abs_pos_embed(  # resize pos embedding when different size from pretrained weights\n            pos_embed_w,\n            new_size=model.patch_embed.grid_size,\n            num_prefix_tokens=num_prefix_tokens,\n            interpolation=interpolation,\n            antialias=antialias,\n            verbose=True,\n        )\n    model.pos_embed.copy_(pos_embed_w)\n    model.norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))\n    model.norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))\n    if isinstance(model.head, nn.Linear) and model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:\n        model.head.weight.copy_(_n2p(w[f'{prefix}head/kernel']))\n        model.head.bias.copy_(_n2p(w[f'{prefix}head/bias']))\n    # NOTE representation layer has been removed, not used in latest 21k/1k pretrained weights\n    # if isinstance(getattr(model.pre_logits, 'fc', None), nn.Linear) and f'{prefix}pre_logits/bias' in w:\n    #     model.pre_logits.fc.weight.copy_(_n2p(w[f'{prefix}pre_logits/kernel']))\n    #     model.pre_logits.fc.bias.copy_(_n2p(w[f'{prefix}pre_logits/bias']))\n    mha_sub, b_sub, ln1_sub = (0, 0, 1) if big_vision else (1, 3, 2)\n    for i, block in enumerate(model.blocks.children()):\n        block_prefix = f'{prefix}Transformer/encoderblock_{i}/'\n        mha_prefix = block_prefix + f'MultiHeadDotProductAttention_{mha_sub}/'\n        block.norm1.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale']))\n        block.norm1.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias']))\n        block.attn.qkv.weight.copy_(torch.cat([\n            _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('query', 'key', 'value')]))\n        block.attn.qkv.bias.copy_(torch.cat([\n            _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('query', 'key', 'value')]))\n        block.attn.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel']).flatten(1))\n        block.attn.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias']))\n        for r in range(2):\n            getattr(block.mlp, f'fc{r + 1}').weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_{b_sub}/Dense_{r}/kernel']))\n            getattr(block.mlp, f'fc{r + 1}').bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_{b_sub}/Dense_{r}/bias']))\n        block.norm2.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_{ln1_sub}/scale']))\n        block.norm2.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_{ln1_sub}/bias']))\n\n\ndef _convert_openai_clip(state_dict, model):\n    out_dict = {}\n    swaps = [\n        ('visual.', ''), ('conv1', 'patch_embed.proj'), ('positional_embedding', 'pos_embed'),\n        ('transformer.resblocks.', 'blocks.'), ('ln_pre', 'norm_pre'), ('ln_post', 'norm'), ('ln_', 'norm'),\n        ('in_proj_', 'qkv.'), ('out_proj', 'proj'), ('mlp.c_fc', 'mlp.fc1'), ('mlp.c_proj', 'mlp.fc2'),\n    ]\n    for k, v in state_dict.items():\n        if not k.startswith('visual.'):\n            continue\n        for sp in swaps:\n            k = k.replace(sp[0], sp[1])\n\n        if k == 'proj':\n            k = 'head.weight'\n            v = v.transpose(0, 1)\n            out_dict['head.bias'] = torch.zeros(v.shape[0])\n        elif k == 'class_embedding':\n            k = 'cls_token'\n            v = v.unsqueeze(0).unsqueeze(1)\n        elif k == 'pos_embed':\n            v = v.unsqueeze(0)\n            if v.shape[1] != model.pos_embed.shape[1]:\n                # To resize pos embedding when using model at different size from pretrained weights\n                v = resize_pos_embed(\n                    v,\n                    model.pos_embed,\n                    0 if getattr(model, 'no_embed_class') else getattr(model, 'num_prefix_tokens', 1),\n                    model.patch_embed.grid_size\n                )\n        out_dict[k] = v\n    return out_dict\n\n\ndef _convert_dinov2(state_dict, model):\n    import re\n    out_dict = {}\n    for k, v in state_dict.items():\n        if k == \"mask_token\":\n            continue\n        elif re.match(r\"blocks\\.(\\d+)\\.mlp\\.w12\\.(?:weight|bias)\", k):\n            out_dict[k.replace(\"w12\", \"fc1\")] = v\n            continue\n        elif re.match(r\"blocks\\.(\\d+)\\.mlp\\.w3\\.(?:weight|bias)\", k):\n            out_dict[k.replace(\"w3\", \"fc2\")] = v\n            continue\n        out_dict[k] = v\n    return out_dict\n\n\ndef _convert_ijepa(state_dict, model):\n    out_dict = {}\n    for k, v in state_dict['encoder'].items():\n        if k.startswith('module.'):\n            k = k[7:]\n        if k.startswith('norm.'):\n            k = 'fc_norm.' + k[5:]\n        out_dict[k] = v\n    return out_dict\n\n\ndef checkpoint_filter_fn(\n        state_dict,\n        model,\n        adapt_layer_scale=False,\n        interpolation='bicubic',\n        antialias=True,\n):\n    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n    import re\n    out_dict = {}\n    state_dict = state_dict.get('model', state_dict)\n    state_dict = state_dict.get('state_dict', state_dict)\n\n    if 'visual.class_embedding' in state_dict:\n        return _convert_openai_clip(state_dict, model)\n\n    if \"mask_token\" in state_dict:\n        state_dict = _convert_dinov2(state_dict, model)\n\n    if \"encoder\" in state_dict:\n        state_dict = _convert_ijepa(state_dict, model)\n\n    for k, v in state_dict.items():\n        if 'patch_embed.proj.weight' in k:\n            O, I, H, W = model.patch_embed.proj.weight.shape\n            if len(v.shape) < 4:\n                # For old models that I trained prior to conv based patchification\n                O, I, H, W = model.patch_embed.proj.weight.shape\n                v = v.reshape(O, -1, H, W)\n            if v.shape[-1] != W or v.shape[-2] != H:\n                v = resample_patch_embed(\n                    v,\n                    (H, W),\n                    interpolation=interpolation,\n                    antialias=antialias,\n                    verbose=True,\n                )\n        elif k == 'pos_embed' and v.shape[1] != model.pos_embed.shape[1]:\n            # To resize pos embedding when using model at different size from pretrained weights\n            num_prefix_tokens = 0 if getattr(model, 'no_embed_class', False) else getattr(model, 'num_prefix_tokens', 1)\n            v = resample_abs_pos_embed(\n                v,\n                new_size=model.patch_embed.grid_size,\n                num_prefix_tokens=num_prefix_tokens,\n                interpolation=interpolation,\n                antialias=antialias,\n                verbose=True,\n            )\n        elif adapt_layer_scale and 'gamma_' in k:\n            # remap layer-scale gamma into sub-module (deit3 models)\n            k = re.sub(r'gamma_([0-9])', r'ls\\1.gamma', k)\n        elif 'pre_logits' in k:\n            # NOTE representation layer removed as not used in latest 21k/1k pretrained weights\n            continue\n        out_dict[k] = v\n    return out_dict\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,\n        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n\n    # re-finetuned augreg 21k FT on in1k weights\n    'vit_base_patch16_224.augreg2_in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/'),\n    'vit_base_patch16_384.augreg2_in21k_ft_in1k': _cfg(),\n    'vit_base_patch8_224.augreg2_in21k_ft_in1k': _cfg(\n        hf_hub_id='timm/'),\n\n    # How to train your ViT (augreg) weights, pretrained on 21k FT on in1k\n    'vit_tiny_patch16_224.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz',\n        hf_hub_id='timm/',\n        custom_load=True),\n    'vit_tiny_patch16_384.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz',\n        hf_hub_id='timm/',\n        custom_load=True, input_size=(3, 384, 384), crop_pct=1.0),\n    'vit_small_patch32_224.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz',\n        hf_hub_id='timm/',\n        custom_load=True),\n    'vit_small_patch32_384.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz',\n        hf_hub_id='timm/',\n        custom_load=True, input_size=(3, 384, 384), crop_pct=1.0),\n    'vit_small_patch16_224.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz',\n        hf_hub_id='timm/',\n        custom_load=True),\n    'vit_small_patch16_384.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz',\n        hf_hub_id='timm/',\n        custom_load=True, input_size=(3, 384, 384), crop_pct=1.0),\n    'vit_base_patch32_224.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz',\n        hf_hub_id='timm/',\n        custom_load=True),\n    'vit_base_patch32_384.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_32-i21k-300ep-lr_0.001-aug_light1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz',\n        hf_hub_id='timm/',\n        custom_load=True, input_size=(3, 384, 384), crop_pct=1.0),\n    'vit_base_patch16_224.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_224.npz',\n        hf_hub_id='timm/',\n        custom_load=True),\n    'vit_base_patch16_384.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz',\n        hf_hub_id='timm/',\n        custom_load=True, input_size=(3, 384, 384), crop_pct=1.0),\n    'vit_base_patch8_224.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_8-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_224.npz',\n        hf_hub_id='timm/',\n        custom_load=True),\n    'vit_large_patch16_224.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224.npz',\n        hf_hub_id='timm/',\n        custom_load=True),\n    'vit_large_patch16_384.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_384.npz',\n        hf_hub_id='timm/',\n        custom_load=True, input_size=(3, 384, 384), crop_pct=1.0),\n\n    # patch models (weights from official Google JAX impl) pretrained on in21k FT on in1k\n    'vit_base_patch16_224.orig_in21k_ft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth',\n        hf_hub_id='timm/'),\n    'vit_base_patch16_384.orig_in21k_ft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_384-83fb41ba.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), crop_pct=1.0),\n    'vit_large_patch32_384.orig_in21k_ft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p32_384-9b920ba8.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), crop_pct=1.0),\n\n    # How to train your ViT (augreg) weights trained on in1k only\n    'vit_small_patch16_224.augreg_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/S_16-i1k-300ep-lr_0.001-aug_medium2-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_224.npz',\n        hf_hub_id='timm/',\n        custom_load=True),\n    'vit_small_patch16_384.augreg_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/S_16-i1k-300ep-lr_0.001-aug_medium2-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz',\n        hf_hub_id='timm/',\n        custom_load=True, input_size=(3, 384, 384), crop_pct=1.0),\n    'vit_base_patch32_224.augreg_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_32-i1k-300ep-lr_0.001-aug_medium2-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224.npz',\n        hf_hub_id='timm/',\n        custom_load=True),\n    'vit_base_patch32_384.augreg_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_32-i1k-300ep-lr_0.001-aug_medium2-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_384.npz',\n        hf_hub_id='timm/',\n        custom_load=True, input_size=(3, 384, 384), crop_pct=1.0),\n    'vit_base_patch16_224.augreg_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_16-i1k-300ep-lr_0.001-aug_strong2-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224.npz',\n        hf_hub_id='timm/',\n        custom_load=True),\n    'vit_base_patch16_384.augreg_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_16-i1k-300ep-lr_0.001-aug_strong2-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_384.npz',\n        hf_hub_id='timm/',\n        custom_load=True, input_size=(3, 384, 384), crop_pct=1.0),\n\n    'vit_large_patch14_224.untrained': _cfg(url=''),\n    'vit_huge_patch14_224.untrained': _cfg(url=''),\n    'vit_giant_patch14_224.untrained': _cfg(url=''),\n    'vit_gigantic_patch14_224.untrained': _cfg(url=''),\n\n    # patch models, imagenet21k (weights from official Google JAX impl)\n    'vit_large_patch32_224.orig_in21k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_patch32_224_in21k-9046d2e7.pth',\n        hf_hub_id='timm/',\n        num_classes=21843),\n    'vit_huge_patch14_224.orig_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/imagenet21k/ViT-H_14.npz',\n        hf_hub_id='timm/',\n        custom_load=True, num_classes=21843),\n\n    # How to train your ViT (augreg) weights, pretrained on in21k\n    'vit_tiny_patch16_224.augreg_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0.npz',\n        hf_hub_id='timm/',\n        custom_load=True, num_classes=21843),\n    'vit_small_patch32_224.augreg_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0.npz',\n        hf_hub_id='timm/',\n        custom_load=True, num_classes=21843),\n    'vit_small_patch16_224.augreg_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0.npz',\n        hf_hub_id='timm/',\n        custom_load=True, num_classes=21843),\n    'vit_base_patch32_224.augreg_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.03-do_0.0-sd_0.0.npz',\n        hf_hub_id='timm/',\n        custom_load=True, num_classes=21843),\n    'vit_base_patch16_224.augreg_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz',\n        hf_hub_id='timm/',\n        custom_load=True, num_classes=21843),\n    'vit_base_patch8_224.augreg_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_8-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz',\n        hf_hub_id='timm/',\n        custom_load=True, num_classes=21843),\n    'vit_large_patch16_224.augreg_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1.npz',\n        hf_hub_id='timm/',\n        custom_load=True, num_classes=21843),\n\n    # SAM trained models (https://arxiv.org/abs/2106.01548)\n    'vit_base_patch32_224.sam_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/sam/ViT-B_32.npz', custom_load=True,\n        hf_hub_id='timm/'),\n    'vit_base_patch16_224.sam_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/sam/ViT-B_16.npz', custom_load=True,\n        hf_hub_id='timm/'),\n\n    # DINO pretrained - https://arxiv.org/abs/2104.14294 (no classifier head, for fine-tune only)\n    'vit_small_patch16_224.dino': _cfg(\n        url='https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0),\n    'vit_small_patch8_224.dino': _cfg(\n        url='https://dl.fbaipublicfiles.com/dino/dino_deitsmall8_pretrain/dino_deitsmall8_pretrain.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0),\n    'vit_base_patch16_224.dino': _cfg(\n        url='https://dl.fbaipublicfiles.com/dino/dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0),\n    'vit_base_patch8_224.dino': _cfg(\n        url='https://dl.fbaipublicfiles.com/dino/dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth',\n        hf_hub_id='timm/',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0),\n    \n    # DINOv2 pretrained - https://arxiv.org/abs/2304.07193 (no classifier head, for fine-tune/features only)\n    'vit_small_patch14_dinov2.lvd142m': _cfg(\n        url='https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth',\n        hf_hub_id='timm/',\n        license='cc-by-nc-4.0',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0,\n        input_size=(3, 518, 518), crop_pct=1.0),\n    'vit_base_patch14_dinov2.lvd142m': _cfg(\n        url='https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth',\n        hf_hub_id='timm/',\n        license='cc-by-nc-4.0',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0,\n        input_size=(3, 518, 518), crop_pct=1.0),\n    'vit_large_patch14_dinov2.lvd142m': _cfg(\n        url='https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth',\n        hf_hub_id='timm/',\n        license='cc-by-nc-4.0',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0,\n        input_size=(3, 518, 518), crop_pct=1.0),\n    'vit_giant_patch14_dinov2.lvd142m': _cfg(\n        url='https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth',\n        hf_hub_id='timm/',\n        license='cc-by-nc-4.0',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0,\n        input_size=(3, 518, 518), crop_pct=1.0),\n\n    # ViT ImageNet-21K-P pretraining by MILL\n    'vit_base_patch16_224_miil.in21k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/vit_base_patch16_224_in21k_miil-887286df.pth',\n        hf_hub_id='timm/',\n        mean=(0., 0., 0.), std=(1., 1., 1.), crop_pct=0.875, interpolation='bilinear', num_classes=11221),\n    'vit_base_patch16_224_miil.in21k_ft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/vit_base_patch16_224_1k_miil_84_4-2deb18e3.pth',\n        hf_hub_id='timm/',\n        mean=(0., 0., 0.), std=(1., 1., 1.), crop_pct=0.875, interpolation='bilinear'),\n\n    # Custom timm variants\n    'vit_base_patch16_rpn_224.sw_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_base_patch16_rpn_224-sw-3b07e89d.pth',\n        hf_hub_id='timm/'),\n    'vit_medium_patch16_gap_240.sw_in12k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), crop_pct=0.95, num_classes=11821),\n    'vit_medium_patch16_gap_256.sw_in12k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 256, 256), crop_pct=0.95),\n    'vit_medium_patch16_gap_384.sw_in12k_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), crop_pct=0.95, crop_mode='squash'),\n    'vit_base_patch16_gap_224': _cfg(),\n\n    # CLIP pretrained image tower and related fine-tuned weights\n    'vit_base_patch32_clip_224.laion2b_ft_in12k_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD),\n    'vit_base_patch32_clip_384.laion2b_ft_in12k_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, input_size=(3, 384, 384)),\n    'vit_base_patch32_clip_448.laion2b_ft_in12k_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, input_size=(3, 448, 448)),\n    'vit_base_patch16_clip_224.laion2b_ft_in12k_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=0.95),\n    'vit_base_patch16_clip_384.laion2b_ft_in12k_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        crop_pct=1.0, input_size=(3, 384, 384), crop_mode='squash'),\n    'vit_large_patch14_clip_224.laion2b_ft_in12k_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD, crop_pct=1.0),\n    'vit_large_patch14_clip_336.laion2b_ft_in12k_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        crop_pct=1.0, input_size=(3, 336, 336), crop_mode='squash'),\n    'vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0),\n    'vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        crop_pct=1.0, input_size=(3, 336, 336), crop_mode='squash'),\n\n    'vit_base_patch32_clip_224.openai_ft_in12k_in1k': _cfg(\n        # hf_hub_id='timm/vit_base_patch32_clip_224.openai_ft_in12k_in1k',  # FIXME weight exists, need to push\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD),\n    'vit_base_patch32_clip_384.openai_ft_in12k_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        crop_pct=0.95, input_size=(3, 384, 384), crop_mode='squash'),\n    'vit_base_patch16_clip_224.openai_ft_in12k_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=0.95),\n    'vit_base_patch16_clip_384.openai_ft_in12k_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        crop_pct=0.95, input_size=(3, 384, 384), crop_mode='squash'),\n    'vit_large_patch14_clip_224.openai_ft_in12k_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0),\n    'vit_large_patch14_clip_336.openai_ft_in12k_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        crop_pct=1.0, input_size=(3, 336, 336), crop_mode='squash'),\n\n    'vit_base_patch32_clip_224.laion2b_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD),\n    'vit_base_patch16_clip_224.laion2b_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0),\n    'vit_base_patch16_clip_384.laion2b_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        crop_pct=1.0, input_size=(3, 384, 384), crop_mode='squash'),\n    'vit_large_patch14_clip_224.laion2b_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD, crop_pct=1.0),\n    'vit_large_patch14_clip_336.laion2b_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        crop_pct=1.0, input_size=(3, 336, 336), crop_mode='squash'),\n    'vit_huge_patch14_clip_224.laion2b_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0),\n    'vit_huge_patch14_clip_336.laion2b_ft_in1k': _cfg(\n        hf_hub_id='',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        crop_pct=1.0, input_size=(3, 336, 336), crop_mode='squash'),\n\n    'vit_base_patch32_clip_224.openai_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD),\n    'vit_base_patch16_clip_224.openai_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD),\n    'vit_base_patch16_clip_384.openai_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        crop_pct=1.0, input_size=(3, 384, 384), crop_mode='squash'),\n    'vit_large_patch14_clip_224.openai_ft_in1k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0),\n\n    'vit_base_patch32_clip_224.laion2b_ft_in12k': _cfg(\n        #hf_hub_id='timm/vit_base_patch32_clip_224.laion2b_ft_in12k',  # FIXME weight exists, need to push\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=11821),\n    'vit_base_patch16_clip_224.laion2b_ft_in12k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=11821),\n    'vit_large_patch14_clip_224.laion2b_ft_in12k': _cfg(\n        hf_hub_id='timm/',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD, crop_pct=1.0, num_classes=11821),\n    'vit_huge_patch14_clip_224.laion2b_ft_in12k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=11821),\n\n    'vit_base_patch32_clip_224.openai_ft_in12k': _cfg(\n        # hf_hub_id='timm/vit_base_patch32_clip_224.openai_ft_in12k',  # FIXME weight exists, need to push\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=11821),\n    'vit_base_patch16_clip_224.openai_ft_in12k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=11821),\n    'vit_large_patch14_clip_224.openai_ft_in12k': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=11821),\n\n    'vit_base_patch32_clip_224.laion2b': _cfg(\n        hf_hub_id='laion/CLIP-ViT-B-32-laion2B-s34B-b79K',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=512),\n    'vit_base_patch16_clip_224.laion2b': _cfg(\n        hf_hub_id='laion/CLIP-ViT-B-16-laion2B-s34B-b88K',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=512),\n    'vit_base_patch16_clip_224.datacompxl': _cfg(\n        hf_hub_id='laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=512),\n    'vit_large_patch14_clip_224.laion2b': _cfg(\n        hf_hub_id='laion/CLIP-ViT-L-14-laion2B-s32B-b82K',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD, crop_pct=1.0, num_classes=768),\n    'vit_large_patch14_clip_224.datacompxl': _cfg(\n        hf_hub_id='laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=768),\n    'vit_huge_patch14_clip_224.laion2b': _cfg(\n        hf_hub_id='laion/CLIP-ViT-H-14-laion2B-s32B-b79K',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=1024),\n    'vit_giant_patch14_clip_224.laion2b': _cfg(\n        hf_hub_id='laion/CLIP-ViT-g-14-laion2B-s12B-b42K',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=1024),\n    'vit_gigantic_patch14_clip_224.laion2b': _cfg(\n        hf_hub_id='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k',\n        hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=1280),\n\n    'vit_base_patch32_clip_224.openai': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=512),\n    'vit_base_patch16_clip_224.openai': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, num_classes=512),\n    'vit_large_patch14_clip_224.openai': _cfg(\n        hf_hub_id='timm/',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD, crop_pct=1.0, num_classes=768),\n    'vit_large_patch14_clip_336.openai': _cfg(\n        hf_hub_id='timm/', hf_hub_filename='open_clip_pytorch_model.bin',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        crop_pct=1.0, input_size=(3, 336, 336), num_classes=768),\n\n    # experimental (may be removed)\n    'vit_base_patch32_plus_256.untrained': _cfg(url='', input_size=(3, 256, 256), crop_pct=0.95),\n    'vit_base_patch16_plus_240.untrained': _cfg(url='', input_size=(3, 240, 240), crop_pct=0.95),\n    'vit_small_patch16_36x1_224.untrained': _cfg(url=''),\n    'vit_small_patch16_18x2_224.untrained': _cfg(url=''),\n    'vit_base_patch16_18x2_224.untrained': _cfg(url=''),\n\n    # EVA fine-tuned weights from MAE style MIM - EVA-CLIP target pretrain\n    # https://github.com/baaivision/EVA/blob/7ecf2c0a370d97967e86d047d7af9188f78d2df3/eva/README.md#eva-l-learning-better-mim-representations-from-eva-clip\n    'eva_large_patch14_196.in22k_ft_in22k_in1k': _cfg(\n        # hf_hub_id='BAAI/EVA', hf_hub_filename='eva_l_psz14_196px_21k_to_1k_ft_88p6.pt',\n        hf_hub_id='timm/', license='mit',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 196, 196), crop_pct=1.0),\n    'eva_large_patch14_336.in22k_ft_in22k_in1k': _cfg(\n        # hf_hub_id='BAAI/EVA', hf_hub_filename='eva_l_psz14_336px_21k_to_1k_ft_89p2.pt',\n        hf_hub_id='timm/', license='mit',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 336, 336), crop_pct=1.0, crop_mode='squash'),\n    'eva_large_patch14_196.in22k_ft_in1k': _cfg(\n        # hf_hub_id='BAAI/EVA', hf_hub_filename='eva_l_psz14_196px_1k_ft_88p0.pt',\n        hf_hub_id='timm/', license='mit',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 196, 196), crop_pct=1.0),\n    'eva_large_patch14_336.in22k_ft_in1k': _cfg(\n        # hf_hub_id='BAAI/EVA', hf_hub_filename='eva_l_psz14_336px_1k_ft_88p65.pt',\n        hf_hub_id='timm/', license='mit',\n        mean=OPENAI_CLIP_MEAN, std=OPENAI_CLIP_STD,\n        input_size=(3, 336, 336), crop_pct=1.0, crop_mode='squash'),\n\n    'flexivit_small.1200ep_in1k': _cfg(\n        url='https://storage.googleapis.com/big_vision/flexivit/flexivit_s_i1k.npz', custom_load=True,\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), crop_pct=0.95),\n    'flexivit_small.600ep_in1k': _cfg(\n        url='https://storage.googleapis.com/big_vision/flexivit/flexivit_s_i1k_600ep.npz', custom_load=True,\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), crop_pct=0.95),\n    'flexivit_small.300ep_in1k': _cfg(\n        url='https://storage.googleapis.com/big_vision/flexivit/flexivit_s_i1k_300ep.npz', custom_load=True,\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), crop_pct=0.95),\n\n    'flexivit_base.1200ep_in1k': _cfg(\n        url='https://storage.googleapis.com/big_vision/flexivit/flexivit_b_i1k.npz', custom_load=True,\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), crop_pct=0.95),\n    'flexivit_base.600ep_in1k': _cfg(\n        url='https://storage.googleapis.com/big_vision/flexivit/flexivit_b_i1k_600ep.npz', custom_load=True,\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), crop_pct=0.95),\n    'flexivit_base.300ep_in1k': _cfg(\n        url='https://storage.googleapis.com/big_vision/flexivit/flexivit_b_i1k_300ep.npz', custom_load=True,\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), crop_pct=0.95),\n    'flexivit_base.1000ep_in21k': _cfg(\n        url='https://storage.googleapis.com/big_vision/flexivit/flexivit_b_i21k_1000ep.npz', custom_load=True,\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), crop_pct=0.95, num_classes=21843),\n    'flexivit_base.300ep_in21k': _cfg(\n        url='https://storage.googleapis.com/big_vision/flexivit/flexivit_b_i21k_300ep.npz', custom_load=True,\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), crop_pct=0.95, num_classes=21843),\n\n    'flexivit_large.1200ep_in1k': _cfg(\n        url='https://storage.googleapis.com/big_vision/flexivit/flexivit_l_i1k.npz', custom_load=True,\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), crop_pct=0.95),\n    'flexivit_large.600ep_in1k': _cfg(\n        url='https://storage.googleapis.com/big_vision/flexivit/flexivit_l_i1k_600ep.npz', custom_load=True,\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), crop_pct=0.95),\n    'flexivit_large.300ep_in1k': _cfg(\n        url='https://storage.googleapis.com/big_vision/flexivit/flexivit_l_i1k_300ep.npz', custom_load=True,\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), crop_pct=0.95),\n\n    'flexivit_base.patch16_in21k': _cfg(\n        url='https://storage.googleapis.com/big_vision/flexivit/vit_b16_i21k_300ep.npz', custom_load=True,\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), crop_pct=0.95, num_classes=21843),\n    'flexivit_base.patch30_in21k': _cfg(\n        url='https://storage.googleapis.com/big_vision/flexivit/vit_b30_i21k_300ep.npz', custom_load=True,\n        hf_hub_id='timm/',\n        input_size=(3, 240, 240), crop_pct=0.95, num_classes=21843),\n\n    'vit_base_patch16_xp_224.untrained': _cfg(url=''),\n    'vit_large_patch14_xp_224.untrained': _cfg(url=''),\n    'vit_huge_patch14_xp_224.untrained': _cfg(url=''),\n\n    'vit_base_patch16_224.mae': _cfg(\n        url='https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth',\n        hf_hub_id='timm/',\n        license='cc-by-nc-4.0',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0),\n    'vit_large_patch16_224.mae': _cfg(\n        url='https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_large.pth',\n        hf_hub_id='timm/',\n        license='cc-by-nc-4.0',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0),\n    'vit_huge_patch14_224.mae': _cfg(\n        url='https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_huge.pth',\n        hf_hub_id='timm/',\n        license='cc-by-nc-4.0',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0),\n    \n    'vit_huge_patch14_224_ijepa.in1k': _cfg(\n        url='https://dl.fbaipublicfiles.com/ijepa/IN1K-vit.h.14-300e.pth.tar',\n        # hf_hub_id='timm/',\n        license='cc-by-nc-4.0',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0),\n    'vit_huge_patch14_224_ijepa.in22k': _cfg(\n        url='https://dl.fbaipublicfiles.com/ijepa/IN22K-vit.h.14-900e.pth.tar',\n        # hf_hub_id='timm/',\n        license='cc-by-nc-4.0',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0),\n    'vit_huge_patch16_448_ijepa.in1k': _cfg(\n        url='https://dl.fbaipublicfiles.com/ijepa/IN1K-vit.h.16-448px-300e.pth.tar',\n        # hf_hub_id='timm/',\n        license='cc-by-nc-4.0',\n        input_size=(3, 448, 448), crop_pct=1.0,\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0),\n    'vit_gigantic_patch16_224_ijepa.in22k': _cfg(\n        url='https://dl.fbaipublicfiles.com/ijepa/IN22K-vit.g.16-600e.pth.tar',\n        # hf_hub_id='timm/',\n        license='cc-by-nc-4.0',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, num_classes=0),\n})\n\n\ndef _create_vision_transformer(variant, pretrained=False, **kwargs):\n    if kwargs.get('features_only', None):\n        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n\n    if 'flexi' in variant:\n        # FIXME Google FlexiViT pretrained models have a strong preference for bilinear patch / embed\n        # interpolation, other pretrained models resize better w/ anti-aliased bicubic interpolation.\n        _filter_fn = partial(checkpoint_filter_fn, interpolation='bilinear', antialias=False)\n    else:\n        _filter_fn = checkpoint_filter_fn\n\n    return build_model_with_cfg(\n        VisionTransformer,\n        variant,\n        pretrained,\n        pretrained_filter_fn=_filter_fn,\n        **kwargs,\n    )\n\n\n@register_model\ndef vit_tiny_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Tiny (Vit-Ti/16)\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3)\n    model = _create_vision_transformer('vit_tiny_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_tiny_patch16_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Tiny (Vit-Ti/16) @ 384x384.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3)\n    model = _create_vision_transformer('vit_tiny_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_small_patch32_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Small (ViT-S/32)\n    \"\"\"\n    model_args = dict(patch_size=32, embed_dim=384, depth=12, num_heads=6)\n    model = _create_vision_transformer('vit_small_patch32_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_small_patch32_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Small (ViT-S/32) at 384x384.\n    \"\"\"\n    model_args = dict(patch_size=32, embed_dim=384, depth=12, num_heads=6)\n    model = _create_vision_transformer('vit_small_patch32_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_small_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Small (ViT-S/16)\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6)\n    model = _create_vision_transformer('vit_small_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_small_patch16_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Small (ViT-S/16)\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6)\n    model = _create_vision_transformer('vit_small_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_small_patch8_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Small (ViT-S/8)\n    \"\"\"\n    model_args = dict(patch_size=8, embed_dim=384, depth=12, num_heads=6)\n    model = _create_vision_transformer('vit_small_patch8_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch32_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Base (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k, source https://github.com/google-research/vision_transformer.\n    \"\"\"\n    model_args = dict(patch_size=32, embed_dim=768, depth=12, num_heads=12)\n    model = _create_vision_transformer('vit_base_patch32_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch32_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Base model (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n    \"\"\"\n    model_args = dict(patch_size=32, embed_dim=768, depth=12, num_heads=12)\n    model = _create_vision_transformer('vit_base_patch32_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12)\n    model = _create_vision_transformer('vit_base_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch16_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12)\n    model = _create_vision_transformer('vit_base_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch8_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Base (ViT-B/8) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.\n    \"\"\"\n    model_args = dict(patch_size=8, embed_dim=768, depth=12, num_heads=12)\n    model = _create_vision_transformer('vit_base_patch8_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_large_patch32_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929). No pretrained weights.\n    \"\"\"\n    model_args = dict(patch_size=32, embed_dim=1024, depth=24, num_heads=16)\n    model = _create_vision_transformer('vit_large_patch32_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_large_patch32_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n    \"\"\"\n    model_args = dict(patch_size=32, embed_dim=1024, depth=24, num_heads=16)\n    model = _create_vision_transformer('vit_large_patch32_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_large_patch16_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16)\n    model = _create_vision_transformer('vit_large_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_large_patch16_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16)\n    model = _create_vision_transformer('vit_large_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_large_patch14_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Large model (ViT-L/14)\n    \"\"\"\n    model_args = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16)\n    model = _create_vision_transformer('vit_large_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_huge_patch14_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Huge model (ViT-H/14) from original paper (https://arxiv.org/abs/2010.11929).\n    \"\"\"\n    model_args = dict(patch_size=14, embed_dim=1280, depth=32, num_heads=16)\n    model = _create_vision_transformer('vit_huge_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_giant_patch14_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Giant (little-g) model (ViT-g/14) from `Scaling Vision Transformers` - https://arxiv.org/abs/2106.04560\n    \"\"\"\n    model_args = dict(patch_size=14, embed_dim=1408, mlp_ratio=48/11, depth=40, num_heads=16)\n    model = _create_vision_transformer('vit_giant_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_gigantic_patch14_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Gigantic (big-G) model (ViT-G/14) from `Scaling Vision Transformers` - https://arxiv.org/abs/2106.04560\n    \"\"\"\n    model_args = dict(patch_size=14, embed_dim=1664, mlp_ratio=64/13, depth=48, num_heads=16)\n    model = _create_vision_transformer(\n        'vit_gigantic_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch16_224_miil(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n    Weights taken from: https://github.com/Alibaba-MIIL/ImageNet21K\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False)\n    model = _create_vision_transformer(\n        'vit_base_patch16_224_miil', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_medium_patch16_gap_240(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Medium (ViT-M/16) w/o class token, w/ avg-pool @ 240x240\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=512, depth=12, num_heads=8, class_token=False,\n        global_pool='avg', qkv_bias=False, init_values=1e-6, fc_norm=False)\n    model = _create_vision_transformer(\n        'vit_medium_patch16_gap_240', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_medium_patch16_gap_256(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Medium (ViT-M/16) w/o class token, w/ avg-pool @ 256x256\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=512, depth=12, num_heads=8, class_token=False,\n        global_pool='avg', qkv_bias=False, init_values=1e-6, fc_norm=False)\n    model = _create_vision_transformer(\n        'vit_medium_patch16_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_medium_patch16_gap_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Medium (ViT-M/16) w/o class token, w/ avg-pool @ 384x384\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=512, depth=12, num_heads=8, class_token=False,\n        global_pool='avg', qkv_bias=False, init_values=1e-6, fc_norm=False)\n    model = _create_vision_transformer(\n        'vit_medium_patch16_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch16_gap_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Base (ViT-B/16) w/o class token, w/ avg-pool @ 256x256\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=768, depth=12, num_heads=16, class_token=False, global_pool='avg', fc_norm=False)\n    model = _create_vision_transformer(\n        'vit_base_patch16_gap_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch32_clip_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-B/32 CLIP image tower @ 224x224\n    \"\"\"\n    model_args = dict(\n        patch_size=32, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=nn.LayerNorm)\n    model = _create_vision_transformer(\n        'vit_base_patch32_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch32_clip_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-B/32 CLIP image tower @ 384x384\n    \"\"\"\n    model_args = dict(\n        patch_size=32, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=nn.LayerNorm)\n    model = _create_vision_transformer(\n        'vit_base_patch32_clip_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch32_clip_448(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-B/32 CLIP image tower @ 448x448\n    \"\"\"\n    model_args = dict(\n        patch_size=32, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=nn.LayerNorm)\n    model = _create_vision_transformer(\n        'vit_base_patch32_clip_448', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch16_clip_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-B/16 CLIP image tower\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=nn.LayerNorm)\n    model = _create_vision_transformer(\n        'vit_base_patch16_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch16_clip_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-B/16 CLIP image tower @ 384x384\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=nn.LayerNorm)\n    model = _create_vision_transformer(\n        'vit_base_patch16_clip_384', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_large_patch14_clip_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Large model (ViT-L/14) CLIP image tower\n    \"\"\"\n    model_args = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16, pre_norm=True, norm_layer=nn.LayerNorm)\n    model = _create_vision_transformer(\n        'vit_large_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_large_patch14_clip_336(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Large model (ViT-L/14) CLIP image tower @ 336x336\n    \"\"\"\n    model_args = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16, pre_norm=True, norm_layer=nn.LayerNorm)\n    model = _create_vision_transformer(\n        'vit_large_patch14_clip_336', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_huge_patch14_clip_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Huge model (ViT-H/14) CLIP image tower.\n    \"\"\"\n    model_args = dict(patch_size=14, embed_dim=1280, depth=32, num_heads=16, pre_norm=True, norm_layer=nn.LayerNorm)\n    model = _create_vision_transformer(\n        'vit_huge_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_huge_patch14_clip_336(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Huge model (ViT-H/14) CLIP image tower @ 336x336\n    \"\"\"\n    model_args = dict(patch_size=14, embed_dim=1280, depth=32, num_heads=16, pre_norm=True, norm_layer=nn.LayerNorm)\n    model = _create_vision_transformer(\n        'vit_huge_patch14_clip_336', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_giant_patch14_clip_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Giant (little-g) model (ViT-g/14) from `Scaling Vision Transformers` - https://arxiv.org/abs/2106.04560\n    Pretrained weights from CLIP image tower.\n    \"\"\"\n    model_args = dict(\n        patch_size=14, embed_dim=1408, mlp_ratio=48/11, depth=40, num_heads=16, pre_norm=True, norm_layer=nn.LayerNorm)\n    model = _create_vision_transformer(\n        'vit_giant_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_gigantic_patch14_clip_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-bigG model (ViT-G/14) from `Scaling Vision Transformers` - https://arxiv.org/abs/2106.04560\n    Pretrained weights from CLIP image tower.\n    \"\"\"\n    model_args = dict(\n        patch_size=14, embed_dim=1664, mlp_ratio=64/13, depth=48, num_heads=16, pre_norm=True, norm_layer=nn.LayerNorm)\n    model = _create_vision_transformer(\n        'vit_gigantic_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n# Experimental models below\n\n@register_model\ndef vit_base_patch32_plus_256(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Base (ViT-B/32+)\n    \"\"\"\n    model_args = dict(patch_size=32, embed_dim=896, depth=12, num_heads=14, init_values=1e-5)\n    model = _create_vision_transformer(\n        'vit_base_patch32_plus_256', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch16_plus_240(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Base (ViT-B/16+)\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=896, depth=12, num_heads=14, init_values=1e-5)\n    model = _create_vision_transformer(\n        'vit_base_patch16_plus_240', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch16_rpn_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Base (ViT-B/16) w/ residual post-norm\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False, init_values=1e-5,\n        class_token=False, block_fn=ResPostBlock, global_pool='avg')\n    model = _create_vision_transformer(\n        'vit_base_patch16_rpn_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_small_patch16_36x1_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Base w/ LayerScale + 36 x 1 (36 block serial) config. Experimental, may remove.\n    Based on `Three things everyone should know about Vision Transformers` - https://arxiv.org/abs/2203.09795\n    Paper focuses on 24x2 + 48x1 for 'Small' width but those are extremely slow.\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=384, depth=36, num_heads=6, init_values=1e-5)\n    model = _create_vision_transformer(\n        'vit_small_patch16_36x1_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_small_patch16_18x2_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Small w/ LayerScale + 18 x 2 (36 block parallel) config. Experimental, may remove.\n    Based on `Three things everyone should know about Vision Transformers` - https://arxiv.org/abs/2203.09795\n    Paper focuses on 24x2 + 48x1 for 'Small' width but those are extremely slow.\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=384, depth=18, num_heads=6, init_values=1e-5, block_fn=ParallelThingsBlock)\n    model = _create_vision_transformer(\n        'vit_small_patch16_18x2_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch16_18x2_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Base w/ LayerScale + 18 x 2 (36 block parallel) config. Experimental, may remove.\n    Based on `Three things everyone should know about Vision Transformers` - https://arxiv.org/abs/2203.09795\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=768, depth=18, num_heads=12, init_values=1e-5, block_fn=ParallelThingsBlock)\n    model = _create_vision_transformer(\n        'vit_base_patch16_18x2_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva_large_patch14_196(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" EVA-large model https://arxiv.org/abs/2211.07636 /via MAE MIM pretrain\"\"\"\n    model_args = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16, global_pool='avg')\n    model = _create_vision_transformer(\n        'eva_large_patch14_196', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef eva_large_patch14_336(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" EVA-large model https://arxiv.org/abs/2211.07636 via MAE MIM pretrain\"\"\"\n    model_args = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16, global_pool='avg')\n    model = _create_vision_transformer('eva_large_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef flexivit_small(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" FlexiViT-Small\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6, no_embed_class=True)\n    model = _create_vision_transformer('flexivit_small', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef flexivit_base(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" FlexiViT-Base\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, no_embed_class=True)\n    model = _create_vision_transformer('flexivit_base', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef flexivit_large(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" FlexiViT-Large\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16, no_embed_class=True)\n    model = _create_vision_transformer('flexivit_large', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch16_xp_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Large model (ViT-L/14) w/ parallel blocks and qk norm enabled.\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=768, depth=12, num_heads=12, pre_norm=True, no_embed_class=True,\n        norm_layer=RmsNorm, block_fn=ParallelScalingBlock, qkv_bias=False, qk_norm=True,\n    )\n    model = _create_vision_transformer(\n        'vit_base_patch16_xp_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_large_patch14_xp_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Large model (ViT-L/14) w/ parallel blocks and qk norm enabled.\n    \"\"\"\n    model_args = dict(\n        patch_size=14, embed_dim=1024, depth=24, num_heads=16, pre_norm=True, no_embed_class=True,\n        norm_layer=RmsNorm, block_fn=ParallelScalingBlock, qkv_bias=False, qk_norm=True,\n    )\n    model = _create_vision_transformer(\n        'vit_large_patch14_xp_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_huge_patch14_xp_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Huge model (ViT-H/14) w/ parallel blocks and qk norm enabled.\n    \"\"\"\n    model_args = dict(\n        patch_size=14, embed_dim=1280, depth=32, num_heads=16, pre_norm=True, no_embed_class=True,\n        norm_layer=RmsNorm, block_fn=ParallelScalingBlock, qkv_bias=False, qk_norm=True,\n    )\n    model = _create_vision_transformer(\n        'vit_huge_patch14_xp_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_small_patch14_dinov2(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-S/14 for DINOv2\n    \"\"\"\n    model_args = dict(\n        patch_size=14, embed_dim=384, depth=12, num_heads=6, init_values=1e-5, img_size=518,\n    )\n    model = _create_vision_transformer(\n        'vit_small_patch14_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_patch14_dinov2(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-B/14 for DINOv2\n    \"\"\"\n    model_args = dict(\n        patch_size=14, embed_dim=768, depth=12, num_heads=12, init_values=1e-5, img_size=518,\n    )\n    model = _create_vision_transformer(\n        'vit_base_patch14_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_large_patch14_dinov2(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-L/14 for DINOv2\n    \"\"\"\n    model_args = dict(\n        patch_size=14, embed_dim=1024, depth=24, num_heads=16, init_values=1e-5, img_size=518,\n    )\n    model = _create_vision_transformer(\n        'vit_large_patch14_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_giant_patch14_dinov2(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-G/14 for DINOv2\n    \"\"\"\n\n    # The hidden_features of SwiGLU is calculated by:\n    # hidden_features = (int(hidden_features * 2 / 3) + 7) // 8 * 8\n    # When embed_dim=1536, hidden_features=4096\n    # With SwiGLUPacked, we need to set hidden_features = 2 * 4096 = 8192\n\n    model_args = dict(\n        patch_size=14, embed_dim=1536, depth=40, num_heads=24, init_values=1e-5, \n        mlp_ratio=2.66667 * 2, mlp_layer=SwiGLUPacked, img_size=518, act_layer=nn.SiLU\n    )\n    model = _create_vision_transformer(\n        'vit_giant_patch14_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_huge_patch14_224_ijepa(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Huge model (ViT-H/14) from `I-JEPA` - https://arxiv.org/abs/2301.08243\n    \"\"\"\n    model_args = dict(patch_size=14, embed_dim=1280, depth=32, num_heads=16, class_token=False, global_pool='avg')\n    model = _create_vision_transformer(\n        'vit_huge_patch14_224_ijepa', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_huge_patch16_448_ijepa(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Huge model (ViT-H/16) from `I-JEPA` - https://arxiv.org/abs/2301.08243\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=1280, depth=32, num_heads=16, class_token=False, global_pool='avg', img_size=448)\n    model = _create_vision_transformer(\n        'vit_huge_patch16_448_ijepa', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_gigantic_patch16_224_ijepa(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" ViT-Gigantic (big-G) model (ViT-G/16) from `I-JEPA - https://arxiv.org/abs/2301.08243\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=1664, mlp_ratio=64/13, depth=48, num_heads=16)\n    model = _create_vision_transformer(\n        'vit_gigantic_patch16_224_ijepa', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\nregister_model_deprecations(__name__, {\n    'vit_tiny_patch16_224_in21k': 'vit_tiny_patch16_224.augreg_in21k',\n    'vit_small_patch32_224_in21k': 'vit_small_patch32_224.augreg_in21k',\n    'vit_small_patch16_224_in21k': 'vit_small_patch16_224.augreg_in21k',\n    'vit_base_patch32_224_in21k': 'vit_base_patch32_224.augreg_in21k',\n    'vit_base_patch16_224_in21k': 'vit_base_patch16_224.augreg_in21k',\n    'vit_base_patch8_224_in21k': 'vit_base_patch8_224.augreg_in21k',\n    'vit_large_patch32_224_in21k': 'vit_large_patch32_224.orig_in21k',\n    'vit_large_patch16_224_in21k': 'vit_large_patch16_224.augreg_in21k',\n    'vit_huge_patch14_224_in21k': 'vit_huge_patch14_224.orig_in21k',\n    'vit_base_patch32_224_sam': 'vit_base_patch32_224.sam',\n    'vit_base_patch16_224_sam': 'vit_base_patch16_224.sam',\n    'vit_small_patch16_224_dino': 'vit_small_patch16_224.dino',\n    'vit_small_patch8_224_dino': 'vit_small_patch8_224.dino',\n    'vit_base_patch16_224_dino': 'vit_base_patch16_224.dino',\n    'vit_base_patch8_224_dino': 'vit_base_patch8_224.dino',\n    'vit_base_patch16_224_miil_in21k': 'vit_base_patch16_224_miil.in21k',\n    'vit_base_patch32_224_clip_laion2b': 'vit_base_patch32_clip_224.laion2b',\n    'vit_large_patch14_224_clip_laion2b': 'vit_large_patch14_clip_224.laion2b',\n    'vit_huge_patch14_224_clip_laion2b': 'vit_huge_patch14_clip_224.laion2b',\n    'vit_giant_patch14_224_clip_laion2b': 'vit_giant_patch14_clip_224.laion2b',\n})\n",
  "\"\"\" Hybrid Vision Transformer (ViT) in PyTorch\n\nA PyTorch implement of the Hybrid Vision Transformers as described in:\n\n'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale'\n    - https://arxiv.org/abs/2010.11929\n\n`How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers`\n    - https://arxiv.org/abs/2106.10270\n\nNOTE These hybrid model definitions depend on code in vision_transformer.py.\nThey were moved here to keep file sizes sane.\n\nHacked together by / Copyright 2020, Ross Wightman\n\"\"\"\nfrom functools import partial\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import StdConv2dSame, StdConv2d, to_2tuple\nfrom ._registry import generate_default_cfgs, register_model, register_model_deprecations\nfrom .resnet import resnet26d, resnet50d\nfrom .resnetv2 import ResNetV2, create_resnetv2_stem\nfrom .vision_transformer import _create_vision_transformer, VisionTransformer\n\n\nclass HybridEmbed(nn.Module):\n    \"\"\" CNN Feature Map Embedding\n    Extract feature map from CNN, flatten, project to embedding dim.\n    \"\"\"\n    def __init__(\n            self,\n            backbone,\n            img_size=224,\n            patch_size=1,\n            feature_size=None,\n            in_chans=3,\n            embed_dim=768,\n            bias=True,\n    ):\n        super().__init__()\n        assert isinstance(backbone, nn.Module)\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.backbone = backbone\n        if feature_size is None:\n            with torch.no_grad():\n                # NOTE Most reliable way of determining output dims is to run forward pass\n                training = backbone.training\n                if training:\n                    backbone.eval()\n                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))\n                if isinstance(o, (list, tuple)):\n                    o = o[-1]  # last feature if backbone outputs list/tuple of features\n                feature_size = o.shape[-2:]\n                feature_dim = o.shape[1]\n                backbone.train(training)\n        else:\n            feature_size = to_2tuple(feature_size)\n            if hasattr(self.backbone, 'feature_info'):\n                feature_dim = self.backbone.feature_info.channels()[-1]\n            else:\n                feature_dim = self.backbone.num_features\n        assert feature_size[0] % patch_size[0] == 0 and feature_size[1] % patch_size[1] == 0\n        self.grid_size = (feature_size[0] // patch_size[0], feature_size[1] // patch_size[1])\n        self.num_patches = self.grid_size[0] * self.grid_size[1]\n        self.proj = nn.Conv2d(feature_dim, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        if isinstance(x, (list, tuple)):\n            x = x[-1]  # last feature if backbone outputs list/tuple of features\n        x = self.proj(x)\n        x = x.flatten(2).transpose(1, 2)\n        return x\n\n\nclass HybridEmbedWithSize(nn.Module):\n    \"\"\" CNN Feature Map Embedding\n    Extract feature map from CNN, flatten, project to embedding dim.\n    \"\"\"\n    def __init__(\n            self,\n            backbone,\n            img_size=224,\n            patch_size=1,\n            feature_size=None,\n            in_chans=3,\n            embed_dim=768,\n            bias=True,\n    ):\n        super().__init__(\n            backbone=backbone,\n            img_size=img_size,\n            patch_size=patch_size,\n            feature_size=feature_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            bias=bias,\n        )\n\n    def forward(self, x) -> Tuple[torch.Tensor, List[int]]:\n        x = self.backbone(x)\n        if isinstance(x, (list, tuple)):\n            x = x[-1]  # last feature if backbone outputs list/tuple of features\n        x = self.proj(x)\n        return x.flatten(2).transpose(1, 2), x.shape[-2:]\n\n\ndef _create_vision_transformer_hybrid(variant, backbone, pretrained=False, **kwargs):\n    embed_layer = partial(HybridEmbed, backbone=backbone)\n    kwargs.setdefault('patch_size', 1)  # default patch size for hybrid models if not set\n    return _create_vision_transformer(variant, pretrained=pretrained, embed_layer=embed_layer, **kwargs)\n\n\ndef _resnetv2(layers=(3, 4, 9), **kwargs):\n    \"\"\" ResNet-V2 backbone helper\"\"\"\n    padding_same = kwargs.get('padding_same', True)\n    stem_type = 'same' if padding_same else ''\n    conv_layer = partial(StdConv2dSame, eps=1e-8) if padding_same else partial(StdConv2d, eps=1e-8)\n    if len(layers):\n        backbone = ResNetV2(\n            layers=layers, num_classes=0, global_pool='', in_chans=kwargs.get('in_chans', 3),\n            preact=False, stem_type=stem_type, conv_layer=conv_layer)\n    else:\n        backbone = create_resnetv2_stem(\n            kwargs.get('in_chans', 3), stem_type=stem_type, preact=False, conv_layer=conv_layer)\n    return backbone\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n        'first_conv': 'patch_embed.backbone.stem.conv', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    # hybrid in-1k models (weights from official JAX impl where they exist)\n    'vit_tiny_r_s16_p8_224.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/R_Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz',\n        hf_hub_id='timm/',\n        custom_load=True,\n        first_conv='patch_embed.backbone.conv'),\n    'vit_tiny_r_s16_p8_384.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/R_Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz',\n        hf_hub_id='timm/',\n        first_conv='patch_embed.backbone.conv', input_size=(3, 384, 384), crop_pct=1.0, custom_load=True),\n    'vit_small_r26_s32_224.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/R26_S_32-i21k-300ep-lr_0.001-aug_light0-wd_0.03-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.03-res_224.npz',\n        hf_hub_id='timm/',\n        custom_load=True,\n    ),\n    'vit_small_r26_s32_384.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/R26_S_32-i21k-300ep-lr_0.001-aug_medium2-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz',\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), crop_pct=1.0, custom_load=True),\n    'vit_base_r26_s32_224.untrained': _cfg(),\n    'vit_base_r50_s16_384.orig_in21k_ft_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_resnet50_384-9fd3c705.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), crop_pct=1.0),\n    'vit_large_r50_s32_224.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/R50_L_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224.npz',\n        hf_hub_id='timm/',\n        custom_load=True,\n    ),\n    'vit_large_r50_s32_384.augreg_in21k_ft_in1k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/R50_L_32-i21k-300ep-lr_0.001-aug_medium2-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz',\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), crop_pct=1.0, custom_load=True,\n    ),\n\n    # hybrid in-21k models (weights from official Google JAX impl where they exist)\n    'vit_tiny_r_s16_p8_224.augreg_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/R_Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0.npz',\n        hf_hub_id='timm/',\n        num_classes=21843, crop_pct=0.9, first_conv='patch_embed.backbone.conv', custom_load=True),\n    'vit_small_r26_s32_224.augreg_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/R26_S_32-i21k-300ep-lr_0.001-aug_medium2-wd_0.03-do_0.0-sd_0.0.npz',\n        hf_hub_id='timm/',\n        num_classes=21843, crop_pct=0.9, custom_load=True),\n    'vit_base_r50_s16_224.orig_in21k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_resnet50_224_in21k-6f7c7740.pth',\n        hf_hub_id='timm/',\n        num_classes=21843, crop_pct=0.9),\n    'vit_large_r50_s32_224.augreg_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/R50_L_32-i21k-300ep-lr_0.001-aug_medium2-wd_0.1-do_0.0-sd_0.0.npz',\n        hf_hub_id='timm/',\n        num_classes=21843, crop_pct=0.9, custom_load=True),\n\n    # hybrid models (using timm resnet backbones)\n    'vit_small_resnet26d_224.untrained': _cfg(\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, first_conv='patch_embed.backbone.conv1.0'),\n    'vit_small_resnet50d_s16_224.untrained': _cfg(\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, first_conv='patch_embed.backbone.conv1.0'),\n    'vit_base_resnet26d_224.untrained': _cfg(\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, first_conv='patch_embed.backbone.conv1.0'),\n    'vit_base_resnet50d_224.untrained': _cfg(\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, first_conv='patch_embed.backbone.conv1.0'),\n})\n\n\n@register_model\ndef vit_tiny_r_s16_p8_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" R+ViT-Ti/S16 w/ 8x8 patch hybrid @ 224 x 224.\n    \"\"\"\n    backbone = _resnetv2(layers=(), **kwargs)\n    model_args = dict(patch_size=8, embed_dim=192, depth=12, num_heads=3)\n    model = _create_vision_transformer_hybrid(\n        'vit_tiny_r_s16_p8_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_tiny_r_s16_p8_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" R+ViT-Ti/S16 w/ 8x8 patch hybrid @ 384 x 384.\n    \"\"\"\n    backbone = _resnetv2(layers=(), **kwargs)\n    model_args = dict(patch_size=8, embed_dim=192, depth=12, num_heads=3)\n    model = _create_vision_transformer_hybrid(\n        'vit_tiny_r_s16_p8_384', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_small_r26_s32_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" R26+ViT-S/S32 hybrid.\n    \"\"\"\n    backbone = _resnetv2((2, 2, 2, 2), **kwargs)\n    model_args = dict(embed_dim=384, depth=12, num_heads=6)\n    model = _create_vision_transformer_hybrid(\n        'vit_small_r26_s32_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_small_r26_s32_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" R26+ViT-S/S32 hybrid.\n    \"\"\"\n    backbone = _resnetv2((2, 2, 2, 2), **kwargs)\n    model_args = dict(embed_dim=384, depth=12, num_heads=6)\n    model = _create_vision_transformer_hybrid(\n        'vit_small_r26_s32_384', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_r26_s32_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" R26+ViT-B/S32 hybrid.\n    \"\"\"\n    backbone = _resnetv2((2, 2, 2, 2), **kwargs)\n    model_args = dict(embed_dim=768, depth=12, num_heads=12)\n    model = _create_vision_transformer_hybrid(\n        'vit_base_r26_s32_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_r50_s16_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" R50+ViT-B/S16 hybrid from original paper (https://arxiv.org/abs/2010.11929).\n    \"\"\"\n    backbone = _resnetv2((3, 4, 9), **kwargs)\n    model_args = dict(embed_dim=768, depth=12, num_heads=12)\n    model = _create_vision_transformer_hybrid(\n        'vit_base_r50_s16_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_r50_s16_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" R50+ViT-B/16 hybrid from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n    \"\"\"\n    backbone = _resnetv2((3, 4, 9), **kwargs)\n    model_args = dict(embed_dim=768, depth=12, num_heads=12)\n    model = _create_vision_transformer_hybrid(\n        'vit_base_r50_s16_384', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_large_r50_s32_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" R50+ViT-L/S32 hybrid.\n    \"\"\"\n    backbone = _resnetv2((3, 4, 6, 3), **kwargs)\n    model_args = dict(embed_dim=1024, depth=24, num_heads=16)\n    model = _create_vision_transformer_hybrid(\n        'vit_large_r50_s32_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_large_r50_s32_384(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" R50+ViT-L/S32 hybrid.\n    \"\"\"\n    backbone = _resnetv2((3, 4, 6, 3), **kwargs)\n    model_args = dict(embed_dim=1024, depth=24, num_heads=16)\n    model = _create_vision_transformer_hybrid(\n        'vit_large_r50_s32_384', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_small_resnet26d_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" Custom ViT small hybrid w/ ResNet26D stride 32. No pretrained weights.\n    \"\"\"\n    backbone = resnet26d(pretrained=pretrained, in_chans=kwargs.get('in_chans', 3), features_only=True, out_indices=[4])\n    model_args = dict(embed_dim=768, depth=8, num_heads=8, mlp_ratio=3)\n    model = _create_vision_transformer_hybrid(\n        'vit_small_resnet26d_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_small_resnet50d_s16_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" Custom ViT small hybrid w/ ResNet50D 3-stages, stride 16. No pretrained weights.\n    \"\"\"\n    backbone = resnet50d(pretrained=pretrained, in_chans=kwargs.get('in_chans', 3), features_only=True, out_indices=[3])\n    model_args = dict(embed_dim=768, depth=8, num_heads=8, mlp_ratio=3)\n    model = _create_vision_transformer_hybrid(\n        'vit_small_resnet50d_s16_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_resnet26d_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" Custom ViT base hybrid w/ ResNet26D stride 32. No pretrained weights.\n    \"\"\"\n    backbone = resnet26d(pretrained=pretrained, in_chans=kwargs.get('in_chans', 3), features_only=True, out_indices=[4])\n    model_args = dict(embed_dim=768, depth=12, num_heads=12)\n    model = _create_vision_transformer_hybrid(\n        'vit_base_resnet26d_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_base_resnet50d_224(pretrained=False, **kwargs) -> VisionTransformer:\n    \"\"\" Custom ViT base hybrid w/ ResNet50D stride 32. No pretrained weights.\n    \"\"\"\n    backbone = resnet50d(pretrained=pretrained, in_chans=kwargs.get('in_chans', 3), features_only=True, out_indices=[4])\n    model_args = dict(embed_dim=768, depth=12, num_heads=12)\n    model = _create_vision_transformer_hybrid(\n        'vit_base_resnet50d_224', backbone=backbone, pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\nregister_model_deprecations(__name__, {\n    'vit_tiny_r_s16_p8_224_in21k': 'vit_tiny_r_s16_p8_224.augreg_in21k',\n    'vit_small_r26_s32_224_in21k': 'vit_small_r26_s32_224.augreg_in21k',\n    'vit_base_r50_s16_224_in21k': 'vit_base_r50_s16_224.orig_in21k',\n    'vit_base_resnet50_224_in21k': 'vit_base_r50_s16_224.orig_in21k',\n    'vit_large_r50_s32_224_in21k': 'vit_large_r50_s32_224.augreg_in21k',\n    'vit_base_resnet50_384': 'vit_base_r50_s16_384.orig_in21k_ft_in1k'\n})\n",
  "\"\"\" EfficientFormer-V2\n\n@article{\n    li2022rethinking,\n    title={Rethinking Vision Transformers for MobileNet Size and Speed},\n    author={Li, Yanyu and Hu, Ju and Wen, Yang and Evangelidis, Georgios and Salahi, Kamyar and Wang, Yanzhi and Tulyakov, Sergey and Ren, Jian},\n    journal={arXiv preprint arXiv:2212.08059},\n    year={2022}\n}\n\nSignificantly refactored and cleaned up for timm from original at: https://github.com/snap-research/EfficientFormer\n\nOriginal code licensed Apache 2.0, Copyright (c) 2022 Snap Inc.\n\nModifications and timm support by / Copyright 2023, Ross Wightman\n\"\"\"\nimport math\nfrom functools import partial\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import create_conv2d, create_norm_layer, get_act_layer, get_norm_layer, ConvNormAct\nfrom timm.layers import DropPath, trunc_normal_, to_2tuple, to_ntuple\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import generate_default_cfgs, register_model\n\n\nEfficientFormer_width = {\n    'L': (40, 80, 192, 384),  # 26m 83.3% 6attn\n    'S2': (32, 64, 144, 288),  # 12m 81.6% 4attn dp0.02\n    'S1': (32, 48, 120, 224),  # 6.1m 79.0\n    'S0': (32, 48, 96, 176),  # 75.0 75.7\n}\n\nEfficientFormer_depth = {\n    'L': (5, 5, 15, 10),  # 26m 83.3%\n    'S2': (4, 4, 12, 8),  # 12m\n    'S1': (3, 3, 9, 6),  # 79.0\n    'S0': (2, 2, 6, 4),  # 75.7\n}\n\nEfficientFormer_expansion_ratios = {\n    'L': (4, 4, (4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4), (4, 4, 4, 3, 3, 3, 3, 4, 4, 4)),\n    'S2': (4, 4, (4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4), (4, 4, 3, 3, 3, 3, 4, 4)),\n    'S1': (4, 4, (4, 4, 3, 3, 3, 3, 4, 4, 4), (4, 4, 3, 3, 4, 4)),\n    'S0': (4, 4, (4, 3, 3, 3, 4, 4), (4, 3, 3, 4)),\n}\n\n\nclass ConvNorm(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size=1,\n            stride=1,\n            padding='',\n            dilation=1,\n            groups=1,\n            bias=True,\n            norm_layer='batchnorm2d',\n            norm_kwargs=None,\n    ):\n        norm_kwargs = norm_kwargs or {}\n        super(ConvNorm, self).__init__()\n        self.conv = create_conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias,\n        )\n        self.bn = create_norm_layer(norm_layer, out_channels, **norm_kwargs)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass Attention2d(torch.nn.Module):\n    attention_bias_cache: Dict[str, torch.Tensor]\n\n    def __init__(\n            self,\n            dim=384,\n            key_dim=32,\n            num_heads=8,\n            attn_ratio=4,\n            resolution=7,\n            act_layer=nn.GELU,\n            stride=None,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        self.scale = key_dim ** -0.5\n        self.key_dim = key_dim\n\n        resolution = to_2tuple(resolution)\n        if stride is not None:\n            resolution = tuple([math.ceil(r / stride) for r in resolution])\n            self.stride_conv = ConvNorm(dim, dim, kernel_size=3, stride=stride, groups=dim)\n            self.upsample = nn.Upsample(scale_factor=stride, mode='bilinear')\n        else:\n            self.stride_conv = None\n            self.upsample = None\n\n        self.resolution = resolution\n        self.N = self.resolution[0] * self.resolution[1]\n        self.d = int(attn_ratio * key_dim)\n        self.dh = int(attn_ratio * key_dim) * num_heads\n        self.attn_ratio = attn_ratio\n        kh = self.key_dim * self.num_heads\n\n        self.q = ConvNorm(dim, kh)\n        self.k = ConvNorm(dim, kh)\n        self.v = ConvNorm(dim, self.dh)\n        self.v_local = ConvNorm(self.dh, self.dh, kernel_size=3, groups=self.dh)\n        self.talking_head1 = nn.Conv2d(self.num_heads, self.num_heads, kernel_size=1)\n        self.talking_head2 = nn.Conv2d(self.num_heads, self.num_heads, kernel_size=1)\n\n        self.act = act_layer()\n        self.proj = ConvNorm(self.dh, dim, 1)\n\n        pos = torch.stack(torch.meshgrid(torch.arange(self.resolution[0]), torch.arange(self.resolution[1]))).flatten(1)\n        rel_pos = (pos[..., :, None] - pos[..., None, :]).abs()\n        rel_pos = (rel_pos[0] * self.resolution[1]) + rel_pos[1]\n        self.attention_biases = torch.nn.Parameter(torch.zeros(num_heads, self.N))\n        self.register_buffer('attention_bias_idxs', torch.LongTensor(rel_pos), persistent=False)\n        self.attention_bias_cache = {}  # per-device attention_biases cache (data-parallel compat)\n\n    @torch.no_grad()\n    def train(self, mode=True):\n        super().train(mode)\n        if mode and self.attention_bias_cache:\n            self.attention_bias_cache = {}  # clear ab cache\n\n    def get_attention_biases(self, device: torch.device) -> torch.Tensor:\n        if torch.jit.is_tracing() or self.training:\n            return self.attention_biases[:, self.attention_bias_idxs]\n        else:\n            device_key = str(device)\n            if device_key not in self.attention_bias_cache:\n                self.attention_bias_cache[device_key] = self.attention_biases[:, self.attention_bias_idxs]\n            return self.attention_bias_cache[device_key]\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        if self.stride_conv is not None:\n            x = self.stride_conv(x)\n\n        q = self.q(x).reshape(B, self.num_heads, -1, self.N).permute(0, 1, 3, 2)\n        k = self.k(x).reshape(B, self.num_heads, -1, self.N).permute(0, 1, 2, 3)\n        v = self.v(x)\n        v_local = self.v_local(v)\n        v = v.reshape(B, self.num_heads, -1, self.N).permute(0, 1, 3, 2)\n\n        attn = (q @ k) * self.scale\n        attn = attn + self.get_attention_biases(x.device)\n        attn = self.talking_head1(attn)\n        attn = attn.softmax(dim=-1)\n        attn = self.talking_head2(attn)\n\n        x = (attn @ v).transpose(2, 3)\n        x = x.reshape(B, self.dh, self.resolution[0], self.resolution[1]) + v_local\n        if self.upsample is not None:\n            x = self.upsample(x)\n\n        x = self.act(x)\n        x = self.proj(x)\n        return x\n\n\nclass LocalGlobalQuery(torch.nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.pool = nn.AvgPool2d(1, 2, 0)\n        self.local = nn.Conv2d(in_dim, in_dim, kernel_size=3, stride=2, padding=1, groups=in_dim)\n        self.proj = ConvNorm(in_dim, out_dim, 1)\n\n    def forward(self, x):\n        local_q = self.local(x)\n        pool_q = self.pool(x)\n        q = local_q + pool_q\n        q = self.proj(q)\n        return q\n\n\nclass Attention2dDownsample(torch.nn.Module):\n    attention_bias_cache: Dict[str, torch.Tensor]\n\n    def __init__(\n            self,\n            dim=384,\n            key_dim=16,\n            num_heads=8,\n            attn_ratio=4,\n            resolution=7,\n            out_dim=None,\n            act_layer=nn.GELU,\n    ):\n        super().__init__()\n\n        self.num_heads = num_heads\n        self.scale = key_dim ** -0.5\n        self.key_dim = key_dim\n        self.resolution = to_2tuple(resolution)\n        self.resolution2 = tuple([math.ceil(r / 2) for r in self.resolution])\n        self.N = self.resolution[0] * self.resolution[1]\n        self.N2 = self.resolution2[0] * self.resolution2[1]\n\n        self.d = int(attn_ratio * key_dim)\n        self.dh = int(attn_ratio * key_dim) * num_heads\n        self.attn_ratio = attn_ratio\n        self.out_dim = out_dim or dim\n        kh = self.key_dim * self.num_heads\n\n        self.q = LocalGlobalQuery(dim, kh)\n        self.k = ConvNorm(dim, kh, 1)\n        self.v = ConvNorm(dim, self.dh, 1)\n        self.v_local = ConvNorm(self.dh, self.dh, kernel_size=3, stride=2, groups=self.dh)\n\n        self.act = act_layer()\n        self.proj = ConvNorm(self.dh, self.out_dim, 1)\n\n        self.attention_biases = nn.Parameter(torch.zeros(num_heads, self.N))\n        k_pos = torch.stack(torch.meshgrid(torch.arange(\n            self.resolution[1]),\n            torch.arange(self.resolution[1]))).flatten(1)\n        q_pos = torch.stack(torch.meshgrid(\n            torch.arange(0, self.resolution[0], step=2),\n            torch.arange(0, self.resolution[1], step=2))).flatten(1)\n        rel_pos = (q_pos[..., :, None] - k_pos[..., None, :]).abs()\n        rel_pos = (rel_pos[0] * self.resolution[1]) + rel_pos[1]\n        self.register_buffer('attention_bias_idxs', rel_pos, persistent=False)\n        self.attention_bias_cache = {}  # per-device attention_biases cache (data-parallel compat)\n\n    @torch.no_grad()\n    def train(self, mode=True):\n        super().train(mode)\n        if mode and self.attention_bias_cache:\n            self.attention_bias_cache = {}  # clear ab cache\n\n    def get_attention_biases(self, device: torch.device) -> torch.Tensor:\n        if torch.jit.is_tracing() or self.training:\n            return self.attention_biases[:, self.attention_bias_idxs]\n        else:\n            device_key = str(device)\n            if device_key not in self.attention_bias_cache:\n                self.attention_bias_cache[device_key] = self.attention_biases[:, self.attention_bias_idxs]\n            return self.attention_bias_cache[device_key]\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n\n        q = self.q(x).reshape(B, self.num_heads, -1, self.N2).permute(0, 1, 3, 2)\n        k = self.k(x).reshape(B, self.num_heads, -1, self.N).permute(0, 1, 2, 3)\n        v = self.v(x)\n        v_local = self.v_local(v)\n        v = v.reshape(B, self.num_heads, -1, self.N).permute(0, 1, 3, 2)\n\n        attn = (q @ k) * self.scale\n        attn = attn + self.get_attention_biases(x.device)\n        attn = attn.softmax(dim=-1)\n\n        x = (attn @ v).transpose(2, 3)\n        x = x.reshape(B, self.dh, self.resolution2[0], self.resolution2[1]) + v_local\n        x = self.act(x)\n        x = self.proj(x)\n        return x\n\n\nclass Downsample(nn.Module):\n    def __init__(\n            self,\n            in_chs,\n            out_chs,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            resolution=7,\n            use_attn=False,\n            act_layer=nn.GELU,\n            norm_layer=nn.BatchNorm2d,\n    ):\n        super().__init__()\n\n        kernel_size = to_2tuple(kernel_size)\n        stride = to_2tuple(stride)\n        padding = to_2tuple(padding)\n        norm_layer = norm_layer or nn.Identity()\n        self.conv = ConvNorm(\n            in_chs,\n            out_chs,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            norm_layer=norm_layer,\n        )\n\n        if use_attn:\n            self.attn = Attention2dDownsample(\n                dim=in_chs,\n                out_dim=out_chs,\n                resolution=resolution,\n                act_layer=act_layer,\n            )\n        else:\n            self.attn = None\n\n    def forward(self, x):\n        out = self.conv(x)\n        if self.attn is not None:\n            return self.attn(x) + out\n        return out\n\n\nclass ConvMlpWithNorm(nn.Module):\n    \"\"\"\n    Implementation of MLP with 1*1 convolutions.\n    Input: tensor with shape [B, C, H, W]\n    \"\"\"\n\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            norm_layer=nn.BatchNorm2d,\n            drop=0.,\n            mid_conv=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = ConvNormAct(\n            in_features, hidden_features, 1,\n            bias=True, norm_layer=norm_layer, act_layer=act_layer)\n        if mid_conv:\n            self.mid = ConvNormAct(\n                hidden_features, hidden_features, 3,\n                groups=hidden_features, bias=True, norm_layer=norm_layer, act_layer=act_layer)\n        else:\n            self.mid = nn.Identity()\n        self.drop1 = nn.Dropout(drop)\n        self.fc2 = ConvNorm(hidden_features, out_features, 1, norm_layer=norm_layer)\n        self.drop2 = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.mid(x)\n        x = self.drop1(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x\n\n\nclass LayerScale2d(nn.Module):\n    def __init__(self, dim, init_values=1e-5, inplace=False):\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        gamma = self.gamma.view(1, -1, 1, 1)\n        return x.mul_(gamma) if self.inplace else x * gamma\n\n\nclass EfficientFormerV2Block(nn.Module):\n    def __init__(\n            self,\n            dim,\n            mlp_ratio=4.,\n            act_layer=nn.GELU,\n            norm_layer=nn.BatchNorm2d,\n            proj_drop=0.,\n            drop_path=0.,\n            layer_scale_init_value=1e-5,\n            resolution=7,\n            stride=None,\n            use_attn=True,\n    ):\n        super().__init__()\n\n        if use_attn:\n            self.token_mixer = Attention2d(\n                dim,\n                resolution=resolution,\n                act_layer=act_layer,\n                stride=stride,\n            )\n            self.ls1 = LayerScale2d(\n                dim, layer_scale_init_value) if layer_scale_init_value is not None else nn.Identity()\n            self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        else:\n            self.token_mixer = None\n            self.ls1 = None\n            self.drop_path1 = None\n\n        self.mlp = ConvMlpWithNorm(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n            drop=proj_drop,\n            mid_conv=True,\n        )\n        self.ls2 = LayerScale2d(\n            dim, layer_scale_init_value) if layer_scale_init_value is not None else nn.Identity()\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        if self.token_mixer is not None:\n            x = x + self.drop_path1(self.ls1(self.token_mixer(x)))\n        x = x + self.drop_path2(self.ls2(self.mlp(x)))\n        return x\n\n\nclass Stem4(nn.Sequential):\n    def __init__(self, in_chs, out_chs, act_layer=nn.GELU, norm_layer=nn.BatchNorm2d):\n        super().__init__()\n        self.stride = 4\n        self.conv1 = ConvNormAct(\n            in_chs, out_chs // 2, kernel_size=3, stride=2, padding=1, bias=True,\n            norm_layer=norm_layer, act_layer=act_layer\n        )\n        self.conv2 = ConvNormAct(\n            out_chs // 2, out_chs, kernel_size=3, stride=2, padding=1, bias=True,\n            norm_layer=norm_layer, act_layer=act_layer\n        )\n\n\nclass EfficientFormerV2Stage(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            dim_out,\n            depth,\n            resolution=7,\n            downsample=True,\n            block_stride=None,\n            downsample_use_attn=False,\n            block_use_attn=False,\n            num_vit=1,\n            mlp_ratio=4.,\n            proj_drop=.0,\n            drop_path=0.,\n            layer_scale_init_value=1e-5,\n            act_layer=nn.GELU,\n            norm_layer=nn.BatchNorm2d,\n\n    ):\n        super().__init__()\n        self.grad_checkpointing = False\n        mlp_ratio = to_ntuple(depth)(mlp_ratio)\n        resolution = to_2tuple(resolution)\n\n        if downsample:\n            self.downsample = Downsample(\n                dim,\n                dim_out,\n                use_attn=downsample_use_attn,\n                resolution=resolution,\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n            )\n            dim = dim_out\n            resolution = tuple([math.ceil(r / 2) for r in resolution])\n        else:\n            assert dim == dim_out\n            self.downsample = nn.Identity()\n\n        blocks = []\n        for block_idx in range(depth):\n            remain_idx = depth - num_vit - 1\n            b = EfficientFormerV2Block(\n                dim,\n                resolution=resolution,\n                stride=block_stride,\n                mlp_ratio=mlp_ratio[block_idx],\n                use_attn=block_use_attn and block_idx > remain_idx,\n                proj_drop=proj_drop,\n                drop_path=drop_path[block_idx],\n                layer_scale_init_value=layer_scale_init_value,\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n            )\n            blocks += [b]\n        self.blocks = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        x = self.downsample(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        return x\n\n\nclass EfficientFormerV2(nn.Module):\n    def __init__(\n            self,\n            depths,\n            in_chans=3,\n            img_size=224,\n            global_pool='avg',\n            embed_dims=None,\n            downsamples=None,\n            mlp_ratios=4,\n            norm_layer='batchnorm2d',\n            norm_eps=1e-5,\n            act_layer='gelu',\n            num_classes=1000,\n            drop_rate=0.,\n            proj_drop_rate=0.,\n            drop_path_rate=0.,\n            layer_scale_init_value=1e-5,\n            num_vit=0,\n            distillation=True,\n    ):\n        super().__init__()\n        assert global_pool in ('avg', '')\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.feature_info = []\n        img_size = to_2tuple(img_size)\n        norm_layer = partial(get_norm_layer(norm_layer), eps=norm_eps)\n        act_layer = get_act_layer(act_layer)\n\n        self.stem = Stem4(in_chans, embed_dims[0], act_layer=act_layer, norm_layer=norm_layer)\n        prev_dim = embed_dims[0]\n        stride = 4\n\n        num_stages = len(depths)\n        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n        downsamples = downsamples or (False,) + (True,) * (len(depths) - 1)\n        mlp_ratios = to_ntuple(num_stages)(mlp_ratios)\n        stages = []\n        for i in range(num_stages):\n            curr_resolution = tuple([math.ceil(s / stride) for s in img_size])\n            stage = EfficientFormerV2Stage(\n                prev_dim,\n                embed_dims[i],\n                depth=depths[i],\n                resolution=curr_resolution,\n                downsample=downsamples[i],\n                block_stride=2 if i == 2 else None,\n                downsample_use_attn=i >= 3,\n                block_use_attn=i >= 2,\n                num_vit=num_vit,\n                mlp_ratio=mlp_ratios[i],\n                proj_drop=proj_drop_rate,\n                drop_path=dpr[i],\n                layer_scale_init_value=layer_scale_init_value,\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n            )\n            if downsamples[i]:\n                stride *= 2\n            prev_dim = embed_dims[i]\n            self.feature_info += [dict(num_chs=prev_dim, reduction=stride, module=f'stages.{i}')]\n            stages.append(stage)\n        self.stages = nn.Sequential(*stages)\n\n        # Classifier head\n        self.num_features = embed_dims[-1]\n        self.norm = norm_layer(embed_dims[-1])\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()\n        self.dist = distillation\n        if self.dist:\n            self.head_dist = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()\n        else:\n            self.head_dist = None\n\n        self.apply(self.init_weights)\n        self.distilled_training = False\n\n    # init for classification\n    def init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {k for k, _ in self.named_parameters() if 'attention_biases' in k}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^stem',  # stem and embed\n            blocks=[(r'^stages\\.(\\d+)', None), (r'^norm', (99999,))]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        for s in self.stages:\n            s.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head, self.head_dist\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n        self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    @torch.jit.ignore\n    def set_distilled_training(self, enable=True):\n        self.distilled_training = enable\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        x = self.stages(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool == 'avg':\n            x = x.mean(dim=(2, 3))\n        x = self.head_drop(x)\n        if pre_logits:\n            return x\n        x, x_dist = self.head(x), self.head_dist(x)\n        if self.distilled_training and self.training and not torch.jit.is_scripting():\n            # only return separate classification predictions when training in distilled mode\n            return x, x_dist\n        else:\n            # during standard train/finetune, inference average the classifier predictions\n            return (x + x_dist) / 2\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None, 'fixed_input_size': True,\n        'crop_pct': .95, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'classifier': ('head', 'head_dist'), 'first_conv': 'stem.conv1.conv',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'efficientformerv2_s0.snap_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n    'efficientformerv2_s1.snap_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n    'efficientformerv2_s2.snap_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n    'efficientformerv2_l.snap_dist_in1k': _cfg(\n        hf_hub_id='timm/',\n    ),\n})\n\n\ndef _create_efficientformerv2(variant, pretrained=False, **kwargs):\n    out_indices = kwargs.pop('out_indices', (0, 1, 2, 3))\n    model = build_model_with_cfg(\n        EfficientFormerV2, variant, pretrained,\n        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),\n        **kwargs)\n    return model\n\n\n@register_model\ndef efficientformerv2_s0(pretrained=False, **kwargs) -> EfficientFormerV2:\n    model_args = dict(\n        depths=EfficientFormer_depth['S0'],\n        embed_dims=EfficientFormer_width['S0'],\n        num_vit=2,\n        drop_path_rate=0.0,\n        mlp_ratios=EfficientFormer_expansion_ratios['S0'],\n    )\n    return _create_efficientformerv2('efficientformerv2_s0', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef efficientformerv2_s1(pretrained=False, **kwargs) -> EfficientFormerV2:\n    model_args = dict(\n        depths=EfficientFormer_depth['S1'],\n        embed_dims=EfficientFormer_width['S1'],\n        num_vit=2,\n        drop_path_rate=0.0,\n        mlp_ratios=EfficientFormer_expansion_ratios['S1'],\n    )\n    return _create_efficientformerv2('efficientformerv2_s1', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef efficientformerv2_s2(pretrained=False, **kwargs) -> EfficientFormerV2:\n    model_args = dict(\n        depths=EfficientFormer_depth['S2'],\n        embed_dims=EfficientFormer_width['S2'],\n        num_vit=4,\n        drop_path_rate=0.02,\n        mlp_ratios=EfficientFormer_expansion_ratios['S2'],\n    )\n    return _create_efficientformerv2('efficientformerv2_s2', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef efficientformerv2_l(pretrained=False, **kwargs) -> EfficientFormerV2:\n    model_args = dict(\n        depths=EfficientFormer_depth['L'],\n        embed_dims=EfficientFormer_width['L'],\n        num_vit=6,\n        drop_path_rate=0.1,\n        mlp_ratios=EfficientFormer_expansion_ratios['L'],\n    )\n    return _create_efficientformerv2('efficientformerv2_l', pretrained=pretrained, **dict(model_args, **kwargs))\n\n",
  "\"\"\" Normalization Free Nets. NFNet, NF-RegNet, NF-ResNet (pre-activation) Models\n\nPaper: `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n    - https://arxiv.org/abs/2101.08692\n\nPaper: `High-Performance Large-Scale Image Recognition Without Normalization`\n    - https://arxiv.org/abs/2102.06171\n\nOfficial Deepmind JAX code: https://github.com/deepmind/deepmind-research/tree/master/nfnets\n\nStatus:\n* These models are a work in progress, experiments ongoing.\n* Pretrained weights for two models so far, more to come.\n* Model details updated to closer match official JAX code now that it's released\n* NF-ResNet, NF-RegNet-B, and NFNet-F models supported\n\nHacked together by / copyright Ross Wightman, 2021.\n\"\"\"\nfrom collections import OrderedDict\nfrom dataclasses import dataclass, replace\nfrom functools import partial\nfrom typing import Callable, Tuple, Optional\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import ClassifierHead, DropPath, AvgPool2dSame, ScaledStdConv2d, ScaledStdConv2dSame, \\\n    get_act_layer, get_act_fn, get_attn, make_divisible\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_module\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import generate_default_cfgs, register_model\n\n__all__ = ['NormFreeNet', 'NfCfg']  # model_registry will add each entrypoint fn to this\n\n\n@dataclass\nclass NfCfg:\n    depths: Tuple[int, int, int, int]\n    channels: Tuple[int, int, int, int]\n    alpha: float = 0.2\n    stem_type: str = '3x3'\n    stem_chs: Optional[int] = None\n    group_size: Optional[int] = None\n    attn_layer: Optional[str] = None\n    attn_kwargs: dict = None\n    attn_gain: float = 2.0  # NF correction gain to apply if attn layer is used\n    width_factor: float = 1.0\n    bottle_ratio: float = 0.5\n    num_features: int = 0  # num out_channels for final conv, no final_conv if 0\n    ch_div: int = 8  # round channels % 8 == 0 to keep tensor-core use optimal\n    reg: bool = False  # enables EfficientNet-like options used in RegNet variants, expand from in_chs, se in middle\n    extra_conv: bool = False  # extra 3x3 bottleneck convolution for NFNet models\n    gamma_in_act: bool = False\n    same_padding: bool = False\n    std_conv_eps: float = 1e-5\n    skipinit: bool = False  # disabled by default, non-trivial performance impact\n    zero_init_fc: bool = False\n    act_layer: str = 'silu'\n\n\nclass GammaAct(nn.Module):\n    def __init__(self, act_type='relu', gamma: float = 1.0, inplace=False):\n        super().__init__()\n        self.act_fn = get_act_fn(act_type)\n        self.gamma = gamma\n        self.inplace = inplace\n\n    def forward(self, x):\n        return self.act_fn(x, inplace=self.inplace).mul_(self.gamma)\n\n\ndef act_with_gamma(act_type, gamma: float = 1.):\n    def _create(inplace=False):\n        return GammaAct(act_type, gamma=gamma, inplace=inplace)\n    return _create\n\n\nclass DownsampleAvg(nn.Module):\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: int,\n            stride: int = 1,\n            dilation: int = 1,\n            first_dilation: Optional[int] = None,\n            conv_layer: Callable = ScaledStdConv2d,\n    ):\n        \"\"\" AvgPool Downsampling as in 'D' ResNet variants. Support for dilation.\"\"\"\n        super(DownsampleAvg, self).__init__()\n        avg_stride = stride if dilation == 1 else 1\n        if stride > 1 or dilation > 1:\n            avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d\n            self.pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)\n        else:\n            self.pool = nn.Identity()\n        self.conv = conv_layer(in_chs, out_chs, 1, stride=1)\n\n    def forward(self, x):\n        return self.conv(self.pool(x))\n\n\n@register_notrace_module  # reason: mul_ causes FX to drop a relevant node. https://github.com/pytorch/pytorch/issues/68301\nclass NormFreeBlock(nn.Module):\n    \"\"\"Normalization-Free pre-activation block.\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: Optional[int] = None,\n            stride: int = 1,\n            dilation: int = 1,\n            first_dilation: Optional[int] = None,\n            alpha: float = 1.0,\n            beta: float = 1.0,\n            bottle_ratio: float = 0.25,\n            group_size: Optional[int] = None,\n            ch_div: int = 1,\n            reg: bool = True,\n            extra_conv: bool = False,\n            skipinit: bool = False,\n            attn_layer: Optional[Callable] = None,\n            attn_gain: bool = 2.0,\n            act_layer: Optional[Callable] = None,\n            conv_layer: Callable = ScaledStdConv2d,\n            drop_path_rate: float = 0.,\n    ):\n        super().__init__()\n        first_dilation = first_dilation or dilation\n        out_chs = out_chs or in_chs\n        # RegNet variants scale bottleneck from in_chs, otherwise scale from out_chs like ResNet\n        mid_chs = make_divisible(in_chs * bottle_ratio if reg else out_chs * bottle_ratio, ch_div)\n        groups = 1 if not group_size else mid_chs // group_size\n        if group_size and group_size % ch_div == 0:\n            mid_chs = group_size * groups  # correct mid_chs if group_size divisible by ch_div, otherwise error\n        self.alpha = alpha\n        self.beta = beta\n        self.attn_gain = attn_gain\n\n        if in_chs != out_chs or stride != 1 or dilation != first_dilation:\n            self.downsample = DownsampleAvg(\n                in_chs,\n                out_chs,\n                stride=stride,\n                dilation=dilation,\n                first_dilation=first_dilation,\n                conv_layer=conv_layer,\n            )\n        else:\n            self.downsample = None\n\n        self.act1 = act_layer()\n        self.conv1 = conv_layer(in_chs, mid_chs, 1)\n        self.act2 = act_layer(inplace=True)\n        self.conv2 = conv_layer(mid_chs, mid_chs, 3, stride=stride, dilation=first_dilation, groups=groups)\n        if extra_conv:\n            self.act2b = act_layer(inplace=True)\n            self.conv2b = conv_layer(mid_chs, mid_chs, 3, stride=1, dilation=dilation, groups=groups)\n        else:\n            self.act2b = None\n            self.conv2b = None\n        if reg and attn_layer is not None:\n            self.attn = attn_layer(mid_chs)  # RegNet blocks apply attn btw conv2 & 3\n        else:\n            self.attn = None\n        self.act3 = act_layer()\n        self.conv3 = conv_layer(mid_chs, out_chs, 1, gain_init=1. if skipinit else 0.)\n        if not reg and attn_layer is not None:\n            self.attn_last = attn_layer(out_chs)  # ResNet blocks apply attn after conv3\n        else:\n            self.attn_last = None\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()\n        self.skipinit_gain = nn.Parameter(torch.tensor(0.)) if skipinit else None\n\n    def forward(self, x):\n        out = self.act1(x) * self.beta\n\n        # shortcut branch\n        shortcut = x\n        if self.downsample is not None:\n            shortcut = self.downsample(out)\n\n        # residual branch\n        out = self.conv1(out)\n        out = self.conv2(self.act2(out))\n        if self.conv2b is not None:\n            out = self.conv2b(self.act2b(out))\n        if self.attn is not None:\n            out = self.attn_gain * self.attn(out)\n        out = self.conv3(self.act3(out))\n        if self.attn_last is not None:\n            out = self.attn_gain * self.attn_last(out)\n        out = self.drop_path(out)\n\n        if self.skipinit_gain is not None:\n            out.mul_(self.skipinit_gain)\n        out = out * self.alpha + shortcut\n        return out\n\n\ndef create_stem(\n        in_chs: int,\n        out_chs: int,\n        stem_type: str = '',\n        conv_layer: Optional[Callable] = None,\n        act_layer: Optional[Callable] = None,\n        preact_feature: bool = True,\n):\n    stem_stride = 2\n    stem_feature = dict(num_chs=out_chs, reduction=2, module='stem.conv')\n    stem = OrderedDict()\n    assert stem_type in ('', 'deep', 'deep_tiered', 'deep_quad', '3x3', '7x7', 'deep_pool', '3x3_pool', '7x7_pool')\n    if 'deep' in stem_type:\n        if 'quad' in stem_type:\n            # 4 deep conv stack as in NFNet-F models\n            assert not 'pool' in stem_type\n            stem_chs = (out_chs // 8, out_chs // 4, out_chs // 2, out_chs)\n            strides = (2, 1, 1, 2)\n            stem_stride = 4\n            stem_feature = dict(num_chs=out_chs // 2, reduction=2, module='stem.conv3')\n        else:\n            if 'tiered' in stem_type:\n                stem_chs = (3 * out_chs // 8, out_chs // 2, out_chs)  # 'T' resnets in resnet.py\n            else:\n                stem_chs = (out_chs // 2, out_chs // 2, out_chs)  # 'D' ResNets\n            strides = (2, 1, 1)\n            stem_feature = dict(num_chs=out_chs // 2, reduction=2, module='stem.conv2')\n        last_idx = len(stem_chs) - 1\n        for i, (c, s) in enumerate(zip(stem_chs, strides)):\n            stem[f'conv{i + 1}'] = conv_layer(in_chs, c, kernel_size=3, stride=s)\n            if i != last_idx:\n                stem[f'act{i + 2}'] = act_layer(inplace=True)\n            in_chs = c\n    elif '3x3' in stem_type:\n        # 3x3 stem conv as in RegNet\n        stem['conv'] = conv_layer(in_chs, out_chs, kernel_size=3, stride=2)\n    else:\n        # 7x7 stem conv as in ResNet\n        stem['conv'] = conv_layer(in_chs, out_chs, kernel_size=7, stride=2)\n\n    if 'pool' in stem_type:\n        stem['pool'] = nn.MaxPool2d(3, stride=2, padding=1)\n        stem_stride = 4\n\n    return nn.Sequential(stem), stem_stride, stem_feature\n\n\n# from https://github.com/deepmind/deepmind-research/tree/master/nfnets\n_nonlin_gamma = dict(\n    identity=1.0,\n    celu=1.270926833152771,\n    elu=1.2716004848480225,\n    gelu=1.7015043497085571,\n    leaky_relu=1.70590341091156,\n    log_sigmoid=1.9193484783172607,\n    log_softmax=1.0002083778381348,\n    relu=1.7139588594436646,\n    relu6=1.7131484746932983,\n    selu=1.0008515119552612,\n    sigmoid=4.803835391998291,\n    silu=1.7881293296813965,\n    softsign=2.338853120803833,\n    softplus=1.9203323125839233,\n    tanh=1.5939117670059204,\n)\n\n\nclass NormFreeNet(nn.Module):\n    \"\"\" Normalization-Free Network\n\n    As described in :\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    and\n    `High-Performance Large-Scale Image Recognition Without Normalization` - https://arxiv.org/abs/2102.06171\n\n    This model aims to cover both the NFRegNet-Bx models as detailed in the paper's code snippets and\n    the (preact) ResNet models described earlier in the paper.\n\n    There are a few differences:\n        * channels are rounded to be divisible by 8 by default (keep tensor core kernels happy),\n            this changes channel dim and param counts slightly from the paper models\n        * activation correcting gamma constants are moved into the ScaledStdConv as it has less performance\n            impact in PyTorch when done with the weight scaling there. This likely wasn't a concern in the JAX impl.\n        * a config option `gamma_in_act` can be enabled to not apply gamma in StdConv as described above, but\n            apply it in each activation. This is slightly slower, numerically different, but matches official impl.\n        * skipinit is disabled by default, it seems to have a rather drastic impact on GPU memory use and throughput\n            for what it is/does. Approx 8-10% throughput loss.\n    \"\"\"\n    def __init__(\n            self,\n            cfg: NfCfg,\n            num_classes: int = 1000,\n            in_chans: int = 3,\n            global_pool: str = 'avg',\n            output_stride: int = 32,\n            drop_rate: float = 0.,\n            drop_path_rate: float = 0.,\n            **kwargs,\n    ):\n        \"\"\"\n        Args:\n            cfg: Model architecture configuration.\n            num_classes: Number of classifier classes.\n            in_chans: Number of input channels.\n            global_pool: Global pooling type.\n            output_stride: Output stride of network, one of (8, 16, 32).\n            drop_rate: Dropout rate.\n            drop_path_rate: Stochastic depth drop-path rate.\n            **kwargs: Extra kwargs overlayed onto cfg.\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        self.grad_checkpointing = False\n\n        cfg = replace(cfg, **kwargs)\n        assert cfg.act_layer in _nonlin_gamma, f\"Please add non-linearity constants for activation ({cfg.act_layer}).\"\n        conv_layer = ScaledStdConv2dSame if cfg.same_padding else ScaledStdConv2d\n        if cfg.gamma_in_act:\n            act_layer = act_with_gamma(cfg.act_layer, gamma=_nonlin_gamma[cfg.act_layer])\n            conv_layer = partial(conv_layer, eps=cfg.std_conv_eps)\n        else:\n            act_layer = get_act_layer(cfg.act_layer)\n            conv_layer = partial(conv_layer, gamma=_nonlin_gamma[cfg.act_layer], eps=cfg.std_conv_eps)\n        attn_layer = partial(get_attn(cfg.attn_layer), **cfg.attn_kwargs) if cfg.attn_layer else None\n\n        stem_chs = make_divisible((cfg.stem_chs or cfg.channels[0]) * cfg.width_factor, cfg.ch_div)\n        self.stem, stem_stride, stem_feat = create_stem(\n            in_chans,\n            stem_chs,\n            cfg.stem_type,\n            conv_layer=conv_layer,\n            act_layer=act_layer,\n        )\n\n        self.feature_info = [stem_feat]\n        drop_path_rates = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(cfg.depths)).split(cfg.depths)]\n        prev_chs = stem_chs\n        net_stride = stem_stride\n        dilation = 1\n        expected_var = 1.0\n        stages = []\n        for stage_idx, stage_depth in enumerate(cfg.depths):\n            stride = 1 if stage_idx == 0 and stem_stride > 2 else 2\n            if net_stride >= output_stride and stride > 1:\n                dilation *= stride\n                stride = 1\n            net_stride *= stride\n            first_dilation = 1 if dilation in (1, 2) else 2\n\n            blocks = []\n            for block_idx in range(cfg.depths[stage_idx]):\n                first_block = block_idx == 0 and stage_idx == 0\n                out_chs = make_divisible(cfg.channels[stage_idx] * cfg.width_factor, cfg.ch_div)\n                blocks += [NormFreeBlock(\n                    in_chs=prev_chs, out_chs=out_chs,\n                    alpha=cfg.alpha,\n                    beta=1. / expected_var ** 0.5,\n                    stride=stride if block_idx == 0 else 1,\n                    dilation=dilation,\n                    first_dilation=first_dilation,\n                    group_size=cfg.group_size,\n                    bottle_ratio=1. if cfg.reg and first_block else cfg.bottle_ratio,\n                    ch_div=cfg.ch_div,\n                    reg=cfg.reg,\n                    extra_conv=cfg.extra_conv,\n                    skipinit=cfg.skipinit,\n                    attn_layer=attn_layer,\n                    attn_gain=cfg.attn_gain,\n                    act_layer=act_layer,\n                    conv_layer=conv_layer,\n                    drop_path_rate=drop_path_rates[stage_idx][block_idx],\n                )]\n                if block_idx == 0:\n                    expected_var = 1.  # expected var is reset after first block of each stage\n                expected_var += cfg.alpha ** 2   # Even if reset occurs, increment expected variance\n                first_dilation = dilation\n                prev_chs = out_chs\n            self.feature_info += [dict(num_chs=prev_chs, reduction=net_stride, module=f'stages.{stage_idx}')]\n            stages += [nn.Sequential(*blocks)]\n        self.stages = nn.Sequential(*stages)\n\n        if cfg.num_features:\n            # The paper NFRegNet models have an EfficientNet-like final head convolution.\n            self.num_features = make_divisible(cfg.width_factor * cfg.num_features, cfg.ch_div)\n            self.final_conv = conv_layer(prev_chs, self.num_features, 1)\n            self.feature_info[-1] = dict(num_chs=self.num_features, reduction=net_stride, module=f'final_conv')\n        else:\n            self.num_features = prev_chs\n            self.final_conv = nn.Identity()\n        self.final_act = act_layer(inplace=cfg.num_features > 0)\n\n        self.head = ClassifierHead(\n            self.num_features,\n            num_classes,\n            pool_type=global_pool,\n            drop_rate=self.drop_rate,\n        )\n\n        for n, m in self.named_modules():\n            if 'fc' in n and isinstance(m, nn.Linear):\n                if cfg.zero_init_fc:\n                    nn.init.zeros_(m.weight)\n                else:\n                    nn.init.normal_(m.weight, 0., .01)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='linear')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^stem',\n            blocks=[\n                (r'^stages\\.(\\d+)' if coarse else r'^stages\\.(\\d+)\\.(\\d+)', None),\n                (r'^final_conv', (99999,))\n            ]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.head.reset(num_classes, global_pool)\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.stages, x)\n        else:\n            x = self.stages(x)\n        x = self.final_conv(x)\n        x = self.final_act(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _nfres_cfg(\n        depths,\n        channels=(256, 512, 1024, 2048),\n        group_size=None,\n        act_layer='relu',\n        attn_layer=None,\n        attn_kwargs=None,\n):\n    attn_kwargs = attn_kwargs or {}\n    cfg = NfCfg(\n        depths=depths,\n        channels=channels,\n        stem_type='7x7_pool',\n        stem_chs=64,\n        bottle_ratio=0.25,\n        group_size=group_size,\n        act_layer=act_layer,\n        attn_layer=attn_layer,\n        attn_kwargs=attn_kwargs,\n    )\n    return cfg\n\n\ndef _nfreg_cfg(depths, channels=(48, 104, 208, 440)):\n    num_features = 1280 * channels[-1] // 440\n    attn_kwargs = dict(rd_ratio=0.5)\n    cfg = NfCfg(\n        depths=depths,\n        channels=channels,\n        stem_type='3x3',\n        group_size=8,\n        width_factor=0.75,\n        bottle_ratio=2.25,\n        num_features=num_features,\n        reg=True,\n        attn_layer='se',\n        attn_kwargs=attn_kwargs,\n    )\n    return cfg\n\n\ndef _nfnet_cfg(\n        depths,\n        channels=(256, 512, 1536, 1536),\n        group_size=128,\n        bottle_ratio=0.5,\n        feat_mult=2.,\n        act_layer='gelu',\n        attn_layer='se',\n        attn_kwargs=None,\n):\n    num_features = int(channels[-1] * feat_mult)\n    attn_kwargs = attn_kwargs if attn_kwargs is not None else dict(rd_ratio=0.5)\n    cfg = NfCfg(\n        depths=depths,\n        channels=channels,\n        stem_type='deep_quad',\n        stem_chs=128,\n        group_size=group_size,\n        bottle_ratio=bottle_ratio,\n        extra_conv=True,\n        num_features=num_features,\n        act_layer=act_layer,\n        attn_layer=attn_layer,\n        attn_kwargs=attn_kwargs,\n    )\n    return cfg\n\n\ndef _dm_nfnet_cfg(\n        depths,\n        channels=(256, 512, 1536, 1536),\n        act_layer='gelu',\n        skipinit=True,\n):\n    cfg = NfCfg(\n        depths=depths,\n        channels=channels,\n        stem_type='deep_quad',\n        stem_chs=128,\n        group_size=128,\n        bottle_ratio=0.5,\n        extra_conv=True,\n        gamma_in_act=True,\n        same_padding=True,\n        skipinit=skipinit,\n        num_features=int(channels[-1] * 2.0),\n        act_layer=act_layer,\n        attn_layer='se',\n        attn_kwargs=dict(rd_ratio=0.5),\n    )\n    return cfg\n\n\nmodel_cfgs = dict(\n    # NFNet-F models w/ GELU compatible with DeepMind weights\n    dm_nfnet_f0=_dm_nfnet_cfg(depths=(1, 2, 6, 3)),\n    dm_nfnet_f1=_dm_nfnet_cfg(depths=(2, 4, 12, 6)),\n    dm_nfnet_f2=_dm_nfnet_cfg(depths=(3, 6, 18, 9)),\n    dm_nfnet_f3=_dm_nfnet_cfg(depths=(4, 8, 24, 12)),\n    dm_nfnet_f4=_dm_nfnet_cfg(depths=(5, 10, 30, 15)),\n    dm_nfnet_f5=_dm_nfnet_cfg(depths=(6, 12, 36, 18)),\n    dm_nfnet_f6=_dm_nfnet_cfg(depths=(7, 14, 42, 21)),\n\n    # NFNet-F models w/ GELU\n    nfnet_f0=_nfnet_cfg(depths=(1, 2, 6, 3)),\n    nfnet_f1=_nfnet_cfg(depths=(2, 4, 12, 6)),\n    nfnet_f2=_nfnet_cfg(depths=(3, 6, 18, 9)),\n    nfnet_f3=_nfnet_cfg(depths=(4, 8, 24, 12)),\n    nfnet_f4=_nfnet_cfg(depths=(5, 10, 30, 15)),\n    nfnet_f5=_nfnet_cfg(depths=(6, 12, 36, 18)),\n    nfnet_f6=_nfnet_cfg(depths=(7, 14, 42, 21)),\n    nfnet_f7=_nfnet_cfg(depths=(8, 16, 48, 24)),\n\n    # Experimental 'light' versions of NFNet-F that are little leaner, w/ SiLU act\n    nfnet_l0=_nfnet_cfg(\n        depths=(1, 2, 6, 3), feat_mult=1.5, group_size=64, bottle_ratio=0.25,\n        attn_kwargs=dict(rd_ratio=0.25, rd_divisor=8), act_layer='silu'),\n    eca_nfnet_l0=_nfnet_cfg(\n        depths=(1, 2, 6, 3), feat_mult=1.5, group_size=64, bottle_ratio=0.25,\n        attn_layer='eca', attn_kwargs=dict(), act_layer='silu'),\n    eca_nfnet_l1=_nfnet_cfg(\n        depths=(2, 4, 12, 6), feat_mult=2, group_size=64, bottle_ratio=0.25,\n        attn_layer='eca', attn_kwargs=dict(), act_layer='silu'),\n    eca_nfnet_l2=_nfnet_cfg(\n        depths=(3, 6, 18, 9), feat_mult=2, group_size=64, bottle_ratio=0.25,\n        attn_layer='eca', attn_kwargs=dict(), act_layer='silu'),\n    eca_nfnet_l3=_nfnet_cfg(\n        depths=(4, 8, 24, 12), feat_mult=2, group_size=64, bottle_ratio=0.25,\n        attn_layer='eca', attn_kwargs=dict(), act_layer='silu'),\n\n    # EffNet influenced RegNet defs.\n    # NOTE: These aren't quite the official ver, ch_div=1 must be set for exact ch counts. I round to ch_div=8.\n    nf_regnet_b0=_nfreg_cfg(depths=(1, 3, 6, 6)),\n    nf_regnet_b1=_nfreg_cfg(depths=(2, 4, 7, 7)),\n    nf_regnet_b2=_nfreg_cfg(depths=(2, 4, 8, 8), channels=(56, 112, 232, 488)),\n    nf_regnet_b3=_nfreg_cfg(depths=(2, 5, 9, 9), channels=(56, 128, 248, 528)),\n    nf_regnet_b4=_nfreg_cfg(depths=(2, 6, 11, 11), channels=(64, 144, 288, 616)),\n    nf_regnet_b5=_nfreg_cfg(depths=(3, 7, 14, 14), channels=(80, 168, 336, 704)),\n\n    # ResNet (preact, D style deep stem/avg down) defs\n    nf_resnet26=_nfres_cfg(depths=(2, 2, 2, 2)),\n    nf_resnet50=_nfres_cfg(depths=(3, 4, 6, 3)),\n    nf_resnet101=_nfres_cfg(depths=(3, 4, 23, 3)),\n\n    nf_seresnet26=_nfres_cfg(depths=(2, 2, 2, 2), attn_layer='se', attn_kwargs=dict(rd_ratio=1/16)),\n    nf_seresnet50=_nfres_cfg(depths=(3, 4, 6, 3), attn_layer='se', attn_kwargs=dict(rd_ratio=1/16)),\n    nf_seresnet101=_nfres_cfg(depths=(3, 4, 23, 3), attn_layer='se', attn_kwargs=dict(rd_ratio=1/16)),\n\n    nf_ecaresnet26=_nfres_cfg(depths=(2, 2, 2, 2), attn_layer='eca', attn_kwargs=dict()),\n    nf_ecaresnet50=_nfres_cfg(depths=(3, 4, 6, 3), attn_layer='eca', attn_kwargs=dict()),\n    nf_ecaresnet101=_nfres_cfg(depths=(3, 4, 23, 3), attn_layer='eca', attn_kwargs=dict()),\n)\n\n\ndef _create_normfreenet(variant, pretrained=False, **kwargs):\n    model_cfg = model_cfgs[variant]\n    feature_cfg = dict(flatten_sequential=True)\n    return build_model_with_cfg(\n        NormFreeNet,\n        variant,\n        pretrained,\n        model_cfg=model_cfg,\n        feature_cfg=feature_cfg,\n        **kwargs,\n    )\n\n\ndef _dcfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.9, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.conv1', 'classifier': 'head.fc',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'dm_nfnet_f0.dm_in1k': _dcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f0-604f9c3a.pth',\n        pool_size=(6, 6), input_size=(3, 192, 192), test_input_size=(3, 256, 256), crop_pct=.9, crop_mode='squash'),\n    'dm_nfnet_f1.dm_in1k': _dcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f1-fc540f82.pth',\n        pool_size=(7, 7), input_size=(3, 224, 224), test_input_size=(3, 320, 320), crop_pct=0.91, crop_mode='squash'),\n    'dm_nfnet_f2.dm_in1k': _dcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f2-89875923.pth',\n        pool_size=(8, 8), input_size=(3, 256, 256), test_input_size=(3, 352, 352), crop_pct=0.92, crop_mode='squash'),\n    'dm_nfnet_f3.dm_in1k': _dcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f3-d74ab3aa.pth',\n        pool_size=(10, 10), input_size=(3, 320, 320), test_input_size=(3, 416, 416), crop_pct=0.94, crop_mode='squash'),\n    'dm_nfnet_f4.dm_in1k': _dcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f4-0ac5b10b.pth',\n        pool_size=(12, 12), input_size=(3, 384, 384), test_input_size=(3, 512, 512), crop_pct=0.951, crop_mode='squash'),\n    'dm_nfnet_f5.dm_in1k': _dcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f5-ecb20ab1.pth',\n        pool_size=(13, 13), input_size=(3, 416, 416), test_input_size=(3, 544, 544), crop_pct=0.954, crop_mode='squash'),\n    'dm_nfnet_f6.dm_in1k': _dcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-dnf-weights/dm_nfnet_f6-e0f12116.pth',\n        pool_size=(14, 14), input_size=(3, 448, 448), test_input_size=(3, 576, 576), crop_pct=0.956, crop_mode='squash'),\n\n    'nfnet_f0': _dcfg(\n        url='', pool_size=(6, 6), input_size=(3, 192, 192), test_input_size=(3, 256, 256)),\n    'nfnet_f1': _dcfg(\n        url='', pool_size=(7, 7), input_size=(3, 224, 224), test_input_size=(3, 320, 320)),\n    'nfnet_f2': _dcfg(\n        url='', pool_size=(8, 8), input_size=(3, 256, 256), test_input_size=(3, 352, 352)),\n    'nfnet_f3': _dcfg(\n        url='', pool_size=(10, 10), input_size=(3, 320, 320), test_input_size=(3, 416, 416)),\n    'nfnet_f4': _dcfg(\n        url='', pool_size=(12, 12), input_size=(3, 384, 384), test_input_size=(3, 512, 512)),\n    'nfnet_f5': _dcfg(\n        url='', pool_size=(13, 13), input_size=(3, 416, 416), test_input_size=(3, 544, 544)),\n    'nfnet_f6': _dcfg(\n        url='', pool_size=(14, 14), input_size=(3, 448, 448), test_input_size=(3, 576, 576)),\n    'nfnet_f7': _dcfg(\n        url='', pool_size=(15, 15), input_size=(3, 480, 480), test_input_size=(3, 608, 608)),\n\n    'nfnet_l0.ra2_in1k': _dcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/nfnet_l0_ra2-45c6688d.pth',\n        pool_size=(7, 7), input_size=(3, 224, 224), test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'eca_nfnet_l0.ra2_in1k': _dcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecanfnet_l0_ra2-e3e9ac50.pth',\n        pool_size=(7, 7), input_size=(3, 224, 224), test_input_size=(3, 288, 288), test_crop_pct=1.0),\n    'eca_nfnet_l1.ra2_in1k': _dcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecanfnet_l1_ra2-7dce93cd.pth',\n        pool_size=(8, 8), input_size=(3, 256, 256), test_input_size=(3, 320, 320), test_crop_pct=1.0),\n    'eca_nfnet_l2.ra3_in1k': _dcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecanfnet_l2_ra3-da781a61.pth',\n        pool_size=(10, 10), input_size=(3, 320, 320), test_input_size=(3, 384, 384), test_crop_pct=1.0),\n    'eca_nfnet_l3': _dcfg(\n        url='',\n        pool_size=(11, 11), input_size=(3, 352, 352), test_input_size=(3, 448, 448), test_crop_pct=1.0),\n\n    'nf_regnet_b0': _dcfg(\n        url='', pool_size=(6, 6), input_size=(3, 192, 192), test_input_size=(3, 256, 256), first_conv='stem.conv'),\n    'nf_regnet_b1.ra2_in1k': _dcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/nf_regnet_b1_256_ra2-ad85cfef.pth',\n        pool_size=(8, 8), input_size=(3, 256, 256), test_input_size=(3, 288, 288), first_conv='stem.conv'),  # NOT to paper spec\n    'nf_regnet_b2': _dcfg(\n        url='', pool_size=(8, 8), input_size=(3, 240, 240), test_input_size=(3, 272, 272), first_conv='stem.conv'),\n    'nf_regnet_b3': _dcfg(\n        url='', pool_size=(9, 9), input_size=(3, 288, 288), test_input_size=(3, 320, 320), first_conv='stem.conv'),\n    'nf_regnet_b4': _dcfg(\n        url='', pool_size=(10, 10), input_size=(3, 320, 320), test_input_size=(3, 384, 384), first_conv='stem.conv'),\n    'nf_regnet_b5': _dcfg(\n        url='', pool_size=(12, 12), input_size=(3, 384, 384), test_input_size=(3, 456, 456), first_conv='stem.conv'),\n\n    'nf_resnet26': _dcfg(url='', first_conv='stem.conv'),\n    'nf_resnet50.ra2_in1k': _dcfg(\n        hf_hub_id='timm/',\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/nf_resnet50_ra2-9f236009.pth',\n        pool_size=(8, 8), input_size=(3, 256, 256), test_input_size=(3, 288, 288), crop_pct=0.94, first_conv='stem.conv'),\n    'nf_resnet101': _dcfg(url='', first_conv='stem.conv'),\n\n    'nf_seresnet26': _dcfg(url='', first_conv='stem.conv'),\n    'nf_seresnet50': _dcfg(url='', first_conv='stem.conv'),\n    'nf_seresnet101': _dcfg(url='', first_conv='stem.conv'),\n\n    'nf_ecaresnet26': _dcfg(url='', first_conv='stem.conv'),\n    'nf_ecaresnet50': _dcfg(url='', first_conv='stem.conv'),\n    'nf_ecaresnet101': _dcfg(url='', first_conv='stem.conv'),\n})\n\n\n@register_model\ndef dm_nfnet_f0(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" NFNet-F0 (DeepMind weight compatible)\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"\n    return _create_normfreenet('dm_nfnet_f0', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef dm_nfnet_f1(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" NFNet-F1 (DeepMind weight compatible)\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"\n    return _create_normfreenet('dm_nfnet_f1', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef dm_nfnet_f2(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" NFNet-F2 (DeepMind weight compatible)\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"\n    return _create_normfreenet('dm_nfnet_f2', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef dm_nfnet_f3(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" NFNet-F3 (DeepMind weight compatible)\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"\n    return _create_normfreenet('dm_nfnet_f3', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef dm_nfnet_f4(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" NFNet-F4 (DeepMind weight compatible)\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"\n    return _create_normfreenet('dm_nfnet_f4', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef dm_nfnet_f5(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" NFNet-F5 (DeepMind weight compatible)\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"\n    return _create_normfreenet('dm_nfnet_f5', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef dm_nfnet_f6(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" NFNet-F6 (DeepMind weight compatible)\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"\n    return _create_normfreenet('dm_nfnet_f6', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nfnet_f0(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" NFNet-F0\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"\n    return _create_normfreenet('nfnet_f0', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nfnet_f1(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" NFNet-F1\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"\n    return _create_normfreenet('nfnet_f1', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nfnet_f2(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" NFNet-F2\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"\n    return _create_normfreenet('nfnet_f2', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nfnet_f3(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" NFNet-F3\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"\n    return _create_normfreenet('nfnet_f3', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nfnet_f4(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" NFNet-F4\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"\n    return _create_normfreenet('nfnet_f4', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nfnet_f5(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" NFNet-F5\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"\n    return _create_normfreenet('nfnet_f5', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nfnet_f6(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" NFNet-F6\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"\n    return _create_normfreenet('nfnet_f6', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nfnet_f7(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" NFNet-F7\n    `High-Performance Large-Scale Image Recognition Without Normalization`\n        - https://arxiv.org/abs/2102.06171\n    \"\"\"\n    return _create_normfreenet('nfnet_f7', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nfnet_l0(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" NFNet-L0b w/ SiLU\n    My experimental 'light' model w/ F0 repeats, 1.5x final_conv mult, 64 group_size, .25 bottleneck & SE ratio\n    \"\"\"\n    return _create_normfreenet('nfnet_l0', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef eca_nfnet_l0(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" ECA-NFNet-L0 w/ SiLU\n    My experimental 'light' model w/ F0 repeats, 1.5x final_conv mult, 64 group_size, .25 bottleneck & ECA attn\n    \"\"\"\n    return _create_normfreenet('eca_nfnet_l0', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef eca_nfnet_l1(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" ECA-NFNet-L1 w/ SiLU\n    My experimental 'light' model w/ F1 repeats, 2.0x final_conv mult, 64 group_size, .25 bottleneck & ECA attn\n    \"\"\"\n    return _create_normfreenet('eca_nfnet_l1', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef eca_nfnet_l2(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" ECA-NFNet-L2 w/ SiLU\n    My experimental 'light' model w/ F2 repeats, 2.0x final_conv mult, 64 group_size, .25 bottleneck & ECA attn\n    \"\"\"\n    return _create_normfreenet('eca_nfnet_l2', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef eca_nfnet_l3(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" ECA-NFNet-L3 w/ SiLU\n    My experimental 'light' model w/ F3 repeats, 2.0x final_conv mult, 64 group_size, .25 bottleneck & ECA attn\n    \"\"\"\n    return _create_normfreenet('eca_nfnet_l3', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nf_regnet_b0(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" Normalization-Free RegNet-B0\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"\n    return _create_normfreenet('nf_regnet_b0', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nf_regnet_b1(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" Normalization-Free RegNet-B1\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"\n    return _create_normfreenet('nf_regnet_b1', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nf_regnet_b2(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" Normalization-Free RegNet-B2\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"\n    return _create_normfreenet('nf_regnet_b2', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nf_regnet_b3(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" Normalization-Free RegNet-B3\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"\n    return _create_normfreenet('nf_regnet_b3', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nf_regnet_b4(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" Normalization-Free RegNet-B4\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"\n    return _create_normfreenet('nf_regnet_b4', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nf_regnet_b5(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" Normalization-Free RegNet-B5\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"\n    return _create_normfreenet('nf_regnet_b5', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nf_resnet26(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" Normalization-Free ResNet-26\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"\n    return _create_normfreenet('nf_resnet26', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nf_resnet50(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" Normalization-Free ResNet-50\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"\n    return _create_normfreenet('nf_resnet50', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nf_resnet101(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" Normalization-Free ResNet-101\n    `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n        - https://arxiv.org/abs/2101.08692\n    \"\"\"\n    return _create_normfreenet('nf_resnet101', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nf_seresnet26(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" Normalization-Free SE-ResNet26\n    \"\"\"\n    return _create_normfreenet('nf_seresnet26', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nf_seresnet50(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" Normalization-Free SE-ResNet50\n    \"\"\"\n    return _create_normfreenet('nf_seresnet50', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nf_seresnet101(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" Normalization-Free SE-ResNet101\n    \"\"\"\n    return _create_normfreenet('nf_seresnet101', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nf_ecaresnet26(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" Normalization-Free ECA-ResNet26\n    \"\"\"\n    return _create_normfreenet('nf_ecaresnet26', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nf_ecaresnet50(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" Normalization-Free ECA-ResNet50\n    \"\"\"\n    return _create_normfreenet('nf_ecaresnet50', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef nf_ecaresnet101(pretrained=False, **kwargs) -> NormFreeNet:\n    \"\"\" Normalization-Free ECA-ResNet101\n    \"\"\"\n    return _create_normfreenet('nf_ecaresnet101', pretrained=pretrained, **kwargs)\n",
  "\"\"\" Relative Position Vision Transformer (ViT) in PyTorch\n\nNOTE: these models are experimental / WIP, expect changes\n\nHacked together by / Copyright 2022, Ross Wightman\n\"\"\"\nimport logging\nimport math\nfrom functools import partial\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom torch.jit import Final\nfrom torch.utils.checkpoint import checkpoint\n\nfrom timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.layers import PatchEmbed, Mlp, DropPath, RelPosMlp, RelPosBias, use_fused_attn\nfrom ._builder import build_model_with_cfg\nfrom ._registry import generate_default_cfgs, register_model\n\n__all__ = ['VisionTransformerRelPos']  # model_registry will add each entrypoint fn to this\n\n_logger = logging.getLogger(__name__)\n\n\nclass RelPosAttention(nn.Module):\n    fused_attn: Final[bool]\n\n    def __init__(\n            self,\n            dim,\n            num_heads=8,\n            qkv_bias=False,\n            qk_norm=False,\n            rel_pos_cls=None,\n            attn_drop=0.,\n            proj_drop=0.,\n            norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.fused_attn = use_fused_attn()\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.rel_pos = rel_pos_cls(num_heads=num_heads) if rel_pos_cls else None\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, shared_rel_pos: Optional[torch.Tensor] = None):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)\n        q = self.q_norm(q)\n        k = self.k_norm(k)\n\n        if self.fused_attn:\n            if self.rel_pos is not None:\n                attn_bias = self.rel_pos.get_bias()\n            elif shared_rel_pos is not None:\n                attn_bias = shared_rel_pos\n            else:\n                attn_bias = None\n\n            x = torch.nn.functional.scaled_dot_product_attention(\n                q, k, v,\n                attn_mask=attn_bias,\n                dropout_p=self.attn_drop.p,\n            )\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n            if self.rel_pos is not None:\n                attn = self.rel_pos(attn, shared_rel_pos=shared_rel_pos)\n            elif shared_rel_pos is not None:\n                attn = attn + shared_rel_pos\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            x = attn @ v\n\n        x = x.transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass LayerScale(nn.Module):\n    def __init__(self, dim, init_values=1e-5, inplace=False):\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n\n\nclass RelPosBlock(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            qk_norm=False,\n            rel_pos_cls=None,\n            init_values=None,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = RelPosAttention(\n            dim,\n            num_heads,\n            qkv_bias=qkv_bias,\n            qk_norm=qk_norm,\n            rel_pos_cls=rel_pos_cls,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n        )\n        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x, shared_rel_pos: Optional[torch.Tensor] = None):\n        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), shared_rel_pos=shared_rel_pos)))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x\n\n\nclass ResPostRelPosBlock(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            qk_norm=False,\n            rel_pos_cls=None,\n            init_values=None,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.init_values = init_values\n\n        self.attn = RelPosAttention(\n            dim,\n            num_heads,\n            qkv_bias=qkv_bias,\n            qk_norm=qk_norm,\n            rel_pos_cls=rel_pos_cls,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n        )\n        self.norm1 = norm_layer(dim)\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n        self.norm2 = norm_layer(dim)\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.init_weights()\n\n    def init_weights(self):\n        # NOTE this init overrides that base model init with specific changes for the block type\n        if self.init_values is not None:\n            nn.init.constant_(self.norm1.weight, self.init_values)\n            nn.init.constant_(self.norm2.weight, self.init_values)\n\n    def forward(self, x, shared_rel_pos: Optional[torch.Tensor] = None):\n        x = x + self.drop_path1(self.norm1(self.attn(x, shared_rel_pos=shared_rel_pos)))\n        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n        return x\n\n\nclass VisionTransformerRelPos(nn.Module):\n    \"\"\" Vision Transformer w/ Relative Position Bias\n\n    Differing from classic vit, this impl\n      * uses relative position index (swin v1 / beit) or relative log coord + mlp (swin v2) pos embed\n      * defaults to no class token (can be enabled)\n      * defaults to global avg pool for head (can be changed)\n      * layer-scale (residual branch gain) enabled\n    \"\"\"\n\n    def __init__(\n            self,\n            img_size=224,\n            patch_size=16,\n            in_chans=3,\n            num_classes=1000,\n            global_pool='avg',\n            embed_dim=768,\n            depth=12,\n            num_heads=12,\n            mlp_ratio=4.,\n            qkv_bias=True,\n            qk_norm=False,\n            init_values=1e-6,\n            class_token=False,\n            fc_norm=False,\n            rel_pos_type='mlp',\n            rel_pos_dim=None,\n            shared_rel_pos=False,\n            drop_rate=0.,\n            proj_drop_rate=0.,\n            attn_drop_rate=0.,\n            drop_path_rate=0.,\n            weight_init='skip',\n            embed_layer=PatchEmbed,\n            norm_layer=None,\n            act_layer=None,\n            block_fn=RelPosBlock\n    ):\n        \"\"\"\n        Args:\n            img_size (int, tuple): input image size\n            patch_size (int, tuple): patch size\n            in_chans (int): number of input channels\n            num_classes (int): number of classes for classification head\n            global_pool (str): type of global pooling for final sequence (default: 'avg')\n            embed_dim (int): embedding dimension\n            depth (int): depth of transformer\n            num_heads (int): number of attention heads\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n            qkv_bias (bool): enable bias for qkv if True\n            qk_norm (bool): Enable normalization of query and key in attention\n            init_values: (float): layer-scale init values\n            class_token (bool): use class token (default: False)\n            fc_norm (bool): use pre classifier norm instead of pre-pool\n            rel_pos_ty pe (str): type of relative position\n            shared_rel_pos (bool): share relative pos across all blocks\n            drop_rate (float): dropout rate\n            proj_drop_rate (float): projection dropout rate\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate\n            weight_init (str): weight init scheme\n            embed_layer (nn.Module): patch embedding layer\n            norm_layer: (nn.Module): normalization layer\n            act_layer: (nn.Module): MLP activation layer\n        \"\"\"\n        super().__init__()\n        assert global_pool in ('', 'avg', 'token')\n        assert class_token or global_pool != 'token'\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        act_layer = act_layer or nn.GELU\n\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.num_prefix_tokens = 1 if class_token else 0\n        self.grad_checkpointing = False\n\n        self.patch_embed = embed_layer(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n        )\n        feat_size = self.patch_embed.grid_size\n\n        rel_pos_args = dict(window_size=feat_size, prefix_tokens=self.num_prefix_tokens)\n        if rel_pos_type.startswith('mlp'):\n            if rel_pos_dim:\n                rel_pos_args['hidden_dim'] = rel_pos_dim\n            if 'swin' in rel_pos_type:\n                rel_pos_args['mode'] = 'swin'\n            rel_pos_cls = partial(RelPosMlp, **rel_pos_args)\n        else:\n            rel_pos_cls = partial(RelPosBias, **rel_pos_args)\n        self.shared_rel_pos = None\n        if shared_rel_pos:\n            self.shared_rel_pos = rel_pos_cls(num_heads=num_heads)\n            # NOTE shared rel pos currently mutually exclusive w/ per-block, but could support both...\n            rel_pos_cls = None\n\n        self.cls_token = nn.Parameter(torch.zeros(1, self.num_prefix_tokens, embed_dim)) if class_token else None\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList([\n            block_fn(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_norm=qk_norm,\n                rel_pos_cls=rel_pos_cls,\n                init_values=init_values,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n            )\n            for i in range(depth)])\n        self.norm = norm_layer(embed_dim) if not fc_norm else nn.Identity()\n\n        # Classifier Head\n        self.fc_norm = norm_layer(embed_dim) if fc_norm else nn.Identity()\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        if weight_init != 'skip':\n            self.init_weights(weight_init)\n\n    def init_weights(self, mode=''):\n        assert mode in ('jax', 'moco', '')\n        if self.cls_token is not None:\n            nn.init.normal_(self.cls_token, std=1e-6)\n        # FIXME weight init scheme using PyTorch defaults curently\n        #named_apply(get_init_weights_vit(mode, head_bias), self)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'cls_token'}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^cls_token|patch_embed',  # stem and embed\n            blocks=[(r'^blocks\\.(\\d+)', None), (r'^norm', (99999,))]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes: int, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in ('', 'avg', 'token')\n            self.global_pool = global_pool\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        if self.cls_token is not None:\n            x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n\n        shared_rel_pos = self.shared_rel_pos.get_bias() if self.shared_rel_pos is not None else None\n        for blk in self.blocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint(blk, x, shared_rel_pos=shared_rel_pos)\n            else:\n                x = blk(x, shared_rel_pos=shared_rel_pos)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool:\n            x = x[:, self.num_prefix_tokens:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]\n        x = self.fc_norm(x)\n        x = self.head_drop(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _create_vision_transformer_relpos(variant, pretrained=False, **kwargs):\n    if kwargs.get('features_only', None):\n        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n\n    model = build_model_with_cfg(VisionTransformerRelPos, variant, pretrained, **kwargs)\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,\n        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'vit_relpos_base_patch32_plus_rpn_256.sw_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_replos_base_patch32_plus_rpn_256-sw-dd486f51.pth',\n        hf_hub_id='timm/',\n        input_size=(3, 256, 256)),\n    'vit_relpos_base_patch16_plus_240.untrained': _cfg(url='', input_size=(3, 240, 240)),\n\n    'vit_relpos_small_patch16_224.sw_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_small_patch16_224-sw-ec2778b4.pth',\n        hf_hub_id='timm/'),\n    'vit_relpos_medium_patch16_224.sw_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_medium_patch16_224-sw-11c174af.pth',\n        hf_hub_id='timm/'),\n    'vit_relpos_base_patch16_224.sw_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_base_patch16_224-sw-49049aed.pth',\n        hf_hub_id='timm/'),\n\n    'vit_srelpos_small_patch16_224.sw_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_srelpos_small_patch16_224-sw-6cdb8849.pth',\n        hf_hub_id='timm/'),\n    'vit_srelpos_medium_patch16_224.sw_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_srelpos_medium_patch16_224-sw-ad702b8c.pth',\n        hf_hub_id='timm/'),\n\n    'vit_relpos_medium_patch16_cls_224.sw_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_medium_patch16_cls_224-sw-cfe8e259.pth',\n        hf_hub_id='timm/'),\n    'vit_relpos_base_patch16_cls_224.untrained': _cfg(),\n    'vit_relpos_base_patch16_clsgap_224.sw_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_base_patch16_gapcls_224-sw-1a341d6c.pth',\n        hf_hub_id='timm/'),\n\n    'vit_relpos_small_patch16_rpn_224.untrained': _cfg(),\n    'vit_relpos_medium_patch16_rpn_224.sw_in1k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/vit_relpos_medium_patch16_rpn_224-sw-5d2befd8.pth',\n        hf_hub_id='timm/'),\n    'vit_relpos_base_patch16_rpn_224.untrained': _cfg(),\n})\n\n\n@register_model\ndef vit_relpos_base_patch32_plus_rpn_256(pretrained=False, **kwargs) -> VisionTransformerRelPos:\n    \"\"\" ViT-Base (ViT-B/32+) w/ relative log-coord position and residual post-norm, no class token\n    \"\"\"\n    model_args = dict(patch_size=32, embed_dim=896, depth=12, num_heads=14, block_fn=ResPostRelPosBlock)\n    model = _create_vision_transformer_relpos(\n        'vit_relpos_base_patch32_plus_rpn_256', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_relpos_base_patch16_plus_240(pretrained=False, **kwargs) -> VisionTransformerRelPos:\n    \"\"\" ViT-Base (ViT-B/16+) w/ relative log-coord position, no class token\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=896, depth=12, num_heads=14)\n    model = _create_vision_transformer_relpos(\n        'vit_relpos_base_patch16_plus_240', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_relpos_small_patch16_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:\n    \"\"\" ViT-Base (ViT-B/16) w/ relative log-coord position, no class token\n    \"\"\"\n    model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6, qkv_bias=False, fc_norm=True)\n    model = _create_vision_transformer_relpos(\n        'vit_relpos_small_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_relpos_medium_patch16_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:\n    \"\"\" ViT-Base (ViT-B/16) w/ relative log-coord position, no class token\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=512, depth=12, num_heads=8, qkv_bias=False, fc_norm=True)\n    model = _create_vision_transformer_relpos(\n        'vit_relpos_medium_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_relpos_base_patch16_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:\n    \"\"\" ViT-Base (ViT-B/16) w/ relative log-coord position, no class token\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False, fc_norm=True)\n    model = _create_vision_transformer_relpos(\n        'vit_relpos_base_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_srelpos_small_patch16_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:\n    \"\"\" ViT-Base (ViT-B/16) w/ shared relative log-coord position, no class token\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=384, depth=12, num_heads=6, qkv_bias=False, fc_norm=False,\n        rel_pos_dim=384, shared_rel_pos=True)\n    model = _create_vision_transformer_relpos(\n        'vit_srelpos_small_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_srelpos_medium_patch16_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:\n    \"\"\" ViT-Base (ViT-B/16) w/ shared relative log-coord position, no class token\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=512, depth=12, num_heads=8, qkv_bias=False, fc_norm=False,\n        rel_pos_dim=512, shared_rel_pos=True)\n    model = _create_vision_transformer_relpos(\n        'vit_srelpos_medium_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_relpos_medium_patch16_cls_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:\n    \"\"\" ViT-Base (ViT-M/16) w/ relative log-coord position, class token present\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=512, depth=12, num_heads=8, qkv_bias=False, fc_norm=False,\n        rel_pos_dim=256, class_token=True, global_pool='token')\n    model = _create_vision_transformer_relpos(\n        'vit_relpos_medium_patch16_cls_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_relpos_base_patch16_cls_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:\n    \"\"\" ViT-Base (ViT-B/16) w/ relative log-coord position, class token present\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False, class_token=True, global_pool='token')\n    model = _create_vision_transformer_relpos(\n        'vit_relpos_base_patch16_cls_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_relpos_base_patch16_clsgap_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:\n    \"\"\" ViT-Base (ViT-B/16) w/ relative log-coord position, class token present\n    NOTE this config is a bit of a mistake, class token was enabled but global avg-pool w/ fc-norm was not disabled\n    Leaving here for comparisons w/ a future re-train as it performs quite well.\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False, fc_norm=True, class_token=True)\n    model = _create_vision_transformer_relpos(\n        'vit_relpos_base_patch16_clsgap_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_relpos_small_patch16_rpn_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:\n    \"\"\" ViT-Base (ViT-B/16) w/ relative log-coord position and residual post-norm, no class token\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=384, depth=12, num_heads=6, qkv_bias=False, block_fn=ResPostRelPosBlock)\n    model = _create_vision_transformer_relpos(\n        'vit_relpos_small_patch16_rpn_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_relpos_medium_patch16_rpn_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:\n    \"\"\" ViT-Base (ViT-B/16) w/ relative log-coord position and residual post-norm, no class token\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=512, depth=12, num_heads=8, qkv_bias=False, block_fn=ResPostRelPosBlock)\n    model = _create_vision_transformer_relpos(\n        'vit_relpos_medium_patch16_rpn_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef vit_relpos_base_patch16_rpn_224(pretrained=False, **kwargs) -> VisionTransformerRelPos:\n    \"\"\" ViT-Base (ViT-B/16) w/ relative log-coord position and residual post-norm, no class token\n    \"\"\"\n    model_args = dict(\n        patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False, block_fn=ResPostRelPosBlock)\n    model = _create_vision_transformer_relpos(\n        'vit_relpos_base_patch16_rpn_224', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n",
  "\"\"\" Nested Transformer (NesT) in PyTorch\n\nA PyTorch implement of Aggregating Nested Transformers as described in:\n\n'Aggregating Nested Transformers'\n    - https://arxiv.org/abs/2105.12723\n\nThe official Jax code is released and available at https://github.com/google-research/nested-transformer. The weights\nhave been converted with convert/convert_nest_flax.py\n\nAcknowledgments:\n* The paper authors for sharing their research, code, and model weights\n* Ross Wightman's existing code off which I based this\n\nCopyright 2021 Alexander Soare\n\"\"\"\n\nimport collections.abc\nimport logging\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import PatchEmbed, Mlp, DropPath, create_classifier, trunc_normal_, _assert\nfrom timm.layers import create_conv2d, create_pool2d, to_ntuple, use_fused_attn, LayerNorm\nfrom ._builder import build_model_with_cfg\nfrom ._features_fx import register_notrace_function\nfrom ._manipulate import checkpoint_seq, named_apply\nfrom ._registry import register_model, generate_default_cfgs, register_model_deprecations\n\n__all__ = ['Nest']  # model_registry will add each entrypoint fn to this\n\n_logger = logging.getLogger(__name__)\n\n\nclass Attention(nn.Module):\n    \"\"\"\n    This is much like `.vision_transformer.Attention` but uses *localised* self attention by accepting an input with\n     an extra \"image block\" dim\n    \"\"\"\n    fused_attn: torch.jit.Final[bool]\n\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n        self.fused_attn = use_fused_attn()\n\n        self.qkv = nn.Linear(dim, 3*dim, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        \"\"\"\n        x is shape: B (batch_size), T (image blocks), N (seq length per image block), C (embed dim)\n        \"\"\" \n        B, T, N, C = x.shape\n        # result of next line is (qkv, B, num (H)eads, T, N, (C')hannels per head)\n        qkv = self.qkv(x).reshape(B, T, N, 3, self.num_heads, C // self.num_heads).permute(3, 0, 4, 1, 2, 5)\n        q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)\n\n        if self.fused_attn:\n            x = F.scaled_dot_product_attention(q, k, v, dropout_p=self.attn_drop.p)\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1) # (B, H, T, N, N)\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            x = attn @ v\n\n        # (B, H, T, N, C'), permute -> (B, T, N, C', H)\n        x = x.permute(0, 2, 3, 4, 1).reshape(B, T, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x  # (B, T, N, C)\n\n\nclass TransformerLayer(nn.Module):\n    \"\"\"\n    This is much like `.vision_transformer.Block` but:\n        - Called TransformerLayer here to allow for \"block\" as defined in the paper (\"non-overlapping image blocks\")\n        - Uses modified Attention layer that handles the \"block\" dimension\n    \"\"\"\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n        )\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n\n    def forward(self, x):\n        y = self.norm1(x)\n        x = x + self.drop_path(self.attn(y))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass ConvPool(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer, pad_type=''):\n        super().__init__()\n        self.conv = create_conv2d(in_channels, out_channels, kernel_size=3, padding=pad_type, bias=True)\n        self.norm = norm_layer(out_channels)\n        self.pool = create_pool2d('max', kernel_size=3, stride=2, padding=pad_type)\n\n    def forward(self, x):\n        \"\"\"\n        x is expected to have shape (B, C, H, W)\n        \"\"\"\n        _assert(x.shape[-2] % 2 == 0, 'BlockAggregation requires even input spatial dims')\n        _assert(x.shape[-1] % 2 == 0, 'BlockAggregation requires even input spatial dims')\n        x = self.conv(x)\n        # Layer norm done over channel dim only\n        x = self.norm(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n        x = self.pool(x)\n        return x  # (B, C, H//2, W//2)\n\n\ndef blockify(x, block_size: int):\n    \"\"\"image to blocks\n    Args:\n        x (Tensor): with shape (B, H, W, C)\n        block_size (int): edge length of a single square block in units of H, W\n    \"\"\"\n    B, H, W, C  = x.shape\n    _assert(H % block_size == 0, '`block_size` must divide input height evenly')\n    _assert(W % block_size == 0, '`block_size` must divide input width evenly')\n    grid_height = H // block_size\n    grid_width = W // block_size\n    x = x.reshape(B, grid_height, block_size, grid_width, block_size, C)\n    x = x.transpose(2, 3).reshape(B, grid_height * grid_width, -1, C)\n    return x  # (B, T, N, C)\n\n\n@register_notrace_function  # reason: int receives Proxy\ndef deblockify(x, block_size: int):\n    \"\"\"blocks to image\n    Args:\n        x (Tensor): with shape (B, T, N, C) where T is number of blocks and N is sequence size per block\n        block_size (int): edge length of a single square block in units of desired H, W\n    \"\"\"\n    B, T, _, C = x.shape\n    grid_size = int(math.sqrt(T))\n    height = width = grid_size * block_size\n    x = x.reshape(B, grid_size, grid_size, block_size, block_size, C)\n    x = x.transpose(2, 3).reshape(B, height, width, C)\n    return x  # (B, H, W, C)\n\n\nclass NestLevel(nn.Module):\n    \"\"\" Single hierarchical level of a Nested Transformer\n    \"\"\"\n    def __init__(\n            self,\n            num_blocks,\n            block_size,\n            seq_length,\n            num_heads,\n            depth,\n            embed_dim,\n            prev_embed_dim=None,\n            mlp_ratio=4.,\n            qkv_bias=True,\n            proj_drop=0.,\n            attn_drop=0.,\n            drop_path=[],\n            norm_layer=None,\n            act_layer=None,\n            pad_type='',\n    ):\n        super().__init__()\n        self.block_size = block_size\n        self.grad_checkpointing = False\n\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_blocks, seq_length, embed_dim))\n\n        if prev_embed_dim is not None:\n            self.pool = ConvPool(prev_embed_dim, embed_dim, norm_layer=norm_layer, pad_type=pad_type)\n        else:\n            self.pool = nn.Identity()\n\n        # Transformer encoder\n        if len(drop_path):\n            assert len(drop_path) == depth, 'Must provide as many drop path rates as there are transformer layers'\n        self.transformer_encoder = nn.Sequential(*[\n            TransformerLayer(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                proj_drop=proj_drop,\n                attn_drop=attn_drop,\n                drop_path=drop_path[i],\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n            )\n            for i in range(depth)])\n\n    def forward(self, x):\n        \"\"\"\n        expects x as (B, C, H, W)\n        \"\"\"\n        x = self.pool(x)\n        x = x.permute(0, 2, 3, 1)  # (B, H', W', C), switch to channels last for transformer\n        x = blockify(x, self.block_size)  # (B, T, N, C')\n        x = x + self.pos_embed\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.transformer_encoder, x)\n        else:\n            x = self.transformer_encoder(x)  # (B, T, N, C')\n        x = deblockify(x, self.block_size)  # (B, H', W', C')\n        # Channel-first for block aggregation, and generally to replicate convnet feature map at each stage\n        return x.permute(0, 3, 1, 2)  # (B, C, H', W')\n\n\nclass Nest(nn.Module):\n    \"\"\" Nested Transformer (NesT)\n\n    A PyTorch impl of : `Aggregating Nested Transformers`\n        - https://arxiv.org/abs/2105.12723\n    \"\"\"\n\n    def __init__(\n            self,\n            img_size=224,\n            in_chans=3,\n            patch_size=4,\n            num_levels=3,\n            embed_dims=(128, 256, 512),\n            num_heads=(4, 8, 16),\n            depths=(2, 2, 20),\n            num_classes=1000,\n            mlp_ratio=4.,\n            qkv_bias=True,\n            drop_rate=0.,\n            proj_drop_rate=0.,\n            attn_drop_rate=0.,\n            drop_path_rate=0.5,\n            norm_layer=None,\n            act_layer=None,\n            pad_type='',\n            weight_init='',\n            global_pool='avg',\n    ):\n        \"\"\"\n        Args:\n            img_size (int, tuple): input image size\n            in_chans (int): number of input channels\n            patch_size (int): patch size\n            num_levels (int): number of block hierarchies (T_d in the paper)\n            embed_dims (int, tuple): embedding dimensions of each level\n            num_heads (int, tuple): number of attention heads for each level\n            depths (int, tuple): number of transformer layers for each level\n            num_classes (int): number of classes for classification head\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim for MLP of transformer layers\n            qkv_bias (bool): enable bias for qkv if True\n            drop_rate (float): dropout rate for MLP of transformer layers, MSA final projection layer, and classifier\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate\n            norm_layer: (nn.Module): normalization layer for transformer layers\n            act_layer: (nn.Module): activation layer in MLP of transformer layers\n            pad_type: str: Type of padding to use '' for PyTorch symmetric, 'same' for TF SAME\n            weight_init: (str): weight init scheme\n            global_pool: (str): type of pooling operation to apply to final feature map\n\n        Notes:\n            - Default values follow NesT-B from the original Jax code.\n            - `embed_dims`, `num_heads`, `depths` should be ints or tuples with length `num_levels`.\n            - For those following the paper, Table A1 may have errors!\n                - https://github.com/google-research/nested-transformer/issues/2\n        \"\"\"\n        super().__init__()\n\n        for param_name in ['embed_dims', 'num_heads', 'depths']:\n            param_value = locals()[param_name]\n            if isinstance(param_value, collections.abc.Sequence):\n                assert len(param_value) == num_levels, f'Require `len({param_name}) == num_levels`'\n\n        embed_dims = to_ntuple(num_levels)(embed_dims)\n        num_heads = to_ntuple(num_levels)(num_heads)\n        depths = to_ntuple(num_levels)(depths)\n        self.num_classes = num_classes\n        self.num_features = embed_dims[-1]\n        self.feature_info = []\n        norm_layer = norm_layer or LayerNorm\n        act_layer = act_layer or nn.GELU\n        self.drop_rate = drop_rate\n        self.num_levels = num_levels\n        if isinstance(img_size, collections.abc.Sequence):\n            assert img_size[0] == img_size[1], 'Model only handles square inputs'\n            img_size = img_size[0]\n        assert img_size % patch_size == 0, '`patch_size` must divide `img_size` evenly'\n        self.patch_size = patch_size\n\n        # Number of blocks at each level\n        self.num_blocks = (4 ** torch.arange(num_levels)).flip(0).tolist()\n        assert (img_size // patch_size) % math.sqrt(self.num_blocks[0]) == 0, \\\n            'First level blocks don\\'t fit evenly. Check `img_size`, `patch_size`, and `num_levels`'\n\n        # Block edge size in units of patches\n        # Hint: (img_size // patch_size) gives number of patches along edge of image. sqrt(self.num_blocks[0]) is the\n        #  number of blocks along edge of image\n        self.block_size = int((img_size // patch_size) // math.sqrt(self.num_blocks[0]))\n        \n        # Patch embedding\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dims[0],\n            flatten=False,\n        )\n        self.num_patches = self.patch_embed.num_patches\n        self.seq_length = self.num_patches // self.num_blocks[0]\n\n        # Build up each hierarchical level\n        levels = []\n        dp_rates = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n        prev_dim = None\n        curr_stride = 4\n        for i in range(len(self.num_blocks)):\n            dim = embed_dims[i]\n            levels.append(NestLevel(\n                self.num_blocks[i],\n                self.block_size,\n                self.seq_length,\n                num_heads[i],\n                depths[i],\n                dim,\n                prev_dim,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                proj_drop=proj_drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dp_rates[i],\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                pad_type=pad_type,\n            ))\n            self.feature_info += [dict(num_chs=dim, reduction=curr_stride, module=f'levels.{i}')]\n            prev_dim = dim\n            curr_stride *= 2\n        self.levels = nn.Sequential(*levels)\n\n        # Final normalization layer\n        self.norm = norm_layer(embed_dims[-1])\n\n        # Classifier\n        global_pool, head = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)\n        self.global_pool = global_pool\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = head\n\n        self.init_weights(weight_init)\n\n    @torch.jit.ignore\n    def init_weights(self, mode=''):\n        assert mode in ('nlhb', '')\n        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.\n        for level in self.levels:\n            trunc_normal_(level.pos_embed, std=.02, a=-2, b=2)\n        named_apply(partial(_init_nest_weights, head_bias=head_bias), self)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {f'level.{i}.pos_embed' for i in range(len(self.levels))}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^patch_embed',  # stem and embed\n            blocks=[\n                (r'^levels\\.(\\d+)' if coarse else r'^levels\\.(\\d+)\\.transformer_encoder\\.(\\d+)', None),\n                (r'^levels\\.(\\d+)\\.(?:pool|pos_embed)', (0,)),\n                (r'^norm', (99999,))\n            ]\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        for l in self.levels:\n            l.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.global_pool, self.head = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = self.levels(x)\n        # Layer norm done over channel dim only (to NHWC and back)\n        x = self.norm(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        x = self.global_pool(x)\n        x = self.head_drop(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _init_nest_weights(module: nn.Module, name: str = '', head_bias: float = 0.):\n    \"\"\" NesT weight initialization\n    Can replicate Jax implementation. Otherwise follows vision_transformer.py\n    \"\"\"\n    if isinstance(module, nn.Linear):\n        if name.startswith('head'):\n            trunc_normal_(module.weight, std=.02, a=-2, b=2)\n            nn.init.constant_(module.bias, head_bias)\n        else:\n            trunc_normal_(module.weight, std=.02, a=-2, b=2)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Conv2d):\n        trunc_normal_(module.weight, std=.02, a=-2, b=2)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n\n\ndef resize_pos_embed(posemb, posemb_new):\n    \"\"\"\n    Rescale the grid of position embeddings when loading from state_dict\n    Expected shape of position embeddings is (1, T, N, C), and considers only square images\n    \"\"\"\n    _logger.info('Resized position embedding: %s to %s', posemb.shape, posemb_new.shape)\n    seq_length_old = posemb.shape[2]\n    num_blocks_new, seq_length_new = posemb_new.shape[1:3]\n    size_new = int(math.sqrt(num_blocks_new*seq_length_new))\n    # First change to (1, C, H, W)\n    posemb = deblockify(posemb, int(math.sqrt(seq_length_old))).permute(0, 3, 1, 2)\n    posemb = F.interpolate(posemb, size=[size_new, size_new], mode='bicubic', align_corners=False)\n    # Now change to new (1, T, N, C)\n    posemb = blockify(posemb.permute(0, 2, 3, 1), int(math.sqrt(seq_length_new)))\n    return posemb\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    \"\"\" resize positional embeddings of pretrained weights \"\"\"\n    pos_embed_keys = [k for k in state_dict.keys() if k.startswith('pos_embed_')]\n    for k in pos_embed_keys:\n        if state_dict[k].shape != getattr(model, k).shape:\n            state_dict[k] = resize_pos_embed(state_dict[k], getattr(model, k))\n    return state_dict\n\n\ndef _create_nest(variant, pretrained=False, **kwargs):\n    model = build_model_with_cfg(\n        Nest,\n        variant,\n        pretrained,\n        feature_cfg=dict(out_indices=(0, 1, 2), flatten_sequential=True),\n        pretrained_filter_fn=checkpoint_filter_fn,\n        **kwargs,\n    )\n\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': [14, 14],\n        'crop_pct': .875, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'nest_base.untrained': _cfg(),\n    'nest_small.untrained': _cfg(),\n    'nest_tiny.untrained': _cfg(),\n    # (weights from official Google JAX impl, require 'SAME' padding)\n    'nest_base_jx.goog_in1k': _cfg(hf_hub_id='timm/'),\n    'nest_small_jx.goog_in1k': _cfg(hf_hub_id='timm/'),\n    'nest_tiny_jx.goog_in1k': _cfg(hf_hub_id='timm/'),\n})\n\n\n@register_model\ndef nest_base(pretrained=False, **kwargs) -> Nest:\n    \"\"\" Nest-B @ 224x224\n    \"\"\"\n    model_kwargs = dict(\n        embed_dims=(128, 256, 512), num_heads=(4, 8, 16), depths=(2, 2, 20), **kwargs)\n    model = _create_nest('nest_base', pretrained=pretrained, **model_kwargs)\n    return model\n\n\n@register_model\ndef nest_small(pretrained=False, **kwargs) -> Nest:\n    \"\"\" Nest-S @ 224x224\n    \"\"\"\n    model_kwargs = dict(embed_dims=(96, 192, 384), num_heads=(3, 6, 12), depths=(2, 2, 20), **kwargs)\n    model = _create_nest('nest_small', pretrained=pretrained, **model_kwargs)\n    return model\n\n\n@register_model\ndef nest_tiny(pretrained=False, **kwargs) -> Nest:\n    \"\"\" Nest-T @ 224x224\n    \"\"\"\n    model_kwargs = dict(embed_dims=(96, 192, 384), num_heads=(3, 6, 12), depths=(2, 2, 8), **kwargs)\n    model = _create_nest('nest_tiny', pretrained=pretrained, **model_kwargs)\n    return model\n\n\n@register_model\ndef nest_base_jx(pretrained=False, **kwargs) -> Nest:\n    \"\"\" Nest-B @ 224x224\n    \"\"\"\n    kwargs.setdefault('pad_type', 'same')\n    model_kwargs = dict(\n        embed_dims=(128, 256, 512), num_heads=(4, 8, 16), depths=(2, 2, 20), **kwargs)\n    model = _create_nest('nest_base_jx', pretrained=pretrained, **model_kwargs)\n    return model\n\n\n@register_model\ndef nest_small_jx(pretrained=False, **kwargs) -> Nest:\n    \"\"\" Nest-S @ 224x224\n    \"\"\"\n    kwargs.setdefault('pad_type', 'same')\n    model_kwargs = dict(embed_dims=(96, 192, 384), num_heads=(3, 6, 12), depths=(2, 2, 20), **kwargs)\n    model = _create_nest('nest_small_jx', pretrained=pretrained, **model_kwargs)\n    return model\n\n\n@register_model\ndef nest_tiny_jx(pretrained=False, **kwargs) -> Nest:\n    \"\"\" Nest-T @ 224x224\n    \"\"\"\n    kwargs.setdefault('pad_type', 'same')\n    model_kwargs = dict(embed_dims=(96, 192, 384), num_heads=(3, 6, 12), depths=(2, 2, 8), **kwargs)\n    model = _create_nest('nest_tiny_jx', pretrained=pretrained, **model_kwargs)\n    return model\n\n\nregister_model_deprecations(__name__, {\n    'jx_nest_base': 'nest_base_jx',\n    'jx_nest_small': 'nest_small_jx',\n    'jx_nest_tiny': 'nest_tiny_jx',\n})",
  "\"\"\" FocalNet\n\nAs described in `Focal Modulation Networks` - https://arxiv.org/abs/2203.11926\n\nSignificant modifications and refactoring from the original impl at https://github.com/microsoft/FocalNet\n\nThis impl is/has:\n* fully convolutional, NCHW tensor layout throughout, seemed to have minimal performance impact but more flexible\n* re-ordered downsample / layer so that striding always at beginning of layer (stage)\n* no input size constraints or input resolution/H/W tracking through the model\n* torchscript fixed and a number of quirks cleaned up\n* feature extraction support via `features_only=True`\n\"\"\"\n# --------------------------------------------------------\n# FocalNets -- Focal Modulation Networks\n# Copyright (c) 2022 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Jianwei Yang (jianwyan@microsoft.com)\n# --------------------------------------------------------\nfrom functools import partial\nfrom typing import Callable, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import Mlp, DropPath, LayerNorm2d, trunc_normal_, ClassifierHead, NormMlpClassifierHead\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import named_apply\nfrom ._registry import generate_default_cfgs, register_model\n\n__all__ = ['FocalNet']\n\n\nclass FocalModulation(nn.Module):\n    def __init__(\n            self,\n            dim: int,\n            focal_window,\n            focal_level: int,\n            focal_factor: int = 2,\n            bias: bool = True,\n            use_post_norm: bool = False,\n            normalize_modulator: bool = False,\n            proj_drop: float = 0.,\n            norm_layer: Callable = LayerNorm2d,\n    ):\n        super().__init__()\n\n        self.dim = dim\n        self.focal_window = focal_window\n        self.focal_level = focal_level\n        self.focal_factor = focal_factor\n        self.use_post_norm = use_post_norm\n        self.normalize_modulator = normalize_modulator\n        self.input_split = [dim, dim, self.focal_level + 1]\n\n        self.f = nn.Conv2d(dim, 2 * dim + (self.focal_level + 1), kernel_size=1, bias=bias)\n        self.h = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n\n        self.act = nn.GELU()\n        self.proj = nn.Conv2d(dim, dim, kernel_size=1)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.focal_layers = nn.ModuleList()\n\n        self.kernel_sizes = []\n        for k in range(self.focal_level):\n            kernel_size = self.focal_factor * k + self.focal_window\n            self.focal_layers.append(nn.Sequential(\n                nn.Conv2d(dim, dim, kernel_size=kernel_size, groups=dim, padding=kernel_size // 2, bias=False),\n                nn.GELU(),\n            ))\n            self.kernel_sizes.append(kernel_size)\n        self.norm = norm_layer(dim) if self.use_post_norm else nn.Identity()\n\n    def forward(self, x):\n        # pre linear projection\n        x = self.f(x)\n        q, ctx, gates = torch.split(x, self.input_split, 1)\n\n        # context aggreation\n        ctx_all = 0\n        for l, focal_layer in enumerate(self.focal_layers):\n            ctx = focal_layer(ctx)\n            ctx_all = ctx_all + ctx * gates[:, l:l + 1]\n        ctx_global = self.act(ctx.mean((2, 3), keepdim=True))\n        ctx_all = ctx_all + ctx_global * gates[:, self.focal_level:]\n\n        # normalize context\n        if self.normalize_modulator:\n            ctx_all = ctx_all / (self.focal_level + 1)\n\n        # focal modulation\n        x_out = q * self.h(ctx_all)\n        x_out = self.norm(x_out)\n\n        # post linear projection\n        x_out = self.proj(x_out)\n        x_out = self.proj_drop(x_out)\n        return x_out\n\n\nclass LayerScale2d(nn.Module):\n    def __init__(self, dim, init_values=1e-5, inplace=False):\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        gamma = self.gamma.view(1, -1, 1, 1)\n        return x.mul_(gamma) if self.inplace else x * gamma\n\n\nclass FocalNetBlock(nn.Module):\n    \"\"\" Focal Modulation Network Block.\n    \"\"\"\n\n    def __init__(\n            self,\n            dim: int,\n            mlp_ratio: float = 4.,\n            focal_level: int = 1,\n            focal_window: int = 3,\n            use_post_norm: bool = False,\n            use_post_norm_in_modulation: bool = False,\n            normalize_modulator: bool = False,\n            layerscale_value: float = 1e-4,\n            proj_drop: float = 0.,\n            drop_path: float = 0.,\n            act_layer: Callable = nn.GELU,\n            norm_layer: Callable = LayerNorm2d,\n    ):\n        \"\"\"\n        Args:\n            dim: Number of input channels.\n            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n            focal_level: Number of focal levels.\n            focal_window: Focal window size at first focal level.\n            use_post_norm: Whether to use layer norm after modulation.\n            use_post_norm_in_modulation: Whether to use layer norm in modulation.\n            layerscale_value: Initial layerscale value.\n            proj_drop: Dropout rate.\n            drop_path: Stochastic depth rate.\n            act_layer: Activation layer.\n            norm_layer: Normalization layer.\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n        self.mlp_ratio = mlp_ratio\n\n        self.focal_window = focal_window\n        self.focal_level = focal_level\n        self.use_post_norm = use_post_norm\n\n        self.norm1 = norm_layer(dim) if not use_post_norm else nn.Identity()\n        self.modulation = FocalModulation(\n            dim,\n            focal_window=focal_window,\n            focal_level=self.focal_level,\n            use_post_norm=use_post_norm_in_modulation,\n            normalize_modulator=normalize_modulator,\n            proj_drop=proj_drop,\n            norm_layer=norm_layer,\n        )\n        self.norm1_post = norm_layer(dim) if use_post_norm else nn.Identity()\n        self.ls1 = LayerScale2d(dim, layerscale_value) if layerscale_value is not None else nn.Identity()\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm2 = norm_layer(dim) if not use_post_norm else nn.Identity()\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            drop=proj_drop,\n            use_conv=True,\n        )\n        self.norm2_post = norm_layer(dim) if use_post_norm else nn.Identity()\n        self.ls2 = LayerScale2d(dim, layerscale_value) if layerscale_value is not None else nn.Identity()\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        shortcut = x\n\n        # Focal Modulation\n        x = self.norm1(x)\n        x = self.modulation(x)\n        x = self.norm1_post(x)\n        x = shortcut + self.drop_path1(self.ls1(x))\n\n        # FFN\n        x = x + self.drop_path2(self.ls2(self.norm2_post(self.mlp(self.norm2(x)))))\n\n        return x\n\n\nclass FocalNetStage(nn.Module):\n    \"\"\" A basic Focal Transformer layer for one stage.\n    \"\"\"\n\n    def __init__(\n            self,\n            dim: int,\n            out_dim: int,\n            depth: int,\n            mlp_ratio: float = 4.,\n            downsample: bool = True,\n            focal_level: int = 1,\n            focal_window: int = 1,\n            use_overlap_down: bool = False,\n            use_post_norm: bool = False,\n            use_post_norm_in_modulation: bool = False,\n            normalize_modulator: bool = False,\n            layerscale_value: float = 1e-4,\n            proj_drop: float = 0.,\n            drop_path: float = 0.,\n            norm_layer: Callable = LayerNorm2d,\n    ):\n        \"\"\"\n        Args:\n            dim: Number of input channels.\n            out_dim: Number of output channels.\n            depth: Number of blocks.\n            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n            downsample: Downsample layer at start of the layer.\n            focal_level: Number of focal levels\n            focal_window: Focal window size at first focal level\n            use_overlap_down: User overlapped convolution in downsample layer.\n            use_post_norm: Whether to use layer norm after modulation.\n            use_post_norm_in_modulation: Whether to use layer norm in modulation.\n            layerscale_value: Initial layerscale value\n            proj_drop: Dropout rate for projections.\n            drop_path: Stochastic depth rate.\n            norm_layer: Normalization layer.\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n        self.depth = depth\n        self.grad_checkpointing = False\n\n        if downsample:\n            self.downsample = Downsample(\n                in_chs=dim,\n                out_chs=out_dim,\n                stride=2,\n                overlap=use_overlap_down,\n                norm_layer=norm_layer,\n            )\n        else:\n            self.downsample = nn.Identity()\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            FocalNetBlock(\n                dim=out_dim,\n                mlp_ratio=mlp_ratio,\n                focal_level=focal_level,\n                focal_window=focal_window,\n                use_post_norm=use_post_norm,\n                use_post_norm_in_modulation=use_post_norm_in_modulation,\n                normalize_modulator=normalize_modulator,\n                layerscale_value=layerscale_value,\n                proj_drop=proj_drop,\n                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                norm_layer=norm_layer,\n            )\n            for i in range(depth)])\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    def forward(self, x):\n        x = self.downsample(x)\n        for blk in self.blocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        return x\n\n\nclass Downsample(nn.Module):\n\n    def __init__(\n            self,\n            in_chs: int,\n            out_chs: int,\n            stride: int = 4,\n            overlap: bool = False,\n            norm_layer: Optional[Callable] = None,\n    ):\n        \"\"\"\n\n        Args:\n            in_chs: Number of input image channels.\n            out_chs: Number of linear projection output channels.\n            stride: Downsample stride.\n            overlap: Use overlapping convolutions if True.\n            norm_layer: Normalization layer.\n        \"\"\"\n        super().__init__()\n        self.stride = stride\n        padding = 0\n        kernel_size = stride\n        if overlap:\n            assert stride in (2, 4)\n            if stride == 4:\n                kernel_size, padding = 7, 2\n            elif stride == 2:\n                kernel_size, padding = 3, 1\n        self.proj = nn.Conv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride, padding=padding)\n        self.norm = norm_layer(out_chs) if norm_layer is not None else nn.Identity()\n\n    def forward(self, x):\n        x = self.proj(x)\n        x = self.norm(x)\n        return x\n\n\nclass FocalNet(nn.Module):\n    \"\"\"\" Focal Modulation Networks (FocalNets)\n    \"\"\"\n\n    def __init__(\n            self,\n            in_chans: int = 3,\n            num_classes: int = 1000,\n            global_pool: str = 'avg',\n            embed_dim: int = 96,\n            depths: Tuple[int, ...] = (2, 2, 6, 2),\n            mlp_ratio: float = 4.,\n            focal_levels: Tuple[int, ...] = (2, 2, 2, 2),\n            focal_windows: Tuple[int, ...] = (3, 3, 3, 3),\n            use_overlap_down: bool = False,\n            use_post_norm: bool = False,\n            use_post_norm_in_modulation: bool = False,\n            normalize_modulator: bool = False,\n            head_hidden_size: Optional[int] = None,\n            head_init_scale: float = 1.0,\n            layerscale_value: Optional[float] = None,\n            drop_rate: bool = 0.,\n            proj_drop_rate: bool = 0.,\n            drop_path_rate: bool = 0.1,\n            norm_layer: Callable = partial(LayerNorm2d, eps=1e-5),\n    ):\n        \"\"\"\n        Args:\n            in_chans: Number of input image channels.\n            num_classes: Number of classes for classification head.\n            embed_dim: Patch embedding dimension.\n            depths: Depth of each Focal Transformer layer.\n            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n            focal_levels: How many focal levels at all stages. Note that this excludes the finest-grain level.\n            focal_windows: The focal window size at all stages.\n            use_overlap_down: Whether to use convolutional embedding.\n            use_post_norm: Whether to use layernorm after modulation (it helps stablize training of large models)\n            layerscale_value: Value for layer scale.\n            drop_rate: Dropout rate.\n            drop_path_rate: Stochastic depth rate.\n            norm_layer: Normalization layer.\n        \"\"\"\n        super().__init__()\n\n        self.num_layers = len(depths)\n        embed_dim = [embed_dim * (2 ** i) for i in range(self.num_layers)]\n\n        self.num_classes = num_classes\n        self.embed_dim = embed_dim\n        self.num_features = embed_dim[-1]\n        self.feature_info = []\n\n        self.stem = Downsample(\n            in_chs=in_chans,\n            out_chs=embed_dim[0],\n            overlap=use_overlap_down,\n            norm_layer=norm_layer,\n        )\n        in_dim = embed_dim[0]\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n        layers = []\n        for i_layer in range(self.num_layers):\n            out_dim = embed_dim[i_layer]\n            layer = FocalNetStage(\n                dim=in_dim,\n                out_dim=out_dim,\n                depth=depths[i_layer],\n                mlp_ratio=mlp_ratio,\n                downsample=i_layer > 0,\n                focal_level=focal_levels[i_layer],\n                focal_window=focal_windows[i_layer],\n                use_overlap_down=use_overlap_down,\n                use_post_norm=use_post_norm,\n                use_post_norm_in_modulation=use_post_norm_in_modulation,\n                normalize_modulator=normalize_modulator,\n                layerscale_value=layerscale_value,\n                proj_drop=proj_drop_rate,\n                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                norm_layer=norm_layer,\n            )\n            in_dim = out_dim\n            layers += [layer]\n            self.feature_info += [dict(num_chs=out_dim, reduction=4 * 2 ** i_layer, module=f'layers.{i_layer}')]\n\n        self.layers = nn.Sequential(*layers)\n\n        if head_hidden_size:\n            self.norm = nn.Identity()\n            self.head = NormMlpClassifierHead(\n                self.num_features,\n                num_classes,\n                hidden_size=head_hidden_size,\n                pool_type=global_pool,\n                drop_rate=drop_rate,\n                norm_layer=norm_layer,\n            )\n        else:\n            self.norm = norm_layer(self.num_features)\n            self.head = ClassifierHead(\n                self.num_features,\n                num_classes,\n                pool_type=global_pool,\n                drop_rate=drop_rate\n            )\n\n        named_apply(partial(_init_weights, head_init_scale=head_init_scale), self)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {''}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^stem',\n            blocks=[\n                (r'^layers\\.(\\d+)', None),\n                (r'^norm', (99999,))\n            ] if coarse else [\n                (r'^layers\\.(\\d+).downsample', (0,)),\n                (r'^layers\\.(\\d+)\\.\\w+\\.(\\d+)', None),\n                (r'^norm', (99999,)),\n            ]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n        for l in self.layers:\n            l.set_grad_checkpointing(enable=enable)\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.head.reset(num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        x = self.layers(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=pre_logits)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef _init_weights(module, name=None, head_init_scale=1.0):\n    if isinstance(module, nn.Conv2d):\n        trunc_normal_(module.weight, std=.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Linear):\n        trunc_normal_(module.weight, std=.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n        if name and 'head.fc' in name:\n            module.weight.data.mul_(head_init_scale)\n            module.bias.data.mul_(head_init_scale)\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': .9, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.proj', 'classifier': 'head.fc',\n        'license': 'mit', **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    \"focalnet_tiny_srf.ms_in1k\": _cfg(\n        hf_hub_id='timm/'),\n    \"focalnet_small_srf.ms_in1k\": _cfg(\n        hf_hub_id='timm/'),\n    \"focalnet_base_srf.ms_in1k\": _cfg(\n        hf_hub_id='timm/'),\n    \"focalnet_tiny_lrf.ms_in1k\": _cfg(\n        hf_hub_id='timm/'),\n    \"focalnet_small_lrf.ms_in1k\": _cfg(\n        hf_hub_id='timm/'),\n    \"focalnet_base_lrf.ms_in1k\": _cfg(\n        hf_hub_id='timm/'),\n\n    \"focalnet_large_fl3.ms_in22k\": _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, num_classes=21842),\n    \"focalnet_large_fl4.ms_in22k\": _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, num_classes=21842),\n    \"focalnet_xlarge_fl3.ms_in22k\": _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, num_classes=21842),\n    \"focalnet_xlarge_fl4.ms_in22k\": _cfg(\n        hf_hub_id='timm/',\n        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, num_classes=21842),\n    \"focalnet_huge_fl3.ms_in22k\": _cfg(\n        hf_hub_id='timm/',\n        num_classes=21842),\n    \"focalnet_huge_fl4.ms_in22k\": _cfg(\n        hf_hub_id='timm/',\n        num_classes=0),\n})\n\n\ndef checkpoint_filter_fn(state_dict, model: FocalNet):\n    state_dict = state_dict.get('model', state_dict)\n    if 'stem.proj.weight' in state_dict:\n        return state_dict\n    import re\n    out_dict = {}\n    dest_dict = model.state_dict()\n    for k, v in state_dict.items():\n        k = re.sub(r'gamma_([0-9])', r'ls\\1.gamma', k)\n        k = k.replace('patch_embed', 'stem')\n        k = re.sub(r'layers.(\\d+).downsample', lambda x: f'layers.{int(x.group(1)) + 1}.downsample', k)\n        if 'norm' in k and k not in dest_dict:\n            k = re.sub(r'norm([0-9])', r'norm\\1_post', k)\n        k = k.replace('ln.', 'norm.')\n        k = k.replace('head', 'head.fc')\n        if k in dest_dict and dest_dict[k].numel() == v.numel() and dest_dict[k].shape != v.shape:\n            v = v.reshape(dest_dict[k].shape)\n        out_dict[k] = v\n    return out_dict\n\n\ndef _create_focalnet(variant, pretrained=False, **kwargs):\n    default_out_indices = tuple(i for i, _ in enumerate(kwargs.get('depths', (1, 1, 3, 1))))\n    out_indices = kwargs.pop('out_indices', default_out_indices)\n\n    model = build_model_with_cfg(\n        FocalNet, variant, pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),\n        **kwargs)\n    return model\n\n\n@register_model\ndef focalnet_tiny_srf(pretrained=False, **kwargs) -> FocalNet:\n    model_kwargs = dict(depths=[2, 2, 6, 2], embed_dim=96, **kwargs)\n    return _create_focalnet('focalnet_tiny_srf', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef focalnet_small_srf(pretrained=False, **kwargs) -> FocalNet:\n    model_kwargs = dict(depths=[2, 2, 18, 2], embed_dim=96, **kwargs)\n    return _create_focalnet('focalnet_small_srf', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef focalnet_base_srf(pretrained=False, **kwargs) -> FocalNet:\n    model_kwargs = dict(depths=[2, 2, 18, 2], embed_dim=128, **kwargs)\n    return _create_focalnet('focalnet_base_srf', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef focalnet_tiny_lrf(pretrained=False, **kwargs) -> FocalNet:\n    model_kwargs = dict(depths=[2, 2, 6, 2], embed_dim=96, focal_levels=[3, 3, 3, 3], **kwargs)\n    return _create_focalnet('focalnet_tiny_lrf', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef focalnet_small_lrf(pretrained=False, **kwargs) -> FocalNet:\n    model_kwargs = dict(depths=[2, 2, 18, 2], embed_dim=96, focal_levels=[3, 3, 3, 3], **kwargs)\n    return _create_focalnet('focalnet_small_lrf', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef focalnet_base_lrf(pretrained=False, **kwargs) -> FocalNet:\n    model_kwargs = dict(depths=[2, 2, 18, 2], embed_dim=128, focal_levels=[3, 3, 3, 3], **kwargs)\n    return _create_focalnet('focalnet_base_lrf', pretrained=pretrained, **model_kwargs)\n\n\n# FocalNet large+ models\n@register_model\ndef focalnet_large_fl3(pretrained=False, **kwargs) -> FocalNet:\n    model_kwargs = dict(\n        depths=[2, 2, 18, 2], embed_dim=192, focal_levels=[3, 3, 3, 3], focal_windows=[5] * 4,\n        use_post_norm=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)\n    return _create_focalnet('focalnet_large_fl3', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef focalnet_large_fl4(pretrained=False, **kwargs) -> FocalNet:\n    model_kwargs = dict(\n        depths=[2, 2, 18, 2], embed_dim=192, focal_levels=[4, 4, 4, 4],\n        use_post_norm=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)\n    return _create_focalnet('focalnet_large_fl4', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef focalnet_xlarge_fl3(pretrained=False, **kwargs) -> FocalNet:\n    model_kwargs = dict(\n        depths=[2, 2, 18, 2], embed_dim=256, focal_levels=[3, 3, 3, 3], focal_windows=[5] * 4,\n        use_post_norm=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)\n    return _create_focalnet('focalnet_xlarge_fl3', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef focalnet_xlarge_fl4(pretrained=False, **kwargs) -> FocalNet:\n    model_kwargs = dict(\n        depths=[2, 2, 18, 2], embed_dim=256, focal_levels=[4, 4, 4, 4],\n        use_post_norm=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)\n    return _create_focalnet('focalnet_xlarge_fl4', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef focalnet_huge_fl3(pretrained=False, **kwargs) -> FocalNet:\n    model_kwargs = dict(\n        depths=[2, 2, 18, 2], embed_dim=352, focal_levels=[3, 3, 3, 3], focal_windows=[3] * 4,\n        use_post_norm=True, use_post_norm_in_modulation=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)\n    return _create_focalnet('focalnet_huge_fl3', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef focalnet_huge_fl4(pretrained=False, **kwargs) -> FocalNet:\n    model_kwargs = dict(\n        depths=[2, 2, 18, 2], embed_dim=352, focal_levels=[4, 4, 4, 4],\n        use_post_norm=True, use_post_norm_in_modulation=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)\n    return _create_focalnet('focalnet_huge_fl4', pretrained=pretrained, **model_kwargs)\n\n",
  "\"\"\" RepViT\n\nPaper: `RepViT: Revisiting Mobile CNN From ViT Perspective`\n    - https://arxiv.org/abs/2307.09283\n\n@misc{wang2023repvit,\n      title={RepViT: Revisiting Mobile CNN From ViT Perspective}, \n      author={Ao Wang and Hui Chen and Zijia Lin and Hengjun Pu and Guiguang Ding},\n      year={2023},\n      eprint={2307.09283},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\nAdapted from official impl at https://github.com/jameslahm/RepViT\n\"\"\"\n\n__all__ = ['RepViT']\n\nimport torch.nn as nn\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom ._registry import register_model, generate_default_cfgs\nfrom ._builder import build_model_with_cfg\nfrom timm.layers import SqueezeExcite, trunc_normal_, to_ntuple, to_2tuple\nfrom ._manipulate import checkpoint_seq\n\nimport torch\n\n\nclass ConvNorm(nn.Sequential):\n    def __init__(self, in_dim, out_dim, ks=1, stride=1, pad=0, dilation=1, groups=1, bn_weight_init=1):\n        super().__init__()\n        self.add_module('c', nn.Conv2d(in_dim, out_dim, ks, stride, pad, dilation, groups, bias=False))\n        self.add_module('bn', nn.BatchNorm2d(out_dim))\n        nn.init.constant_(self.bn.weight, bn_weight_init)\n        nn.init.constant_(self.bn.bias, 0)\n\n    @torch.no_grad()\n    def fuse(self):\n        c, bn = self._modules.values()\n        w = bn.weight / (bn.running_var + bn.eps) ** 0.5\n        w = c.weight * w[:, None, None, None]\n        b = bn.bias - bn.running_mean * bn.weight / (bn.running_var + bn.eps) ** 0.5\n        m = nn.Conv2d(\n            w.size(1) * self.c.groups,\n            w.size(0),\n            w.shape[2:],\n            stride=self.c.stride,\n            padding=self.c.padding,\n            dilation=self.c.dilation,\n            groups=self.c.groups,\n            device=c.weight.device,\n        )\n        m.weight.data.copy_(w)\n        m.bias.data.copy_(b)\n        return m\n\n\nclass NormLinear(nn.Sequential):\n    def __init__(self, in_dim, out_dim, bias=True, std=0.02):\n        super().__init__()\n        self.add_module('bn', nn.BatchNorm1d(in_dim))\n        self.add_module('l', nn.Linear(in_dim, out_dim, bias=bias))\n        trunc_normal_(self.l.weight, std=std)\n        if bias:\n            nn.init.constant_(self.l.bias, 0)\n\n    @torch.no_grad()\n    def fuse(self):\n        bn, l = self._modules.values()\n        w = bn.weight / (bn.running_var + bn.eps) ** 0.5\n        b = bn.bias - self.bn.running_mean * self.bn.weight / (bn.running_var + bn.eps) ** 0.5\n        w = l.weight * w[None, :]\n        if l.bias is None:\n            b = b @ self.l.weight.T\n        else:\n            b = (l.weight @ b[:, None]).view(-1) + self.l.bias\n        m = nn.Linear(w.size(1), w.size(0), device=l.weight.device)\n        m.weight.data.copy_(w)\n        m.bias.data.copy_(b)\n        return m\n\n\nclass RepVGGDW(nn.Module):\n    def __init__(self, ed, kernel_size):\n        super().__init__()\n        self.conv = ConvNorm(ed, ed, kernel_size, 1, (kernel_size - 1) // 2, groups=ed)\n        self.conv1 = ConvNorm(ed, ed, 1, 1, 0, groups=ed)\n        self.dim = ed\n\n    def forward(self, x):\n        return self.conv(x) + self.conv1(x) + x\n\n    @torch.no_grad()\n    def fuse(self):\n        conv = self.conv.fuse()\n        conv1 = self.conv1.fuse()\n\n        conv_w = conv.weight\n        conv_b = conv.bias\n        conv1_w = conv1.weight\n        conv1_b = conv1.bias\n\n        conv1_w = nn.functional.pad(conv1_w, [1, 1, 1, 1])\n\n        identity = nn.functional.pad(\n            torch.ones(conv1_w.shape[0], conv1_w.shape[1], 1, 1, device=conv1_w.device), [1, 1, 1, 1]\n        )\n\n        final_conv_w = conv_w + conv1_w + identity\n        final_conv_b = conv_b + conv1_b\n\n        conv.weight.data.copy_(final_conv_w)\n        conv.bias.data.copy_(final_conv_b)\n        return conv\n\n\nclass RepViTMlp(nn.Module):\n    def __init__(self, in_dim, hidden_dim, act_layer):\n        super().__init__()\n        self.conv1 = ConvNorm(in_dim, hidden_dim, 1, 1, 0)\n        self.act = act_layer()\n        self.conv2 = ConvNorm(hidden_dim, in_dim, 1, 1, 0, bn_weight_init=0)\n\n    def forward(self, x):\n        return self.conv2(self.act(self.conv1(x)))\n\n\nclass RepViTBlock(nn.Module):\n    def __init__(self, in_dim, mlp_ratio, kernel_size, use_se, act_layer):\n        super(RepViTBlock, self).__init__()\n\n        self.token_mixer = RepVGGDW(in_dim, kernel_size)\n        self.se = SqueezeExcite(in_dim, 0.25) if use_se else nn.Identity()\n        self.channel_mixer = RepViTMlp(in_dim, in_dim * mlp_ratio, act_layer)\n\n    def forward(self, x):\n        x = self.token_mixer(x)\n        x = self.se(x)\n        identity = x\n        x = self.channel_mixer(x)\n        return identity + x\n\n\nclass RepViTStem(nn.Module):\n    def __init__(self, in_chs, out_chs, act_layer):\n        super().__init__()\n        self.conv1 = ConvNorm(in_chs, out_chs // 2, 3, 2, 1)\n        self.act1 = act_layer()\n        self.conv2 = ConvNorm(out_chs // 2, out_chs, 3, 2, 1)\n        self.stride = 4\n\n    def forward(self, x):\n        return self.conv2(self.act1(self.conv1(x)))\n\n\nclass RepViTDownsample(nn.Module):\n    def __init__(self, in_dim, mlp_ratio, out_dim, kernel_size, act_layer):\n        super().__init__()\n        self.pre_block = RepViTBlock(in_dim, mlp_ratio, kernel_size, use_se=False, act_layer=act_layer)\n        self.spatial_downsample = ConvNorm(in_dim, in_dim, kernel_size, 2, (kernel_size - 1) // 2, groups=in_dim)\n        self.channel_downsample = ConvNorm(in_dim, out_dim, 1, 1)\n        self.ffn = RepViTMlp(out_dim, out_dim * mlp_ratio, act_layer)\n\n    def forward(self, x):\n        x = self.pre_block(x)\n        x = self.spatial_downsample(x)\n        x = self.channel_downsample(x)\n        identity = x\n        x = self.ffn(x)\n        return x + identity\n\n\nclass RepViTClassifier(nn.Module):\n    def __init__(self, dim, num_classes, distillation=False):\n        super().__init__()\n        self.head = NormLinear(dim, num_classes) if num_classes > 0 else nn.Identity()\n        self.distillation = distillation\n        if distillation:\n            self.head_dist = NormLinear(dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward(self, x):\n        if self.distillation:\n            x1, x2 = self.head(x), self.head_dist(x)\n            if (not self.training) or torch.jit.is_scripting():\n                return (x1 + x2) / 2\n            else:\n                return x1, x2\n        else:\n            x = self.head(x)\n            return x\n\n    @torch.no_grad()\n    def fuse(self):\n        if not self.num_classes > 0:\n            return nn.Identity()\n        head = self.head.fuse()\n        if self.distillation:\n            head_dist = self.head_dist.fuse()\n            head.weight += head_dist.weight\n            head.bias += head_dist.bias\n            head.weight /= 2\n            head.bias /= 2\n            return head\n        else:\n            return head\n\n\nclass RepViTStage(nn.Module):\n    def __init__(self, in_dim, out_dim, depth, mlp_ratio, act_layer, kernel_size=3, downsample=True):\n        super().__init__()\n        if downsample:\n            self.downsample = RepViTDownsample(in_dim, mlp_ratio, out_dim, kernel_size, act_layer)\n        else:\n            assert in_dim == out_dim\n            self.downsample = nn.Identity()\n\n        blocks = []\n        use_se = True\n        for _ in range(depth):\n            blocks.append(RepViTBlock(out_dim, mlp_ratio, kernel_size, use_se, act_layer))\n            use_se = not use_se\n\n        self.blocks = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        x = self.downsample(x)\n        x = self.blocks(x)\n        return x\n\n\nclass RepViT(nn.Module):\n    def __init__(\n        self,\n        in_chans=3,\n        img_size=224,\n        embed_dim=(48,),\n        depth=(2,),\n        mlp_ratio=2,\n        global_pool='avg',\n        kernel_size=3,\n        num_classes=1000,\n        act_layer=nn.GELU,\n        distillation=True,\n    ):\n        super(RepViT, self).__init__()\n        self.grad_checkpointing = False\n        self.global_pool = global_pool\n        self.embed_dim = embed_dim\n        self.num_classes = num_classes\n\n        in_dim = embed_dim[0]\n        self.stem = RepViTStem(in_chans, in_dim, act_layer)\n        stride = self.stem.stride\n        resolution = tuple([i // p for i, p in zip(to_2tuple(img_size), to_2tuple(stride))])\n\n        num_stages = len(embed_dim)\n        mlp_ratios = to_ntuple(num_stages)(mlp_ratio)\n\n        self.feature_info = []\n        stages = []\n        for i in range(num_stages):\n            downsample = True if i != 0 else False\n            stages.append(\n                RepViTStage(\n                    in_dim,\n                    embed_dim[i],\n                    depth[i],\n                    mlp_ratio=mlp_ratios[i],\n                    act_layer=act_layer,\n                    kernel_size=kernel_size,\n                    downsample=downsample,\n                )\n            )\n            stage_stride = 2 if downsample else 1\n            stride *= stage_stride\n            resolution = tuple([(r - 1) // stage_stride + 1 for r in resolution])\n            self.feature_info += [dict(num_chs=embed_dim[i], reduction=stride, module=f'stages.{i}')]\n            in_dim = embed_dim[i]\n        self.stages = nn.Sequential(*stages)\n\n        self.num_features = embed_dim[-1]\n        self.head = RepViTClassifier(embed_dim[-1], num_classes, distillation)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^stem',  # stem and embed\n            blocks=[(r'^blocks\\.(\\d+)', None), (r'^norm', (99999,))]\n        )\n        return matcher\n    \n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=None, distillation=False):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = (\n            RepViTClassifier(self.embed_dim[-1], num_classes, distillation) if num_classes > 0 else nn.Identity()\n        )\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.stages, x)\n        else:\n            x = self.stages(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool == 'avg':\n            x = nn.functional.adaptive_avg_pool2d(x, 1).flatten(1)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n    @torch.no_grad()\n    def fuse(self):\n        def fuse_children(net):\n            for child_name, child in net.named_children():\n                if hasattr(child, 'fuse'):\n                    fused = child.fuse()\n                    setattr(net, child_name, fused)\n                    fuse_children(fused)\n                else:\n                    fuse_children(child)\n\n        fuse_children(self)\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000,\n        'input_size': (3, 224, 224),\n        'pool_size': (7, 7),\n        'crop_pct': 0.95,\n        'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN,\n        'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.conv1.c',\n        'classifier': ('head.head.l', 'head.head_dist.l'),\n        **kwargs,\n    }\n\n\ndefault_cfgs = generate_default_cfgs(\n    {\n        'repvit_m1.dist_in1k': _cfg(\n            url='https://github.com/THU-MIG/RepViT/releases/download/v1.0/repvit_m1_distill_300_timm.pth'\n        ),\n        'repvit_m2.dist_in1k': _cfg(\n            url='https://github.com/THU-MIG/RepViT/releases/download/v1.0/repvit_m2_distill_300_timm.pth'\n        ),\n        'repvit_m3.dist_in1k': _cfg(\n            url='https://github.com/THU-MIG/RepViT/releases/download/v1.0/repvit_m3_distill_300_timm.pth'\n        ),\n    }\n)\n\n\ndef _create_repvit(variant, pretrained=False, **kwargs):\n    out_indices = kwargs.pop('out_indices', (0, 1, 2, 3))\n    model = build_model_with_cfg(\n        RepViT, variant, pretrained, feature_cfg=dict(flatten_sequential=True, out_indices=out_indices), **kwargs\n    )\n    return model\n\n\n@register_model\ndef repvit_m1(pretrained=False, **kwargs):\n    \"\"\"\n    Constructs a RepViT-M1 model\n    \"\"\"\n    model_args = dict(embed_dim=(48, 96, 192, 384), depth=(2, 2, 14, 2))\n    return _create_repvit('repvit_m1', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef repvit_m2(pretrained=False, **kwargs):\n    \"\"\"\n    Constructs a RepViT-M2 model\n    \"\"\"\n    model_args = dict(embed_dim=(64, 128, 256, 512), depth=(2, 2, 12, 2))\n    return _create_repvit('repvit_m2', pretrained=pretrained, **dict(model_args, **kwargs))\n\n\n@register_model\ndef repvit_m3(pretrained=False, **kwargs):\n    \"\"\"\n    Constructs a RepViT-M3 model\n    \"\"\"\n    model_args = dict(embed_dim=(64, 128, 256, 512), depth=(4, 4, 18, 2))\n    return _create_repvit('repvit_m3', pretrained=pretrained, **dict(model_args, **kwargs))\n",
  "\"\"\" HRNet\n\nCopied from https://github.com/HRNet/HRNet-Image-Classification\n\nOriginal header:\n  Copyright (c) Microsoft\n  Licensed under the MIT License.\n  Written by Bin Xiao (Bin.Xiao@microsoft.com)\n  Modified by Ke Sun (sunk@mail.ustc.edu.cn)\n\"\"\"\nimport logging\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.layers import create_classifier\nfrom ._builder import build_model_with_cfg, pretrained_cfg_for_features\nfrom ._features import FeatureInfo\nfrom ._registry import register_model, generate_default_cfgs\nfrom .resnet import BasicBlock, Bottleneck  # leveraging ResNet block_types w/ additional features like SE\n\n__all__ = ['HighResolutionNet', 'HighResolutionNetFeatures']  # model_registry will add each entrypoint fn to this\n\n_BN_MOMENTUM = 0.1\n_logger = logging.getLogger(__name__)\n\n\ncfg_cls = dict(\n    hrnet_w18_small=dict(\n        stem_width=64,\n        stage1=dict(\n            num_modules=1,\n            num_branches=1,\n            block_type='BOTTLENECK',\n            num_blocks=(1,),\n            num_channels=(32,),\n            fuse_method='SUM',\n        ),\n        stage2=dict(\n            num_modules=1,\n            num_branches=2,\n            block_type='BASIC',\n            num_blocks=(2, 2),\n            num_channels=(16, 32),\n            fuse_method='SUM'\n        ),\n        stage3=dict(\n            num_modules=1,\n            num_branches=3,\n            block_type='BASIC',\n            num_blocks=(2, 2, 2),\n            num_channels=(16, 32, 64),\n            fuse_method='SUM'\n        ),\n        stage4=dict(\n            num_modules=1,\n            num_branches=4,\n            block_type='BASIC',\n            num_blocks=(2, 2, 2, 2),\n            num_channels=(16, 32, 64, 128),\n            fuse_method='SUM',\n        ),\n    ),\n\n    hrnet_w18_small_v2=dict(\n        stem_width=64,\n        stage1=dict(\n            num_modules=1,\n            num_branches=1,\n            block_type='BOTTLENECK',\n            num_blocks=(2,),\n            num_channels=(64,),\n            fuse_method='SUM',\n        ),\n        stage2=dict(\n            num_modules=1,\n            num_branches=2,\n            block_type='BASIC',\n            num_blocks=(2, 2),\n            num_channels=(18, 36),\n            fuse_method='SUM'\n        ),\n        stage3=dict(\n            num_modules=3,\n            num_branches=3,\n            block_type='BASIC',\n            num_blocks=(2, 2, 2),\n            num_channels=(18, 36, 72),\n            fuse_method='SUM'\n        ),\n        stage4=dict(\n            num_modules=2,\n            num_branches=4,\n            block_type='BASIC',\n            num_blocks=(2, 2, 2, 2),\n            num_channels=(18, 36, 72, 144),\n            fuse_method='SUM',\n        ),\n    ),\n\n    hrnet_w18=dict(\n        stem_width=64,\n        stage1=dict(\n            num_modules=1,\n            num_branches=1,\n            block_type='BOTTLENECK',\n            num_blocks=(4,),\n            num_channels=(64,),\n            fuse_method='SUM',\n        ),\n        stage2=dict(\n            num_modules=1,\n            num_branches=2,\n            block_type='BASIC',\n            num_blocks=(4, 4),\n            num_channels=(18, 36),\n            fuse_method='SUM'\n        ),\n        stage3=dict(\n            num_modules=4,\n            num_branches=3,\n            block_type='BASIC',\n            num_blocks=(4, 4, 4),\n            num_channels=(18, 36, 72),\n            fuse_method='SUM'\n        ),\n        stage4=dict(\n            num_modules=3,\n            num_branches=4,\n            block_type='BASIC',\n            num_blocks=(4, 4, 4, 4),\n            num_channels=(18, 36, 72, 144),\n            fuse_method='SUM',\n        ),\n    ),\n\n    hrnet_w30=dict(\n        stem_width=64,\n        stage1=dict(\n            num_modules=1,\n            num_branches=1,\n            block_type='BOTTLENECK',\n            num_blocks=(4,),\n            num_channels=(64,),\n            fuse_method='SUM',\n        ),\n        stage2=dict(\n            num_modules=1,\n            num_branches=2,\n            block_type='BASIC',\n            num_blocks=(4, 4),\n            num_channels=(30, 60),\n            fuse_method='SUM'\n        ),\n        stage3=dict(\n            num_modules=4,\n            num_branches=3,\n            block_type='BASIC',\n            num_blocks=(4, 4, 4),\n            num_channels=(30, 60, 120),\n            fuse_method='SUM'\n        ),\n        stage4=dict(\n            num_modules=3,\n            num_branches=4,\n            block_type='BASIC',\n            num_blocks=(4, 4, 4, 4),\n            num_channels=(30, 60, 120, 240),\n            fuse_method='SUM',\n        ),\n    ),\n\n    hrnet_w32=dict(\n        stem_width=64,\n        stage1=dict(\n            num_modules=1,\n            num_branches=1,\n            block_type='BOTTLENECK',\n            num_blocks=(4,),\n            num_channels=(64,),\n            fuse_method='SUM',\n        ),\n        stage2=dict(\n            num_modules=1,\n            num_branches=2,\n            block_type='BASIC',\n            num_blocks=(4, 4),\n            num_channels=(32, 64),\n            fuse_method='SUM'\n        ),\n        stage3=dict(\n            num_modules=4,\n            num_branches=3,\n            block_type='BASIC',\n            num_blocks=(4, 4, 4),\n            num_channels=(32, 64, 128),\n            fuse_method='SUM'\n        ),\n        stage4=dict(\n            num_modules=3,\n            num_branches=4,\n            block_type='BASIC',\n            num_blocks=(4, 4, 4, 4),\n            num_channels=(32, 64, 128, 256),\n            fuse_method='SUM',\n        ),\n    ),\n\n    hrnet_w40=dict(\n        stem_width=64,\n        stage1=dict(\n            num_modules=1,\n            num_branches=1,\n            block_type='BOTTLENECK',\n            num_blocks=(4,),\n            num_channels=(64,),\n            fuse_method='SUM',\n        ),\n        stage2=dict(\n            num_modules=1,\n            num_branches=2,\n            block_type='BASIC',\n            num_blocks=(4, 4),\n            num_channels=(40, 80),\n            fuse_method='SUM'\n        ),\n        stage3=dict(\n            num_modules=4,\n            num_branches=3,\n            block_type='BASIC',\n            num_blocks=(4, 4, 4),\n            num_channels=(40, 80, 160),\n            fuse_method='SUM'\n        ),\n        stage4=dict(\n            num_modules=3,\n            num_branches=4,\n            block_type='BASIC',\n            num_blocks=(4, 4, 4, 4),\n            num_channels=(40, 80, 160, 320),\n            fuse_method='SUM',\n        ),\n    ),\n\n    hrnet_w44=dict(\n        stem_width=64,\n        stage1=dict(\n            num_modules=1,\n            num_branches=1,\n            block_type='BOTTLENECK',\n            num_blocks=(4,),\n            num_channels=(64,),\n            fuse_method='SUM',\n        ),\n        stage2=dict(\n            num_modules=1,\n            num_branches=2,\n            block_type='BASIC',\n            num_blocks=(4, 4),\n            num_channels=(44, 88),\n            fuse_method='SUM'\n        ),\n        stage3=dict(\n            num_modules=4,\n            num_branches=3,\n            block_type='BASIC',\n            num_blocks=(4, 4, 4),\n            num_channels=(44, 88, 176),\n            fuse_method='SUM'\n        ),\n        stage4=dict(\n            num_modules=3,\n            num_branches=4,\n            block_type='BASIC',\n            num_blocks=(4, 4, 4, 4),\n            num_channels=(44, 88, 176, 352),\n            fuse_method='SUM',\n        ),\n    ),\n\n    hrnet_w48=dict(\n        stem_width=64,\n        stage1=dict(\n            num_modules=1,\n            num_branches=1,\n            block_type='BOTTLENECK',\n            num_blocks=(4,),\n            num_channels=(64,),\n            fuse_method='SUM',\n        ),\n        stage2=dict(\n            num_modules=1,\n            num_branches=2,\n            block_type='BASIC',\n            num_blocks=(4, 4),\n            num_channels=(48, 96),\n            fuse_method='SUM'\n        ),\n        stage3=dict(\n            num_modules=4,\n            num_branches=3,\n            block_type='BASIC',\n            num_blocks=(4, 4, 4),\n            num_channels=(48, 96, 192),\n            fuse_method='SUM'\n        ),\n        stage4=dict(\n            num_modules=3,\n            num_branches=4,\n            block_type='BASIC',\n            num_blocks=(4, 4, 4, 4),\n            num_channels=(48, 96, 192, 384),\n            fuse_method='SUM',\n        ),\n    ),\n\n    hrnet_w64=dict(\n        stem_width=64,\n        stage1=dict(\n            num_modules=1,\n            num_branches=1,\n            block_type='BOTTLENECK',\n            num_blocks=(4,),\n            num_channels=(64,),\n            fuse_method='SUM',\n        ),\n        stage2=dict(\n            num_modules=1,\n            num_branches=2,\n            block_type='BASIC',\n            num_blocks=(4, 4),\n            num_channels=(64, 128),\n            fuse_method='SUM'\n        ),\n        stage3=dict(\n            num_modules=4,\n            num_branches=3,\n            block_type='BASIC',\n            num_blocks=(4, 4, 4),\n            num_channels=(64, 128, 256),\n            fuse_method='SUM'\n        ),\n        stage4=dict(\n            num_modules=3,\n            num_branches=4,\n            block_type='BASIC',\n            num_blocks=(4, 4, 4, 4),\n            num_channels=(64, 128, 256, 512),\n            fuse_method='SUM',\n        ),\n    )\n)\n\n\nclass HighResolutionModule(nn.Module):\n    def __init__(\n            self,\n            num_branches,\n            block_types,\n            num_blocks,\n            num_in_chs,\n            num_channels,\n            fuse_method,\n            multi_scale_output=True,\n    ):\n        super(HighResolutionModule, self).__init__()\n        self._check_branches(\n            num_branches,\n            block_types,\n            num_blocks,\n            num_in_chs,\n            num_channels,\n        )\n\n        self.num_in_chs = num_in_chs\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches,\n            block_types,\n            num_blocks,\n            num_channels,\n        )\n        self.fuse_layers = self._make_fuse_layers()\n        self.fuse_act = nn.ReLU(False)\n\n    def _check_branches(self, num_branches, block_types, num_blocks, num_in_chs, num_channels):\n        error_msg = ''\n        if num_branches != len(num_blocks):\n            error_msg = 'num_branches({}) <> num_blocks({})'.format(num_branches, len(num_blocks))\n        elif num_branches != len(num_channels):\n            error_msg = 'num_branches({}) <> num_channels({})'.format(num_branches, len(num_channels))\n        elif num_branches != len(num_in_chs):\n            error_msg = 'num_branches({}) <> num_in_chs({})'.format(num_branches, len(num_in_chs))\n        if error_msg:\n            _logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self, branch_index, block_type, num_blocks, num_channels, stride=1):\n        downsample = None\n        if stride != 1 or self.num_in_chs[branch_index] != num_channels[branch_index] * block_type.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.num_in_chs[branch_index], num_channels[branch_index] * block_type.expansion,\n                    kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(num_channels[branch_index] * block_type.expansion, momentum=_BN_MOMENTUM),\n            )\n\n        layers = [block_type(self.num_in_chs[branch_index], num_channels[branch_index], stride, downsample)]\n        self.num_in_chs[branch_index] = num_channels[branch_index] * block_type.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(block_type(self.num_in_chs[branch_index], num_channels[branch_index]))\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block_type, num_blocks, num_channels):\n        branches = []\n        for i in range(num_branches):\n            branches.append(self._make_one_branch(i, block_type, num_blocks, num_channels))\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return nn.Identity()\n\n        num_branches = self.num_branches\n        num_in_chs = self.num_in_chs\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(nn.Sequential(\n                        nn.Conv2d(num_in_chs[j], num_in_chs[i], 1, 1, 0, bias=False),\n                        nn.BatchNorm2d(num_in_chs[i], momentum=_BN_MOMENTUM),\n                        nn.Upsample(scale_factor=2 ** (j - i), mode='nearest')))\n                elif j == i:\n                    fuse_layer.append(nn.Identity())\n                else:\n                    conv3x3s = []\n                    for k in range(i - j):\n                        if k == i - j - 1:\n                            num_out_chs_conv3x3 = num_in_chs[i]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_in_chs[j], num_out_chs_conv3x3, 3, 2, 1, bias=False),\n                                nn.BatchNorm2d(num_out_chs_conv3x3, momentum=_BN_MOMENTUM)\n                            ))\n                        else:\n                            num_out_chs_conv3x3 = num_in_chs[j]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_in_chs[j], num_out_chs_conv3x3, 3, 2, 1, bias=False),\n                                nn.BatchNorm2d(num_out_chs_conv3x3, momentum=_BN_MOMENTUM),\n                                nn.ReLU(False)\n                            ))\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_in_chs(self):\n        return self.num_in_chs\n\n    def forward(self, x: List[torch.Tensor]) -> List[torch.Tensor]:\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i, branch in enumerate(self.branches):\n            x[i] = branch(x[i])\n\n        x_fuse = []\n        for i, fuse_outer in enumerate(self.fuse_layers):\n            y = None\n            for j, f in enumerate(fuse_outer):\n                if y is None:\n                    y = f(x[j])\n                else:\n                    y = y + f(x[j])\n            x_fuse.append(self.fuse_act(y))\n        return x_fuse\n\n\nclass SequentialList(nn.Sequential):\n\n    def __init__(self, *args):\n        super(SequentialList, self).__init__(*args)\n\n    @torch.jit._overload_method  # noqa: F811\n    def forward(self, x):\n        # type: (List[torch.Tensor]) -> (List[torch.Tensor])\n        pass\n\n    @torch.jit._overload_method  # noqa: F811\n    def forward(self, x):\n        # type: (torch.Tensor) -> (List[torch.Tensor])\n        pass\n\n    def forward(self, x) -> List[torch.Tensor]:\n        for module in self:\n            x = module(x)\n        return x\n\n\n@torch.jit.interface\nclass ModuleInterface(torch.nn.Module):\n    def forward(self, input: torch.Tensor) -> torch.Tensor: # `input` has a same name in Sequential forward\n        pass\n\n\nblock_types_dict = {\n    'BASIC': BasicBlock,\n    'BOTTLENECK': Bottleneck\n}\n\n\nclass HighResolutionNet(nn.Module):\n\n    def __init__(\n            self,\n            cfg,\n            in_chans=3,\n            num_classes=1000,\n            output_stride=32,\n            global_pool='avg',\n            drop_rate=0.0,\n            head='classification',\n            **kwargs,\n    ):\n        super(HighResolutionNet, self).__init__()\n        self.num_classes = num_classes\n        assert output_stride == 32  # FIXME support dilation\n\n        cfg.update(**kwargs)\n        stem_width = cfg['stem_width']\n        self.conv1 = nn.Conv2d(in_chans, stem_width, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stem_width, momentum=_BN_MOMENTUM)\n        self.act1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(stem_width, 64, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(64, momentum=_BN_MOMENTUM)\n        self.act2 = nn.ReLU(inplace=True)\n\n        self.stage1_cfg = cfg['stage1']\n        num_channels = self.stage1_cfg['num_channels'][0]\n        block_type = block_types_dict[self.stage1_cfg['block_type']]\n        num_blocks = self.stage1_cfg['num_blocks'][0]\n        self.layer1 = self._make_layer(block_type, 64, num_channels, num_blocks)\n        stage1_out_channel = block_type.expansion * num_channels\n\n        self.stage2_cfg = cfg['stage2']\n        num_channels = self.stage2_cfg['num_channels']\n        block_type = block_types_dict[self.stage2_cfg['block_type']]\n        num_channels = [num_channels[i] * block_type.expansion for i in range(len(num_channels))]\n        self.transition1 = self._make_transition_layer([stage1_out_channel], num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(self.stage2_cfg, num_channels)\n\n        self.stage3_cfg = cfg['stage3']\n        num_channels = self.stage3_cfg['num_channels']\n        block_type = block_types_dict[self.stage3_cfg['block_type']]\n        num_channels = [num_channels[i] * block_type.expansion for i in range(len(num_channels))]\n        self.transition2 = self._make_transition_layer(pre_stage_channels, num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(self.stage3_cfg, num_channels)\n\n        self.stage4_cfg = cfg['stage4']\n        num_channels = self.stage4_cfg['num_channels']\n        block_type = block_types_dict[self.stage4_cfg['block_type']]\n        num_channels = [num_channels[i] * block_type.expansion for i in range(len(num_channels))]\n        self.transition3 = self._make_transition_layer(pre_stage_channels, num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(self.stage4_cfg, num_channels, multi_scale_output=True)\n\n        self.head = head\n        self.head_channels = None  # set if _make_head called\n        head_conv_bias = cfg.pop('head_conv_bias', True)\n        if head == 'classification':\n            # Classification Head\n            self.num_features = 2048\n            self.incre_modules, self.downsamp_modules, self.final_layer = self._make_head(\n                pre_stage_channels,\n                conv_bias=head_conv_bias,\n            )\n            self.global_pool, self.head_drop, self.classifier = create_classifier(\n                self.num_features,\n                self.num_classes,\n                pool_type=global_pool,\n                drop_rate=drop_rate,\n            )\n        else:\n            if head == 'incre':\n                self.num_features = 2048\n                self.incre_modules, _, _ = self._make_head(pre_stage_channels, incre_only=True)\n            else:\n                self.num_features = 256\n                self.incre_modules = None\n            self.global_pool = nn.Identity()\n            self.head_drop = nn.Identity()\n            self.classifier = nn.Identity()\n\n        curr_stride = 2\n        # module names aren't actually valid here, hook or FeatureNet based extraction would not work\n        self.feature_info = [dict(num_chs=64, reduction=curr_stride, module='stem')]\n        for i, c in enumerate(self.head_channels if self.head_channels else num_channels):\n            curr_stride *= 2\n            c = c * 4 if self.head_channels else c  # head block_type expansion factor of 4\n            self.feature_info += [dict(num_chs=c, reduction=curr_stride, module=f'stage{i + 1}')]\n\n        self.init_weights()\n\n    def _make_head(self, pre_stage_channels, incre_only=False, conv_bias=True):\n        head_block_type = Bottleneck\n        self.head_channels = [32, 64, 128, 256]\n\n        # Increasing the #channels on each resolution\n        # from C, 2C, 4C, 8C to 128, 256, 512, 1024\n        incre_modules = []\n        for i, channels in enumerate(pre_stage_channels):\n            incre_modules.append(self._make_layer(head_block_type, channels, self.head_channels[i], 1, stride=1))\n        incre_modules = nn.ModuleList(incre_modules)\n        if incre_only:\n            return incre_modules, None, None\n\n        # downsampling modules\n        downsamp_modules = []\n        for i in range(len(pre_stage_channels) - 1):\n            in_channels = self.head_channels[i] * head_block_type.expansion\n            out_channels = self.head_channels[i + 1] * head_block_type.expansion\n            downsamp_module = nn.Sequential(\n                nn.Conv2d(\n                    in_channels=in_channels, out_channels=out_channels,\n                    kernel_size=3, stride=2, padding=1, bias=conv_bias),\n                nn.BatchNorm2d(out_channels, momentum=_BN_MOMENTUM),\n                nn.ReLU(inplace=True)\n            )\n            downsamp_modules.append(downsamp_module)\n        downsamp_modules = nn.ModuleList(downsamp_modules)\n\n        final_layer = nn.Sequential(\n            nn.Conv2d(\n                in_channels=self.head_channels[3] * head_block_type.expansion, out_channels=self.num_features,\n                kernel_size=1, stride=1, padding=0, bias=conv_bias),\n            nn.BatchNorm2d(self.num_features, momentum=_BN_MOMENTUM),\n            nn.ReLU(inplace=True)\n        )\n\n        return incre_modules, downsamp_modules, final_layer\n\n    def _make_transition_layer(self, num_channels_pre_layer, num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(nn.Sequential(\n                        nn.Conv2d(num_channels_pre_layer[i], num_channels_cur_layer[i], 3, 1, 1, bias=False),\n                        nn.BatchNorm2d(num_channels_cur_layer[i], momentum=_BN_MOMENTUM),\n                        nn.ReLU(inplace=True)))\n                else:\n                    transition_layers.append(nn.Identity())\n            else:\n                conv3x3s = []\n                for j in range(i + 1 - num_branches_pre):\n                    _in_chs = num_channels_pre_layer[-1]\n                    _out_chs = num_channels_cur_layer[i] if j == i - num_branches_pre else _in_chs\n                    conv3x3s.append(nn.Sequential(\n                        nn.Conv2d(_in_chs, _out_chs, 3, 2, 1, bias=False),\n                        nn.BatchNorm2d(_out_chs, momentum=_BN_MOMENTUM),\n                        nn.ReLU(inplace=True)))\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block_type, inplanes, planes, block_types, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block_type.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block_type.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block_type.expansion, momentum=_BN_MOMENTUM),\n            )\n\n        layers = [block_type(inplanes, planes, stride, downsample)]\n        inplanes = planes * block_type.expansion\n        for i in range(1, block_types):\n            layers.append(block_type(inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, num_in_chs, multi_scale_output=True):\n        num_modules = layer_config['num_modules']\n        num_branches = layer_config['num_branches']\n        num_blocks = layer_config['num_blocks']\n        num_channels = layer_config['num_channels']\n        block_type = block_types_dict[layer_config['block_type']]\n        fuse_method = layer_config['fuse_method']\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            reset_multi_scale_output = multi_scale_output or i < num_modules - 1\n            modules.append(HighResolutionModule(\n                num_branches, block_type, num_blocks, num_in_chs, num_channels, fuse_method, reset_multi_scale_output)\n            )\n            num_in_chs = modules[-1].get_num_in_chs()\n\n        return SequentialList(*modules), num_in_chs\n\n    @torch.jit.ignore\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(\n            stem=r'^conv[12]|bn[12]',\n            block_types=r'^(?:layer|stage|transition)(\\d+)' if coarse else [\n                (r'^layer(\\d+)\\.(\\d+)', None),\n                (r'^stage(\\d+)\\.(\\d+)', None),\n                (r'^transition(\\d+)', (99999,)),\n            ],\n        )\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, \"gradient checkpointing not supported\"\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.classifier\n\n    def reset_classifier(self, num_classes, global_pool='avg'):\n        self.num_classes = num_classes\n        self.global_pool, self.classifier = create_classifier(\n            self.num_features, self.num_classes, pool_type=global_pool)\n\n    def stages(self, x) -> List[torch.Tensor]:\n        x = self.layer1(x)\n\n        xl = [t(x) for i, t in enumerate(self.transition1)]\n        yl = self.stage2(xl)\n\n        xl = [t(yl[-1]) if not isinstance(t, nn.Identity) else yl[i] for i, t in enumerate(self.transition2)]\n        yl = self.stage3(xl)\n\n        xl = [t(yl[-1]) if not isinstance(t, nn.Identity) else yl[i] for i, t in enumerate(self.transition3)]\n        yl = self.stage4(xl)\n        return yl\n\n    def forward_features(self, x):\n        # Stem\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.act2(x)\n\n        # Stages\n        yl = self.stages(x)\n        if self.incre_modules is None or self.downsamp_modules is None:\n            return yl\n\n        y = None\n        for i, incre in enumerate(self.incre_modules):\n            if y is None:\n                y = incre(yl[i])\n            else:\n                down: ModuleInterface = self.downsamp_modules[i - 1]  # needed for torchscript module indexing\n                y = incre(yl[i]) + down.forward(y)\n\n        y = self.final_layer(y)\n        return y\n\n    def forward_head(self, x, pre_logits: bool = False):\n        # Classification Head\n        x = self.global_pool(x)\n        x = self.head_drop(x)\n        return x if pre_logits else self.classifier(x)\n\n    def forward(self, x):\n        y = self.forward_features(x)\n        x = self.forward_head(y)\n        return x\n\n\nclass HighResolutionNetFeatures(HighResolutionNet):\n    \"\"\"HighResolutionNet feature extraction\n\n    The design of HRNet makes it easy to grab feature maps, this class provides a simple wrapper to do so.\n    It would be more complicated to use the FeatureNet helpers.\n\n    The `feature_location=incre` allows grabbing increased channel count features using part of the\n    classification head. If `feature_location=''` the default HRNet features are returned. First stem\n    conv is used for stride 2 features.\n    \"\"\"\n\n    def __init__(\n            self,\n            cfg,\n            in_chans=3,\n            num_classes=1000,\n            output_stride=32,\n            global_pool='avg',\n            drop_rate=0.0,\n            feature_location='incre',\n            out_indices=(0, 1, 2, 3, 4),\n            **kwargs,\n    ):\n        assert feature_location in ('incre', '')\n        super(HighResolutionNetFeatures, self).__init__(\n            cfg,\n            in_chans=in_chans,\n            num_classes=num_classes,\n            output_stride=output_stride,\n            global_pool=global_pool,\n            drop_rate=drop_rate,\n            head=feature_location,\n            **kwargs,\n        )\n        self.feature_info = FeatureInfo(self.feature_info, out_indices)\n        self._out_idx = {f['index'] for f in self.feature_info.get_dicts()}\n\n    def forward_features(self, x):\n        assert False, 'Not supported'\n\n    def forward(self, x) -> List[torch.tensor]:\n        out = []\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        if 0 in self._out_idx:\n            out.append(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.act2(x)\n        x = self.stages(x)\n        if self.incre_modules is not None:\n            x = [incre(f) for f, incre in zip(x, self.incre_modules)]\n        for i, f in enumerate(x):\n            if i + 1 in self._out_idx:\n                out.append(f)\n        return out\n\n\ndef _create_hrnet(variant, pretrained=False, cfg_variant=None, **model_kwargs):\n    model_cls = HighResolutionNet\n    features_only = False\n    kwargs_filter = None\n    if model_kwargs.pop('features_only', False):\n        model_cls = HighResolutionNetFeatures\n        kwargs_filter = ('num_classes', 'global_pool')\n        features_only = True\n    cfg_variant = cfg_variant or variant\n    model = build_model_with_cfg(\n        model_cls,\n        variant,\n        pretrained,\n        model_cfg=cfg_cls[cfg_variant],\n        pretrained_strict=not features_only,\n        kwargs_filter=kwargs_filter,\n        **model_kwargs,\n    )\n    if features_only:\n        model.pretrained_cfg = pretrained_cfg_for_features(model.default_cfg)\n        model.default_cfg = model.pretrained_cfg  # backwards compat\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bilinear',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'conv1', 'classifier': 'classifier',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'hrnet_w18_small.ms_in1k': _cfg(hf_hub_id='timm/'),\n    'hrnet_w18_small_v2.ms_in1k': _cfg(hf_hub_id='timm/'),\n    'hrnet_w18.ms_aug_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.95,\n    ),\n    'hrnet_w18.ms_in1k': _cfg(hf_hub_id='timm/'),\n    'hrnet_w30.ms_in1k': _cfg(hf_hub_id='timm/'),\n    'hrnet_w32.ms_in1k': _cfg(hf_hub_id='timm/'),\n    'hrnet_w40.ms_in1k': _cfg(hf_hub_id='timm/'),\n    'hrnet_w44.ms_in1k': _cfg(hf_hub_id='timm/'),\n    'hrnet_w48.ms_in1k': _cfg(hf_hub_id='timm/'),\n    'hrnet_w64.ms_in1k': _cfg(hf_hub_id='timm/'),\n\n    'hrnet_w18_ssld.paddle_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.95, test_crop_pct=1.0, test_input_size=(3, 288, 288)\n    ),\n    'hrnet_w48_ssld.paddle_in1k': _cfg(\n        hf_hub_id='timm/',\n        crop_pct=0.95, test_crop_pct=1.0, test_input_size=(3, 288, 288)\n    ),\n})\n\n\n@register_model\ndef hrnet_w18_small(pretrained=False, **kwargs) -> HighResolutionNet:\n    return _create_hrnet('hrnet_w18_small', pretrained, **kwargs)\n\n\n@register_model\ndef hrnet_w18_small_v2(pretrained=False, **kwargs) -> HighResolutionNet:\n    return _create_hrnet('hrnet_w18_small_v2', pretrained, **kwargs)\n\n\n@register_model\ndef hrnet_w18(pretrained=False, **kwargs) -> HighResolutionNet:\n    return _create_hrnet('hrnet_w18', pretrained, **kwargs)\n\n\n@register_model\ndef hrnet_w30(pretrained=False, **kwargs) -> HighResolutionNet:\n    return _create_hrnet('hrnet_w30', pretrained, **kwargs)\n\n\n@register_model\ndef hrnet_w32(pretrained=False, **kwargs) -> HighResolutionNet:\n    return _create_hrnet('hrnet_w32', pretrained, **kwargs)\n\n\n@register_model\ndef hrnet_w40(pretrained=False, **kwargs) -> HighResolutionNet:\n    return _create_hrnet('hrnet_w40', pretrained, **kwargs)\n\n\n@register_model\ndef hrnet_w44(pretrained=False, **kwargs) -> HighResolutionNet:\n    return _create_hrnet('hrnet_w44', pretrained, **kwargs)\n\n\n@register_model\ndef hrnet_w48(pretrained=False, **kwargs) -> HighResolutionNet:\n    return _create_hrnet('hrnet_w48', pretrained, **kwargs)\n\n\n@register_model\ndef hrnet_w64(pretrained=False, **kwargs) -> HighResolutionNet:\n    return _create_hrnet('hrnet_w64', pretrained, **kwargs)\n\n\n@register_model\ndef hrnet_w18_ssld(pretrained=False, **kwargs) -> HighResolutionNet:\n    kwargs.setdefault('head_conv_bias', False)\n    return _create_hrnet('hrnet_w18_ssld', cfg_variant='hrnet_w18', pretrained=pretrained, **kwargs)\n\n\n@register_model\ndef hrnet_w48_ssld(pretrained=False, **kwargs) -> HighResolutionNet:\n    kwargs.setdefault('head_conv_bias', False)\n    return _create_hrnet('hrnet_w48_ssld', cfg_variant='hrnet_w48', pretrained=pretrained, **kwargs)\n\n",
  "\"\"\" Sequencer\n\nPaper: `Sequencer: Deep LSTM for Image Classification` - https://arxiv.org/pdf/2205.01972.pdf\n\n\"\"\"\n#  Copyright (c) 2022. Yuki Tatsunami\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n\nimport math\nfrom functools import partial\nfrom itertools import accumulate\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, DEFAULT_CROP_PCT\nfrom timm.layers import lecun_normal_, DropPath, Mlp, PatchEmbed, ClassifierHead\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import named_apply\nfrom ._registry import register_model, generate_default_cfgs\n\n__all__ = ['Sequencer2d']  # model_registry will add each entrypoint fn to this\n\n\ndef _init_weights(module: nn.Module, name: str, head_bias: float = 0., flax=False):\n    if isinstance(module, nn.Linear):\n        if name.startswith('head'):\n            nn.init.zeros_(module.weight)\n            nn.init.constant_(module.bias, head_bias)\n        else:\n            if flax:\n                # Flax defaults\n                lecun_normal_(module.weight)\n                if module.bias is not None:\n                    nn.init.zeros_(module.bias)\n            else:\n                nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    if 'mlp' in name:\n                        nn.init.normal_(module.bias, std=1e-6)\n                    else:\n                        nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Conv2d):\n        lecun_normal_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d, nn.GroupNorm)):\n        nn.init.ones_(module.weight)\n        nn.init.zeros_(module.bias)\n    elif isinstance(module, (nn.RNN, nn.GRU, nn.LSTM)):\n        stdv = 1.0 / math.sqrt(module.hidden_size)\n        for weight in module.parameters():\n            nn.init.uniform_(weight, -stdv, stdv)\n    elif hasattr(module, 'init_weights'):\n        module.init_weights()\n\n\nclass RNNIdentity(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(RNNIdentity, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, None]:\n        return x, None\n\n\nclass RNN2dBase(nn.Module):\n\n    def __init__(\n            self,\n            input_size: int,\n            hidden_size: int,\n            num_layers: int = 1,\n            bias: bool = True,\n            bidirectional: bool = True,\n            union=\"cat\",\n            with_fc=True,\n    ):\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = 2 * hidden_size if bidirectional else hidden_size\n        self.union = union\n\n        self.with_vertical = True\n        self.with_horizontal = True\n        self.with_fc = with_fc\n\n        self.fc = None\n        if with_fc:\n            if union == \"cat\":\n                self.fc = nn.Linear(2 * self.output_size, input_size)\n            elif union == \"add\":\n                self.fc = nn.Linear(self.output_size, input_size)\n            elif union == \"vertical\":\n                self.fc = nn.Linear(self.output_size, input_size)\n                self.with_horizontal = False\n            elif union == \"horizontal\":\n                self.fc = nn.Linear(self.output_size, input_size)\n                self.with_vertical = False\n            else:\n                raise ValueError(\"Unrecognized union: \" + union)\n        elif union == \"cat\":\n            pass\n            if 2 * self.output_size != input_size:\n                raise ValueError(f\"The output channel {2 * self.output_size} is different from the input channel {input_size}.\")\n        elif union == \"add\":\n            pass\n            if self.output_size != input_size:\n                raise ValueError(f\"The output channel {self.output_size} is different from the input channel {input_size}.\")\n        elif union == \"vertical\":\n            if self.output_size != input_size:\n                raise ValueError(f\"The output channel {self.output_size} is different from the input channel {input_size}.\")\n            self.with_horizontal = False\n        elif union == \"horizontal\":\n            if self.output_size != input_size:\n                raise ValueError(f\"The output channel {self.output_size} is different from the input channel {input_size}.\")\n            self.with_vertical = False\n        else:\n            raise ValueError(\"Unrecognized union: \" + union)\n\n        self.rnn_v = RNNIdentity()\n        self.rnn_h = RNNIdentity()\n\n    def forward(self, x):\n        B, H, W, C = x.shape\n\n        if self.with_vertical:\n            v = x.permute(0, 2, 1, 3)\n            v = v.reshape(-1, H, C)\n            v, _ = self.rnn_v(v)\n            v = v.reshape(B, W, H, -1)\n            v = v.permute(0, 2, 1, 3)\n        else:\n            v = None\n\n        if self.with_horizontal:\n            h = x.reshape(-1, W, C)\n            h, _ = self.rnn_h(h)\n            h = h.reshape(B, H, W, -1)\n        else:\n            h = None\n\n        if v is not None and h is not None:\n            if self.union == \"cat\":\n                x = torch.cat([v, h], dim=-1)\n            else:\n                x = v + h\n        elif v is not None:\n            x = v\n        elif h is not None:\n            x = h\n\n        if self.fc is not None:\n            x = self.fc(x)\n\n        return x\n\n\nclass LSTM2d(RNN2dBase):\n\n    def __init__(\n            self,\n            input_size: int,\n            hidden_size: int,\n            num_layers: int = 1,\n            bias: bool = True,\n            bidirectional: bool = True,\n            union=\"cat\",\n            with_fc=True,\n    ):\n        super().__init__(input_size, hidden_size, num_layers, bias, bidirectional, union, with_fc)\n        if self.with_vertical:\n            self.rnn_v = nn.LSTM(\n                input_size,\n                hidden_size,\n                num_layers,\n                batch_first=True,\n                bias=bias,\n                bidirectional=bidirectional,\n            )\n        if self.with_horizontal:\n            self.rnn_h = nn.LSTM(\n                input_size,\n                hidden_size,\n                num_layers,\n                batch_first=True,\n                bias=bias,\n                bidirectional=bidirectional,\n            )\n\n\nclass Sequencer2dBlock(nn.Module):\n    def __init__(\n            self,\n            dim,\n            hidden_size,\n            mlp_ratio=3.0,\n            rnn_layer=LSTM2d,\n            mlp_layer=Mlp,\n            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n            act_layer=nn.GELU,\n            num_layers=1,\n            bidirectional=True,\n            union=\"cat\",\n            with_fc=True,\n            drop=0.,\n            drop_path=0.,\n    ):\n        super().__init__()\n        channels_dim = int(mlp_ratio * dim)\n        self.norm1 = norm_layer(dim)\n        self.rnn_tokens = rnn_layer(\n            dim,\n            hidden_size,\n            num_layers=num_layers,\n            bidirectional=bidirectional,\n            union=union,\n            with_fc=with_fc,\n        )\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        self.mlp_channels = mlp_layer(dim, channels_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.rnn_tokens(self.norm1(x)))\n        x = x + self.drop_path(self.mlp_channels(self.norm2(x)))\n        return x\n\n\nclass Shuffle(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        if self.training:\n            B, H, W, C = x.shape\n            r = torch.randperm(H * W)\n            x = x.reshape(B, -1, C)\n            x = x[:, r, :].reshape(B, H, W, -1)\n        return x\n\n\nclass Downsample2d(nn.Module):\n    def __init__(self, input_dim, output_dim, patch_size):\n        super().__init__()\n        self.down = nn.Conv2d(input_dim, output_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        x = x.permute(0, 3, 1, 2)\n        x = self.down(x)\n        x = x.permute(0, 2, 3, 1)\n        return x\n\n\nclass Sequencer2dStage(nn.Module):\n    def __init__(\n            self,\n            dim,\n            dim_out,\n            depth,\n            patch_size,\n            hidden_size,\n            mlp_ratio,\n            downsample=False,\n            block_layer=Sequencer2dBlock,\n            rnn_layer=LSTM2d,\n            mlp_layer=Mlp,\n            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n            act_layer=nn.GELU,\n            num_layers=1,\n            bidirectional=True,\n            union=\"cat\",\n            with_fc=True,\n            drop=0.,\n            drop_path=0.,\n    ):\n        super().__init__()\n        if downsample:\n            self.downsample = Downsample2d(dim, dim_out, patch_size)\n        else:\n            assert dim == dim_out\n            self.downsample = nn.Identity()\n\n        blocks = []\n        for block_idx in range(depth):\n            blocks.append(block_layer(\n                dim_out,\n                hidden_size,\n                mlp_ratio=mlp_ratio,\n                rnn_layer=rnn_layer,\n                mlp_layer=mlp_layer,\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                num_layers=num_layers,\n                bidirectional=bidirectional,\n                union=union,\n                with_fc=with_fc,\n                drop=drop,\n                drop_path=drop_path[block_idx] if isinstance(drop_path, (list, tuple)) else drop_path,\n            ))\n        self.blocks = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        x = self.downsample(x)\n        x = self.blocks(x)\n        return x\n\n\nclass Sequencer2d(nn.Module):\n    def __init__(\n            self,\n            num_classes=1000,\n            img_size=224,\n            in_chans=3,\n            global_pool='avg',\n            layers=(4, 3, 8, 3),\n            patch_sizes=(7, 2, 2, 1),\n            embed_dims=(192, 384, 384, 384),\n            hidden_sizes=(48, 96, 96, 96),\n            mlp_ratios=(3.0, 3.0, 3.0, 3.0),\n            block_layer=Sequencer2dBlock,\n            rnn_layer=LSTM2d,\n            mlp_layer=Mlp,\n            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n            act_layer=nn.GELU,\n            num_rnn_layers=1,\n            bidirectional=True,\n            union=\"cat\",\n            with_fc=True,\n            drop_rate=0.,\n            drop_path_rate=0.,\n            nlhb=False,\n            stem_norm=False,\n    ):\n        super().__init__()\n        assert global_pool in ('', 'avg')\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.num_features = embed_dims[-1]  # num_features for consistency with other models\n        self.feature_dim = -1  # channel dim index for feature outputs (rank 4, NHWC)\n        self.output_fmt = 'NHWC'\n        self.feature_info = []\n\n        self.stem = PatchEmbed(\n            img_size=None,\n            patch_size=patch_sizes[0],\n            in_chans=in_chans,\n            embed_dim=embed_dims[0],\n            norm_layer=norm_layer if stem_norm else None,\n            flatten=False,\n            output_fmt='NHWC',\n        )\n\n        assert len(layers) == len(patch_sizes) == len(embed_dims) == len(hidden_sizes) == len(mlp_ratios)\n        reductions = list(accumulate(patch_sizes, lambda x, y: x * y))\n        stages = []\n        prev_dim = embed_dims[0]\n        for i, _ in enumerate(embed_dims):\n            stages += [Sequencer2dStage(\n                prev_dim,\n                embed_dims[i],\n                depth=layers[i],\n                downsample=i > 0,\n                patch_size=patch_sizes[i],\n                hidden_size=hidden_sizes[i],\n                mlp_ratio=mlp_ratios[i],\n                block_layer=block_layer,\n                rnn_layer=rnn_layer,\n                mlp_layer=mlp_layer,\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                num_layers=num_rnn_layers,\n                bidirectional=bidirectional,\n                union=union,\n                with_fc=with_fc,\n                drop=drop_rate,\n                drop_path=drop_path_rate,\n            )]\n            prev_dim = embed_dims[i]\n            self.feature_info += [dict(num_chs=prev_dim, reduction=reductions[i], module=f'stages.{i}')]\n\n        self.stages = nn.Sequential(*stages)\n        self.norm = norm_layer(embed_dims[-1])\n        self.head = ClassifierHead(\n            self.num_features,\n            num_classes,\n            pool_type=global_pool,\n            drop_rate=drop_rate,\n            input_fmt=self.output_fmt,\n        )\n\n        self.init_weights(nlhb=nlhb)\n\n    def init_weights(self, nlhb=False):\n        head_bias = -math.log(self.num_classes) if nlhb else 0.\n        named_apply(partial(_init_weights, head_bias=head_bias), module=self)  # depth-first\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        return dict(\n            stem=r'^stem',\n            blocks=[\n                (r'^stages\\.(\\d+)', None),\n                (r'^norm', (99999,))\n            ] if coarse else [\n                (r'^stages\\.(\\d+)\\.blocks\\.(\\d+)', None),\n                (r'^stages\\.(\\d+)\\.downsample', (0,)),\n                (r'^norm', (99999,))\n            ]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        assert not enable, 'gradient checkpointing not supported'\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        self.head.reset(num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        x = self.stages(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return self.head(x, pre_logits=True) if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    \"\"\" Remap original checkpoints -> timm \"\"\"\n    if 'stages.0.blocks.0.norm1.weight' in state_dict:\n        return state_dict  # already translated checkpoint\n    if 'model' in state_dict:\n        state_dict = state_dict['model']\n\n    import re\n    out_dict = {}\n    for k, v in state_dict.items():\n        k = re.sub(r'blocks.([0-9]+).([0-9]+).down', lambda x: f'stages.{int(x.group(1)) + 1}.downsample.down', k)\n        k = re.sub(r'blocks.([0-9]+).([0-9]+)', r'stages.\\1.blocks.\\2', k)\n        k = k.replace('head.', 'head.fc.')\n        out_dict[k] = v\n\n    return out_dict\n\n\ndef _create_sequencer2d(variant, pretrained=False, **kwargs):\n    default_out_indices = tuple(range(3))\n    out_indices = kwargs.pop('out_indices', default_out_indices)\n\n    model = build_model_with_cfg(\n        Sequencer2d,\n        variant,\n        pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),\n        **kwargs,\n    )\n    return model\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': DEFAULT_CROP_PCT, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'first_conv': 'stem.proj', 'classifier': 'head.fc',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'sequencer2d_s.in1k': _cfg(hf_hub_id='timm/'),\n    'sequencer2d_m.in1k': _cfg(hf_hub_id='timm/'),\n    'sequencer2d_l.in1k': _cfg(hf_hub_id='timm/'),\n})\n\n\n@register_model\ndef sequencer2d_s(pretrained=False, **kwargs) -> Sequencer2d:\n    model_args = dict(\n        layers=[4, 3, 8, 3],\n        patch_sizes=[7, 2, 1, 1],\n        embed_dims=[192, 384, 384, 384],\n        hidden_sizes=[48, 96, 96, 96],\n        mlp_ratios=[3.0, 3.0, 3.0, 3.0],\n        rnn_layer=LSTM2d,\n        bidirectional=True,\n        union=\"cat\",\n        with_fc=True,\n    )\n    model = _create_sequencer2d('sequencer2d_s', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef sequencer2d_m(pretrained=False, **kwargs) -> Sequencer2d:\n    model_args = dict(\n        layers=[4, 3, 14, 3],\n        patch_sizes=[7, 2, 1, 1],\n        embed_dims=[192, 384, 384, 384],\n        hidden_sizes=[48, 96, 96, 96],\n        mlp_ratios=[3.0, 3.0, 3.0, 3.0],\n        rnn_layer=LSTM2d,\n        bidirectional=True,\n        union=\"cat\",\n        with_fc=True,\n        **kwargs)\n    model = _create_sequencer2d('sequencer2d_m', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n\n\n@register_model\ndef sequencer2d_l(pretrained=False, **kwargs) -> Sequencer2d:\n    model_args = dict(\n        layers=[8, 8, 16, 4],\n        patch_sizes=[7, 2, 1, 1],\n        embed_dims=[192, 384, 384, 384],\n        hidden_sizes=[48, 96, 96, 96],\n        mlp_ratios=[3.0, 3.0, 3.0, 3.0],\n        rnn_layer=LSTM2d,\n        bidirectional=True,\n        union=\"cat\",\n        with_fc=True,\n        **kwargs)\n    model = _create_sequencer2d('sequencer2d_l', pretrained=pretrained, **dict(model_args, **kwargs))\n    return model\n",
  "\"\"\"\nTResNet: High Performance GPU-Dedicated Architecture\nhttps://arxiv.org/pdf/2003.13630.pdf\n\nOriginal model: https://github.com/mrT23/TResNet\n\n\"\"\"\nfrom collections import OrderedDict\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\nfrom timm.layers import SpaceToDepth, BlurPool2d, ClassifierHead, SEModule,\\\n    ConvNormActAa, ConvNormAct, DropPath\nfrom ._builder import build_model_with_cfg\nfrom ._manipulate import checkpoint_seq\nfrom ._registry import register_model, generate_default_cfgs, register_model_deprecations\n\n__all__ = ['TResNet']  # model_registry will add each entrypoint fn to this\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(\n            self,\n            inplanes,\n            planes,\n            stride=1,\n            downsample=None,\n            use_se=True,\n            aa_layer=None,\n            drop_path_rate=0.\n    ):\n        super(BasicBlock, self).__init__()\n        self.downsample = downsample\n        self.stride = stride\n        act_layer = partial(nn.LeakyReLU, negative_slope=1e-3)\n\n        if stride == 1:\n            self.conv1 = ConvNormAct(inplanes, planes, kernel_size=3, stride=1, act_layer=act_layer)\n        else:\n            self.conv1 = ConvNormActAa(\n                inplanes, planes, kernel_size=3, stride=2, act_layer=act_layer, aa_layer=aa_layer)\n\n        self.conv2 = ConvNormAct(planes, planes, kernel_size=3, stride=1, apply_act=False, act_layer=None)\n        self.act = nn.ReLU(inplace=True)\n\n        rd_chs = max(planes * self.expansion // 4, 64)\n        self.se = SEModule(planes * self.expansion, rd_channels=rd_chs) if use_se else None\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()\n\n    def forward(self, x):\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n        else:\n            shortcut = x\n        out = self.conv1(x)\n        out = self.conv2(out)\n        if self.se is not None:\n            out = self.se(out)\n        out = self.drop_path(out) + shortcut\n        out = self.act(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(\n            self,\n            inplanes,\n            planes,\n            stride=1,\n            downsample=None,\n            use_se=True,\n            act_layer=None,\n            aa_layer=None,\n            drop_path_rate=0.,\n    ):\n        super(Bottleneck, self).__init__()\n        self.downsample = downsample\n        self.stride = stride\n        act_layer = act_layer or partial(nn.LeakyReLU, negative_slope=1e-3)\n\n        self.conv1 = ConvNormAct(\n            inplanes, planes, kernel_size=1, stride=1, act_layer=act_layer)\n        if stride == 1:\n            self.conv2 = ConvNormAct(\n                planes, planes, kernel_size=3, stride=1, act_layer=act_layer)\n        else:\n            self.conv2 = ConvNormActAa(\n                planes, planes, kernel_size=3, stride=2, act_layer=act_layer, aa_layer=aa_layer)\n\n        reduction_chs = max(planes * self.expansion // 8, 64)\n        self.se = SEModule(planes, rd_channels=reduction_chs) if use_se else None\n\n        self.conv3 = ConvNormAct(\n            planes, planes * self.expansion, kernel_size=1, stride=1, apply_act=False, act_layer=None)\n\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()\n        self.act = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n        else:\n            shortcut = x\n        out = self.conv1(x)\n        out = self.conv2(out)\n        if self.se is not None:\n            out = self.se(out)\n        out = self.conv3(out)\n        out = self.drop_path(out) + shortcut\n        out = self.act(out)\n        return out\n\n\nclass TResNet(nn.Module):\n    def __init__(\n            self,\n            layers,\n            in_chans=3,\n            num_classes=1000,\n            width_factor=1.0,\n            v2=False,\n            global_pool='fast',\n            drop_rate=0.,\n            drop_path_rate=0.,\n    ):\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        self.grad_checkpointing = False\n        super(TResNet, self).__init__()\n\n        aa_layer = BlurPool2d\n        act_layer = nn.LeakyReLU\n\n        # TResnet stages\n        self.inplanes = int(64 * width_factor)\n        self.planes = int(64 * width_factor)\n        if v2:\n            self.inplanes = self.inplanes // 8 * 8\n            self.planes = self.planes // 8 * 8\n\n        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(layers)).split(layers)]\n        conv1 = ConvNormAct(in_chans * 16, self.planes, stride=1, kernel_size=3, act_layer=act_layer)\n        layer1 = self._make_layer(\n            Bottleneck if v2 else BasicBlock,\n            self.planes, layers[0], stride=1, use_se=True, aa_layer=aa_layer, drop_path_rate=dpr[0])\n        layer2 = self._make_layer(\n            Bottleneck if v2 else BasicBlock,\n            self.planes * 2, layers[1], stride=2, use_se=True, aa_layer=aa_layer, drop_path_rate=dpr[1])\n        layer3 = self._make_layer(\n            Bottleneck,\n            self.planes * 4, layers[2], stride=2, use_se=True, aa_layer=aa_layer, drop_path_rate=dpr[2])\n        layer4 = self._make_layer(\n            Bottleneck,\n            self.planes * 8, layers[3], stride=2, use_se=False, aa_layer=aa_layer, drop_path_rate=dpr[3])\n\n        # body\n        self.body = nn.Sequential(OrderedDict([\n            ('s2d', SpaceToDepth()),\n            ('conv1', conv1),\n            ('layer1', layer1),\n            ('layer2', layer2),\n            ('layer3', layer3),\n            ('layer4', layer4),\n        ]))\n\n        self.feature_info = [\n            dict(num_chs=self.planes, reduction=2, module=''),  # Not with S2D?\n            dict(num_chs=self.planes * (Bottleneck.expansion if v2 else 1), reduction=4, module='body.layer1'),\n            dict(num_chs=self.planes * 2 * (Bottleneck.expansion if v2 else 1), reduction=8, module='body.layer2'),\n            dict(num_chs=self.planes * 4 * Bottleneck.expansion, reduction=16, module='body.layer3'),\n            dict(num_chs=self.planes * 8 * Bottleneck.expansion, reduction=32, module='body.layer4'),\n        ]\n\n        # head\n        self.num_features = (self.planes * 8) * Bottleneck.expansion\n        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=drop_rate)\n\n        # model initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n            if isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n\n        # residual connections special initialization\n        for m in self.modules():\n            if isinstance(m, BasicBlock):\n                nn.init.zeros_(m.conv2.bn.weight)\n            if isinstance(m, Bottleneck):\n                nn.init.zeros_(m.conv3.bn.weight)\n\n    def _make_layer(self, block, planes, blocks, stride=1, use_se=True, aa_layer=None, drop_path_rate=0.):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            layers = []\n            if stride == 2:\n                # avg pooling before 1x1 conv\n                layers.append(nn.AvgPool2d(kernel_size=2, stride=2, ceil_mode=True, count_include_pad=False))\n            layers += [ConvNormAct(\n                self.inplanes, planes * block.expansion, kernel_size=1, stride=1, apply_act=False, act_layer=None)]\n            downsample = nn.Sequential(*layers)\n\n        layers = []\n        for i in range(blocks):\n            layers.append(block(\n                self.inplanes,\n                planes,\n                stride=stride if i == 0 else 1,\n                downsample=downsample if i == 0 else None,\n                use_se=use_se,\n                aa_layer=aa_layer,\n                drop_path_rate=drop_path_rate[i] if isinstance(drop_path_rate, list) else drop_path_rate,\n            ))\n            self.inplanes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):\n        matcher = dict(stem=r'^body\\.conv1', blocks=r'^body\\.layer(\\d+)' if coarse else r'^body\\.layer(\\d+)\\.(\\d+)')\n        return matcher\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.head.reset(num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = self.body.s2d(x)\n            x = self.body.conv1(x)\n            x = checkpoint_seq([\n                self.body.layer1,\n                self.body.layer2,\n                self.body.layer3,\n                self.body.layer4],\n                x, flatten=True)\n        else:\n            x = self.body(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    if 'body.conv1.conv.weight' in state_dict:\n        return state_dict\n\n    import re\n    state_dict = state_dict.get('model', state_dict)\n    state_dict = state_dict.get('state_dict', state_dict)\n    out_dict = {}\n    for k, v in state_dict.items():\n        k = re.sub(r'conv(\\d+)\\.0.0', lambda x: f'conv{int(x.group(1))}.conv', k)\n        k = re.sub(r'conv(\\d+)\\.0.1', lambda x: f'conv{int(x.group(1))}.bn', k)\n        k = re.sub(r'conv(\\d+)\\.0', lambda x: f'conv{int(x.group(1))}.conv', k)\n        k = re.sub(r'conv(\\d+)\\.1', lambda x: f'conv{int(x.group(1))}.bn', k)\n        k = re.sub(r'downsample\\.(\\d+)\\.0', lambda x: f'downsample.{int(x.group(1))}.conv', k)\n        k = re.sub(r'downsample\\.(\\d+)\\.1', lambda x: f'downsample.{int(x.group(1))}.bn', k)\n        if k.endswith('bn.weight'):\n            # convert weight from inplace_abn to batchnorm\n            v = v.abs().add(1e-5)\n        out_dict[k] = v\n    return out_dict\n\n\ndef _create_tresnet(variant, pretrained=False, **kwargs):\n    return build_model_with_cfg(\n        TResNet,\n        variant,\n        pretrained,\n        pretrained_filter_fn=checkpoint_filter_fn,\n        feature_cfg=dict(out_indices=(1, 2, 3, 4), flatten_sequential=True),\n        **kwargs,\n    )\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n        'crop_pct': 0.875, 'interpolation': 'bilinear',\n        'mean': (0., 0., 0.), 'std': (1., 1., 1.),\n        'first_conv': 'body.conv1.conv', 'classifier': 'head.fc',\n        **kwargs\n    }\n\n\ndefault_cfgs = generate_default_cfgs({\n    'tresnet_m.miil_in21k_ft_in1k': _cfg(hf_hub_id='timm/'),\n    'tresnet_m.miil_in21k': _cfg(hf_hub_id='timm/', num_classes=11221),\n    'tresnet_m.miil_in1k': _cfg(hf_hub_id='timm/'),\n    'tresnet_l.miil_in1k': _cfg(hf_hub_id='timm/'),\n    'tresnet_xl.miil_in1k': _cfg(hf_hub_id='timm/'),\n    'tresnet_m.miil_in1k_448': _cfg(\n        input_size=(3, 448, 448), pool_size=(14, 14),\n        hf_hub_id='timm/'),\n    'tresnet_l.miil_in1k_448': _cfg(\n        input_size=(3, 448, 448), pool_size=(14, 14),\n        hf_hub_id='timm/'),\n    'tresnet_xl.miil_in1k_448': _cfg(\n        input_size=(3, 448, 448), pool_size=(14, 14),\n        hf_hub_id='timm/'),\n\n    'tresnet_v2_l.miil_in21k_ft_in1k': _cfg(hf_hub_id='timm/'),\n    'tresnet_v2_l.miil_in21k': _cfg(hf_hub_id='timm/', num_classes=11221),\n})\n\n\n@register_model\ndef tresnet_m(pretrained=False, **kwargs) -> TResNet:\n    model_kwargs = dict(layers=[3, 4, 11, 3], **kwargs)\n    return _create_tresnet('tresnet_m', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef tresnet_l(pretrained=False, **kwargs) -> TResNet:\n    model_kwargs = dict(layers=[4, 5, 18, 3], width_factor=1.2, **kwargs)\n    return _create_tresnet('tresnet_l', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef tresnet_xl(pretrained=False, **kwargs) -> TResNet:\n    model_kwargs = dict(layers=[4, 5, 24, 3], width_factor=1.3, **kwargs)\n    return _create_tresnet('tresnet_xl', pretrained=pretrained, **model_kwargs)\n\n\n@register_model\ndef tresnet_v2_l(pretrained=False, **kwargs) -> TResNet:\n    model_kwargs = dict(layers=[3, 4, 23, 3], width_factor=1.0, v2=True, **kwargs)\n    return _create_tresnet('tresnet_v2_l', pretrained=pretrained, **model_kwargs)\n\n\nregister_model_deprecations(__name__, {\n    'tresnet_m_miil_in21k': 'tresnet_m.miil_in21k',\n    'tresnet_m_448': 'tresnet_m.miil_in1k_448',\n    'tresnet_l_448': 'tresnet_l.miil_in1k_448',\n    'tresnet_xl_448': 'tresnet_xl.miil_in1k_448',\n})",
  "# NOTE timm.models.layers is DEPRECATED, please use timm.layers, this is here to reduce breakages in transition\nfrom timm.layers.activations import *\nfrom timm.layers.adaptive_avgmax_pool import \\\n    adaptive_avgmax_pool2d, select_adaptive_pool2d, AdaptiveAvgMaxPool2d, SelectAdaptivePool2d\nfrom timm.layers.attention_pool2d import AttentionPool2d, RotAttentionPool2d, RotaryEmbedding\nfrom timm.layers.blur_pool import BlurPool2d\nfrom timm.layers.classifier import ClassifierHead, create_classifier\nfrom timm.layers.cond_conv2d import CondConv2d, get_condconv_initializer\nfrom timm.layers.config import is_exportable, is_scriptable, is_no_jit, set_exportable, set_scriptable, set_no_jit,\\\n    set_layer_config\nfrom timm.layers.conv2d_same import Conv2dSame, conv2d_same\nfrom timm.layers.conv_bn_act import ConvNormAct, ConvNormActAa, ConvBnAct\nfrom timm.layers.create_act import create_act_layer, get_act_layer, get_act_fn\nfrom timm.layers.create_attn import get_attn, create_attn\nfrom timm.layers.create_conv2d import create_conv2d\nfrom timm.layers.create_norm import get_norm_layer, create_norm_layer\nfrom timm.layers.create_norm_act import get_norm_act_layer, create_norm_act_layer, get_norm_act_layer\nfrom timm.layers.drop import DropBlock2d, DropPath, drop_block_2d, drop_path\nfrom timm.layers.eca import EcaModule, CecaModule, EfficientChannelAttn, CircularEfficientChannelAttn\nfrom timm.layers.evo_norm import EvoNorm2dB0, EvoNorm2dB1, EvoNorm2dB2,\\\n    EvoNorm2dS0, EvoNorm2dS0a, EvoNorm2dS1, EvoNorm2dS1a, EvoNorm2dS2, EvoNorm2dS2a\nfrom timm.layers.fast_norm import is_fast_norm, set_fast_norm, fast_group_norm, fast_layer_norm\nfrom timm.layers.filter_response_norm import FilterResponseNormTlu2d, FilterResponseNormAct2d\nfrom timm.layers.gather_excite import GatherExcite\nfrom timm.layers.global_context import GlobalContext\nfrom timm.layers.helpers import to_ntuple, to_2tuple, to_3tuple, to_4tuple, make_divisible, extend_tuple\nfrom timm.layers.inplace_abn import InplaceAbn\nfrom timm.layers.linear import Linear\nfrom timm.layers.mixed_conv2d import MixedConv2d\nfrom timm.layers.mlp import Mlp, GluMlp, GatedMlp, ConvMlp\nfrom timm.layers.non_local_attn import NonLocalAttn, BatNonLocalAttn\nfrom timm.layers.norm import GroupNorm, GroupNorm1, LayerNorm, LayerNorm2d\nfrom timm.layers.norm_act import BatchNormAct2d, GroupNormAct, convert_sync_batchnorm\nfrom timm.layers.padding import get_padding, get_same_padding, pad_same\nfrom timm.layers.patch_embed import PatchEmbed\nfrom timm.layers.pool2d_same import AvgPool2dSame, create_pool2d\nfrom timm.layers.squeeze_excite import SEModule, SqueezeExcite, EffectiveSEModule, EffectiveSqueezeExcite\nfrom timm.layers.selective_kernel import SelectiveKernel\nfrom timm.layers.separable_conv import SeparableConv2d, SeparableConvNormAct\nfrom timm.layers.space_to_depth import SpaceToDepthModule\nfrom timm.layers.split_attn import SplitAttn\nfrom timm.layers.split_batchnorm import SplitBatchNorm2d, convert_splitbn_model\nfrom timm.layers.std_conv import StdConv2d, StdConv2dSame, ScaledStdConv2d, ScaledStdConv2dSame\nfrom timm.layers.test_time_pool import TestTimePoolHead, apply_test_time_pool\nfrom timm.layers.trace_utils import _assert, _float_to_int\nfrom timm.layers.weight_init import trunc_normal_, trunc_normal_tf_, variance_scaling_, lecun_normal_\n\nimport warnings\nwarnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", DeprecationWarning)\n",
  "\"\"\" Squeeze-and-Excitation Channel Attention\n\nAn SE implementation originally based on PyTorch SE-Net impl.\nHas since evolved with additional functionality / configuration.\n\nPaper: `Squeeze-and-Excitation Networks` - https://arxiv.org/abs/1709.01507\n\nAlso included is Effective Squeeze-Excitation (ESE).\nPaper: `CenterMask : Real-Time Anchor-Free Instance Segmentation` - https://arxiv.org/abs/1911.06667\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nfrom torch import nn as nn\n\nfrom .create_act import create_act_layer\nfrom .helpers import make_divisible\n\n\nclass SEModule(nn.Module):\n    \"\"\" SE Module as defined in original SE-Nets with a few additions\n    Additions include:\n        * divisor can be specified to keep channels % div == 0 (default: 8)\n        * reduction channels can be specified directly by arg (if rd_channels is set)\n        * reduction channels can be specified by float rd_ratio (default: 1/16)\n        * global max pooling can be added to the squeeze aggregation\n        * customizable activation, normalization, and gate layer\n    \"\"\"\n    def __init__(\n            self, channels, rd_ratio=1. / 16, rd_channels=None, rd_divisor=8, add_maxpool=False,\n            bias=True, act_layer=nn.ReLU, norm_layer=None, gate_layer='sigmoid'):\n        super(SEModule, self).__init__()\n        self.add_maxpool = add_maxpool\n        if not rd_channels:\n            rd_channels = make_divisible(channels * rd_ratio, rd_divisor, round_limit=0.)\n        self.fc1 = nn.Conv2d(channels, rd_channels, kernel_size=1, bias=bias)\n        self.bn = norm_layer(rd_channels) if norm_layer else nn.Identity()\n        self.act = create_act_layer(act_layer, inplace=True)\n        self.fc2 = nn.Conv2d(rd_channels, channels, kernel_size=1, bias=bias)\n        self.gate = create_act_layer(gate_layer)\n\n    def forward(self, x):\n        x_se = x.mean((2, 3), keepdim=True)\n        if self.add_maxpool:\n            # experimental codepath, may remove or change\n            x_se = 0.5 * x_se + 0.5 * x.amax((2, 3), keepdim=True)\n        x_se = self.fc1(x_se)\n        x_se = self.act(self.bn(x_se))\n        x_se = self.fc2(x_se)\n        return x * self.gate(x_se)\n\n\nSqueezeExcite = SEModule  # alias\n\n\nclass EffectiveSEModule(nn.Module):\n    \"\"\" 'Effective Squeeze-Excitation\n    From `CenterMask : Real-Time Anchor-Free Instance Segmentation` - https://arxiv.org/abs/1911.06667\n    \"\"\"\n    def __init__(self, channels, add_maxpool=False, gate_layer='hard_sigmoid', **_):\n        super(EffectiveSEModule, self).__init__()\n        self.add_maxpool = add_maxpool\n        self.fc = nn.Conv2d(channels, channels, kernel_size=1, padding=0)\n        self.gate = create_act_layer(gate_layer)\n\n    def forward(self, x):\n        x_se = x.mean((2, 3), keepdim=True)\n        if self.add_maxpool:\n            # experimental codepath, may remove or change\n            x_se = 0.5 * x_se + 0.5 * x.amax((2, 3), keepdim=True)\n        x_se = self.fc(x_se)\n        return x * self.gate(x_se)\n\n\nEffectiveSqueezeExcite = EffectiveSEModule  # alias\n\n\nclass SqueezeExciteCl(nn.Module):\n    \"\"\" SE Module as defined in original SE-Nets with a few additions\n    Additions include:\n        * divisor can be specified to keep channels % div == 0 (default: 8)\n        * reduction channels can be specified directly by arg (if rd_channels is set)\n        * reduction channels can be specified by float rd_ratio (default: 1/16)\n        * global max pooling can be added to the squeeze aggregation\n        * customizable activation, normalization, and gate layer\n    \"\"\"\n    def __init__(\n            self, channels, rd_ratio=1. / 16, rd_channels=None, rd_divisor=8,\n            bias=True, act_layer=nn.ReLU, gate_layer='sigmoid'):\n        super().__init__()\n        if not rd_channels:\n            rd_channels = make_divisible(channels * rd_ratio, rd_divisor, round_limit=0.)\n        self.fc1 = nn.Linear(channels, rd_channels, bias=bias)\n        self.act = create_act_layer(act_layer, inplace=True)\n        self.fc2 = nn.Linear(rd_channels, channels, bias=bias)\n        self.gate = create_act_layer(gate_layer)\n\n    def forward(self, x):\n        x_se = x.mean((1, 2), keepdims=True)  # FIXME avg dim [1:n-1], don't assume 2D NHWC\n        x_se = self.fc1(x_se)\n        x_se = self.act(x_se)\n        x_se = self.fc2(x_se)\n        return x * self.gate(x_se)",
  "from .activations import *\nfrom .adaptive_avgmax_pool import \\\n    adaptive_avgmax_pool2d, select_adaptive_pool2d, AdaptiveAvgMaxPool2d, SelectAdaptivePool2d\nfrom .attention_pool2d import AttentionPool2d, RotAttentionPool2d, RotaryEmbedding\nfrom .blur_pool import BlurPool2d\nfrom .classifier import ClassifierHead, create_classifier, NormMlpClassifierHead\nfrom .cond_conv2d import CondConv2d, get_condconv_initializer\nfrom .config import is_exportable, is_scriptable, is_no_jit, use_fused_attn, \\\n    set_exportable, set_scriptable, set_no_jit, set_layer_config, set_fused_attn\nfrom .conv2d_same import Conv2dSame, conv2d_same\nfrom .conv_bn_act import ConvNormAct, ConvNormActAa, ConvBnAct\nfrom .create_act import create_act_layer, get_act_layer, get_act_fn\nfrom .create_attn import get_attn, create_attn\nfrom .create_conv2d import create_conv2d\nfrom .create_norm import get_norm_layer, create_norm_layer\nfrom .create_norm_act import get_norm_act_layer, create_norm_act_layer, get_norm_act_layer\nfrom .drop import DropBlock2d, DropPath, drop_block_2d, drop_path\nfrom .eca import EcaModule, CecaModule, EfficientChannelAttn, CircularEfficientChannelAttn\nfrom .evo_norm import EvoNorm2dB0, EvoNorm2dB1, EvoNorm2dB2,\\\n    EvoNorm2dS0, EvoNorm2dS0a, EvoNorm2dS1, EvoNorm2dS1a, EvoNorm2dS2, EvoNorm2dS2a\nfrom .fast_norm import is_fast_norm, set_fast_norm, fast_group_norm, fast_layer_norm\nfrom .filter_response_norm import FilterResponseNormTlu2d, FilterResponseNormAct2d\nfrom .format import Format, get_channel_dim, get_spatial_dim, nchw_to, nhwc_to\nfrom .gather_excite import GatherExcite\nfrom .global_context import GlobalContext\nfrom .helpers import to_ntuple, to_2tuple, to_3tuple, to_4tuple, make_divisible, extend_tuple\nfrom .inplace_abn import InplaceAbn\nfrom .linear import Linear\nfrom .mixed_conv2d import MixedConv2d\nfrom .mlp import Mlp, GluMlp, GatedMlp, SwiGLU, SwiGLUPacked, ConvMlp, GlobalResponseNormMlp\nfrom .non_local_attn import NonLocalAttn, BatNonLocalAttn\nfrom .norm import GroupNorm, GroupNorm1, LayerNorm, LayerNorm2d, RmsNorm\nfrom .norm_act import BatchNormAct2d, GroupNormAct, GroupNorm1Act, LayerNormAct, LayerNormAct2d,\\\n    SyncBatchNormAct, convert_sync_batchnorm, FrozenBatchNormAct2d, freeze_batch_norm_2d, unfreeze_batch_norm_2d\nfrom .padding import get_padding, get_same_padding, pad_same\nfrom .patch_dropout import PatchDropout\nfrom .patch_embed import PatchEmbed, PatchEmbedWithSize, resample_patch_embed\nfrom .pool2d_same import AvgPool2dSame, create_pool2d\nfrom .pos_embed import resample_abs_pos_embed, resample_abs_pos_embed_nhwc\nfrom .pos_embed_rel import RelPosMlp, RelPosBias, RelPosBiasTf, gen_relative_position_index, gen_relative_log_coords\nfrom .pos_embed_sincos import pixel_freq_bands, freq_bands, build_sincos2d_pos_embed, build_fourier_pos_embed, \\\n    build_rotary_pos_embed, apply_rot_embed, apply_rot_embed_cat, apply_rot_embed_list, apply_keep_indices_nlc, \\\n    FourierEmbed, RotaryEmbedding, RotaryEmbeddingCat\nfrom .squeeze_excite import SEModule, SqueezeExcite, EffectiveSEModule, EffectiveSqueezeExcite\nfrom .selective_kernel import SelectiveKernel\nfrom .separable_conv import SeparableConv2d, SeparableConvNormAct\nfrom .space_to_depth import SpaceToDepthModule, SpaceToDepth, DepthToSpace\nfrom .split_attn import SplitAttn\nfrom .split_batchnorm import SplitBatchNorm2d, convert_splitbn_model\nfrom .std_conv import StdConv2d, StdConv2dSame, ScaledStdConv2d, ScaledStdConv2dSame\nfrom .test_time_pool import TestTimePoolHead, apply_test_time_pool\nfrom .trace_utils import _assert, _float_to_int\nfrom .weight_init import trunc_normal_, trunc_normal_tf_, variance_scaling_, lecun_normal_\n",
  "\"\"\"\nBlurPool layer inspired by\n - Kornia's Max_BlurPool2d\n - Making Convolutional Networks Shift-Invariant Again :cite:`zhang2019shiftinvar`\n\nHacked together by Chris Ha and Ross Wightman\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom .padding import get_padding\n\n\nclass BlurPool2d(nn.Module):\n    r\"\"\"Creates a module that computes blurs and downsample a given feature map.\n    See :cite:`zhang2019shiftinvar` for more details.\n    Corresponds to the Downsample class, which does blurring and subsampling\n\n    Args:\n        channels = Number of input channels\n        filt_size (int): binomial filter size for blurring. currently supports 3 (default) and 5.\n        stride (int): downsampling filter stride\n\n    Returns:\n        torch.Tensor: the transformed tensor.\n    \"\"\"\n    def __init__(self, channels, filt_size=3, stride=2) -> None:\n        super(BlurPool2d, self).__init__()\n        assert filt_size > 1\n        self.channels = channels\n        self.filt_size = filt_size\n        self.stride = stride\n        self.padding = [get_padding(filt_size, stride, dilation=1)] * 4\n        coeffs = torch.tensor((np.poly1d((0.5, 0.5)) ** (self.filt_size - 1)).coeffs.astype(np.float32))\n        blur_filter = (coeffs[:, None] * coeffs[None, :])[None, None, :, :].repeat(self.channels, 1, 1, 1)\n        self.register_buffer('filt', blur_filter, persistent=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = F.pad(x, self.padding, 'reflect')\n        return F.conv2d(x, self.filt, stride=self.stride, groups=self.channels)\n",
  "\"\"\" Model / Layer Config singleton state\n\"\"\"\nimport os\nimport warnings\nfrom typing import Any, Optional\n\nimport torch\n\n__all__ = [\n    'is_exportable', 'is_scriptable', 'is_no_jit', 'use_fused_attn',\n    'set_exportable', 'set_scriptable', 'set_no_jit', 'set_layer_config', 'set_fused_attn'\n]\n\n# Set to True if prefer to have layers with no jit optimization (includes activations)\n_NO_JIT = False\n\n# Set to True if prefer to have activation layers with no jit optimization\n# NOTE not currently used as no difference between no_jit and no_activation jit as only layers obeying\n# the jit flags so far are activations. This will change as more layers are updated and/or added.\n_NO_ACTIVATION_JIT = False\n\n# Set to True if exporting a model with Same padding via ONNX\n_EXPORTABLE = False\n\n# Set to True if wanting to use torch.jit.script on a model\n_SCRIPTABLE = False\n\n\n# use torch.scaled_dot_product_attention where possible\n_HAS_FUSED_ATTN = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\nif 'TIMM_FUSED_ATTN' in os.environ:\n    _USE_FUSED_ATTN = int(os.environ['TIMM_FUSED_ATTN'])\nelse:\n    _USE_FUSED_ATTN = 1  # 0 == off, 1 == on (for tested use), 2 == on (for experimental use)\n\n\ndef is_no_jit():\n    return _NO_JIT\n\n\nclass set_no_jit:\n    def __init__(self, mode: bool) -> None:\n        global _NO_JIT\n        self.prev = _NO_JIT\n        _NO_JIT = mode\n\n    def __enter__(self) -> None:\n        pass\n\n    def __exit__(self, *args: Any) -> bool:\n        global _NO_JIT\n        _NO_JIT = self.prev\n        return False\n\n\ndef is_exportable():\n    return _EXPORTABLE\n\n\nclass set_exportable:\n    def __init__(self, mode: bool) -> None:\n        global _EXPORTABLE\n        self.prev = _EXPORTABLE\n        _EXPORTABLE = mode\n\n    def __enter__(self) -> None:\n        pass\n\n    def __exit__(self, *args: Any) -> bool:\n        global _EXPORTABLE\n        _EXPORTABLE = self.prev\n        return False\n\n\ndef is_scriptable():\n    return _SCRIPTABLE\n\n\nclass set_scriptable:\n    def __init__(self, mode: bool) -> None:\n        global _SCRIPTABLE\n        self.prev = _SCRIPTABLE\n        _SCRIPTABLE = mode\n\n    def __enter__(self) -> None:\n        pass\n\n    def __exit__(self, *args: Any) -> bool:\n        global _SCRIPTABLE\n        _SCRIPTABLE = self.prev\n        return False\n\n\nclass set_layer_config:\n    \"\"\" Layer config context manager that allows setting all layer config flags at once.\n    If a flag arg is None, it will not change the current value.\n    \"\"\"\n    def __init__(\n            self,\n            scriptable: Optional[bool] = None,\n            exportable: Optional[bool] = None,\n            no_jit: Optional[bool] = None,\n            no_activation_jit: Optional[bool] = None):\n        global _SCRIPTABLE\n        global _EXPORTABLE\n        global _NO_JIT\n        global _NO_ACTIVATION_JIT\n        self.prev = _SCRIPTABLE, _EXPORTABLE, _NO_JIT, _NO_ACTIVATION_JIT\n        if scriptable is not None:\n            _SCRIPTABLE = scriptable\n        if exportable is not None:\n            _EXPORTABLE = exportable\n        if no_jit is not None:\n            _NO_JIT = no_jit\n        if no_activation_jit is not None:\n            _NO_ACTIVATION_JIT = no_activation_jit\n\n    def __enter__(self) -> None:\n        pass\n\n    def __exit__(self, *args: Any) -> bool:\n        global _SCRIPTABLE\n        global _EXPORTABLE\n        global _NO_JIT\n        global _NO_ACTIVATION_JIT\n        _SCRIPTABLE, _EXPORTABLE, _NO_JIT, _NO_ACTIVATION_JIT = self.prev\n        return False\n\n\ndef use_fused_attn(experimental: bool = False) -> bool:\n    # NOTE: ONNX export cannot handle F.scaled_dot_product_attention as of pytorch 2.0\n    if not _HAS_FUSED_ATTN or _EXPORTABLE:\n        return False\n    if experimental:\n        return _USE_FUSED_ATTN > 1\n    return _USE_FUSED_ATTN > 0\n\n\ndef set_fused_attn(enable: bool = True, experimental: bool = False):\n    global _USE_FUSED_ATTN\n    if not _HAS_FUSED_ATTN:\n        warnings.warn('This version of pytorch does not have F.scaled_dot_product_attention, fused_attn flag ignored.')\n        return\n    if experimental and enable:\n        _USE_FUSED_ATTN = 2\n    elif enable:\n        _USE_FUSED_ATTN = 1\n    else:\n        _USE_FUSED_ATTN = 0\n",
  "\"\"\" Linear layer (alternate definition)\n\"\"\"\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn as nn\n\n\nclass Linear(nn.Linear):\n    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n\n    Wraps torch.nn.Linear to support AMP + torchscript usage by manually casting\n    weight & bias to input.dtype to work around an issue w/ torch.addmm in this use case.\n    \"\"\"\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        if torch.jit.is_scripting():\n            bias = self.bias.to(dtype=input.dtype) if self.bias is not None else None\n            return F.linear(input, self.weight.to(dtype=input.dtype), bias=bias)\n        else:\n            return F.linear(input, self.weight, self.bias)\n",
  "\"\"\" Attention Factory\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport torch\nfrom functools import partial\n\nfrom .bottleneck_attn import BottleneckAttn\nfrom .cbam import CbamModule, LightCbamModule\nfrom .eca import EcaModule, CecaModule\nfrom .gather_excite import GatherExcite\nfrom .global_context import GlobalContext\nfrom .halo_attn import HaloAttn\nfrom .lambda_layer import LambdaLayer\nfrom .non_local_attn import NonLocalAttn, BatNonLocalAttn\nfrom .selective_kernel import SelectiveKernel\nfrom .split_attn import SplitAttn\nfrom .squeeze_excite import SEModule, EffectiveSEModule\n\n\ndef get_attn(attn_type):\n    if isinstance(attn_type, torch.nn.Module):\n        return attn_type\n    module_cls = None\n    if attn_type:\n        if isinstance(attn_type, str):\n            attn_type = attn_type.lower()\n            # Lightweight attention modules (channel and/or coarse spatial).\n            # Typically added to existing network architecture blocks in addition to existing convolutions.\n            if attn_type == 'se':\n                module_cls = SEModule\n            elif attn_type == 'ese':\n                module_cls = EffectiveSEModule\n            elif attn_type == 'eca':\n                module_cls = EcaModule\n            elif attn_type == 'ecam':\n                module_cls = partial(EcaModule, use_mlp=True)\n            elif attn_type == 'ceca':\n                module_cls = CecaModule\n            elif attn_type == 'ge':\n                module_cls = GatherExcite\n            elif attn_type == 'gc':\n                module_cls = GlobalContext\n            elif attn_type == 'gca':\n                module_cls = partial(GlobalContext, fuse_add=True, fuse_scale=False)\n            elif attn_type == 'cbam':\n                module_cls = CbamModule\n            elif attn_type == 'lcbam':\n                module_cls = LightCbamModule\n\n            # Attention / attention-like modules w/ significant params\n            # Typically replace some of the existing workhorse convs in a network architecture.\n            # All of these accept a stride argument and can spatially downsample the input.\n            elif attn_type == 'sk':\n                module_cls = SelectiveKernel\n            elif attn_type == 'splat':\n                module_cls = SplitAttn\n\n            # Self-attention / attention-like modules w/ significant compute and/or params\n            # Typically replace some of the existing workhorse convs in a network architecture.\n            # All of these accept a stride argument and can spatially downsample the input.\n            elif attn_type == 'lambda':\n                return LambdaLayer\n            elif attn_type == 'bottleneck':\n                return BottleneckAttn\n            elif attn_type == 'halo':\n                return HaloAttn\n            elif attn_type == 'nl':\n                module_cls = NonLocalAttn\n            elif attn_type == 'bat':\n                module_cls = BatNonLocalAttn\n\n            # Woops!\n            else:\n                assert False, \"Invalid attn module (%s)\" % attn_type\n        elif isinstance(attn_type, bool):\n            if attn_type:\n                module_cls = SEModule\n        else:\n            module_cls = attn_type\n    return module_cls\n\n\ndef create_attn(attn_type, channels, **kwargs):\n    module_cls = get_attn(attn_type)\n    if module_cls is not None:\n        # NOTE: it's expected the first (positional) argument of all attention layers is the # input channels\n        return module_cls(channels, **kwargs)\n    return None\n",
  "\"\"\" Sin-cos, fourier, rotary position embedding modules and functions\n\nHacked together by / Copyright 2022 Ross Wightman\n\"\"\"\nimport math\nfrom typing import List, Tuple, Optional, Union\n\nimport torch\nfrom torch import nn as nn\n\nfrom .trace_utils import _assert\n\n\ndef pixel_freq_bands(\n        num_bands: int,\n        max_freq: float = 224.,\n        linear_bands: bool = True,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[torch.device] = None,\n):\n    if linear_bands:\n        bands = torch.linspace(1.0, max_freq / 2, num_bands, dtype=dtype, device=device)\n    else:\n        bands = 2 ** torch.linspace(0, math.log(max_freq, 2) - 1, num_bands, dtype=dtype, device=device)\n    return bands * torch.pi\n\n\ndef freq_bands(\n        num_bands: int,\n        temperature: float = 10000.,\n        step: int = 2,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[torch.device] = None,\n) -> torch.Tensor:\n    bands = 1. / (temperature ** (torch.arange(0, num_bands, step, dtype=dtype, device=device) / num_bands))\n    return bands\n\n\ndef build_sincos2d_pos_embed(\n        feat_shape: List[int],\n        dim: int = 64,\n        temperature: float = 10000.,\n        reverse_coord: bool = False,\n        interleave_sin_cos: bool = False,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[torch.device] = None\n) -> torch.Tensor:\n    \"\"\"\n\n    Args:\n        feat_shape:\n        dim:\n        temperature:\n        reverse_coord: stack grid order W, H instead of H, W\n        interleave_sin_cos: sin, cos, sin, cos stack instead of sin, sin, cos, cos\n        dtype:\n        device:\n\n    Returns:\n\n    \"\"\"\n    assert dim % 4 == 0, 'Embed dimension must be divisible by 4 for sin-cos 2D position embedding'\n    pos_dim = dim // 4\n    bands = freq_bands(pos_dim, temperature=temperature, step=1, dtype=dtype, device=device)\n\n    if reverse_coord:\n        feat_shape = feat_shape[::-1]  # stack W, H instead of H, W\n    grid = torch.stack(torch.meshgrid(\n        [torch.arange(s, device=device, dtype=dtype) for s in feat_shape])).flatten(1).transpose(0, 1)\n    pos2 = grid.unsqueeze(-1) * bands.unsqueeze(0)\n    # FIXME add support for unflattened spatial dim?\n\n    stack_dim = 2 if interleave_sin_cos else 1  # stack sin, cos, sin, cos  instead of sin sin cos cos\n    pos_emb = torch.stack([torch.sin(pos2), torch.cos(pos2)], dim=stack_dim).flatten(1)\n    return pos_emb\n\n\ndef build_fourier_pos_embed(\n        feat_shape: List[int],\n        bands: Optional[torch.Tensor] = None,\n        num_bands: int = 64,\n        max_res: int = 224,\n        temperature: float = 10000.,\n        linear_bands: bool = False,\n        include_grid: bool = False,\n        in_pixels: bool = True,\n        ref_feat_shape: Optional[List[int]] = None,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[torch.device] = None,\n) -> List[torch.Tensor]:\n    \"\"\"\n\n    Args:\n        feat_shape: Feature shape for embedding.\n        bands: Pre-calculated frequency bands.\n        num_bands: Number of frequency bands (determines output dim).\n        max_res: Maximum resolution for pixel based freq.\n        temperature: Temperature for non-pixel freq.\n        linear_bands: Linear band spacing for pixel based freq.\n        include_grid: Include the spatial grid in output.\n        in_pixels: Output in pixel freq.\n        ref_feat_shape: Reference feature shape for resize / fine-tune.\n        dtype: Output dtype.\n        device: Output device.\n\n    Returns:\n\n    \"\"\"\n    if bands is None:\n        if in_pixels:\n            bands = pixel_freq_bands(\n                num_bands,\n                float(max_res),\n                linear_bands=linear_bands,\n                dtype=dtype,\n                device=device,\n            )\n        else:\n            bands = freq_bands(\n                num_bands,\n                temperature=temperature,\n                step=1,\n                dtype=dtype,\n                device=device,\n            )\n    else:\n        if device is None:\n            device = bands.device\n        if dtype is None:\n            dtype = bands.dtype\n\n    if in_pixels:\n        t = [torch.linspace(-1., 1., steps=s, device=device, dtype=dtype) for s in feat_shape]\n    else:\n        t = [torch.arange(s, device=device, dtype=dtype) for s in feat_shape]\n\n    if ref_feat_shape is not None:\n        # eva's scheme for resizing rope embeddings (ref shape = pretrain)\n        t = [x / f * r for x, f, r in zip(t, feat_shape, ref_feat_shape)]\n\n    grid = torch.stack(torch.meshgrid(t), dim=-1)\n    grid = grid.unsqueeze(-1)\n    pos = grid * bands\n\n    pos_sin, pos_cos = pos.sin(), pos.cos()\n    out = [grid, pos_sin, pos_cos] if include_grid else [pos_sin, pos_cos]\n    return out\n\n\nclass FourierEmbed(nn.Module):\n\n    def __init__(\n            self,\n            max_res: int = 224,\n            num_bands: int = 64,\n            concat_grid=True,\n            keep_spatial=False,\n    ):\n        super().__init__()\n        self.max_res = max_res\n        self.num_bands = num_bands\n        self.concat_grid = concat_grid\n        self.keep_spatial = keep_spatial\n        self.register_buffer(\n            'bands',\n            pixel_freq_bands(max_res, num_bands),\n            persistent=False,\n        )\n\n    def forward(self, x):\n        B, C = x.shape[:2]\n        feat_shape = x.shape[2:]\n        emb = build_fourier_pos_embed(\n            feat_shape,\n            self.bands,\n            include_grid=self.concat_grid,\n            dtype=x.dtype,\n            device=x.device,\n        )\n        emb = torch.cat(emb, dim=-1)\n        emb = emb.transpose(-1, -2).flatten(len(feat_shape))\n        batch_expand = (B,) + (-1,) * (x.ndim - 1)\n\n        # FIXME support nD\n        if self.keep_spatial:\n            x = torch.cat([x, emb.unsqueeze(0).expand(batch_expand).permute(0, 3, 1, 2)], dim=1)\n        else:\n            x = torch.cat([x.permute(0, 2, 3, 1), emb.unsqueeze(0).expand(batch_expand)], dim=-1)\n            x = x.reshape(B, feat_shape.numel(), -1)\n\n        return x\n\n\ndef rot(x):\n    return torch.stack([-x[..., 1::2], x[..., ::2]], -1).reshape(x.shape)\n\n\ndef apply_rot_embed(x: torch.Tensor, sin_emb, cos_emb):\n    if sin_emb.ndim == 3:\n        return x * cos_emb.unsqueeze(1).expand_as(x) + rot(x) * sin_emb.unsqueeze(1).expand_as(x)\n    return x * cos_emb + rot(x) * sin_emb\n\n\ndef apply_rot_embed_list(x: List[torch.Tensor], sin_emb, cos_emb):\n    if isinstance(x, torch.Tensor):\n        x = [x]\n    return [t * cos_emb + rot(t) * sin_emb for t in x]\n\n\ndef apply_rot_embed_cat(x: torch.Tensor, emb):\n    sin_emb, cos_emb = emb.tensor_split(2, -1)\n    if sin_emb.ndim == 3:\n        return x * cos_emb.unsqueeze(1).expand_as(x) + rot(x) * sin_emb.unsqueeze(1).expand_as(x)\n    return x * cos_emb + rot(x) * sin_emb\n\n\ndef apply_keep_indices_nlc(x, pos_embed, keep_indices):\n    pos_embed = pos_embed.unsqueeze(0).expand(x.shape[0], -1, -1)\n    pos_embed = pos_embed.gather(1, keep_indices.unsqueeze(-1).expand(-1, -1, pos_embed.shape[-1]))\n    return pos_embed\n\n\ndef build_rotary_pos_embed(\n        feat_shape: List[int],\n        bands: Optional[torch.Tensor] = None,\n        dim: int = 64,\n        max_res: int = 224,\n        temperature: float = 10000.,\n        linear_bands: bool = False,\n        in_pixels: bool = True,\n        ref_feat_shape: Optional[List[int]] = None,\n        dtype: torch.dtype = torch.float32,\n        device: Optional[torch.device] = None,\n):\n    \"\"\"\n\n    Args:\n        feat_shape: Spatial shape of the target tensor for embedding.\n        bands: Optional pre-generated frequency bands\n        dim: Output dimension of embedding tensor.\n        max_res: Maximum resolution for pixel mode.\n        temperature: Temperature (inv freq) for non-pixel mode\n        linear_bands: Linearly (instead of log) spaced bands for pixel mode\n        in_pixels: Pixel vs language (inv freq) mode.\n        dtype: Output dtype.\n        device: Output device.\n\n    Returns:\n\n    \"\"\"\n    sin_emb, cos_emb = build_fourier_pos_embed(\n        feat_shape,\n        bands=bands,\n        num_bands=dim // 4,\n        max_res=max_res,\n        temperature=temperature,\n        linear_bands=linear_bands,\n        in_pixels=in_pixels,\n        ref_feat_shape=ref_feat_shape,\n        device=device,\n        dtype=dtype,\n    )\n    num_spatial_dim = 1\n    # this would be much nicer as a .numel() call to torch.Size(), but torchscript sucks\n    for x in feat_shape:\n        num_spatial_dim *= x\n    sin_emb = sin_emb.reshape(num_spatial_dim, -1).repeat_interleave(2, -1)\n    cos_emb = cos_emb.reshape(num_spatial_dim, -1).repeat_interleave(2, -1)\n    return sin_emb, cos_emb\n\n\nclass RotaryEmbedding(nn.Module):\n    \"\"\" Rotary position embedding\n\n    NOTE: This is my initial attempt at impl rotary embedding for spatial use, it has not\n    been well tested, and will likely change. It will be moved to its own file.\n\n    The following impl/resources were referenced for this impl:\n    * https://github.com/lucidrains/vit-pytorch/blob/6f3a5fcf0bca1c5ec33a35ef48d97213709df4ba/vit_pytorch/rvt.py\n    * https://blog.eleuther.ai/rotary-embeddings/\n    \"\"\"\n\n    def __init__(\n            self,\n            dim,\n            max_res=224,\n            temperature=10000,\n            in_pixels=True,\n            linear_bands: bool = False,\n            feat_shape: Optional[List[int]] = None,\n            ref_feat_shape: Optional[List[int]] = None,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.max_res = max_res\n        self.temperature = temperature\n        self.in_pixels = in_pixels\n        self.feat_shape = feat_shape\n        self.ref_feat_shape = ref_feat_shape\n\n        if feat_shape is None:\n            # only cache bands\n            if in_pixels:\n                bands = pixel_freq_bands(\n                    dim // 4,\n                    float(max_res),\n                    linear_bands=linear_bands,\n                )\n            else:\n                bands = freq_bands(\n                    dim // 4,\n                    temperature=temperature,\n                    step=1,\n                )\n                print(bands)\n            self.register_buffer(\n                'bands',\n                bands,\n                persistent=False,\n            )\n            self.pos_embed_sin = None\n            self.pos_embed_cos = None\n        else:\n            # cache full sin/cos embeddings if shape provided up front\n            emb_sin, emb_cos = build_rotary_pos_embed(\n                feat_shape=feat_shape,\n                dim=dim,\n                max_res=max_res,\n                linear_bands=linear_bands,\n                in_pixels=in_pixels,\n                ref_feat_shape=self.ref_feat_shape,\n            )\n            self.bands = None\n            self.register_buffer(\n                'pos_embed_sin',\n                emb_sin,\n                persistent=False,\n            )\n            self.register_buffer(\n                'pos_embed_cos',\n                emb_cos,\n                persistent=False,\n            )\n\n    def get_embed(self, shape: Optional[List[int]] = None):\n        if self.bands is not None:\n            # rebuild embeddings every call, use if target shape changes\n            assert shape is not None\n            return build_rotary_pos_embed(\n                shape,\n                self.bands,\n                in_pixels=self.in_pixels,\n            )\n        else:\n            return self.pos_embed_sin, self.pos_embed_cos\n\n    def forward(self, x):\n        # assuming channel-first tensor where spatial dim are >= 2\n        sin_emb, cos_emb = self.get_embed(x.shape[2:])\n        return apply_rot_embed(x, sin_emb, cos_emb)\n\n\nclass RotaryEmbeddingCat(nn.Module):\n    \"\"\" Rotary position embedding w/ concatenatd sin & cos\n\n    The following impl/resources were referenced for this impl:\n    * https://github.com/lucidrains/vit-pytorch/blob/6f3a5fcf0bca1c5ec33a35ef48d97213709df4ba/vit_pytorch/rvt.py\n    * https://blog.eleuther.ai/rotary-embeddings/\n    \"\"\"\n\n    def __init__(\n            self,\n            dim,\n            max_res=224,\n            temperature=10000,\n            in_pixels=True,\n            linear_bands: bool = False,\n            feat_shape: Optional[List[int]] = None,\n            ref_feat_shape: Optional[List[int]] = None,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.max_res = max_res\n        self.temperature = temperature\n        self.in_pixels = in_pixels\n        self.feat_shape = feat_shape\n        self.ref_feat_shape = ref_feat_shape\n\n        if feat_shape is None:\n            # only cache bands\n            if in_pixels:\n                bands = pixel_freq_bands(\n                    dim // 4,\n                    float(max_res),\n                    linear_bands=linear_bands,\n                )\n            else:\n                bands = freq_bands(\n                    dim // 4,\n                    temperature=temperature,\n                    step=1,\n                )\n                print(bands)\n            self.register_buffer(\n                'bands',\n                bands,\n                persistent=False,\n            )\n            self.embed = None\n        else:\n            # cache full sin/cos embeddings if shape provided up front\n            embeds = build_rotary_pos_embed(\n                feat_shape=feat_shape,\n                dim=dim,\n                max_res=max_res,\n                linear_bands=linear_bands,\n                in_pixels=in_pixels,\n                ref_feat_shape=self.ref_feat_shape,\n            )\n            self.bands = None\n            self.register_buffer(\n                'pos_embed',\n                torch.cat(embeds, -1),\n                persistent=False,\n            )\n\n    def get_embed(self, shape: Optional[List[int]] = None):\n        if self.bands is not None:\n            # rebuild embeddings every call, use if target shape changes\n            _assert(shape is not None, 'valid shape needed')\n            embeds = build_rotary_pos_embed(\n                shape,\n                self.bands,\n                in_pixels=self.in_pixels,\n            )\n            return torch.cat(embeds, -1)\n        else:\n            return self.pos_embed\n\n    def forward(self, x):\n        # assuming channel-first tensor where spatial dim are >= 2\n        pos_embed = self.get_embed(x.shape[2:])\n        return apply_rot_embed_cat(x, pos_embed)\n",
  "\"\"\" Activation Factory\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom typing import Union, Callable, Type\n\nfrom .activations import *\nfrom .activations_jit import *\nfrom .activations_me import *\nfrom .config import is_exportable, is_scriptable, is_no_jit\n\n# PyTorch has an optimized, native 'silu' (aka 'swish') operator as of PyTorch 1.7.\n# Also hardsigmoid, hardswish, and soon mish. This code will use native version if present.\n# Eventually, the custom SiLU, Mish, Hard*, layers will be removed and only native variants will be used.\n_has_silu = 'silu' in dir(torch.nn.functional)\n_has_hardswish = 'hardswish' in dir(torch.nn.functional)\n_has_hardsigmoid = 'hardsigmoid' in dir(torch.nn.functional)\n_has_mish = 'mish' in dir(torch.nn.functional)\n\n\n_ACT_FN_DEFAULT = dict(\n    silu=F.silu if _has_silu else swish,\n    swish=F.silu if _has_silu else swish,\n    mish=F.mish if _has_mish else mish,\n    relu=F.relu,\n    relu6=F.relu6,\n    leaky_relu=F.leaky_relu,\n    elu=F.elu,\n    celu=F.celu,\n    selu=F.selu,\n    gelu=gelu,\n    gelu_tanh=gelu_tanh,\n    sigmoid=sigmoid,\n    tanh=tanh,\n    hard_sigmoid=F.hardsigmoid if _has_hardsigmoid else hard_sigmoid,\n    hard_swish=F.hardswish if _has_hardswish else hard_swish,\n    hard_mish=hard_mish,\n)\n\n_ACT_FN_JIT = dict(\n    silu=F.silu if _has_silu else swish_jit,\n    swish=F.silu if _has_silu else swish_jit,\n    mish=F.mish if _has_mish else mish_jit,\n    hard_sigmoid=F.hardsigmoid if _has_hardsigmoid else hard_sigmoid_jit,\n    hard_swish=F.hardswish if _has_hardswish else hard_swish_jit,\n    hard_mish=hard_mish_jit\n)\n\n_ACT_FN_ME = dict(\n    silu=F.silu if _has_silu else swish_me,\n    swish=F.silu if _has_silu else swish_me,\n    mish=F.mish if _has_mish else mish_me,\n    hard_sigmoid=F.hardsigmoid if _has_hardsigmoid else hard_sigmoid_me,\n    hard_swish=F.hardswish if _has_hardswish else hard_swish_me,\n    hard_mish=hard_mish_me,\n)\n\n_ACT_FNS = (_ACT_FN_ME, _ACT_FN_JIT, _ACT_FN_DEFAULT)\nfor a in _ACT_FNS:\n    a.setdefault('hardsigmoid', a.get('hard_sigmoid'))\n    a.setdefault('hardswish', a.get('hard_swish'))\n\n\n_ACT_LAYER_DEFAULT = dict(\n    silu=nn.SiLU if _has_silu else Swish,\n    swish=nn.SiLU if _has_silu else Swish,\n    mish=nn.Mish if _has_mish else Mish,\n    relu=nn.ReLU,\n    relu6=nn.ReLU6,\n    leaky_relu=nn.LeakyReLU,\n    elu=nn.ELU,\n    prelu=PReLU,\n    celu=nn.CELU,\n    selu=nn.SELU,\n    gelu=GELU,\n    gelu_tanh=GELUTanh,\n    sigmoid=Sigmoid,\n    tanh=Tanh,\n    hard_sigmoid=nn.Hardsigmoid if _has_hardsigmoid else HardSigmoid,\n    hard_swish=nn.Hardswish if _has_hardswish else HardSwish,\n    hard_mish=HardMish,\n    identity=nn.Identity,\n)\n\n_ACT_LAYER_JIT = dict(\n    silu=nn.SiLU if _has_silu else SwishJit,\n    swish=nn.SiLU if _has_silu else SwishJit,\n    mish=nn.Mish if _has_mish else MishJit,\n    hard_sigmoid=nn.Hardsigmoid if _has_hardsigmoid else HardSigmoidJit,\n    hard_swish=nn.Hardswish if _has_hardswish else HardSwishJit,\n    hard_mish=HardMishJit\n)\n\n_ACT_LAYER_ME = dict(\n    silu=nn.SiLU if _has_silu else SwishMe,\n    swish=nn.SiLU if _has_silu else SwishMe,\n    mish=nn.Mish if _has_mish else MishMe,\n    hard_sigmoid=nn.Hardsigmoid if _has_hardsigmoid else HardSigmoidMe,\n    hard_swish=nn.Hardswish if _has_hardswish else HardSwishMe,\n    hard_mish=HardMishMe,\n)\n\n_ACT_LAYERS = (_ACT_LAYER_ME, _ACT_LAYER_JIT, _ACT_LAYER_DEFAULT)\nfor a in _ACT_LAYERS:\n    a.setdefault('hardsigmoid', a.get('hard_sigmoid'))\n    a.setdefault('hardswish', a.get('hard_swish'))\n\n\ndef get_act_fn(name: Union[Callable, str] = 'relu'):\n    \"\"\" Activation Function Factory\n    Fetching activation fns by name with this function allows export or torch script friendly\n    functions to be returned dynamically based on current config.\n    \"\"\"\n    if not name:\n        return None\n    if isinstance(name, Callable):\n        return name\n    if not (is_no_jit() or is_exportable() or is_scriptable()):\n        # If not exporting or scripting the model, first look for a memory-efficient version with\n        # custom autograd, then fallback\n        if name in _ACT_FN_ME:\n            return _ACT_FN_ME[name]\n    if not (is_no_jit() or is_exportable()):\n        if name in _ACT_FN_JIT:\n            return _ACT_FN_JIT[name]\n    return _ACT_FN_DEFAULT[name]\n\n\ndef get_act_layer(name: Union[Type[nn.Module], str] = 'relu'):\n    \"\"\" Activation Layer Factory\n    Fetching activation layers by name with this function allows export or torch script friendly\n    functions to be returned dynamically based on current config.\n    \"\"\"\n    if not name:\n        return None\n    if not isinstance(name, str):\n        # callable, module, etc\n        return name\n    if not (is_no_jit() or is_exportable() or is_scriptable()):\n        if name in _ACT_LAYER_ME:\n            return _ACT_LAYER_ME[name]\n    if not (is_no_jit() or is_exportable()):\n        if name in _ACT_LAYER_JIT:\n            return _ACT_LAYER_JIT[name]\n    return _ACT_LAYER_DEFAULT[name]\n\n\ndef create_act_layer(name: Union[nn.Module, str], inplace=None, **kwargs):\n    act_layer = get_act_layer(name)\n    if act_layer is None:\n        return None\n    if inplace is None:\n        return act_layer(**kwargs)\n    try:\n        return act_layer(inplace=inplace, **kwargs)\n    except TypeError:\n        # recover if act layer doesn't have inplace arg\n        return act_layer(**kwargs)\n",
  "\"\"\" Split Attention Conv2d (for ResNeSt Models)\n\nPaper: `ResNeSt: Split-Attention Networks` - /https://arxiv.org/abs/2004.08955\n\nAdapted from original PyTorch impl at https://github.com/zhanghang1989/ResNeSt\n\nModified for torchscript compat, performance, and consistency with timm by Ross Wightman\n\"\"\"\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom .helpers import make_divisible\n\n\nclass RadixSoftmax(nn.Module):\n    def __init__(self, radix, cardinality):\n        super(RadixSoftmax, self).__init__()\n        self.radix = radix\n        self.cardinality = cardinality\n\n    def forward(self, x):\n        batch = x.size(0)\n        if self.radix > 1:\n            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n            x = F.softmax(x, dim=1)\n            x = x.reshape(batch, -1)\n        else:\n            x = torch.sigmoid(x)\n        return x\n\n\nclass SplitAttn(nn.Module):\n    \"\"\"Split-Attention (aka Splat)\n    \"\"\"\n    def __init__(self, in_channels, out_channels=None, kernel_size=3, stride=1, padding=None,\n                 dilation=1, groups=1, bias=False, radix=2, rd_ratio=0.25, rd_channels=None, rd_divisor=8,\n                 act_layer=nn.ReLU, norm_layer=None, drop_layer=None, **kwargs):\n        super(SplitAttn, self).__init__()\n        out_channels = out_channels or in_channels\n        self.radix = radix\n        mid_chs = out_channels * radix\n        if rd_channels is None:\n            attn_chs = make_divisible(in_channels * radix * rd_ratio, min_value=32, divisor=rd_divisor)\n        else:\n            attn_chs = rd_channels * radix\n\n        padding = kernel_size // 2 if padding is None else padding\n        self.conv = nn.Conv2d(\n            in_channels, mid_chs, kernel_size, stride, padding, dilation,\n            groups=groups * radix, bias=bias, **kwargs)\n        self.bn0 = norm_layer(mid_chs) if norm_layer else nn.Identity()\n        self.drop = drop_layer() if drop_layer is not None else nn.Identity()\n        self.act0 = act_layer(inplace=True)\n        self.fc1 = nn.Conv2d(out_channels, attn_chs, 1, groups=groups)\n        self.bn1 = norm_layer(attn_chs) if norm_layer else nn.Identity()\n        self.act1 = act_layer(inplace=True)\n        self.fc2 = nn.Conv2d(attn_chs, mid_chs, 1, groups=groups)\n        self.rsoftmax = RadixSoftmax(radix, groups)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn0(x)\n        x = self.drop(x)\n        x = self.act0(x)\n\n        B, RC, H, W = x.shape\n        if self.radix > 1:\n            x = x.reshape((B, self.radix, RC // self.radix, H, W))\n            x_gap = x.sum(dim=1)\n        else:\n            x_gap = x\n        x_gap = x_gap.mean((2, 3), keepdim=True)\n        x_gap = self.fc1(x_gap)\n        x_gap = self.bn1(x_gap)\n        x_gap = self.act1(x_gap)\n        x_attn = self.fc2(x_gap)\n\n        x_attn = self.rsoftmax(x_attn).view(B, -1, 1, 1)\n        if self.radix > 1:\n            out = (x * x_attn.reshape((B, self.radix, RC // self.radix, 1, 1))).sum(dim=1)\n        else:\n            out = x * x_attn\n        return out.contiguous()\n",
  "import torch\nimport math\nimport warnings\n\nfrom torch.nn.init import _calculate_fan_in_and_fan_out\n\n\ndef _trunc_normal_(tensor, mean, std, a, b):\n    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n    def norm_cdf(x):\n        # Computes standard normal cumulative distribution function\n        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\n    if (mean < a - 2 * std) or (mean > b + 2 * std):\n        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n                      \"The distribution of values may be incorrect.\",\n                      stacklevel=2)\n\n    # Values are generated by using a truncated uniform distribution and\n    # then using the inverse CDF for the normal distribution.\n    # Get upper and lower cdf values\n    l = norm_cdf((a - mean) / std)\n    u = norm_cdf((b - mean) / std)\n\n    # Uniformly fill tensor with values from [l, u], then translate to\n    # [2l-1, 2u-1].\n    tensor.uniform_(2 * l - 1, 2 * u - 1)\n\n    # Use inverse cdf transform for normal distribution to get truncated\n    # standard normal\n    tensor.erfinv_()\n\n    # Transform to proper mean, std\n    tensor.mul_(std * math.sqrt(2.))\n    tensor.add_(mean)\n\n    # Clamp to ensure it's in the proper range\n    tensor.clamp_(min=a, max=b)\n    return tensor\n\n\ndef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n    # type: (Tensor, float, float, float, float) -> Tensor\n    r\"\"\"Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n\n    NOTE: this impl is similar to the PyTorch trunc_normal_, the bounds [a, b] are\n    applied while sampling the normal with mean/std applied, therefore a, b args\n    should be adjusted to match the range of mean, std args.\n\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution\n        std: the standard deviation of the normal distribution\n        a: the minimum cutoff value\n        b: the maximum cutoff value\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.trunc_normal_(w)\n    \"\"\"\n    with torch.no_grad():\n        return _trunc_normal_(tensor, mean, std, a, b)\n\n\ndef trunc_normal_tf_(tensor, mean=0., std=1., a=-2., b=2.):\n    # type: (Tensor, float, float, float, float) -> Tensor\n    r\"\"\"Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n\n    NOTE: this 'tf' variant behaves closer to Tensorflow / JAX impl where the\n    bounds [a, b] are applied when sampling the normal distribution with mean=0, std=1.0\n    and the result is subsquently scaled and shifted by the mean and std args.\n\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution\n        std: the standard deviation of the normal distribution\n        a: the minimum cutoff value\n        b: the maximum cutoff value\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.trunc_normal_(w)\n    \"\"\"\n    with torch.no_grad():\n        _trunc_normal_(tensor, 0, 1.0, a, b)\n        tensor.mul_(std).add_(mean)\n    return tensor\n\n\ndef variance_scaling_(tensor, scale=1.0, mode='fan_in', distribution='normal'):\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n    if mode == 'fan_in':\n        denom = fan_in\n    elif mode == 'fan_out':\n        denom = fan_out\n    elif mode == 'fan_avg':\n        denom = (fan_in + fan_out) / 2\n\n    variance = scale / denom\n\n    if distribution == \"truncated_normal\":\n        # constant is stddev of standard normal truncated to (-2, 2)\n        trunc_normal_tf_(tensor, std=math.sqrt(variance) / .87962566103423978)\n    elif distribution == \"normal\":\n        with torch.no_grad():\n            tensor.normal_(std=math.sqrt(variance))\n    elif distribution == \"uniform\":\n        bound = math.sqrt(3 * variance)\n        with torch.no_grad():\n            tensor.uniform_(-bound, bound)\n    else:\n        raise ValueError(f\"invalid distribution {distribution}\")\n\n\ndef lecun_normal_(tensor):\n    variance_scaling_(tensor, mode='fan_in', distribution='truncated_normal')\n",
  "\"\"\" Depthwise Separable Conv Modules\n\nBasic DWS convs. Other variations of DWS exist with batch norm or activations between the\nDW and PW convs such as the Depthwise modules in MobileNetV2 / EfficientNet and Xception.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom torch import nn as nn\n\nfrom .create_conv2d import create_conv2d\nfrom .create_norm_act import get_norm_act_layer\n\n\nclass SeparableConvNormAct(nn.Module):\n    \"\"\" Separable Conv w/ trailing Norm and Activation\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1, padding='', bias=False,\n                 channel_multiplier=1.0, pw_kernel_size=1, norm_layer=nn.BatchNorm2d, act_layer=nn.ReLU,\n                 apply_act=True, drop_layer=None):\n        super(SeparableConvNormAct, self).__init__()\n\n        self.conv_dw = create_conv2d(\n            in_channels, int(in_channels * channel_multiplier), kernel_size,\n            stride=stride, dilation=dilation, padding=padding, depthwise=True)\n\n        self.conv_pw = create_conv2d(\n            int(in_channels * channel_multiplier), out_channels, pw_kernel_size, padding=padding, bias=bias)\n\n        norm_act_layer = get_norm_act_layer(norm_layer, act_layer)\n        norm_kwargs = dict(drop_layer=drop_layer) if drop_layer is not None else {}\n        self.bn = norm_act_layer(out_channels, apply_act=apply_act, **norm_kwargs)\n\n    @property\n    def in_channels(self):\n        return self.conv_dw.in_channels\n\n    @property\n    def out_channels(self):\n        return self.conv_pw.out_channels\n\n    def forward(self, x):\n        x = self.conv_dw(x)\n        x = self.conv_pw(x)\n        x = self.bn(x)\n        return x\n\n\nSeparableConvBnAct = SeparableConvNormAct\n\n\nclass SeparableConv2d(nn.Module):\n    \"\"\" Separable Conv\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1, padding='', bias=False,\n                 channel_multiplier=1.0, pw_kernel_size=1):\n        super(SeparableConv2d, self).__init__()\n\n        self.conv_dw = create_conv2d(\n            in_channels, int(in_channels * channel_multiplier), kernel_size,\n            stride=stride, dilation=dilation, padding=padding, depthwise=True)\n\n        self.conv_pw = create_conv2d(\n            int(in_channels * channel_multiplier), out_channels, pw_kernel_size, padding=padding, bias=bias)\n\n    @property\n    def in_channels(self):\n        return self.conv_dw.in_channels\n\n    @property\n    def out_channels(self):\n        return self.conv_pw.out_channels\n\n    def forward(self, x):\n        x = self.conv_dw(x)\n        x = self.conv_pw(x)\n        return x\n",
  "\"\"\" EvoNorm in PyTorch\n\nBased on `Evolving Normalization-Activation Layers` - https://arxiv.org/abs/2004.02967\n@inproceedings{NEURIPS2020,\n author = {Liu, Hanxiao and Brock, Andy and Simonyan, Karen and Le, Quoc},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},\n pages = {13539--13550},\n publisher = {Curran Associates, Inc.},\n title = {Evolving Normalization-Activation Layers},\n url = {https://proceedings.neurips.cc/paper/2020/file/9d4c03631b8b0c85ae08bf05eda37d0f-Paper.pdf},\n volume = {33},\n year = {2020}\n}\n\nAn attempt at getting decent performing EvoNorms running in PyTorch.\nWhile faster than other PyTorch impl, still quite a ways off the built-in BatchNorm\nin terms of memory usage and throughput on GPUs.\n\nI'm testing these modules on TPU w/ PyTorch XLA. Promising start but\ncurrently working around some issues with builtin torch/tensor.var/std. Unlike\nGPU, similar train speeds for EvoNormS variants and BatchNorm.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom typing import Sequence, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .create_act import create_act_layer\nfrom .trace_utils import _assert\n\n\ndef instance_std(x, eps: float = 1e-5):\n    std = x.float().var(dim=(2, 3), unbiased=False, keepdim=True).add(eps).sqrt().to(x.dtype)\n    return std.expand(x.shape)\n\n\ndef instance_std_tpu(x, eps: float = 1e-5):\n    std = manual_var(x, dim=(2, 3)).add(eps).sqrt()\n    return std.expand(x.shape)\n# instance_std = instance_std_tpu\n\n\ndef instance_rms(x, eps: float = 1e-5):\n    rms = x.float().square().mean(dim=(2, 3), keepdim=True).add(eps).sqrt().to(x.dtype)\n    return rms.expand(x.shape)\n\n\ndef manual_var(x, dim: Union[int, Sequence[int]], diff_sqm: bool = False):\n    xm = x.mean(dim=dim, keepdim=True)\n    if diff_sqm:\n        # difference of squared mean and mean squared, faster on TPU can be less stable\n        var = ((x * x).mean(dim=dim, keepdim=True) - (xm * xm)).clamp(0)\n    else:\n        var = ((x - xm) * (x - xm)).mean(dim=dim, keepdim=True)\n    return var\n\n\ndef group_std(x, groups: int = 32, eps: float = 1e-5, flatten: bool = False):\n    B, C, H, W = x.shape\n    x_dtype = x.dtype\n    _assert(C % groups == 0, '')\n    if flatten:\n        x = x.reshape(B, groups, -1)  # FIXME simpler shape causing TPU / XLA issues\n        std = x.float().var(dim=2, unbiased=False, keepdim=True).add(eps).sqrt().to(x_dtype)\n    else:\n        x = x.reshape(B, groups, C // groups, H, W)\n        std = x.float().var(dim=(2, 3, 4), unbiased=False, keepdim=True).add(eps).sqrt().to(x_dtype)\n    return std.expand(x.shape).reshape(B, C, H, W)\n\n\ndef group_std_tpu(x, groups: int = 32, eps: float = 1e-5, diff_sqm: bool = False, flatten: bool = False):\n    # This is a workaround for some stability / odd behaviour of .var and .std\n    # running on PyTorch XLA w/ TPUs. These manual var impl are producing much better results\n    B, C, H, W = x.shape\n    _assert(C % groups == 0, '')\n    if flatten:\n        x = x.reshape(B, groups, -1)  # FIXME simpler shape causing TPU / XLA issues\n        var = manual_var(x, dim=-1, diff_sqm=diff_sqm)\n    else:\n        x = x.reshape(B, groups, C // groups, H, W)\n        var = manual_var(x, dim=(2, 3, 4), diff_sqm=diff_sqm)\n    return var.add(eps).sqrt().expand(x.shape).reshape(B, C, H, W)\n#group_std = group_std_tpu  # FIXME TPU temporary\n\n\ndef group_rms(x, groups: int = 32, eps: float = 1e-5):\n    B, C, H, W = x.shape\n    _assert(C % groups == 0, '')\n    x_dtype = x.dtype\n    x = x.reshape(B, groups, C // groups, H, W)\n    rms = x.float().square().mean(dim=(2, 3, 4), keepdim=True).add(eps).sqrt_().to(x_dtype)\n    return rms.expand(x.shape).reshape(B, C, H, W)\n\n\nclass EvoNorm2dB0(nn.Module):\n    def __init__(self, num_features, apply_act=True, momentum=0.1, eps=1e-3, **_):\n        super().__init__()\n        self.apply_act = apply_act  # apply activation (non-linearity)\n        self.momentum = momentum\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(num_features))\n        self.bias = nn.Parameter(torch.zeros(num_features))\n        self.v = nn.Parameter(torch.ones(num_features)) if apply_act else None\n        self.register_buffer('running_var', torch.ones(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.ones_(self.weight)\n        nn.init.zeros_(self.bias)\n        if self.v is not None:\n            nn.init.ones_(self.v)\n\n    def forward(self, x):\n        _assert(x.dim() == 4, 'expected 4D input')\n        x_dtype = x.dtype\n        v_shape = (1, -1, 1, 1)\n        if self.v is not None:\n            if self.training:\n                var = x.float().var(dim=(0, 2, 3), unbiased=False)\n                # var = manual_var(x, dim=(0, 2, 3)).squeeze()\n                n = x.numel() / x.shape[1]\n                self.running_var.copy_(\n                    self.running_var * (1 - self.momentum) +\n                    var.detach() * self.momentum * (n / (n - 1)))\n            else:\n                var = self.running_var\n            left = var.add(self.eps).sqrt_().to(x_dtype).view(v_shape).expand_as(x)\n            v = self.v.to(x_dtype).view(v_shape)\n            right = x * v + instance_std(x, self.eps)\n            x = x / left.max(right)\n        return x * self.weight.to(x_dtype).view(v_shape) + self.bias.to(x_dtype).view(v_shape)\n\n\nclass EvoNorm2dB1(nn.Module):\n    def __init__(self, num_features, apply_act=True, momentum=0.1, eps=1e-5, **_):\n        super().__init__()\n        self.apply_act = apply_act  # apply activation (non-linearity)\n        self.momentum = momentum\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(num_features))\n        self.bias = nn.Parameter(torch.zeros(num_features))\n        self.register_buffer('running_var', torch.ones(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.ones_(self.weight)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, x):\n        _assert(x.dim() == 4, 'expected 4D input')\n        x_dtype = x.dtype\n        v_shape = (1, -1, 1, 1)\n        if self.apply_act:\n            if self.training:\n                var = x.float().var(dim=(0, 2, 3), unbiased=False)\n                n = x.numel() / x.shape[1]\n                self.running_var.copy_(\n                    self.running_var * (1 - self.momentum) +\n                    var.detach().to(self.running_var.dtype) * self.momentum * (n / (n - 1)))\n            else:\n                var = self.running_var\n            var = var.to(x_dtype).view(v_shape)\n            left = var.add(self.eps).sqrt_()\n            right = (x + 1) * instance_rms(x, self.eps)\n            x = x / left.max(right)\n        return x * self.weight.view(v_shape).to(x_dtype) + self.bias.view(v_shape).to(x_dtype)\n\n\nclass EvoNorm2dB2(nn.Module):\n    def __init__(self, num_features, apply_act=True, momentum=0.1, eps=1e-5, **_):\n        super().__init__()\n        self.apply_act = apply_act  # apply activation (non-linearity)\n        self.momentum = momentum\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(num_features))\n        self.bias = nn.Parameter(torch.zeros(num_features))\n        self.register_buffer('running_var', torch.ones(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.ones_(self.weight)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, x):\n        _assert(x.dim() == 4, 'expected 4D input')\n        x_dtype = x.dtype\n        v_shape = (1, -1, 1, 1)\n        if self.apply_act:\n            if self.training:\n                var = x.float().var(dim=(0, 2, 3), unbiased=False)\n                n = x.numel() / x.shape[1]\n                self.running_var.copy_(\n                    self.running_var * (1 - self.momentum) +\n                    var.detach().to(self.running_var.dtype) * self.momentum * (n / (n - 1)))\n            else:\n                var = self.running_var\n            var = var.to(x_dtype).view(v_shape)\n            left = var.add(self.eps).sqrt_()\n            right = instance_rms(x, self.eps) - x\n            x = x / left.max(right)\n        return x * self.weight.view(v_shape).to(x_dtype) + self.bias.view(v_shape).to(x_dtype)\n\n\nclass EvoNorm2dS0(nn.Module):\n    def __init__(self, num_features, groups=32, group_size=None, apply_act=True, eps=1e-5, **_):\n        super().__init__()\n        self.apply_act = apply_act  # apply activation (non-linearity)\n        if group_size:\n            assert num_features % group_size == 0\n            self.groups = num_features // group_size\n        else:\n            self.groups = groups\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(num_features))\n        self.bias = nn.Parameter(torch.zeros(num_features))\n        self.v = nn.Parameter(torch.ones(num_features)) if apply_act else None\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.ones_(self.weight)\n        nn.init.zeros_(self.bias)\n        if self.v is not None:\n            nn.init.ones_(self.v)\n\n    def forward(self, x):\n        _assert(x.dim() == 4, 'expected 4D input')\n        x_dtype = x.dtype\n        v_shape = (1, -1, 1, 1)\n        if self.v is not None:\n            v = self.v.view(v_shape).to(x_dtype)\n            x = x * (x * v).sigmoid() / group_std(x, self.groups, self.eps)\n        return x * self.weight.view(v_shape).to(x_dtype) + self.bias.view(v_shape).to(x_dtype)\n\n\nclass EvoNorm2dS0a(EvoNorm2dS0):\n    def __init__(self, num_features, groups=32, group_size=None, apply_act=True, eps=1e-3, **_):\n        super().__init__(\n            num_features, groups=groups, group_size=group_size, apply_act=apply_act, eps=eps)\n\n    def forward(self, x):\n        _assert(x.dim() == 4, 'expected 4D input')\n        x_dtype = x.dtype\n        v_shape = (1, -1, 1, 1)\n        d = group_std(x, self.groups, self.eps)\n        if self.v is not None:\n            v = self.v.view(v_shape).to(x_dtype)\n            x = x * (x * v).sigmoid()\n        x = x / d\n        return x * self.weight.view(v_shape).to(x_dtype) + self.bias.view(v_shape).to(x_dtype)\n\n\nclass EvoNorm2dS1(nn.Module):\n    def __init__(\n            self, num_features, groups=32, group_size=None,\n            apply_act=True, act_layer=None, eps=1e-5, **_):\n        super().__init__()\n        act_layer = act_layer or nn.SiLU\n        self.apply_act = apply_act  # apply activation (non-linearity)\n        if act_layer is not None and apply_act:\n            self.act = create_act_layer(act_layer)\n        else:\n            self.act = nn.Identity()\n        if group_size:\n            assert num_features % group_size == 0\n            self.groups = num_features // group_size\n        else:\n            self.groups = groups\n        self.eps = eps\n        self.pre_act_norm = False\n        self.weight = nn.Parameter(torch.ones(num_features))\n        self.bias = nn.Parameter(torch.zeros(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.ones_(self.weight)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, x):\n        _assert(x.dim() == 4, 'expected 4D input')\n        x_dtype = x.dtype\n        v_shape = (1, -1, 1, 1)\n        if self.apply_act:\n            x = self.act(x) / group_std(x, self.groups, self.eps)\n        return x * self.weight.view(v_shape).to(x_dtype) + self.bias.view(v_shape).to(x_dtype)\n\n\nclass EvoNorm2dS1a(EvoNorm2dS1):\n    def __init__(\n            self, num_features, groups=32, group_size=None,\n            apply_act=True, act_layer=None, eps=1e-3, **_):\n        super().__init__(\n            num_features, groups=groups, group_size=group_size, apply_act=apply_act, act_layer=act_layer, eps=eps)\n\n    def forward(self, x):\n        _assert(x.dim() == 4, 'expected 4D input')\n        x_dtype = x.dtype\n        v_shape = (1, -1, 1, 1)\n        x = self.act(x) / group_std(x, self.groups, self.eps)\n        return x * self.weight.view(v_shape).to(x_dtype) + self.bias.view(v_shape).to(x_dtype)\n\n\nclass EvoNorm2dS2(nn.Module):\n    def __init__(\n            self, num_features, groups=32, group_size=None,\n            apply_act=True, act_layer=None, eps=1e-5, **_):\n        super().__init__()\n        act_layer = act_layer or nn.SiLU\n        self.apply_act = apply_act  # apply activation (non-linearity)\n        if act_layer is not None and apply_act:\n            self.act = create_act_layer(act_layer)\n        else:\n            self.act = nn.Identity()\n        if group_size:\n            assert num_features % group_size == 0\n            self.groups = num_features // group_size\n        else:\n            self.groups = groups\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(num_features))\n        self.bias = nn.Parameter(torch.zeros(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.ones_(self.weight)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, x):\n        _assert(x.dim() == 4, 'expected 4D input')\n        x_dtype = x.dtype\n        v_shape = (1, -1, 1, 1)\n        if self.apply_act:\n            x = self.act(x) / group_rms(x, self.groups, self.eps)\n        return x * self.weight.view(v_shape).to(x_dtype) + self.bias.view(v_shape).to(x_dtype)\n\n\nclass EvoNorm2dS2a(EvoNorm2dS2):\n    def __init__(\n            self, num_features, groups=32, group_size=None,\n            apply_act=True, act_layer=None, eps=1e-3, **_):\n        super().__init__(\n            num_features, groups=groups, group_size=group_size, apply_act=apply_act, act_layer=act_layer, eps=eps)\n\n    def forward(self, x):\n        _assert(x.dim() == 4, 'expected 4D input')\n        x_dtype = x.dtype\n        v_shape = (1, -1, 1, 1)\n        x = self.act(x) / group_rms(x, self.groups, self.eps)\n        return x * self.weight.view(v_shape).to(x_dtype) + self.bias.view(v_shape).to(x_dtype)\n",
  "\"\"\" Global Context Attention Block\n\nPaper: `GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond`\n    - https://arxiv.org/abs/1904.11492\n\nOfficial code consulted as reference: https://github.com/xvjiarui/GCNet\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nfrom torch import nn as nn\nimport torch.nn.functional as F\n\nfrom .create_act import create_act_layer, get_act_layer\nfrom .helpers import make_divisible\nfrom .mlp import ConvMlp\nfrom .norm import LayerNorm2d\n\n\nclass GlobalContext(nn.Module):\n\n    def __init__(self, channels, use_attn=True, fuse_add=False, fuse_scale=True, init_last_zero=False,\n                 rd_ratio=1./8, rd_channels=None, rd_divisor=1, act_layer=nn.ReLU, gate_layer='sigmoid'):\n        super(GlobalContext, self).__init__()\n        act_layer = get_act_layer(act_layer)\n\n        self.conv_attn = nn.Conv2d(channels, 1, kernel_size=1, bias=True) if use_attn else None\n\n        if rd_channels is None:\n            rd_channels = make_divisible(channels * rd_ratio, rd_divisor, round_limit=0.)\n        if fuse_add:\n            self.mlp_add = ConvMlp(channels, rd_channels, act_layer=act_layer, norm_layer=LayerNorm2d)\n        else:\n            self.mlp_add = None\n        if fuse_scale:\n            self.mlp_scale = ConvMlp(channels, rd_channels, act_layer=act_layer, norm_layer=LayerNorm2d)\n        else:\n            self.mlp_scale = None\n\n        self.gate = create_act_layer(gate_layer)\n        self.init_last_zero = init_last_zero\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        if self.conv_attn is not None:\n            nn.init.kaiming_normal_(self.conv_attn.weight, mode='fan_in', nonlinearity='relu')\n        if self.mlp_add is not None:\n            nn.init.zeros_(self.mlp_add.fc2.weight)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n\n        if self.conv_attn is not None:\n            attn = self.conv_attn(x).reshape(B, 1, H * W)  # (B, 1, H * W)\n            attn = F.softmax(attn, dim=-1).unsqueeze(3)  # (B, 1, H * W, 1)\n            context = x.reshape(B, C, H * W).unsqueeze(1) @ attn\n            context = context.view(B, C, 1, 1)\n        else:\n            context = x.mean(dim=(2, 3), keepdim=True)\n\n        if self.mlp_scale is not None:\n            mlp_x = self.mlp_scale(context)\n            x = x * self.gate(mlp_x)\n        if self.mlp_add is not None:\n            mlp_x = self.mlp_add(context)\n            x = x + mlp_x\n\n        return x\n",
  "\"\"\" Test Time Pooling (Average-Max Pool)\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport logging\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom .adaptive_avgmax_pool import adaptive_avgmax_pool2d\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass TestTimePoolHead(nn.Module):\n    def __init__(self, base, original_pool=7):\n        super(TestTimePoolHead, self).__init__()\n        self.base = base\n        self.original_pool = original_pool\n        base_fc = self.base.get_classifier()\n        if isinstance(base_fc, nn.Conv2d):\n            self.fc = base_fc\n        else:\n            self.fc = nn.Conv2d(\n                self.base.num_features, self.base.num_classes, kernel_size=1, bias=True)\n            self.fc.weight.data.copy_(base_fc.weight.data.view(self.fc.weight.size()))\n            self.fc.bias.data.copy_(base_fc.bias.data.view(self.fc.bias.size()))\n        self.base.reset_classifier(0)  # delete original fc layer\n\n    def forward(self, x):\n        x = self.base.forward_features(x)\n        x = F.avg_pool2d(x, kernel_size=self.original_pool, stride=1)\n        x = self.fc(x)\n        x = adaptive_avgmax_pool2d(x, 1)\n        return x.view(x.size(0), -1)\n\n\ndef apply_test_time_pool(model, config, use_test_size=False):\n    test_time_pool = False\n    if not hasattr(model, 'default_cfg') or not model.default_cfg:\n        return model, False\n    if use_test_size and 'test_input_size' in model.default_cfg:\n        df_input_size = model.default_cfg['test_input_size']\n    else:\n        df_input_size = model.default_cfg['input_size']\n    if config['input_size'][-1] > df_input_size[-1] and config['input_size'][-2] > df_input_size[-2]:\n        _logger.info('Target input size %s > pretrained default %s, using test time pooling' %\n                     (str(config['input_size'][-2:]), str(df_input_size[-2:])))\n        model = TestTimePoolHead(model, original_pool=model.default_cfg['pool_size'])\n        test_time_pool = True\n    return model, test_time_pool\n",
  "\"\"\" Activations\n\nA collection of jit-scripted activations fn and modules with a common interface so that they can\neasily be swapped. All have an `inplace` arg even if not used.\n\nAll jit scripted activations are lacking in-place variations on purpose, scripted kernel fusion does not\ncurrently work across in-place op boundaries, thus performance is equal to or less than the non-scripted\nversions if they contain in-place ops.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\n\n@torch.jit.script\ndef swish_jit(x, inplace: bool = False):\n    \"\"\"Swish - Described in: https://arxiv.org/abs/1710.05941\n    \"\"\"\n    return x.mul(x.sigmoid())\n\n\n@torch.jit.script\ndef mish_jit(x, _inplace: bool = False):\n    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n    \"\"\"\n    return x.mul(F.softplus(x).tanh())\n\n\nclass SwishJit(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(SwishJit, self).__init__()\n\n    def forward(self, x):\n        return swish_jit(x)\n\n\nclass MishJit(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(MishJit, self).__init__()\n\n    def forward(self, x):\n        return mish_jit(x)\n\n\n@torch.jit.script\ndef hard_sigmoid_jit(x, inplace: bool = False):\n    # return F.relu6(x + 3.) / 6.\n    return (x + 3).clamp(min=0, max=6).div(6.)  # clamp seems ever so slightly faster?\n\n\nclass HardSigmoidJit(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(HardSigmoidJit, self).__init__()\n\n    def forward(self, x):\n        return hard_sigmoid_jit(x)\n\n\n@torch.jit.script\ndef hard_swish_jit(x, inplace: bool = False):\n    # return x * (F.relu6(x + 3.) / 6)\n    return x * (x + 3).clamp(min=0, max=6).div(6.)  # clamp seems ever so slightly faster?\n\n\nclass HardSwishJit(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(HardSwishJit, self).__init__()\n\n    def forward(self, x):\n        return hard_swish_jit(x)\n\n\n@torch.jit.script\ndef hard_mish_jit(x, inplace: bool = False):\n    \"\"\" Hard Mish\n    Experimental, based on notes by Mish author Diganta Misra at\n      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n    \"\"\"\n    return 0.5 * x * (x + 2).clamp(min=0, max=2)\n\n\nclass HardMishJit(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(HardMishJit, self).__init__()\n\n    def forward(self, x):\n        return hard_mish_jit(x)\n",
  "\"\"\" MLP module w/ dropout and configurable activation layer\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom functools import partial\n\nfrom torch import nn as nn\n\nfrom .grn import GlobalResponseNorm\nfrom .helpers import to_2tuple\n\n\nclass Mlp(nn.Module):\n    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            norm_layer=None,\n            bias=True,\n            drop=0.,\n            use_conv=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = to_2tuple(bias)\n        drop_probs = to_2tuple(drop)\n        linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear\n\n        self.fc1 = linear_layer(in_features, hidden_features, bias=bias[0])\n        self.act = act_layer()\n        self.drop1 = nn.Dropout(drop_probs[0])\n        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n        self.fc2 = linear_layer(hidden_features, out_features, bias=bias[1])\n        self.drop2 = nn.Dropout(drop_probs[1])\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop1(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x\n\n\nclass GluMlp(nn.Module):\n    \"\"\" MLP w/ GLU style gating\n    See: https://arxiv.org/abs/1612.08083, https://arxiv.org/abs/2002.05202\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.Sigmoid,\n            norm_layer=None,\n            bias=True,\n            drop=0.,\n            use_conv=False,\n            gate_last=True,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        assert hidden_features % 2 == 0\n        bias = to_2tuple(bias)\n        drop_probs = to_2tuple(drop)\n        linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear\n        self.chunk_dim = 1 if use_conv else -1\n        self.gate_last = gate_last  # use second half of width for gate\n\n        self.fc1 = linear_layer(in_features, hidden_features, bias=bias[0])\n        self.act = act_layer()\n        self.drop1 = nn.Dropout(drop_probs[0])\n        self.norm = norm_layer(hidden_features // 2) if norm_layer is not None else nn.Identity()\n        self.fc2 = linear_layer(hidden_features // 2, out_features, bias=bias[1])\n        self.drop2 = nn.Dropout(drop_probs[1])\n\n    def init_weights(self):\n        # override init of fc1 w/ gate portion set to weight near zero, bias=1\n        fc1_mid = self.fc1.bias.shape[0] // 2\n        nn.init.ones_(self.fc1.bias[fc1_mid:])\n        nn.init.normal_(self.fc1.weight[fc1_mid:], std=1e-6)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x1, x2 = x.chunk(2, dim=self.chunk_dim)\n        x = x1 * self.act(x2) if self.gate_last else self.act(x1) * x2\n        x = self.drop1(x)\n        x = self.norm(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x\n\n\nSwiGLUPacked = partial(GluMlp, act_layer=nn.SiLU, gate_last=False)\n\n\nclass SwiGLU(nn.Module):\n    \"\"\" SwiGLU\n    NOTE: GluMLP above can implement SwiGLU, but this impl has split fc1 and\n    better matches some other common impl which makes mapping checkpoints simpler.\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.SiLU,\n            norm_layer=None,\n            bias=True,\n            drop=0.,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = to_2tuple(bias)\n        drop_probs = to_2tuple(drop)\n\n        self.fc1_g = nn.Linear(in_features, hidden_features, bias=bias[0])\n        self.fc1_x = nn.Linear(in_features, hidden_features, bias=bias[0])\n        self.act = act_layer()\n        self.drop1 = nn.Dropout(drop_probs[0])\n        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1])\n        self.drop2 = nn.Dropout(drop_probs[1])\n\n        self.drop = nn.Dropout(drop)\n\n    def init_weights(self):\n        # override init of fc1 w/ gate portion set to weight near zero, bias=1\n        nn.init.ones_(self.fc1_g.bias)\n        nn.init.normal_(self.fc1_g.weight, std=1e-6)\n\n    def forward(self, x):\n        x_gate = self.fc1_g(x)\n        x = self.fc1_x(x)\n        x = self.act(x_gate) * x\n        x = self.drop1(x)\n        x = self.norm(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x\n\n\nclass GatedMlp(nn.Module):\n    \"\"\" MLP as used in gMLP\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            norm_layer=None,\n            gate_layer=None,\n            bias=True,\n            drop=0.,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = to_2tuple(bias)\n        drop_probs = to_2tuple(drop)\n\n        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias[0])\n        self.act = act_layer()\n        self.drop1 = nn.Dropout(drop_probs[0])\n        if gate_layer is not None:\n            assert hidden_features % 2 == 0\n            self.gate = gate_layer(hidden_features)\n            hidden_features = hidden_features // 2  # FIXME base reduction on gate property?\n        else:\n            self.gate = nn.Identity()\n        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1])\n        self.drop2 = nn.Dropout(drop_probs[1])\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop1(x)\n        x = self.gate(x)\n        x = self.norm(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x\n\n\nclass ConvMlp(nn.Module):\n    \"\"\" MLP using 1x1 convs that keeps spatial dims\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.ReLU,\n            norm_layer=None,\n            bias=True,\n            drop=0.,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = to_2tuple(bias)\n\n        self.fc1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, bias=bias[0])\n        self.norm = norm_layer(hidden_features) if norm_layer else nn.Identity()\n        self.act = act_layer()\n        self.drop = nn.Dropout(drop)\n        self.fc2 = nn.Conv2d(hidden_features, out_features, kernel_size=1, bias=bias[1])\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.norm(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        return x\n\n\nclass GlobalResponseNormMlp(nn.Module):\n    \"\"\" MLP w/ Global Response Norm (see grn.py), nn.Linear or 1x1 Conv2d\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            bias=True,\n            drop=0.,\n            use_conv=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = to_2tuple(bias)\n        drop_probs = to_2tuple(drop)\n        linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear\n\n        self.fc1 = linear_layer(in_features, hidden_features, bias=bias[0])\n        self.act = act_layer()\n        self.drop1 = nn.Dropout(drop_probs[0])\n        self.grn = GlobalResponseNorm(hidden_features, channels_last=not use_conv)\n        self.fc2 = linear_layer(hidden_features, out_features, bias=bias[1])\n        self.drop2 = nn.Dropout(drop_probs[1])\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop1(x)\n        x = self.grn(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x\n",
  "\"\"\" PyTorch Mixed Convolution\n\nPaper: MixConv: Mixed Depthwise Convolutional Kernels (https://arxiv.org/abs/1907.09595)\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport torch\nfrom torch import nn as nn\n\nfrom .conv2d_same import create_conv2d_pad\n\n\ndef _split_channels(num_chan, num_groups):\n    split = [num_chan // num_groups for _ in range(num_groups)]\n    split[0] += num_chan - sum(split)\n    return split\n\n\nclass MixedConv2d(nn.ModuleDict):\n    \"\"\" Mixed Grouped Convolution\n\n    Based on MDConv and GroupedConv in MixNet impl:\n      https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mixnet/custom_layers.py\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=3,\n                 stride=1, padding='', dilation=1, depthwise=False, **kwargs):\n        super(MixedConv2d, self).__init__()\n\n        kernel_size = kernel_size if isinstance(kernel_size, list) else [kernel_size]\n        num_groups = len(kernel_size)\n        in_splits = _split_channels(in_channels, num_groups)\n        out_splits = _split_channels(out_channels, num_groups)\n        self.in_channels = sum(in_splits)\n        self.out_channels = sum(out_splits)\n        for idx, (k, in_ch, out_ch) in enumerate(zip(kernel_size, in_splits, out_splits)):\n            conv_groups = in_ch if depthwise else 1\n            # use add_module to keep key space clean\n            self.add_module(\n                str(idx),\n                create_conv2d_pad(\n                    in_ch, out_ch, k, stride=stride,\n                    padding=padding, dilation=dilation, groups=conv_groups, **kwargs)\n            )\n        self.splits = in_splits\n\n    def forward(self, x):\n        x_split = torch.split(x, self.splits, 1)\n        x_out = [c(x_split[i]) for i, c in enumerate(self.values())]\n        x = torch.cat(x_out, 1)\n        return x\n",
  "\"\"\" CBAM (sort-of) Attention\n\nExperimental impl of CBAM: Convolutional Block Attention Module: https://arxiv.org/abs/1807.06521\n\nWARNING: Results with these attention layers have been mixed. They can significantly reduce performance on\nsome tasks, especially fine-grained it seems. I may end up removing this impl.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\nfrom torch import nn as nn\nimport torch.nn.functional as F\n\nfrom .conv_bn_act import ConvNormAct\nfrom .create_act import create_act_layer, get_act_layer\nfrom .helpers import make_divisible\n\n\nclass ChannelAttn(nn.Module):\n    \"\"\" Original CBAM channel attention module, currently avg + max pool variant only.\n    \"\"\"\n    def __init__(\n            self, channels, rd_ratio=1./16, rd_channels=None, rd_divisor=1,\n            act_layer=nn.ReLU, gate_layer='sigmoid', mlp_bias=False):\n        super(ChannelAttn, self).__init__()\n        if not rd_channels:\n            rd_channels = make_divisible(channels * rd_ratio, rd_divisor, round_limit=0.)\n        self.fc1 = nn.Conv2d(channels, rd_channels, 1, bias=mlp_bias)\n        self.act = act_layer(inplace=True)\n        self.fc2 = nn.Conv2d(rd_channels, channels, 1, bias=mlp_bias)\n        self.gate = create_act_layer(gate_layer)\n\n    def forward(self, x):\n        x_avg = self.fc2(self.act(self.fc1(x.mean((2, 3), keepdim=True))))\n        x_max = self.fc2(self.act(self.fc1(x.amax((2, 3), keepdim=True))))\n        return x * self.gate(x_avg + x_max)\n\n\nclass LightChannelAttn(ChannelAttn):\n    \"\"\"An experimental 'lightweight' that sums avg + max pool first\n    \"\"\"\n    def __init__(\n            self, channels, rd_ratio=1./16, rd_channels=None, rd_divisor=1,\n            act_layer=nn.ReLU, gate_layer='sigmoid', mlp_bias=False):\n        super(LightChannelAttn, self).__init__(\n            channels, rd_ratio, rd_channels, rd_divisor, act_layer, gate_layer, mlp_bias)\n\n    def forward(self, x):\n        x_pool = 0.5 * x.mean((2, 3), keepdim=True) + 0.5 * x.amax((2, 3), keepdim=True)\n        x_attn = self.fc2(self.act(self.fc1(x_pool)))\n        return x * F.sigmoid(x_attn)\n\n\nclass SpatialAttn(nn.Module):\n    \"\"\" Original CBAM spatial attention module\n    \"\"\"\n    def __init__(self, kernel_size=7, gate_layer='sigmoid'):\n        super(SpatialAttn, self).__init__()\n        self.conv = ConvNormAct(2, 1, kernel_size, apply_act=False)\n        self.gate = create_act_layer(gate_layer)\n\n    def forward(self, x):\n        x_attn = torch.cat([x.mean(dim=1, keepdim=True), x.amax(dim=1, keepdim=True)], dim=1)\n        x_attn = self.conv(x_attn)\n        return x * self.gate(x_attn)\n\n\nclass LightSpatialAttn(nn.Module):\n    \"\"\"An experimental 'lightweight' variant that sums avg_pool and max_pool results.\n    \"\"\"\n    def __init__(self, kernel_size=7, gate_layer='sigmoid'):\n        super(LightSpatialAttn, self).__init__()\n        self.conv = ConvNormAct(1, 1, kernel_size, apply_act=False)\n        self.gate = create_act_layer(gate_layer)\n\n    def forward(self, x):\n        x_attn = 0.5 * x.mean(dim=1, keepdim=True) + 0.5 * x.amax(dim=1, keepdim=True)\n        x_attn = self.conv(x_attn)\n        return x * self.gate(x_attn)\n\n\nclass CbamModule(nn.Module):\n    def __init__(\n            self, channels, rd_ratio=1./16, rd_channels=None, rd_divisor=1,\n            spatial_kernel_size=7, act_layer=nn.ReLU, gate_layer='sigmoid', mlp_bias=False):\n        super(CbamModule, self).__init__()\n        self.channel = ChannelAttn(\n            channels, rd_ratio=rd_ratio, rd_channels=rd_channels,\n            rd_divisor=rd_divisor, act_layer=act_layer, gate_layer=gate_layer, mlp_bias=mlp_bias)\n        self.spatial = SpatialAttn(spatial_kernel_size, gate_layer=gate_layer)\n\n    def forward(self, x):\n        x = self.channel(x)\n        x = self.spatial(x)\n        return x\n\n\nclass LightCbamModule(nn.Module):\n    def __init__(\n            self, channels, rd_ratio=1./16, rd_channels=None, rd_divisor=1,\n            spatial_kernel_size=7, act_layer=nn.ReLU, gate_layer='sigmoid', mlp_bias=False):\n        super(LightCbamModule, self).__init__()\n        self.channel = LightChannelAttn(\n            channels, rd_ratio=rd_ratio, rd_channels=rd_channels,\n            rd_divisor=rd_divisor, act_layer=act_layer, gate_layer=gate_layer, mlp_bias=mlp_bias)\n        self.spatial = LightSpatialAttn(spatial_kernel_size)\n\n    def forward(self, x):\n        x = self.channel(x)\n        x = self.spatial(x)\n        return x\n\n",
  "\"\"\" Image to Patch Embedding using Conv2d\n\nA convolution based approach to patchifying a 2D image w/ embedding projection.\n\nBased on code in:\n  * https://github.com/google-research/vision_transformer\n  * https://github.com/google-research/big_vision/tree/main/big_vision\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport logging\nfrom typing import Callable, List, Optional, Tuple, Union\n\nimport torch\nfrom torch import nn as nn\nimport torch.nn.functional as F\n\nfrom .format import Format, nchw_to\nfrom .helpers import to_2tuple\nfrom .trace_utils import _assert\n\n_logger = logging.getLogger(__name__)\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" 2D Image to Patch Embedding\n    \"\"\"\n    output_fmt: Format\n\n    def __init__(\n            self,\n            img_size: Optional[int] = 224,\n            patch_size: int = 16,\n            in_chans: int = 3,\n            embed_dim: int = 768,\n            norm_layer: Optional[Callable] = None,\n            flatten: bool = True,\n            output_fmt: Optional[str] = None,\n            bias: bool = True,\n            strict_img_size: bool = True,\n    ):\n        super().__init__()\n        self.patch_size = to_2tuple(patch_size)\n        if img_size is not None:\n            self.img_size = to_2tuple(img_size)\n            self.grid_size = tuple([s // p for s, p in zip(self.img_size, self.patch_size)])\n            self.num_patches = self.grid_size[0] * self.grid_size[1]\n        else:\n            self.img_size = None\n            self.grid_size = None\n            self.num_patches = None\n\n        if output_fmt is not None:\n            self.flatten = False\n            self.output_fmt = Format(output_fmt)\n        else:\n            # flatten spatial dim and transpose to channels last, kept for bwd compat\n            self.flatten = flatten\n            self.output_fmt = Format.NCHW\n        self.strict_img_size = strict_img_size\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        if self.img_size is not None:\n            if self.strict_img_size:\n                _assert(H == self.img_size[0], f\"Input height ({H}) doesn't match model ({self.img_size[0]}).\")\n                _assert(W == self.img_size[1], f\"Input width ({W}) doesn't match model ({self.img_size[1]}).\")\n            else:\n                _assert(\n                    H % self.patch_size[0] == 0,\n                    f\"Input height ({H}) should be divisible by patch size ({self.patch_size[0]}).\"\n                )\n                _assert(\n                    W % self.patch_size[1] == 0,\n                    f\"Input width ({W}) should be divisible by patch size ({self.patch_size[1]}).\"\n                )\n\n        x = self.proj(x)\n        if self.flatten:\n            x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC\n        elif self.output_fmt != Format.NCHW:\n            x = nchw_to(x, self.output_fmt)\n        x = self.norm(x)\n        return x\n\n\nclass PatchEmbedWithSize(PatchEmbed):\n    \"\"\" 2D Image to Patch Embedding\n    \"\"\"\n    output_fmt: Format\n\n    def __init__(\n            self,\n            img_size: Optional[int] = 224,\n            patch_size: int = 16,\n            in_chans: int = 3,\n            embed_dim: int = 768,\n            norm_layer: Optional[Callable] = None,\n            flatten: bool = True,\n            output_fmt: Optional[str] = None,\n            bias: bool = True,\n    ):\n        super().__init__(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer,\n            flatten=flatten,\n            output_fmt=output_fmt,\n            bias=bias,\n        )\n\n    def forward(self, x) -> Tuple[torch.Tensor, List[int]]:\n        B, C, H, W = x.shape\n        if self.img_size is not None:\n            _assert(H % self.patch_size[0] == 0, f\"Input image height ({H}) must be divisible by patch size ({self.patch_size[0]}).\")\n            _assert(W % self.patch_size[1] == 0, f\"Input image width ({W}) must be divisible by patch size ({self.patch_size[1]}).\")\n\n        x = self.proj(x)\n        grid_size = x.shape[-2:]\n        if self.flatten:\n            x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC\n        elif self.output_fmt != Format.NCHW:\n            x = nchw_to(x, self.output_fmt)\n        x = self.norm(x)\n        return x, grid_size\n\n\ndef resample_patch_embed(\n        patch_embed,\n        new_size: List[int],\n        interpolation: str = 'bicubic',\n        antialias: bool = True,\n        verbose: bool = False,\n):\n    \"\"\"Resample the weights of the patch embedding kernel to target resolution.\n    We resample the patch embedding kernel by approximately inverting the effect\n    of patch resizing.\n\n    Code based on:\n      https://github.com/google-research/big_vision/blob/b00544b81f8694488d5f36295aeb7972f3755ffe/big_vision/models/proj/flexi/vit.py\n\n    With this resizing, we can for example load a B/8 filter into a B/16 model\n    and, on 2x larger input image, the result will match.\n\n    Args:\n        patch_embed: original parameter to be resized.\n        new_size (tuple(int, int): target shape (height, width)-only.\n        interpolation (str): interpolation for resize\n        antialias (bool): use anti-aliasing filter in resize\n        verbose (bool): log operation\n    Returns:\n        Resized patch embedding kernel.\n    \"\"\"\n    import numpy as np\n    try:\n        import functorch\n        vmap = functorch.vmap\n    except ImportError:\n        if hasattr(torch, 'vmap'):\n            vmap = torch.vmap\n        else:\n            assert False, \"functorch or a version of torch with vmap is required for FlexiViT resizing.\"\n\n    assert len(patch_embed.shape) == 4, \"Four dimensions expected\"\n    assert len(new_size) == 2, \"New shape should only be hw\"\n    old_size = patch_embed.shape[-2:]\n    if tuple(old_size) == tuple(new_size):\n        return patch_embed\n\n    if verbose:\n        _logger.info(f\"Resize patch embedding {patch_embed.shape} to {new_size}, w/ {interpolation} interpolation.\")\n\n    def resize(x_np, _new_size):\n        x_tf = torch.Tensor(x_np)[None, None, ...]\n        x_upsampled = F.interpolate(\n            x_tf, size=_new_size, mode=interpolation, antialias=antialias)[0, 0, ...].numpy()\n        return x_upsampled\n\n    def get_resize_mat(_old_size, _new_size):\n        mat = []\n        for i in range(np.prod(_old_size)):\n            basis_vec = np.zeros(_old_size)\n            basis_vec[np.unravel_index(i, _old_size)] = 1.\n            mat.append(resize(basis_vec, _new_size).reshape(-1))\n        return np.stack(mat).T\n\n    resize_mat = get_resize_mat(old_size, new_size)\n    resize_mat_pinv = torch.Tensor(np.linalg.pinv(resize_mat.T))\n\n    def resample_kernel(kernel):\n        resampled_kernel = resize_mat_pinv @ kernel.reshape(-1)\n        return resampled_kernel.reshape(new_size)\n\n    v_resample_kernel = vmap(vmap(resample_kernel, 0, 0), 1, 1)\n    return v_resample_kernel(patch_embed)\n\n\n# def divs(n, m=None):\n#     m = m or n // 2\n#     if m == 1:\n#         return [1]\n#     if n % m == 0:\n#         return [m] + divs(n, m - 1)\n#     return divs(n, m - 1)\n#\n#\n# class FlexiPatchEmbed(nn.Module):\n#     \"\"\" 2D Image to Patch Embedding w/ Flexible Patch sizes (FlexiViT)\n#     FIXME WIP\n#     \"\"\"\n#     def __init__(\n#             self,\n#             img_size=240,\n#             patch_size=16,\n#             in_chans=3,\n#             embed_dim=768,\n#             base_img_size=240,\n#             base_patch_size=32,\n#             norm_layer=None,\n#             flatten=True,\n#             bias=True,\n#     ):\n#         super().__init__()\n#         self.img_size = to_2tuple(img_size)\n#         self.patch_size = to_2tuple(patch_size)\n#         self.num_patches = 0\n#\n#         # full range for 240 = (5, 6, 8, 10, 12, 14, 15, 16, 20, 24, 30, 40, 48)\n#         self.seqhw = (6, 8, 10, 12, 14, 15, 16, 20, 24, 30)\n#\n#         self.base_img_size = to_2tuple(base_img_size)\n#         self.base_patch_size = to_2tuple(base_patch_size)\n#         self.base_grid_size = tuple([i // p for i, p in zip(self.base_img_size, self.base_patch_size)])\n#         self.base_num_patches = self.base_grid_size[0] * self.base_grid_size[1]\n#\n#         self.flatten = flatten\n#         self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=self.patch_size, stride=self.patch_size, bias=bias)\n#         self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n#\n#     def forward(self, x):\n#         B, C, H, W = x.shape\n#\n#         if self.patch_size == self.base_patch_size:\n#             weight = self.proj.weight\n#         else:\n#             weight = resample_patch_embed(self.proj.weight, self.patch_size)\n#         patch_size = self.patch_size\n#         x = F.conv2d(x, weight, bias=self.proj.bias, stride=patch_size)\n#         if self.flatten:\n#             x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n#         x = self.norm(x)\n#         return x\n",
  "\"\"\" Create Conv2d Factory Method\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nfrom .mixed_conv2d import MixedConv2d\nfrom .cond_conv2d import CondConv2d\nfrom .conv2d_same import create_conv2d_pad\n\n\ndef create_conv2d(in_channels, out_channels, kernel_size, **kwargs):\n    \"\"\" Select a 2d convolution implementation based on arguments\n    Creates and returns one of torch.nn.Conv2d, Conv2dSame, MixedConv2d, or CondConv2d.\n\n    Used extensively by EfficientNet, MobileNetv3 and related networks.\n    \"\"\"\n    if isinstance(kernel_size, list):\n        assert 'num_experts' not in kwargs  # MixNet + CondConv combo not supported currently\n        if 'groups' in kwargs:\n            groups = kwargs.pop('groups')\n            if groups == in_channels:\n                kwargs['depthwise'] = True\n            else:\n                assert groups == 1\n        # We're going to use only lists for defining the MixedConv2d kernel groups,\n        # ints, tuples, other iterables will continue to pass to normal conv and specify h, w.\n        m = MixedConv2d(in_channels, out_channels, kernel_size, **kwargs)\n    else:\n        depthwise = kwargs.pop('depthwise', False)\n        # for DW out_channels must be multiple of in_channels as must have out_channels % groups == 0\n        groups = in_channels if depthwise else kwargs.pop('groups', 1)\n        if 'num_experts' in kwargs and kwargs['num_experts'] > 0:\n            m = CondConv2d(in_channels, out_channels, kernel_size, groups=groups, **kwargs)\n        else:\n            m = create_conv2d_pad(in_channels, out_channels, kernel_size, groups=groups, **kwargs)\n    return m\n",
  "\"\"\" PyTorch Conditionally Parameterized Convolution (CondConv)\n\nPaper: CondConv: Conditionally Parameterized Convolutions for Efficient Inference\n(https://arxiv.org/abs/1904.04971)\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport math\nfrom functools import partial\nimport numpy as np\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\nfrom .helpers import to_2tuple\nfrom .conv2d_same import conv2d_same\nfrom .padding import get_padding_value\n\n\ndef get_condconv_initializer(initializer, num_experts, expert_shape):\n    def condconv_initializer(weight):\n        \"\"\"CondConv initializer function.\"\"\"\n        num_params = np.prod(expert_shape)\n        if (len(weight.shape) != 2 or weight.shape[0] != num_experts or\n                weight.shape[1] != num_params):\n            raise (ValueError(\n                'CondConv variables must have shape [num_experts, num_params]'))\n        for i in range(num_experts):\n            initializer(weight[i].view(expert_shape))\n    return condconv_initializer\n\n\nclass CondConv2d(nn.Module):\n    \"\"\" Conditionally Parameterized Convolution\n    Inspired by: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/condconv/condconv_layers.py\n\n    Grouped convolution hackery for parallel execution of the per-sample kernel filters inspired by this discussion:\n    https://github.com/pytorch/pytorch/issues/17983\n    \"\"\"\n    __constants__ = ['in_channels', 'out_channels', 'dynamic_padding']\n\n    def __init__(self, in_channels, out_channels, kernel_size=3,\n                 stride=1, padding='', dilation=1, groups=1, bias=False, num_experts=4):\n        super(CondConv2d, self).__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = to_2tuple(kernel_size)\n        self.stride = to_2tuple(stride)\n        padding_val, is_padding_dynamic = get_padding_value(\n            padding, kernel_size, stride=stride, dilation=dilation)\n        self.dynamic_padding = is_padding_dynamic  # if in forward to work with torchscript\n        self.padding = to_2tuple(padding_val)\n        self.dilation = to_2tuple(dilation)\n        self.groups = groups\n        self.num_experts = num_experts\n\n        self.weight_shape = (self.out_channels, self.in_channels // self.groups) + self.kernel_size\n        weight_num_param = 1\n        for wd in self.weight_shape:\n            weight_num_param *= wd\n        self.weight = torch.nn.Parameter(torch.Tensor(self.num_experts, weight_num_param))\n\n        if bias:\n            self.bias_shape = (self.out_channels,)\n            self.bias = torch.nn.Parameter(torch.Tensor(self.num_experts, self.out_channels))\n        else:\n            self.register_parameter('bias', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        init_weight = get_condconv_initializer(\n            partial(nn.init.kaiming_uniform_, a=math.sqrt(5)), self.num_experts, self.weight_shape)\n        init_weight(self.weight)\n        if self.bias is not None:\n            fan_in = np.prod(self.weight_shape[1:])\n            bound = 1 / math.sqrt(fan_in)\n            init_bias = get_condconv_initializer(\n                partial(nn.init.uniform_, a=-bound, b=bound), self.num_experts, self.bias_shape)\n            init_bias(self.bias)\n\n    def forward(self, x, routing_weights):\n        B, C, H, W = x.shape\n        weight = torch.matmul(routing_weights, self.weight)\n        new_weight_shape = (B * self.out_channels, self.in_channels // self.groups) + self.kernel_size\n        weight = weight.view(new_weight_shape)\n        bias = None\n        if self.bias is not None:\n            bias = torch.matmul(routing_weights, self.bias)\n            bias = bias.view(B * self.out_channels)\n        # move batch elements with channels so each batch element can be efficiently convolved with separate kernel\n        # reshape instead of view to work with channels_last input\n        x = x.reshape(1, B * C, H, W)\n        if self.dynamic_padding:\n            out = conv2d_same(\n                x, weight, bias, stride=self.stride, padding=self.padding,\n                dilation=self.dilation, groups=self.groups * B)\n        else:\n            out = F.conv2d(\n                x, weight, bias, stride=self.stride, padding=self.padding,\n                dilation=self.dilation, groups=self.groups * B)\n        out = out.permute([1, 0, 2, 3]).view(B, self.out_channels, out.shape[-2], out.shape[-1])\n\n        # Literal port (from TF definition)\n        # x = torch.split(x, 1, 0)\n        # weight = torch.split(weight, 1, 0)\n        # if self.bias is not None:\n        #     bias = torch.matmul(routing_weights, self.bias)\n        #     bias = torch.split(bias, 1, 0)\n        # else:\n        #     bias = [None] * B\n        # out = []\n        # for xi, wi, bi in zip(x, weight, bias):\n        #     wi = wi.view(*self.weight_shape)\n        #     if bi is not None:\n        #         bi = bi.view(*self.bias_shape)\n        #     out.append(self.conv_fn(\n        #         xi, wi, bi, stride=self.stride, padding=self.padding,\n        #         dilation=self.dilation, groups=self.groups))\n        # out = torch.cat(out, 0)\n        return out\n",
  "import torch\nimport torch.nn as nn\n\n\nclass SpaceToDepth(nn.Module):\n    bs: torch.jit.Final[int]\n\n    def __init__(self, block_size=4):\n        super().__init__()\n        assert block_size == 4\n        self.bs = block_size\n\n    def forward(self, x):\n        N, C, H, W = x.size()\n        x = x.view(N, C, H // self.bs, self.bs, W // self.bs, self.bs)  # (N, C, H//bs, bs, W//bs, bs)\n        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)\n        x = x.view(N, C * self.bs * self.bs, H // self.bs, W // self.bs)  # (N, C*bs^2, H//bs, W//bs)\n        return x\n\n\n@torch.jit.script\nclass SpaceToDepthJit:\n    def __call__(self, x: torch.Tensor):\n        # assuming hard-coded that block_size==4 for acceleration\n        N, C, H, W = x.size()\n        x = x.view(N, C, H // 4, 4, W // 4, 4)  # (N, C, H//bs, bs, W//bs, bs)\n        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)\n        x = x.view(N, C * 16, H // 4, W // 4)  # (N, C*bs^2, H//bs, W//bs)\n        return x\n\n\nclass SpaceToDepthModule(nn.Module):\n    def __init__(self, no_jit=False):\n        super().__init__()\n        if not no_jit:\n            self.op = SpaceToDepthJit()\n        else:\n            self.op = SpaceToDepth()\n\n    def forward(self, x):\n        return self.op(x)\n\n\nclass DepthToSpace(nn.Module):\n\n    def __init__(self, block_size):\n        super().__init__()\n        self.bs = block_size\n\n    def forward(self, x):\n        N, C, H, W = x.size()\n        x = x.view(N, self.bs, self.bs, C // (self.bs ** 2), H, W)  # (N, bs, bs, C//bs^2, H, W)\n        x = x.permute(0, 3, 4, 1, 5, 2).contiguous()  # (N, C//bs^2, H, bs, W, bs)\n        x = x.view(N, C // (self.bs ** 2), H * self.bs, W * self.bs)  # (N, C//bs^2, H * bs, W * bs)\n        return x\n",
  "\"\"\" Normalization + Activation Layers\n\nProvides Norm+Act fns for standard PyTorch norm layers such as\n* BatchNorm\n* GroupNorm\n* LayerNorm\n\nThis allows swapping with alternative layers that are natively both norm + act such as\n* EvoNorm (evo_norm.py)\n* FilterResponseNorm (filter_response_norm.py)\n* InplaceABN (inplace_abn.py)\n\nHacked together by / Copyright 2022 Ross Wightman\n\"\"\"\nfrom typing import Union, List, Optional, Any\n\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\nfrom torchvision.ops.misc import FrozenBatchNorm2d\n\nfrom .create_act import get_act_layer\nfrom .fast_norm import is_fast_norm, fast_group_norm, fast_layer_norm\nfrom .trace_utils import _assert\n\n\ndef _create_act(act_layer, act_kwargs=None, inplace=False, apply_act=True):\n    act_layer = get_act_layer(act_layer)  # string -> nn.Module\n    act_kwargs = act_kwargs or {}\n    if act_layer is not None and apply_act:\n        if inplace:\n            act_kwargs['inplace'] = inplace\n        act = act_layer(**act_kwargs)\n    else:\n        act = nn.Identity()\n    return act\n\n\nclass BatchNormAct2d(nn.BatchNorm2d):\n    \"\"\"BatchNorm + Activation\n\n    This module performs BatchNorm + Activation in a manner that will remain backwards\n    compatible with weights trained with separate bn, act. This is why we inherit from BN\n    instead of composing it as a .bn member.\n    \"\"\"\n    def __init__(\n            self,\n            num_features,\n            eps=1e-5,\n            momentum=0.1,\n            affine=True,\n            track_running_stats=True,\n            apply_act=True,\n            act_layer=nn.ReLU,\n            act_kwargs=None,\n            inplace=True,\n            drop_layer=None,\n            device=None,\n            dtype=None,\n    ):\n        try:\n            factory_kwargs = {'device': device, 'dtype': dtype}\n            super(BatchNormAct2d, self).__init__(\n                num_features,\n                eps=eps,\n                momentum=momentum,\n                affine=affine,\n                track_running_stats=track_running_stats,\n                **factory_kwargs,\n            )\n        except TypeError:\n            # NOTE for backwards compat with old PyTorch w/o factory device/dtype support\n            super(BatchNormAct2d, self).__init__(\n                num_features,\n                eps=eps,\n                momentum=momentum,\n                affine=affine,\n                track_running_stats=track_running_stats,\n            )\n        self.drop = drop_layer() if drop_layer is not None else nn.Identity()\n        self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)\n\n    def forward(self, x):\n        # cut & paste of torch.nn.BatchNorm2d.forward impl to avoid issues with torchscript and tracing\n        _assert(x.ndim == 4, f'expected 4D input (got {x.ndim}D input)')\n\n        # exponential_average_factor is set to self.momentum\n        # (when it is available) only so that it gets updated\n        # in ONNX graph when this node is exported to ONNX.\n        if self.momentum is None:\n            exponential_average_factor = 0.0\n        else:\n            exponential_average_factor = self.momentum\n\n        if self.training and self.track_running_stats:\n            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n            if self.num_batches_tracked is not None:  # type: ignore[has-type]\n                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n                if self.momentum is None:  # use cumulative moving average\n                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n                else:  # use exponential moving average\n                    exponential_average_factor = self.momentum\n\n        r\"\"\"\n        Decide whether the mini-batch stats should be used for normalization rather than the buffers.\n        Mini-batch stats are used in training mode, and in eval mode when buffers are None.\n        \"\"\"\n        if self.training:\n            bn_training = True\n        else:\n            bn_training = (self.running_mean is None) and (self.running_var is None)\n\n        r\"\"\"\n        Buffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\n        passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\n        used for normalization (i.e. in eval mode when buffers are not None).\n        \"\"\"\n        x = F.batch_norm(\n            x,\n            # If buffers are not to be tracked, ensure that they won't be updated\n            self.running_mean if not self.training or self.track_running_stats else None,\n            self.running_var if not self.training or self.track_running_stats else None,\n            self.weight,\n            self.bias,\n            bn_training,\n            exponential_average_factor,\n            self.eps,\n        )\n        x = self.drop(x)\n        x = self.act(x)\n        return x\n\n\nclass SyncBatchNormAct(nn.SyncBatchNorm):\n    # Thanks to Selim Seferbekov (https://github.com/rwightman/pytorch-image-models/issues/1254)\n    # This is a quick workaround to support SyncBatchNorm for timm BatchNormAct2d layers\n    # but ONLY when used in conjunction with the timm conversion function below.\n    # Do not create this module directly or use the PyTorch conversion function.\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = super().forward(x)  # SyncBN doesn't work with torchscript anyways, so this is fine\n        if hasattr(self, \"drop\"):\n            x = self.drop(x)\n        if hasattr(self, \"act\"):\n            x = self.act(x)\n        return x\n\n\ndef convert_sync_batchnorm(module, process_group=None):\n    # convert both BatchNorm and BatchNormAct layers to Synchronized variants\n    module_output = module\n    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n        if isinstance(module, BatchNormAct2d):\n            # convert timm norm + act layer\n            module_output = SyncBatchNormAct(\n                module.num_features,\n                module.eps,\n                module.momentum,\n                module.affine,\n                module.track_running_stats,\n                process_group=process_group,\n            )\n            # set act and drop attr from the original module\n            module_output.act = module.act\n            module_output.drop = module.drop\n        else:\n            # convert standard BatchNorm layers\n            module_output = torch.nn.SyncBatchNorm(\n                module.num_features,\n                module.eps,\n                module.momentum,\n                module.affine,\n                module.track_running_stats,\n                process_group,\n            )\n        if module.affine:\n            with torch.no_grad():\n                module_output.weight = module.weight\n                module_output.bias = module.bias\n        module_output.running_mean = module.running_mean\n        module_output.running_var = module.running_var\n        module_output.num_batches_tracked = module.num_batches_tracked\n        if hasattr(module, \"qconfig\"):\n            module_output.qconfig = module.qconfig\n    for name, child in module.named_children():\n        module_output.add_module(name, convert_sync_batchnorm(child, process_group))\n    del module\n    return module_output\n\n\nclass FrozenBatchNormAct2d(torch.nn.Module):\n    \"\"\"\n    BatchNormAct2d where the batch statistics and the affine parameters are fixed\n\n    Args:\n        num_features (int): Number of features ``C`` from an expected input of size ``(N, C, H, W)``\n        eps (float): a value added to the denominator for numerical stability. Default: 1e-5\n    \"\"\"\n\n    def __init__(\n        self,\n        num_features: int,\n        eps: float = 1e-5,\n        apply_act=True,\n        act_layer=nn.ReLU,\n        act_kwargs=None,\n        inplace=True,\n        drop_layer=None,\n    ):\n        super().__init__()\n        self.eps = eps\n        self.register_buffer(\"weight\", torch.ones(num_features))\n        self.register_buffer(\"bias\", torch.zeros(num_features))\n        self.register_buffer(\"running_mean\", torch.zeros(num_features))\n        self.register_buffer(\"running_var\", torch.ones(num_features))\n\n        self.drop = drop_layer() if drop_layer is not None else nn.Identity()\n        self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)\n\n    def _load_from_state_dict(\n        self,\n        state_dict: dict,\n        prefix: str,\n        local_metadata: dict,\n        strict: bool,\n        missing_keys: List[str],\n        unexpected_keys: List[str],\n        error_msgs: List[str],\n    ):\n        num_batches_tracked_key = prefix + \"num_batches_tracked\"\n        if num_batches_tracked_key in state_dict:\n            del state_dict[num_batches_tracked_key]\n\n        super()._load_from_state_dict(\n            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # move reshapes to the beginning\n        # to make it fuser-friendly\n        w = self.weight.reshape(1, -1, 1, 1)\n        b = self.bias.reshape(1, -1, 1, 1)\n        rv = self.running_var.reshape(1, -1, 1, 1)\n        rm = self.running_mean.reshape(1, -1, 1, 1)\n        scale = w * (rv + self.eps).rsqrt()\n        bias = b - rm * scale\n        x = x * scale + bias\n        x = self.act(self.drop(x))\n        return x\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}({self.weight.shape[0]}, eps={self.eps}, act={self.act})\"\n\n\ndef freeze_batch_norm_2d(module):\n    \"\"\"\n    Converts all `BatchNorm2d` and `SyncBatchNorm` or `BatchNormAct2d` and `SyncBatchNormAct2d` layers\n    of provided module into `FrozenBatchNorm2d` or `FrozenBatchNormAct2d` respectively.\n\n    Args:\n        module (torch.nn.Module): Any PyTorch module.\n\n    Returns:\n        torch.nn.Module: Resulting module\n\n    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762\n    \"\"\"\n    res = module\n    if isinstance(module, (BatchNormAct2d, SyncBatchNormAct)):\n        res = FrozenBatchNormAct2d(module.num_features)\n        res.num_features = module.num_features\n        res.affine = module.affine\n        if module.affine:\n            res.weight.data = module.weight.data.clone().detach()\n            res.bias.data = module.bias.data.clone().detach()\n        res.running_mean.data = module.running_mean.data\n        res.running_var.data = module.running_var.data\n        res.eps = module.eps\n        res.drop = module.drop\n        res.act = module.act\n    elif isinstance(module, (torch.nn.modules.batchnorm.BatchNorm2d, torch.nn.modules.batchnorm.SyncBatchNorm)):\n        res = FrozenBatchNorm2d(module.num_features)\n        res.num_features = module.num_features\n        res.affine = module.affine\n        if module.affine:\n            res.weight.data = module.weight.data.clone().detach()\n            res.bias.data = module.bias.data.clone().detach()\n        res.running_mean.data = module.running_mean.data\n        res.running_var.data = module.running_var.data\n        res.eps = module.eps\n    else:\n        for name, child in module.named_children():\n            new_child = freeze_batch_norm_2d(child)\n            if new_child is not child:\n                res.add_module(name, new_child)\n    return res\n\n\ndef unfreeze_batch_norm_2d(module):\n    \"\"\"\n    Converts all `FrozenBatchNorm2d` layers of provided module into `BatchNorm2d`. If `module` is itself and instance\n    of `FrozenBatchNorm2d`, it is converted into `BatchNorm2d` and returned. Otherwise, the module is walked\n    recursively and submodules are converted in place.\n\n    Args:\n        module (torch.nn.Module): Any PyTorch module.\n\n    Returns:\n        torch.nn.Module: Resulting module\n\n    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762\n    \"\"\"\n    res = module\n    if isinstance(module, FrozenBatchNormAct2d):\n        res = BatchNormAct2d(module.num_features)\n        if module.affine:\n            res.weight.data = module.weight.data.clone().detach()\n            res.bias.data = module.bias.data.clone().detach()\n        res.running_mean.data = module.running_mean.data\n        res.running_var.data = module.running_var.data\n        res.eps = module.eps\n        res.drop = module.drop\n        res.act = module.act\n    elif isinstance(module, FrozenBatchNorm2d):\n        res = torch.nn.BatchNorm2d(module.num_features)\n        if module.affine:\n            res.weight.data = module.weight.data.clone().detach()\n            res.bias.data = module.bias.data.clone().detach()\n        res.running_mean.data = module.running_mean.data\n        res.running_var.data = module.running_var.data\n        res.eps = module.eps\n    else:\n        for name, child in module.named_children():\n            new_child = unfreeze_batch_norm_2d(child)\n            if new_child is not child:\n                res.add_module(name, new_child)\n    return res\n\n\ndef _num_groups(num_channels, num_groups, group_size):\n    if group_size:\n        assert num_channels % group_size == 0\n        return num_channels // group_size\n    return num_groups\n\n\nclass GroupNormAct(nn.GroupNorm):\n    # NOTE num_channel and num_groups order flipped for easier layer swaps / binding of fixed args\n    def __init__(\n            self,\n            num_channels,\n            num_groups=32,\n            eps=1e-5,\n            affine=True,\n            group_size=None,\n            apply_act=True,\n            act_layer=nn.ReLU,\n            act_kwargs=None,\n            inplace=True,\n            drop_layer=None,\n    ):\n        super(GroupNormAct, self).__init__(\n            _num_groups(num_channels, num_groups, group_size),\n            num_channels,\n            eps=eps,\n            affine=affine,\n        )\n        self.drop = drop_layer() if drop_layer is not None else nn.Identity()\n        self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)\n\n        self._fast_norm = is_fast_norm()\n\n    def forward(self, x):\n        if self._fast_norm:\n            x = fast_group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n        else:\n            x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n        x = self.drop(x)\n        x = self.act(x)\n        return x\n\n\nclass GroupNorm1Act(nn.GroupNorm):\n    def __init__(\n            self,\n            num_channels,\n            eps=1e-5,\n            affine=True,\n            apply_act=True,\n            act_layer=nn.ReLU,\n            act_kwargs=None,\n            inplace=True,\n            drop_layer=None,\n    ):\n        super(GroupNorm1Act, self).__init__(1, num_channels, eps=eps, affine=affine)\n        self.drop = drop_layer() if drop_layer is not None else nn.Identity()\n        self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)\n\n        self._fast_norm = is_fast_norm()\n\n    def forward(self, x):\n        if self._fast_norm:\n            x = fast_group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n        else:\n            x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n        x = self.drop(x)\n        x = self.act(x)\n        return x\n\n\nclass LayerNormAct(nn.LayerNorm):\n    def __init__(\n            self,\n            normalization_shape: Union[int, List[int], torch.Size],\n            eps=1e-5,\n            affine=True,\n            apply_act=True,\n            act_layer=nn.ReLU,\n            act_kwargs=None,\n            inplace=True,\n            drop_layer=None,\n    ):\n        super(LayerNormAct, self).__init__(normalization_shape, eps=eps, elementwise_affine=affine)\n        self.drop = drop_layer() if drop_layer is not None else nn.Identity()\n        act_layer = get_act_layer(act_layer)  # string -> nn.Module\n        self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)\n\n        self._fast_norm = is_fast_norm()\n\n    def forward(self, x):\n        if self._fast_norm:\n            x = fast_layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        else:\n            x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        x = self.drop(x)\n        x = self.act(x)\n        return x\n\n\nclass LayerNormAct2d(nn.LayerNorm):\n    def __init__(\n            self,\n            num_channels,\n            eps=1e-5,\n            affine=True,\n            apply_act=True,\n            act_layer=nn.ReLU,\n            act_kwargs=None,\n            inplace=True,\n            drop_layer=None,\n    ):\n        super(LayerNormAct2d, self).__init__(num_channels, eps=eps, elementwise_affine=affine)\n        self.drop = drop_layer() if drop_layer is not None else nn.Identity()\n        self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)\n        self._fast_norm = is_fast_norm()\n\n    def forward(self, x):\n        x = x.permute(0, 2, 3, 1)\n        if self._fast_norm:\n            x = fast_layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        else:\n            x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        x = x.permute(0, 3, 1, 2)\n        x = self.drop(x)\n        x = self.act(x)\n        return x\n",
  "\"\"\" Relative position embedding modules and functions\n\nHacked together by / Copyright 2022 Ross Wightman\n\"\"\"\nimport math\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .mlp import Mlp\nfrom .weight_init import trunc_normal_\n\n\ndef gen_relative_position_index(\n        q_size: Tuple[int, int],\n        k_size: Optional[Tuple[int, int]] = None,\n        class_token: bool = False,\n) -> torch.Tensor:\n    # Adapted with significant modifications from Swin / BeiT codebases\n    # get pair-wise relative position index for each token inside the window\n    if k_size is None:\n        coords = torch.stack(\n            torch.meshgrid([\n                torch.arange(q_size[0]),\n                torch.arange(q_size[1])\n            ])\n        ).flatten(1)  # 2, Wh, Ww\n        relative_coords = coords[:, :, None] - coords[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0)  # Qh*Qw, Kh*Kw, 2\n        num_relative_distance = (2 * q_size[0] - 1) * (2 * q_size[1] - 1) + 3\n    else:\n        # FIXME different q vs k sizes is a WIP, need to better offset the two grids?\n        q_coords = torch.stack(\n            torch.meshgrid([\n                torch.arange(q_size[0]),\n                torch.arange(q_size[1])\n            ])\n        ).flatten(1)  # 2, Wh, Ww\n        k_coords = torch.stack(\n            torch.meshgrid([\n                torch.arange(k_size[0]),\n                torch.arange(k_size[1])\n            ])\n        ).flatten(1)\n        relative_coords = q_coords[:, :, None] - k_coords[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0)  # Qh*Qw, Kh*Kw, 2\n        # relative_coords[:, :, 0] += max(q_size[0], k_size[0]) - 1  # shift to start from 0\n        # relative_coords[:, :, 1] += max(q_size[1], k_size[1]) - 1\n        # relative_coords[:, :, 0] *= k_size[1] + q_size[1] - 1\n        # relative_position_index = relative_coords.sum(-1)  # Qh*Qw, Kh*Kw\n        num_relative_distance = (q_size[0] + k_size[0] - 1) * (q_size[1] + q_size[1] - 1) + 3\n\n    _, relative_position_index = torch.unique(relative_coords.view(-1, 2), return_inverse=True, dim=0)\n\n    if class_token:\n        # handle cls to token & token 2 cls & cls to cls as per beit for rel pos bias\n        # NOTE not intended or tested with MLP log-coords\n        relative_position_index = F.pad(relative_position_index, [1, 0, 1, 0])\n        relative_position_index[0, 0:] = num_relative_distance - 3\n        relative_position_index[0:, 0] = num_relative_distance - 2\n        relative_position_index[0, 0] = num_relative_distance - 1\n\n    return relative_position_index.contiguous()\n\n\nclass RelPosBias(nn.Module):\n    \"\"\" Relative Position Bias\n    Adapted from Swin-V1 relative position bias impl, modularized.\n    \"\"\"\n\n    def __init__(self, window_size, num_heads, prefix_tokens=0):\n        super().__init__()\n        assert prefix_tokens <= 1\n        self.window_size = window_size\n        self.window_area = window_size[0] * window_size[1]\n        self.bias_shape = (self.window_area + prefix_tokens,) * 2 + (num_heads,)\n\n        num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3 * prefix_tokens\n        self.relative_position_bias_table = nn.Parameter(torch.zeros(num_relative_distance, num_heads))\n        self.register_buffer(\n            \"relative_position_index\",\n            gen_relative_position_index(self.window_size, class_token=prefix_tokens > 0).view(-1),\n            persistent=False,\n        )\n\n        self.init_weights()\n\n    def init_weights(self):\n        trunc_normal_(self.relative_position_bias_table, std=.02)\n\n    def get_bias(self) -> torch.Tensor:\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index]\n        # win_h * win_w, win_h * win_w, num_heads\n        relative_position_bias = relative_position_bias.view(self.bias_shape).permute(2, 0, 1)\n        return relative_position_bias.unsqueeze(0).contiguous()\n\n    def forward(self, attn, shared_rel_pos: Optional[torch.Tensor] = None):\n        return attn + self.get_bias()\n\n\ndef gen_relative_log_coords(\n        win_size: Tuple[int, int],\n        pretrained_win_size: Tuple[int, int] = (0, 0),\n        mode='swin',\n):\n    assert mode in ('swin', 'cr')\n    # as per official swin-v2 impl, supporting timm specific 'cr' log coords as well\n    relative_coords_h = torch.arange(-(win_size[0] - 1), win_size[0], dtype=torch.float32)\n    relative_coords_w = torch.arange(-(win_size[1] - 1), win_size[1], dtype=torch.float32)\n    relative_coords_table = torch.stack(torch.meshgrid([relative_coords_h, relative_coords_w]))\n    relative_coords_table = relative_coords_table.permute(1, 2, 0).contiguous()  # 2*Wh-1, 2*Ww-1, 2\n    if mode == 'swin':\n        if pretrained_win_size[0] > 0:\n            relative_coords_table[:, :, 0] /= (pretrained_win_size[0] - 1)\n            relative_coords_table[:, :, 1] /= (pretrained_win_size[1] - 1)\n        else:\n            relative_coords_table[:, :, 0] /= (win_size[0] - 1)\n            relative_coords_table[:, :, 1] /= (win_size[1] - 1)\n        relative_coords_table *= 8  # normalize to -8, 8\n        relative_coords_table = torch.sign(relative_coords_table) * torch.log2(\n            1.0 + relative_coords_table.abs()) / math.log2(8)\n    else:\n        # mode == 'cr'\n        relative_coords_table = torch.sign(relative_coords_table) * torch.log(\n            1.0 + relative_coords_table.abs())\n\n    return relative_coords_table\n\n\nclass RelPosMlp(nn.Module):\n    \"\"\" Log-Coordinate Relative Position MLP\n    Based on ideas presented in Swin-V2 paper (https://arxiv.org/abs/2111.09883)\n\n    This impl covers the 'swin' implementation as well as two timm specific modes ('cr', and 'rw')\n    \"\"\"\n    def __init__(\n            self,\n            window_size,\n            num_heads=8,\n            hidden_dim=128,\n            prefix_tokens=0,\n            mode='cr',\n            pretrained_window_size=(0, 0)\n    ):\n        super().__init__()\n        self.window_size = window_size\n        self.window_area = self.window_size[0] * self.window_size[1]\n        self.prefix_tokens = prefix_tokens\n        self.num_heads = num_heads\n        self.bias_shape = (self.window_area,) * 2 + (num_heads,)\n        if mode == 'swin':\n            self.bias_act = nn.Sigmoid()\n            self.bias_gain = 16\n            mlp_bias = (True, False)\n        else:\n            self.bias_act = nn.Identity()\n            self.bias_gain = None\n            mlp_bias = True\n\n        self.mlp = Mlp(\n            2,  # x, y\n            hidden_features=hidden_dim,\n            out_features=num_heads,\n            act_layer=nn.ReLU,\n            bias=mlp_bias,\n            drop=(0.125, 0.)\n        )\n\n        self.register_buffer(\n            \"relative_position_index\",\n            gen_relative_position_index(window_size).view(-1),\n            persistent=False)\n\n        # get relative_coords_table\n        self.register_buffer(\n            \"rel_coords_log\",\n            gen_relative_log_coords(window_size, pretrained_window_size, mode=mode),\n            persistent=False)\n\n    def get_bias(self) -> torch.Tensor:\n        relative_position_bias = self.mlp(self.rel_coords_log)\n        if self.relative_position_index is not None:\n            relative_position_bias = relative_position_bias.view(-1, self.num_heads)[self.relative_position_index]\n            relative_position_bias = relative_position_bias.view(self.bias_shape)\n        relative_position_bias = relative_position_bias.permute(2, 0, 1)\n        relative_position_bias = self.bias_act(relative_position_bias)\n        if self.bias_gain is not None:\n            relative_position_bias = self.bias_gain * relative_position_bias\n        if self.prefix_tokens:\n            relative_position_bias = F.pad(relative_position_bias, [self.prefix_tokens, 0, self.prefix_tokens, 0])\n        return relative_position_bias.unsqueeze(0).contiguous()\n\n    def forward(self, attn, shared_rel_pos: Optional[torch.Tensor] = None):\n        return attn + self.get_bias()\n\n\ndef generate_lookup_tensor(\n        length: int,\n        max_relative_position: Optional[int] = None,\n):\n    \"\"\"Generate a one_hot lookup tensor to reindex embeddings along one dimension.\n\n    Args:\n        length: the length to reindex to.\n        max_relative_position: the maximum relative position to consider.\n            Relative position embeddings for distances above this threshold\n            are zeroed out.\n    Returns:\n        a lookup Tensor of size [length, length, vocab_size] that satisfies\n            ret[n,m,v] = 1{m - n + max_relative_position = v}.\n    \"\"\"\n    if max_relative_position is None:\n        max_relative_position = length - 1\n    # Return the cached lookup tensor, otherwise compute it and cache it.\n    vocab_size = 2 * max_relative_position + 1\n    ret = torch.zeros(length, length, vocab_size)\n    for i in range(length):\n        for x in range(length):\n            v = x - i + max_relative_position\n            if abs(x - i) > max_relative_position:\n                continue\n            ret[i, x, v] = 1\n    return ret\n\n\ndef reindex_2d_einsum_lookup(\n        relative_position_tensor,\n        height: int,\n        width: int,\n        height_lookup: torch.Tensor,\n        width_lookup: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"Reindex 2d relative position bias with 2 independent einsum lookups.\n\n    Adapted from:\n     https://github.com/google-research/maxvit/blob/2e06a7f1f70c76e64cd3dabe5cd1b8c1a23c9fb7/maxvit/models/attention_utils.py\n\n    Args:\n        relative_position_tensor: tensor of shape\n            [..., vocab_height, vocab_width, ...].\n        height: height to reindex to.\n        width: width to reindex to.\n        height_lookup: one-hot height lookup\n        width_lookup: one-hot width lookup\n    Returns:\n        reindexed_tensor: a Tensor of shape\n            [..., height * width, height * width, ...]\n    \"\"\"\n    reindexed_tensor = torch.einsum('nhw,ixh->nixw', relative_position_tensor, height_lookup)\n    reindexed_tensor = torch.einsum('nixw,jyw->nijxy', reindexed_tensor, width_lookup)\n    area = height * width\n    return reindexed_tensor.reshape(relative_position_tensor.shape[0], area, area)\n\n\nclass RelPosBiasTf(nn.Module):\n    \"\"\" Relative Position Bias Impl (Compatible with Tensorflow MaxViT models)\n    Adapted from:\n     https://github.com/google-research/maxvit/blob/2e06a7f1f70c76e64cd3dabe5cd1b8c1a23c9fb7/maxvit/models/attention_utils.py\n    \"\"\"\n    def __init__(self, window_size, num_heads, prefix_tokens=0):\n        super().__init__()\n        assert prefix_tokens <= 1\n        self.window_size = window_size\n        self.window_area = window_size[0] * window_size[1]\n        self.num_heads = num_heads\n\n        vocab_height = 2 * window_size[0] - 1\n        vocab_width = 2 * window_size[1] - 1\n        self.bias_shape = (self.num_heads, vocab_height, vocab_width)\n        self.relative_position_bias_table = nn.Parameter(torch.zeros(self.bias_shape))\n        self.register_buffer('height_lookup', generate_lookup_tensor(window_size[0]), persistent=False)\n        self.register_buffer('width_lookup', generate_lookup_tensor(window_size[1]), persistent=False)\n        self.init_weights()\n\n    def init_weights(self):\n        nn.init.normal_(self.relative_position_bias_table, std=.02)\n\n    def get_bias(self) -> torch.Tensor:\n        # FIXME change to not use one-hot/einsum?\n        return reindex_2d_einsum_lookup(\n            self.relative_position_bias_table,\n            self.window_size[0],\n            self.window_size[1],\n            self.height_lookup,\n            self.width_lookup\n        )\n\n    def forward(self, attn, shared_rel_pos: Optional[torch.Tensor] = None):\n        return attn + self.get_bias()\n",
  "\"\"\" DropBlock, DropPath\n\nPyTorch implementations of DropBlock and DropPath (Stochastic Depth) regularization layers.\n\nPapers:\nDropBlock: A regularization method for convolutional networks (https://arxiv.org/abs/1810.12890)\n\nDeep Networks with Stochastic Depth (https://arxiv.org/abs/1603.09382)\n\nCode:\nDropBlock impl inspired by two Tensorflow impl that I liked:\n - https://github.com/tensorflow/tpu/blob/master/models/official/resnet/resnet_model.py#L74\n - https://github.com/clovaai/assembled-cnn/blob/master/nets/blocks.py\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef drop_block_2d(\n        x, drop_prob: float = 0.1, block_size: int = 7, gamma_scale: float = 1.0,\n        with_noise: bool = False, inplace: bool = False, batchwise: bool = False):\n    \"\"\" DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\n\n    DropBlock with an experimental gaussian noise option. This layer has been tested on a few training\n    runs with success, but needs further validation and possibly optimization for lower runtime impact.\n    \"\"\"\n    B, C, H, W = x.shape\n    total_size = W * H\n    clipped_block_size = min(block_size, min(W, H))\n    # seed_drop_rate, the gamma parameter\n    gamma = gamma_scale * drop_prob * total_size / clipped_block_size ** 2 / (\n            (W - block_size + 1) * (H - block_size + 1))\n\n    # Forces the block to be inside the feature map.\n    w_i, h_i = torch.meshgrid(torch.arange(W).to(x.device), torch.arange(H).to(x.device))\n    valid_block = ((w_i >= clipped_block_size // 2) & (w_i < W - (clipped_block_size - 1) // 2)) & \\\n                  ((h_i >= clipped_block_size // 2) & (h_i < H - (clipped_block_size - 1) // 2))\n    valid_block = torch.reshape(valid_block, (1, 1, H, W)).to(dtype=x.dtype)\n\n    if batchwise:\n        # one mask for whole batch, quite a bit faster\n        uniform_noise = torch.rand((1, C, H, W), dtype=x.dtype, device=x.device)\n    else:\n        uniform_noise = torch.rand_like(x)\n    block_mask = ((2 - gamma - valid_block + uniform_noise) >= 1).to(dtype=x.dtype)\n    block_mask = -F.max_pool2d(\n        -block_mask,\n        kernel_size=clipped_block_size,  # block_size,\n        stride=1,\n        padding=clipped_block_size // 2)\n\n    if with_noise:\n        normal_noise = torch.randn((1, C, H, W), dtype=x.dtype, device=x.device) if batchwise else torch.randn_like(x)\n        if inplace:\n            x.mul_(block_mask).add_(normal_noise * (1 - block_mask))\n        else:\n            x = x * block_mask + normal_noise * (1 - block_mask)\n    else:\n        normalize_scale = (block_mask.numel() / block_mask.to(dtype=torch.float32).sum().add(1e-7)).to(x.dtype)\n        if inplace:\n            x.mul_(block_mask * normalize_scale)\n        else:\n            x = x * block_mask * normalize_scale\n    return x\n\n\ndef drop_block_fast_2d(\n        x: torch.Tensor, drop_prob: float = 0.1, block_size: int = 7,\n        gamma_scale: float = 1.0, with_noise: bool = False, inplace: bool = False):\n    \"\"\" DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\n\n    DropBlock with an experimental gaussian noise option. Simplied from above without concern for valid\n    block mask at edges.\n    \"\"\"\n    B, C, H, W = x.shape\n    total_size = W * H\n    clipped_block_size = min(block_size, min(W, H))\n    gamma = gamma_scale * drop_prob * total_size / clipped_block_size ** 2 / (\n            (W - block_size + 1) * (H - block_size + 1))\n\n    block_mask = torch.empty_like(x).bernoulli_(gamma)\n    block_mask = F.max_pool2d(\n        block_mask.to(x.dtype), kernel_size=clipped_block_size, stride=1, padding=clipped_block_size // 2)\n\n    if with_noise:\n        normal_noise = torch.empty_like(x).normal_()\n        if inplace:\n            x.mul_(1. - block_mask).add_(normal_noise * block_mask)\n        else:\n            x = x * (1. - block_mask) + normal_noise * block_mask\n    else:\n        block_mask = 1 - block_mask\n        normalize_scale = (block_mask.numel() / block_mask.to(dtype=torch.float32).sum().add(1e-6)).to(dtype=x.dtype)\n        if inplace:\n            x.mul_(block_mask * normalize_scale)\n        else:\n            x = x * block_mask * normalize_scale\n    return x\n\n\nclass DropBlock2d(nn.Module):\n    \"\"\" DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\n    \"\"\"\n\n    def __init__(\n            self,\n            drop_prob: float = 0.1,\n            block_size: int = 7,\n            gamma_scale: float = 1.0,\n            with_noise: bool = False,\n            inplace: bool = False,\n            batchwise: bool = False,\n            fast: bool = True):\n        super(DropBlock2d, self).__init__()\n        self.drop_prob = drop_prob\n        self.gamma_scale = gamma_scale\n        self.block_size = block_size\n        self.with_noise = with_noise\n        self.inplace = inplace\n        self.batchwise = batchwise\n        self.fast = fast  # FIXME finish comparisons of fast vs not\n\n    def forward(self, x):\n        if not self.training or not self.drop_prob:\n            return x\n        if self.fast:\n            return drop_block_fast_2d(\n                x, self.drop_prob, self.block_size, self.gamma_scale, self.with_noise, self.inplace)\n        else:\n            return drop_block_2d(\n                x, self.drop_prob, self.block_size, self.gamma_scale, self.with_noise, self.inplace, self.batchwise)\n\n\ndef drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n    'survival rate' as the argument.\n\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0 and scale_by_keep:\n        random_tensor.div_(keep_prob)\n    return x * random_tensor\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self.scale_by_keep = scale_by_keep\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n\n    def extra_repr(self):\n        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n",
  "\"\"\" Layer/Module Helpers\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom itertools import repeat\nimport collections.abc\n\n\n# From PyTorch internals\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n            return tuple(x)\n        return tuple(repeat(x, n))\n    return parse\n\n\nto_1tuple = _ntuple(1)\nto_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = _ntuple\n\n\ndef make_divisible(v, divisor=8, min_value=None, round_limit=.9):\n    min_value = min_value or divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < round_limit * v:\n        new_v += divisor\n    return new_v\n\n\ndef extend_tuple(x, n):\n    # pdas a tuple to specified n by padding with last value\n    if not isinstance(x, (tuple, list)):\n        x = (x,)\n    else:\n        x = tuple(x)\n    pad_n = n - len(x)\n    if pad_n <= 0:\n        return x[:n]\n    return x + (x[-1],) * pad_n\n",
  "\"\"\" PyTorch selectable adaptive pooling\nAdaptive pooling with the ability to select the type of pooling from:\n    * 'avg' - Average pooling\n    * 'max' - Max pooling\n    * 'avgmax' - Sum of average and max pooling re-scaled by 0.5\n    * 'avgmaxc' - Concatenation of average and max pooling along feature dim, doubles feature dim\n\nBoth a functional and a nn.Module version of the pooling is provided.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom typing import Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .format import get_spatial_dim, get_channel_dim\n\n_int_tuple_2_t = Union[int, Tuple[int, int]]\n\n\ndef adaptive_pool_feat_mult(pool_type='avg'):\n    if pool_type.endswith('catavgmax'):\n        return 2\n    else:\n        return 1\n\n\ndef adaptive_avgmax_pool2d(x, output_size: _int_tuple_2_t = 1):\n    x_avg = F.adaptive_avg_pool2d(x, output_size)\n    x_max = F.adaptive_max_pool2d(x, output_size)\n    return 0.5 * (x_avg + x_max)\n\n\ndef adaptive_catavgmax_pool2d(x, output_size: _int_tuple_2_t = 1):\n    x_avg = F.adaptive_avg_pool2d(x, output_size)\n    x_max = F.adaptive_max_pool2d(x, output_size)\n    return torch.cat((x_avg, x_max), 1)\n\n\ndef select_adaptive_pool2d(x, pool_type='avg', output_size: _int_tuple_2_t = 1):\n    \"\"\"Selectable global pooling function with dynamic input kernel size\n    \"\"\"\n    if pool_type == 'avg':\n        x = F.adaptive_avg_pool2d(x, output_size)\n    elif pool_type == 'avgmax':\n        x = adaptive_avgmax_pool2d(x, output_size)\n    elif pool_type == 'catavgmax':\n        x = adaptive_catavgmax_pool2d(x, output_size)\n    elif pool_type == 'max':\n        x = F.adaptive_max_pool2d(x, output_size)\n    else:\n        assert False, 'Invalid pool type: %s' % pool_type\n    return x\n\n\nclass FastAdaptiveAvgPool(nn.Module):\n    def __init__(self, flatten: bool = False, input_fmt: F = 'NCHW'):\n        super(FastAdaptiveAvgPool, self).__init__()\n        self.flatten = flatten\n        self.dim = get_spatial_dim(input_fmt)\n\n    def forward(self, x):\n        return x.mean(self.dim, keepdim=not self.flatten)\n\n\nclass FastAdaptiveMaxPool(nn.Module):\n    def __init__(self, flatten: bool = False, input_fmt: str = 'NCHW'):\n        super(FastAdaptiveMaxPool, self).__init__()\n        self.flatten = flatten\n        self.dim = get_spatial_dim(input_fmt)\n\n    def forward(self, x):\n        return x.amax(self.dim, keepdim=not self.flatten)\n\n\nclass FastAdaptiveAvgMaxPool(nn.Module):\n    def __init__(self, flatten: bool = False, input_fmt: str = 'NCHW'):\n        super(FastAdaptiveAvgMaxPool, self).__init__()\n        self.flatten = flatten\n        self.dim = get_spatial_dim(input_fmt)\n\n    def forward(self, x):\n        x_avg = x.mean(self.dim, keepdim=not self.flatten)\n        x_max = x.amax(self.dim, keepdim=not self.flatten)\n        return 0.5 * x_avg + 0.5 * x_max\n\n\nclass FastAdaptiveCatAvgMaxPool(nn.Module):\n    def __init__(self, flatten: bool = False, input_fmt: str = 'NCHW'):\n        super(FastAdaptiveCatAvgMaxPool, self).__init__()\n        self.flatten = flatten\n        self.dim_reduce = get_spatial_dim(input_fmt)\n        if flatten:\n            self.dim_cat = 1\n        else:\n            self.dim_cat = get_channel_dim(input_fmt)\n\n    def forward(self, x):\n        x_avg = x.mean(self.dim_reduce, keepdim=not self.flatten)\n        x_max = x.amax(self.dim_reduce, keepdim=not self.flatten)\n        return torch.cat((x_avg, x_max), self.dim_cat)\n\n\nclass AdaptiveAvgMaxPool2d(nn.Module):\n    def __init__(self, output_size: _int_tuple_2_t = 1):\n        super(AdaptiveAvgMaxPool2d, self).__init__()\n        self.output_size = output_size\n\n    def forward(self, x):\n        return adaptive_avgmax_pool2d(x, self.output_size)\n\n\nclass AdaptiveCatAvgMaxPool2d(nn.Module):\n    def __init__(self, output_size: _int_tuple_2_t = 1):\n        super(AdaptiveCatAvgMaxPool2d, self).__init__()\n        self.output_size = output_size\n\n    def forward(self, x):\n        return adaptive_catavgmax_pool2d(x, self.output_size)\n\n\nclass SelectAdaptivePool2d(nn.Module):\n    \"\"\"Selectable global pooling layer with dynamic input kernel size\n    \"\"\"\n    def __init__(\n            self,\n            output_size: _int_tuple_2_t = 1,\n            pool_type: str = 'fast',\n            flatten: bool = False,\n            input_fmt: str = 'NCHW',\n    ):\n        super(SelectAdaptivePool2d, self).__init__()\n        assert input_fmt in ('NCHW', 'NHWC')\n        self.pool_type = pool_type or ''  # convert other falsy values to empty string for consistent TS typing\n        if not pool_type:\n            self.pool = nn.Identity()  # pass through\n            self.flatten = nn.Flatten(1) if flatten else nn.Identity()\n        elif pool_type.startswith('fast') or input_fmt != 'NCHW':\n            assert output_size == 1, 'Fast pooling and non NCHW input formats require output_size == 1.'\n            if pool_type.endswith('avgmax'):\n                self.pool = FastAdaptiveAvgMaxPool(flatten, input_fmt=input_fmt)\n            elif pool_type.endswith('catavgmax'):\n                self.pool = FastAdaptiveCatAvgMaxPool(flatten, input_fmt=input_fmt)\n            elif pool_type.endswith('max'):\n                self.pool = FastAdaptiveMaxPool(flatten, input_fmt=input_fmt)\n            else:\n                self.pool = FastAdaptiveAvgPool(flatten, input_fmt=input_fmt)\n            self.flatten = nn.Identity()\n        else:\n            assert input_fmt == 'NCHW'\n            if pool_type == 'avgmax':\n                self.pool = AdaptiveAvgMaxPool2d(output_size)\n            elif pool_type == 'catavgmax':\n                self.pool = AdaptiveCatAvgMaxPool2d(output_size)\n            elif pool_type == 'max':\n                self.pool = nn.AdaptiveMaxPool2d(output_size)\n            else:\n                self.pool = nn.AdaptiveAvgPool2d(output_size)\n            self.flatten = nn.Flatten(1) if flatten else nn.Identity()\n\n    def is_identity(self):\n        return not self.pool_type\n\n    def forward(self, x):\n        x = self.pool(x)\n        x = self.flatten(x)\n        return x\n\n    def feat_mult(self):\n        return adaptive_pool_feat_mult(self.pool_type)\n\n    def __repr__(self):\n        return self.__class__.__name__ + ' (' \\\n               + 'pool_type=' + self.pool_type \\\n               + ', flatten=' + str(self.flatten) + ')'\n\n",
  "\"\"\" Filter Response Norm in PyTorch\n\nBased on `Filter Response Normalization Layer` - https://arxiv.org/abs/1911.09737\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport torch\nimport torch.nn as nn\n\nfrom .create_act import create_act_layer\nfrom .trace_utils import _assert\n\n\ndef inv_instance_rms(x, eps: float = 1e-5):\n    rms = x.square().float().mean(dim=(2, 3), keepdim=True).add(eps).rsqrt().to(x.dtype)\n    return rms.expand(x.shape)\n\n\nclass FilterResponseNormTlu2d(nn.Module):\n    def __init__(self, num_features, apply_act=True, eps=1e-5, rms=True, **_):\n        super(FilterResponseNormTlu2d, self).__init__()\n        self.apply_act = apply_act  # apply activation (non-linearity)\n        self.rms = rms\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(num_features))\n        self.bias = nn.Parameter(torch.zeros(num_features))\n        self.tau = nn.Parameter(torch.zeros(num_features)) if apply_act else None\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.ones_(self.weight)\n        nn.init.zeros_(self.bias)\n        if self.tau is not None:\n            nn.init.zeros_(self.tau)\n\n    def forward(self, x):\n        _assert(x.dim() == 4, 'expected 4D input')\n        x_dtype = x.dtype\n        v_shape = (1, -1, 1, 1)\n        x = x * inv_instance_rms(x, self.eps)\n        x = x * self.weight.view(v_shape).to(dtype=x_dtype) + self.bias.view(v_shape).to(dtype=x_dtype)\n        return torch.maximum(x, self.tau.reshape(v_shape).to(dtype=x_dtype)) if self.tau is not None else x\n\n\nclass FilterResponseNormAct2d(nn.Module):\n    def __init__(self, num_features, apply_act=True, act_layer=nn.ReLU, inplace=None, rms=True, eps=1e-5, **_):\n        super(FilterResponseNormAct2d, self).__init__()\n        if act_layer is not None and apply_act:\n            self.act = create_act_layer(act_layer, inplace=inplace)\n        else:\n            self.act = nn.Identity()\n        self.rms = rms\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(num_features))\n        self.bias = nn.Parameter(torch.zeros(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.ones_(self.weight)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, x):\n        _assert(x.dim() == 4, 'expected 4D input')\n        x_dtype = x.dtype\n        v_shape = (1, -1, 1, 1)\n        x = x * inv_instance_rms(x, self.eps)\n        x = x * self.weight.view(v_shape).to(dtype=x_dtype) + self.bias.view(v_shape).to(dtype=x_dtype)\n        return self.act(x)\n",
  "\"\"\" Lambda Layer\n\nPaper: `LambdaNetworks: Modeling Long-Range Interactions Without Attention`\n    - https://arxiv.org/abs/2102.08602\n\n@misc{2102.08602,\nAuthor = {Irwan Bello},\nTitle = {LambdaNetworks: Modeling Long-Range Interactions Without Attention},\nYear = {2021},\n}\n\nStatus:\nThis impl is a WIP. Code snippets in the paper were used as reference but\ngood chance some details are missing/wrong.\n\nI've only implemented local lambda conv based pos embeddings.\n\nFor a PyTorch impl that includes other embedding options checkout\nhttps://github.com/lucidrains/lambda-networks\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom .helpers import to_2tuple, make_divisible\nfrom .weight_init import trunc_normal_\n\n\ndef rel_pos_indices(size):\n    size = to_2tuple(size)\n    pos = torch.stack(torch.meshgrid(torch.arange(size[0]), torch.arange(size[1]))).flatten(1)\n    rel_pos = pos[:, None, :] - pos[:, :, None]\n    rel_pos[0] += size[0] - 1\n    rel_pos[1] += size[1] - 1\n    return rel_pos  # 2, H * W, H * W\n\n\nclass LambdaLayer(nn.Module):\n    \"\"\"Lambda Layer\n\n    Paper: `LambdaNetworks: Modeling Long-Range Interactions Without Attention`\n        - https://arxiv.org/abs/2102.08602\n\n    NOTE: intra-depth parameter 'u' is fixed at 1. It did not appear worth the complexity to add.\n\n    The internal dimensions of the lambda module are controlled via the interaction of several arguments.\n      * the output dimension of the module is specified by dim_out, which falls back to input dim if not set\n      * the value (v) dimension is set to dim_out // num_heads, the v projection determines the output dim\n      * the query (q) and key (k) dimension are determined by\n        * dim_head = (dim_out * attn_ratio // num_heads) if dim_head is None\n        * q = num_heads * dim_head, k = dim_head\n      * as seen above, attn_ratio determines the ratio of q and k relative to the output if dim_head not set\n\n    Args:\n        dim (int): input dimension to the module\n        dim_out (int): output dimension of the module, same as dim if not set\n        feat_size (Tuple[int, int]): size of input feature_map for relative pos variant H, W\n        stride (int): output stride of the module, avg pool used if stride == 2\n        num_heads (int): parallel attention heads.\n        dim_head (int): dimension of query and key heads, calculated from dim_out * attn_ratio // num_heads if not set\n        r (int): local lambda convolution radius. Use lambda conv if set, else relative pos if not. (default: 9)\n        qk_ratio (float): ratio of q and k dimensions to output dimension when dim_head not set. (default: 1.0)\n        qkv_bias (bool): add bias to q, k, and v projections\n    \"\"\"\n    def __init__(\n            self, dim, dim_out=None, feat_size=None, stride=1, num_heads=4, dim_head=16, r=9,\n            qk_ratio=1.0, qkv_bias=False):\n        super().__init__()\n        dim_out = dim_out or dim\n        assert dim_out % num_heads == 0, ' should be divided by num_heads'\n        self.dim_qk = dim_head or make_divisible(dim_out * qk_ratio, divisor=8) // num_heads\n        self.num_heads = num_heads\n        self.dim_v = dim_out // num_heads\n\n        self.qkv = nn.Conv2d(\n            dim,\n            num_heads * self.dim_qk + self.dim_qk + self.dim_v,\n            kernel_size=1, bias=qkv_bias)\n        self.norm_q = nn.BatchNorm2d(num_heads * self.dim_qk)\n        self.norm_v = nn.BatchNorm2d(self.dim_v)\n\n        if r is not None:\n            # local lambda convolution for pos\n            self.conv_lambda = nn.Conv3d(1, self.dim_qk, (r, r, 1), padding=(r // 2, r // 2, 0))\n            self.pos_emb = None\n            self.rel_pos_indices = None\n        else:\n            # relative pos embedding\n            assert feat_size is not None\n            feat_size = to_2tuple(feat_size)\n            rel_size = [2 * s - 1 for s in feat_size]\n            self.conv_lambda = None\n            self.pos_emb = nn.Parameter(torch.zeros(rel_size[0], rel_size[1], self.dim_qk))\n            self.register_buffer('rel_pos_indices', rel_pos_indices(feat_size), persistent=False)\n\n        self.pool = nn.AvgPool2d(2, 2) if stride == 2 else nn.Identity()\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        trunc_normal_(self.qkv.weight, std=self.qkv.weight.shape[1] ** -0.5)  # fan-in\n        if self.conv_lambda is not None:\n            trunc_normal_(self.conv_lambda.weight, std=self.dim_qk ** -0.5)\n        if self.pos_emb is not None:\n            trunc_normal_(self.pos_emb, std=.02)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        M = H * W\n        qkv = self.qkv(x)\n        q, k, v = torch.split(qkv, [\n            self.num_heads * self.dim_qk, self.dim_qk, self.dim_v], dim=1)\n        q = self.norm_q(q).reshape(B, self.num_heads, self.dim_qk, M).transpose(-1, -2)  # B, num_heads, M, K\n        v = self.norm_v(v).reshape(B, self.dim_v, M).transpose(-1, -2)  # B, M, V\n        k = F.softmax(k.reshape(B, self.dim_qk, M), dim=-1)  # B, K, M\n\n        content_lam = k @ v  # B, K, V\n        content_out = q @ content_lam.unsqueeze(1)  # B, num_heads, M, V\n\n        if self.pos_emb is None:\n            position_lam = self.conv_lambda(v.reshape(B, 1, H, W, self.dim_v))  # B, H, W, V, K\n            position_lam = position_lam.reshape(B, 1, self.dim_qk, H * W, self.dim_v).transpose(2, 3)  # B, 1, M, K, V\n        else:\n            # FIXME relative pos embedding path not fully verified\n            pos_emb = self.pos_emb[self.rel_pos_indices[0], self.rel_pos_indices[1]].expand(B, -1, -1, -1)\n            position_lam = (pos_emb.transpose(-1, -2) @ v.unsqueeze(1)).unsqueeze(1)  # B, 1, M, K, V\n        position_out = (q.unsqueeze(-2) @ position_lam).squeeze(-2)  # B, num_heads, M, V\n\n        out = (content_out + position_out).transpose(-1, -2).reshape(B, C, H, W)  # B, C (num_heads * V), H, W\n        out = self.pool(out)\n        return out\n",
  "\"\"\" Selective Kernel Convolution/Attention\n\nPaper: Selective Kernel Networks (https://arxiv.org/abs/1903.06586)\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\nfrom torch import nn as nn\n\nfrom .conv_bn_act import ConvNormActAa\nfrom .helpers import make_divisible\nfrom .trace_utils import _assert\n\n\ndef _kernel_valid(k):\n    if isinstance(k, (list, tuple)):\n        for ki in k:\n            return _kernel_valid(ki)\n    assert k >= 3 and k % 2\n\n\nclass SelectiveKernelAttn(nn.Module):\n    def __init__(self, channels, num_paths=2, attn_channels=32, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):\n        \"\"\" Selective Kernel Attention Module\n\n        Selective Kernel attention mechanism factored out into its own module.\n\n        \"\"\"\n        super(SelectiveKernelAttn, self).__init__()\n        self.num_paths = num_paths\n        self.fc_reduce = nn.Conv2d(channels, attn_channels, kernel_size=1, bias=False)\n        self.bn = norm_layer(attn_channels)\n        self.act = act_layer(inplace=True)\n        self.fc_select = nn.Conv2d(attn_channels, channels * num_paths, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        _assert(x.shape[1] == self.num_paths, '')\n        x = x.sum(1).mean((2, 3), keepdim=True)\n        x = self.fc_reduce(x)\n        x = self.bn(x)\n        x = self.act(x)\n        x = self.fc_select(x)\n        B, C, H, W = x.shape\n        x = x.view(B, self.num_paths, C // self.num_paths, H, W)\n        x = torch.softmax(x, dim=1)\n        return x\n\n\nclass SelectiveKernel(nn.Module):\n\n    def __init__(self, in_channels, out_channels=None, kernel_size=None, stride=1, dilation=1, groups=1,\n                 rd_ratio=1./16, rd_channels=None, rd_divisor=8, keep_3x3=True, split_input=True,\n                 act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d, aa_layer=None, drop_layer=None):\n        \"\"\" Selective Kernel Convolution Module\n\n        As described in Selective Kernel Networks (https://arxiv.org/abs/1903.06586) with some modifications.\n\n        Largest change is the input split, which divides the input channels across each convolution path, this can\n        be viewed as a grouping of sorts, but the output channel counts expand to the module level value. This keeps\n        the parameter count from ballooning when the convolutions themselves don't have groups, but still provides\n        a noteworthy increase in performance over similar param count models without this attention layer. -Ross W\n\n        Args:\n            in_channels (int):  module input (feature) channel count\n            out_channels (int):  module output (feature) channel count\n            kernel_size (int, list): kernel size for each convolution branch\n            stride (int): stride for convolutions\n            dilation (int): dilation for module as a whole, impacts dilation of each branch\n            groups (int): number of groups for each branch\n            rd_ratio (int, float): reduction factor for attention features\n            keep_3x3 (bool): keep all branch convolution kernels as 3x3, changing larger kernels for dilations\n            split_input (bool): split input channels evenly across each convolution branch, keeps param count lower,\n                can be viewed as grouping by path, output expands to module out_channels count\n            act_layer (nn.Module): activation layer to use\n            norm_layer (nn.Module): batchnorm/norm layer to use\n            aa_layer (nn.Module): anti-aliasing module\n            drop_layer (nn.Module): spatial drop module in convs (drop block, etc)\n        \"\"\"\n        super(SelectiveKernel, self).__init__()\n        out_channels = out_channels or in_channels\n        kernel_size = kernel_size or [3, 5]  # default to one 3x3 and one 5x5 branch. 5x5 -> 3x3 + dilation\n        _kernel_valid(kernel_size)\n        if not isinstance(kernel_size, list):\n            kernel_size = [kernel_size] * 2\n        if keep_3x3:\n            dilation = [dilation * (k - 1) // 2 for k in kernel_size]\n            kernel_size = [3] * len(kernel_size)\n        else:\n            dilation = [dilation] * len(kernel_size)\n        self.num_paths = len(kernel_size)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.split_input = split_input\n        if self.split_input:\n            assert in_channels % self.num_paths == 0\n            in_channels = in_channels // self.num_paths\n        groups = min(out_channels, groups)\n\n        conv_kwargs = dict(\n            stride=stride, groups=groups, act_layer=act_layer, norm_layer=norm_layer,\n            aa_layer=aa_layer, drop_layer=drop_layer)\n        self.paths = nn.ModuleList([\n            ConvNormActAa(in_channels, out_channels, kernel_size=k, dilation=d, **conv_kwargs)\n            for k, d in zip(kernel_size, dilation)])\n\n        attn_channels = rd_channels or make_divisible(out_channels * rd_ratio, divisor=rd_divisor)\n        self.attn = SelectiveKernelAttn(out_channels, self.num_paths, attn_channels)\n\n    def forward(self, x):\n        if self.split_input:\n            x_split = torch.split(x, self.in_channels // self.num_paths, 1)\n            x_paths = [op(x_split[i]) for i, op in enumerate(self.paths)]\n        else:\n            x_paths = [op(x) for op in self.paths]\n        x = torch.stack(x_paths, dim=1)\n        x_attn = self.attn(x)\n        x = x * x_attn\n        x = torch.sum(x, dim=1)\n        return x\n",
  "from typing import Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\n\n\nclass PatchDropout(nn.Module):\n    \"\"\"\n    https://arxiv.org/abs/2212.00794\n    \"\"\"\n    return_indices: torch.jit.Final[bool]\n\n    def __init__(\n            self,\n            prob: float = 0.5,\n            num_prefix_tokens: int = 1,\n            ordered: bool = False,\n            return_indices: bool = False,\n    ):\n        super().__init__()\n        assert 0 <= prob < 1.\n        self.prob = prob\n        self.num_prefix_tokens = num_prefix_tokens  # exclude CLS token (or other prefix tokens)\n        self.ordered = ordered\n        self.return_indices = return_indices\n\n    def forward(self, x) -> Union[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]:\n        if not self.training or self.prob == 0.:\n            if self.return_indices:\n                return x, None\n            return x\n\n        if self.num_prefix_tokens:\n            prefix_tokens, x = x[:, :self.num_prefix_tokens], x[:, self.num_prefix_tokens:]\n        else:\n            prefix_tokens = None\n\n        B = x.shape[0]\n        L = x.shape[1]\n        num_keep = max(1, int(L * (1. - self.prob)))\n        keep_indices = torch.argsort(torch.randn(B, L, device=x.device), dim=-1)[:, :num_keep]\n        if self.ordered:\n            # NOTE does not need to maintain patch order in typical transformer use,\n            # but possibly useful for debug / visualization\n            keep_indices = keep_indices.sort(dim=-1)[0]\n        x = x.gather(1, keep_indices.unsqueeze(-1).expand((-1, -1) + x.shape[2:]))\n\n        if prefix_tokens is not None:\n            x = torch.cat((prefix_tokens, x), dim=1)\n\n        if self.return_indices:\n            return x, keep_indices\n        return x\n",
  "\"\"\" Bottleneck Self Attention (Bottleneck Transformers)\n\nPaper: `Bottleneck Transformers for Visual Recognition` - https://arxiv.org/abs/2101.11605\n\n@misc{2101.11605,\nAuthor = {Aravind Srinivas and Tsung-Yi Lin and Niki Parmar and Jonathon Shlens and Pieter Abbeel and Ashish Vaswani},\nTitle = {Bottleneck Transformers for Visual Recognition},\nYear = {2021},\n}\n\nBased on ref gist at: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2\n\nThis impl is a WIP but given that it is based on the ref gist likely not too far off.\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .helpers import to_2tuple, make_divisible\nfrom .weight_init import trunc_normal_\nfrom .trace_utils import _assert\n\n\ndef rel_logits_1d(q, rel_k, permute_mask: List[int]):\n    \"\"\" Compute relative logits along one dimension\n\n    As per: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2\n    Originally from: `Attention Augmented Convolutional Networks` - https://arxiv.org/abs/1904.09925\n\n    Args:\n        q: (batch, heads, height, width, dim)\n        rel_k: (2 * width - 1, dim)\n        permute_mask: permute output dim according to this\n    \"\"\"\n    B, H, W, dim = q.shape\n    x = (q @ rel_k.transpose(-1, -2))\n    x = x.reshape(-1, W, 2 * W -1)\n\n    # pad to shift from relative to absolute indexing\n    x_pad = F.pad(x, [0, 1]).flatten(1)\n    x_pad = F.pad(x_pad, [0, W - 1])\n\n    # reshape and slice out the padded elements\n    x_pad = x_pad.reshape(-1, W + 1, 2 * W - 1)\n    x = x_pad[:, :W, W - 1:]\n\n    # reshape and tile\n    x = x.reshape(B, H, 1, W, W).expand(-1, -1, H, -1, -1)\n    return x.permute(permute_mask)\n\n\nclass PosEmbedRel(nn.Module):\n    \"\"\" Relative Position Embedding\n    As per: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2\n    Originally from: `Attention Augmented Convolutional Networks` - https://arxiv.org/abs/1904.09925\n    \"\"\"\n    def __init__(self, feat_size, dim_head, scale):\n        super().__init__()\n        self.height, self.width = to_2tuple(feat_size)\n        self.dim_head = dim_head\n        self.height_rel = nn.Parameter(torch.randn(self.height * 2 - 1, dim_head) * scale)\n        self.width_rel = nn.Parameter(torch.randn(self.width * 2 - 1, dim_head) * scale)\n\n    def forward(self, q):\n        B, HW, _ = q.shape\n\n        # relative logits in width dimension.\n        q = q.reshape(B, self.height, self.width, -1)\n        rel_logits_w = rel_logits_1d(q, self.width_rel, permute_mask=(0, 1, 3, 2, 4))\n\n        # relative logits in height dimension.\n        q = q.transpose(1, 2)\n        rel_logits_h = rel_logits_1d(q, self.height_rel, permute_mask=(0, 3, 1, 4, 2))\n\n        rel_logits = rel_logits_h + rel_logits_w\n        rel_logits = rel_logits.reshape(B, HW, HW)\n        return rel_logits\n\n\nclass BottleneckAttn(nn.Module):\n    \"\"\" Bottleneck Attention\n    Paper: `Bottleneck Transformers for Visual Recognition` - https://arxiv.org/abs/2101.11605\n\n    The internal dimensions of the attention module are controlled by the interaction of several arguments.\n      * the output dimension of the module is specified by dim_out, which falls back to input dim if not set\n      * the value (v) dimension is set to dim_out // num_heads, the v projection determines the output dim\n      * the query and key (qk) dimensions are determined by\n        * num_heads * dim_head if dim_head is not None\n        * num_heads * (dim_out * attn_ratio // num_heads) if dim_head is None\n      * as seen above, attn_ratio determines the ratio of q and k relative to the output if dim_head not used\n\n    Args:\n        dim (int): input dimension to the module\n        dim_out (int): output dimension of the module, same as dim if not set\n        stride (int): output stride of the module, avg pool used if stride == 2 (default: 1).\n        num_heads (int): parallel attention heads (default: 4)\n        dim_head (int): dimension of query and key heads, calculated from dim_out * attn_ratio // num_heads if not set\n        qk_ratio (float): ratio of q and k dimensions to output dimension when dim_head not set. (default: 1.0)\n        qkv_bias (bool): add bias to q, k, and v projections\n        scale_pos_embed (bool): scale the position embedding as well as Q @ K\n    \"\"\"\n    def __init__(\n            self, dim, dim_out=None, feat_size=None, stride=1, num_heads=4, dim_head=None,\n            qk_ratio=1.0, qkv_bias=False, scale_pos_embed=False):\n        super().__init__()\n        assert feat_size is not None, 'A concrete feature size matching expected input (H, W) is required'\n        dim_out = dim_out or dim\n        assert dim_out % num_heads == 0\n        self.num_heads = num_heads\n        self.dim_head_qk = dim_head or make_divisible(dim_out * qk_ratio, divisor=8) // num_heads\n        self.dim_head_v = dim_out // self.num_heads\n        self.dim_out_qk = num_heads * self.dim_head_qk\n        self.dim_out_v = num_heads * self.dim_head_v\n        self.scale = self.dim_head_qk ** -0.5\n        self.scale_pos_embed = scale_pos_embed\n\n        self.qkv = nn.Conv2d(dim, self.dim_out_qk * 2 + self.dim_out_v, 1, bias=qkv_bias)\n\n        # NOTE I'm only supporting relative pos embedding for now\n        self.pos_embed = PosEmbedRel(feat_size, dim_head=self.dim_head_qk, scale=self.scale)\n\n        self.pool = nn.AvgPool2d(2, 2) if stride == 2 else nn.Identity()\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        trunc_normal_(self.qkv.weight, std=self.qkv.weight.shape[1] ** -0.5)  # fan-in\n        trunc_normal_(self.pos_embed.height_rel, std=self.scale)\n        trunc_normal_(self.pos_embed.width_rel, std=self.scale)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        _assert(H == self.pos_embed.height, '')\n        _assert(W == self.pos_embed.width, '')\n\n        x = self.qkv(x)  # B, (2 * dim_head_qk + dim_head_v) * num_heads, H, W\n\n        # NOTE head vs channel split ordering in qkv projection was decided before I allowed qk to differ from v\n        # So, this is more verbose than if heads were before qkv splits, but throughput is not impacted.\n        q, k, v = torch.split(x, [self.dim_out_qk, self.dim_out_qk, self.dim_out_v], dim=1)\n        q = q.reshape(B * self.num_heads, self.dim_head_qk, -1).transpose(-1, -2)\n        k = k.reshape(B * self.num_heads, self.dim_head_qk, -1)  # no transpose, for q @ k\n        v = v.reshape(B * self.num_heads, self.dim_head_v, -1).transpose(-1, -2)\n\n        if self.scale_pos_embed:\n            attn = (q @ k + self.pos_embed(q)) * self.scale  # B * num_heads, H * W, H * W\n        else:\n            attn = (q @ k) * self.scale + self.pos_embed(q)\n        attn = attn.softmax(dim=-1)\n\n        out = (attn @ v).transpose(-1, -2).reshape(B, self.dim_out_v, H, W)  # B, dim_out, H, W\n        out = self.pool(out)\n        return out\n",
  "\"\"\" Split BatchNorm\n\nA PyTorch BatchNorm layer that splits input batch into N equal parts and passes each through\na separate BN layer. The first split is passed through the parent BN layers with weight/bias\nkeys the same as the original BN. All other splits pass through BN sub-layers under the '.aux_bn'\nnamespace.\n\nThis allows easily removing the auxiliary BN layers after training to efficiently\nachieve the 'Auxiliary BatchNorm' as described in the AdvProp Paper, section 4.2,\n'Disentangled Learning via An Auxiliary BN'\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\nimport torch.nn as nn\n\n\nclass SplitBatchNorm2d(torch.nn.BatchNorm2d):\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n                 track_running_stats=True, num_splits=2):\n        super().__init__(num_features, eps, momentum, affine, track_running_stats)\n        assert num_splits > 1, 'Should have at least one aux BN layer (num_splits at least 2)'\n        self.num_splits = num_splits\n        self.aux_bn = nn.ModuleList([\n            nn.BatchNorm2d(num_features, eps, momentum, affine, track_running_stats) for _ in range(num_splits - 1)])\n\n    def forward(self, input: torch.Tensor):\n        if self.training:  # aux BN only relevant while training\n            split_size = input.shape[0] // self.num_splits\n            assert input.shape[0] == split_size * self.num_splits, \"batch size must be evenly divisible by num_splits\"\n            split_input = input.split(split_size)\n            x = [super().forward(split_input[0])]\n            for i, a in enumerate(self.aux_bn):\n                x.append(a(split_input[i + 1]))\n            return torch.cat(x, dim=0)\n        else:\n            return super().forward(input)\n\n\ndef convert_splitbn_model(module, num_splits=2):\n    \"\"\"\n    Recursively traverse module and its children to replace all instances of\n    ``torch.nn.modules.batchnorm._BatchNorm`` with `SplitBatchnorm2d`.\n    Args:\n        module (torch.nn.Module): input module\n        num_splits: number of separate batchnorm layers to split input across\n    Example::\n        >>> # model is an instance of torch.nn.Module\n        >>> model = timm.models.convert_splitbn_model(model, num_splits=2)\n    \"\"\"\n    mod = module\n    if isinstance(module, torch.nn.modules.instancenorm._InstanceNorm):\n        return module\n    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n        mod = SplitBatchNorm2d(\n            module.num_features, module.eps, module.momentum, module.affine,\n            module.track_running_stats, num_splits=num_splits)\n        mod.running_mean = module.running_mean\n        mod.running_var = module.running_var\n        mod.num_batches_tracked = module.num_batches_tracked\n        if module.affine:\n            mod.weight.data = module.weight.data.clone().detach()\n            mod.bias.data = module.bias.data.clone().detach()\n        for aux in mod.aux_bn:\n            aux.running_mean = module.running_mean.clone()\n            aux.running_var = module.running_var.clone()\n            aux.num_batches_tracked = module.num_batches_tracked.clone()\n            if module.affine:\n                aux.weight.data = module.weight.data.clone().detach()\n                aux.bias.data = module.bias.data.clone().detach()\n    for name, child in module.named_children():\n        mod.add_module(name, convert_splitbn_model(child, num_splits=num_splits))\n    del module\n    return mod\n",
  "\"\"\" Conv2d w/ Same Padding\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Tuple, Optional\n\nfrom .config import is_exportable, is_scriptable\nfrom .padding import pad_same, pad_same_arg, get_padding_value\n\n\n_USE_EXPORT_CONV = False\n\n\ndef conv2d_same(\n        x,\n        weight: torch.Tensor,\n        bias: Optional[torch.Tensor] = None,\n        stride: Tuple[int, int] = (1, 1),\n        padding: Tuple[int, int] = (0, 0),\n        dilation: Tuple[int, int] = (1, 1),\n        groups: int = 1,\n):\n    x = pad_same(x, weight.shape[-2:], stride, dilation)\n    return F.conv2d(x, weight, bias, stride, (0, 0), dilation, groups)\n\n\nclass Conv2dSame(nn.Conv2d):\n    \"\"\" Tensorflow like 'SAME' convolution wrapper for 2D convolutions\n    \"\"\"\n\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=1,\n            padding=0,\n            dilation=1,\n            groups=1,\n            bias=True,\n    ):\n        super(Conv2dSame, self).__init__(\n            in_channels, out_channels, kernel_size,\n            stride, 0, dilation, groups, bias,\n        )\n\n    def forward(self, x):\n        return conv2d_same(\n            x, self.weight, self.bias,\n            self.stride, self.padding, self.dilation, self.groups,\n        )\n\n\nclass Conv2dSameExport(nn.Conv2d):\n    \"\"\" ONNX export friendly Tensorflow like 'SAME' convolution wrapper for 2D convolutions\n\n    NOTE: This does not currently work with torch.jit.script\n    \"\"\"\n\n    # pylint: disable=unused-argument\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=1,\n            padding=0,\n            dilation=1,\n            groups=1,\n            bias=True,\n    ):\n        super(Conv2dSameExport, self).__init__(\n            in_channels, out_channels, kernel_size,\n            stride, 0, dilation, groups, bias,\n        )\n        self.pad = None\n        self.pad_input_size = (0, 0)\n\n    def forward(self, x):\n        input_size = x.size()[-2:]\n        if self.pad is None:\n            pad_arg = pad_same_arg(input_size, self.weight.size()[-2:], self.stride, self.dilation)\n            self.pad = nn.ZeroPad2d(pad_arg)\n            self.pad_input_size = input_size\n\n        x = self.pad(x)\n        return F.conv2d(\n            x, self.weight, self.bias,\n            self.stride, self.padding, self.dilation, self.groups,\n        )\n\n\ndef create_conv2d_pad(in_chs, out_chs, kernel_size, **kwargs):\n    padding = kwargs.pop('padding', '')\n    kwargs.setdefault('bias', False)\n    padding, is_dynamic = get_padding_value(padding, kernel_size, **kwargs)\n    if is_dynamic:\n        if _USE_EXPORT_CONV and is_exportable():\n            # older PyTorch ver needed this to export same padding reasonably\n            assert not is_scriptable()  # Conv2DSameExport does not work with jit\n            return Conv2dSameExport(in_chs, out_chs, kernel_size, **kwargs)\n        else:\n            return Conv2dSame(in_chs, out_chs, kernel_size, **kwargs)\n    else:\n        return nn.Conv2d(in_chs, out_chs, kernel_size, padding=padding, **kwargs)\n\n\n",
  "\"\"\" Global Response Normalization Module\n\nBased on the GRN layer presented in\n`ConvNeXt-V2 - Co-designing and Scaling ConvNets with Masked Autoencoders` - https://arxiv.org/abs/2301.00808\n\nThis implementation\n* works for both NCHW and NHWC tensor layouts\n* uses affine param names matching existing torch norm layers\n* slightly improves eager mode performance via fused addcmul\n\nHacked together by / Copyright 2023 Ross Wightman\n\"\"\"\n\nimport torch\nfrom torch import nn as nn\n\n\nclass GlobalResponseNorm(nn.Module):\n    \"\"\" Global Response Normalization layer\n    \"\"\"\n    def __init__(self, dim, eps=1e-6, channels_last=True):\n        super().__init__()\n        self.eps = eps\n        if channels_last:\n            self.spatial_dim = (1, 2)\n            self.channel_dim = -1\n            self.wb_shape = (1, 1, 1, -1)\n        else:\n            self.spatial_dim = (2, 3)\n            self.channel_dim = 1\n            self.wb_shape = (1, -1, 1, 1)\n\n        self.weight = nn.Parameter(torch.zeros(dim))\n        self.bias = nn.Parameter(torch.zeros(dim))\n\n    def forward(self, x):\n        x_g = x.norm(p=2, dim=self.spatial_dim, keepdim=True)\n        x_n = x_g / (x_g.mean(dim=self.channel_dim, keepdim=True) + self.eps)\n        return x + torch.addcmul(self.bias.view(self.wb_shape), self.weight.view(self.wb_shape), x * x_n)\n",
  "import torch\nfrom torch import nn as nn\n\ntry:\n    from inplace_abn.functions import inplace_abn, inplace_abn_sync\n    has_iabn = True\nexcept ImportError:\n    has_iabn = False\n\n    def inplace_abn(x, weight, bias, running_mean, running_var,\n                    training=True, momentum=0.1, eps=1e-05, activation=\"leaky_relu\", activation_param=0.01):\n        raise ImportError(\n            \"Please install InplaceABN:'pip install git+https://github.com/mapillary/inplace_abn.git@v1.0.12'\")\n\n    def inplace_abn_sync(**kwargs):\n        inplace_abn(**kwargs)\n\n\nclass InplaceAbn(nn.Module):\n    \"\"\"Activated Batch Normalization\n\n    This gathers a BatchNorm and an activation function in a single module\n\n    Parameters\n    ----------\n    num_features : int\n        Number of feature channels in the input and output.\n    eps : float\n        Small constant to prevent numerical issues.\n    momentum : float\n        Momentum factor applied to compute running statistics.\n    affine : bool\n        If `True` apply learned scale and shift transformation after normalization.\n    act_layer : str or nn.Module type\n        Name or type of the activation functions, one of: `leaky_relu`, `elu`\n    act_param : float\n        Negative slope for the `leaky_relu` activation.\n    \"\"\"\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, apply_act=True,\n                 act_layer=\"leaky_relu\", act_param=0.01, drop_layer=None):\n        super(InplaceAbn, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n        self.momentum = momentum\n        if apply_act:\n            if isinstance(act_layer, str):\n                assert act_layer in ('leaky_relu', 'elu', 'identity', '')\n                self.act_name = act_layer if act_layer else 'identity'\n            else:\n                # convert act layer passed as type to string\n                if act_layer == nn.ELU:\n                    self.act_name = 'elu'\n                elif act_layer == nn.LeakyReLU:\n                    self.act_name = 'leaky_relu'\n                elif act_layer is None or act_layer == nn.Identity:\n                    self.act_name = 'identity'\n                else:\n                    assert False, f'Invalid act layer {act_layer.__name__} for IABN'\n        else:\n            self.act_name = 'identity'\n        self.act_param = act_param\n        if self.affine:\n            self.weight = nn.Parameter(torch.ones(num_features))\n            self.bias = nn.Parameter(torch.zeros(num_features))\n        else:\n            self.register_parameter('weight', None)\n            self.register_parameter('bias', None)\n        self.register_buffer('running_mean', torch.zeros(num_features))\n        self.register_buffer('running_var', torch.ones(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.constant_(self.running_mean, 0)\n        nn.init.constant_(self.running_var, 1)\n        if self.affine:\n            nn.init.constant_(self.weight, 1)\n            nn.init.constant_(self.bias, 0)\n\n    def forward(self, x):\n        output = inplace_abn(\n            x, self.weight, self.bias, self.running_mean, self.running_var,\n            self.training, self.momentum, self.eps, self.act_name, self.act_param)\n        if isinstance(output, tuple):\n            output = output[0]\n        return output\n",
  "\"\"\" NormAct (Normalizaiton + Activation Layer) Factory\n\nCreate norm + act combo modules that attempt to be backwards compatible with separate norm + act\nisntances in models. Where these are used it will be possible to swap separate BN + act layers with\ncombined modules like IABN or EvoNorms.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport types\nimport functools\n\nfrom .evo_norm import *\nfrom .filter_response_norm import FilterResponseNormAct2d, FilterResponseNormTlu2d\nfrom .norm_act import BatchNormAct2d, GroupNormAct, LayerNormAct, LayerNormAct2d\nfrom .inplace_abn import InplaceAbn\n\n_NORM_ACT_MAP = dict(\n    batchnorm=BatchNormAct2d,\n    batchnorm2d=BatchNormAct2d,\n    groupnorm=GroupNormAct,\n    groupnorm1=functools.partial(GroupNormAct, num_groups=1),\n    layernorm=LayerNormAct,\n    layernorm2d=LayerNormAct2d,\n    evonormb0=EvoNorm2dB0,\n    evonormb1=EvoNorm2dB1,\n    evonormb2=EvoNorm2dB2,\n    evonorms0=EvoNorm2dS0,\n    evonorms0a=EvoNorm2dS0a,\n    evonorms1=EvoNorm2dS1,\n    evonorms1a=EvoNorm2dS1a,\n    evonorms2=EvoNorm2dS2,\n    evonorms2a=EvoNorm2dS2a,\n    frn=FilterResponseNormAct2d,\n    frntlu=FilterResponseNormTlu2d,\n    inplaceabn=InplaceAbn,\n    iabn=InplaceAbn,\n)\n_NORM_ACT_TYPES = {m for n, m in _NORM_ACT_MAP.items()}\n# has act_layer arg to define act type\n_NORM_ACT_REQUIRES_ARG = {\n    BatchNormAct2d, GroupNormAct, LayerNormAct, LayerNormAct2d, FilterResponseNormAct2d, InplaceAbn}\n\n\ndef create_norm_act_layer(layer_name, num_features, act_layer=None, apply_act=True, jit=False, **kwargs):\n    layer = get_norm_act_layer(layer_name, act_layer=act_layer)\n    layer_instance = layer(num_features, apply_act=apply_act, **kwargs)\n    if jit:\n        layer_instance = torch.jit.script(layer_instance)\n    return layer_instance\n\n\ndef get_norm_act_layer(norm_layer, act_layer=None):\n    assert isinstance(norm_layer, (type, str,  types.FunctionType, functools.partial))\n    assert act_layer is None or isinstance(act_layer, (type, str, types.FunctionType, functools.partial))\n    norm_act_kwargs = {}\n\n    # unbind partial fn, so args can be rebound later\n    if isinstance(norm_layer, functools.partial):\n        norm_act_kwargs.update(norm_layer.keywords)\n        norm_layer = norm_layer.func\n\n    if isinstance(norm_layer, str):\n        layer_name = norm_layer.replace('_', '').lower().split('-')[0]\n        norm_act_layer = _NORM_ACT_MAP.get(layer_name, None)\n    elif norm_layer in _NORM_ACT_TYPES:\n        norm_act_layer = norm_layer\n    elif isinstance(norm_layer,  types.FunctionType):\n        # if function type, must be a lambda/fn that creates a norm_act layer\n        norm_act_layer = norm_layer\n    else:\n        type_name = norm_layer.__name__.lower()\n        if type_name.startswith('batchnorm'):\n            norm_act_layer = BatchNormAct2d\n        elif type_name.startswith('groupnorm'):\n            norm_act_layer = GroupNormAct\n        elif type_name.startswith('groupnorm1'):\n            norm_act_layer = functools.partial(GroupNormAct, num_groups=1)\n        elif type_name.startswith('layernorm2d'):\n            norm_act_layer = LayerNormAct2d\n        elif type_name.startswith('layernorm'):\n            norm_act_layer = LayerNormAct\n        else:\n            assert False, f\"No equivalent norm_act layer for {type_name}\"\n\n    if norm_act_layer in _NORM_ACT_REQUIRES_ARG:\n        # pass `act_layer` through for backwards compat where `act_layer=None` implies no activation.\n        # In the future, may force use of `apply_act` with `act_layer` arg bound to relevant NormAct types\n        norm_act_kwargs.setdefault('act_layer', act_layer)\n    if norm_act_kwargs:\n        norm_act_layer = functools.partial(norm_act_layer, **norm_act_kwargs)  # bind/rebind args\n    return norm_act_layer\n",
  "from enum import Enum\nfrom typing import Union\n\nimport torch\n\n\nclass Format(str, Enum):\n    NCHW = 'NCHW'\n    NHWC = 'NHWC'\n    NCL = 'NCL'\n    NLC = 'NLC'\n\n\nFormatT = Union[str, Format]\n\n\ndef get_spatial_dim(fmt: FormatT):\n    fmt = Format(fmt)\n    if fmt is Format.NLC:\n        dim = (1,)\n    elif fmt is Format.NCL:\n        dim = (2,)\n    elif fmt is Format.NHWC:\n        dim = (1, 2)\n    else:\n        dim = (2, 3)\n    return dim\n\n\ndef get_channel_dim(fmt: FormatT):\n    fmt = Format(fmt)\n    if fmt is Format.NHWC:\n        dim = 3\n    elif fmt is Format.NLC:\n        dim = 2\n    else:\n        dim = 1\n    return dim\n\n\ndef nchw_to(x: torch.Tensor, fmt: Format):\n    if fmt == Format.NHWC:\n        x = x.permute(0, 2, 3, 1)\n    elif fmt == Format.NLC:\n        x = x.flatten(2).transpose(1, 2)\n    elif fmt == Format.NCL:\n        x = x.flatten(2)\n    return x\n\n\ndef nhwc_to(x: torch.Tensor, fmt: Format):\n    if fmt == Format.NCHW:\n        x = x.permute(0, 3, 1, 2)\n    elif fmt == Format.NLC:\n        x = x.flatten(1, 2)\n    elif fmt == Format.NCL:\n        x = x.flatten(1, 2).transpose(1, 2)\n    return x\n",
  "\"\"\" Halo Self Attention\n\nPaper: `Scaling Local Self-Attention for Parameter Efficient Visual Backbones`\n    - https://arxiv.org/abs/2103.12731\n\n@misc{2103.12731,\nAuthor = {Ashish Vaswani and Prajit Ramachandran and Aravind Srinivas and Niki Parmar and Blake Hechtman and\n    Jonathon Shlens},\nTitle = {Scaling Local Self-Attention for Parameter Efficient Visual Backbones},\nYear = {2021},\n}\n\nStatus:\nThis impl is a WIP, there is no official ref impl and some details in paper weren't clear to me.\nThe attention mechanism works but it's slow as implemented.\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nfrom typing import List\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom .helpers import make_divisible\nfrom .weight_init import trunc_normal_\nfrom .trace_utils import _assert\n\n\ndef rel_logits_1d(q, rel_k, permute_mask: List[int]):\n    \"\"\" Compute relative logits along one dimension\n\n    As per: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2\n    Originally from: `Attention Augmented Convolutional Networks` - https://arxiv.org/abs/1904.09925\n\n    Args:\n        q: (batch, height, width, dim)\n        rel_k: (2 * window - 1, dim)\n        permute_mask: permute output dim according to this\n    \"\"\"\n    B, H, W, dim = q.shape\n    rel_size = rel_k.shape[0]\n    win_size = (rel_size + 1) // 2\n\n    x = (q @ rel_k.transpose(-1, -2))\n    x = x.reshape(-1, W, rel_size)\n\n    # pad to shift from relative to absolute indexing\n    x_pad = F.pad(x, [0, 1]).flatten(1)\n    x_pad = F.pad(x_pad, [0, rel_size - W])\n\n    # reshape and slice out the padded elements\n    x_pad = x_pad.reshape(-1, W + 1, rel_size)\n    x = x_pad[:, :W, win_size - 1:]\n\n    # reshape and tile\n    x = x.reshape(B, H, 1, W, win_size).expand(-1, -1, win_size, -1, -1)\n    return x.permute(permute_mask)\n\n\nclass PosEmbedRel(nn.Module):\n    \"\"\" Relative Position Embedding\n    As per: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2\n    Originally from: `Attention Augmented Convolutional Networks` - https://arxiv.org/abs/1904.09925\n\n    \"\"\"\n    def __init__(self, block_size, win_size, dim_head, scale):\n        \"\"\"\n        Args:\n            block_size (int): block size\n            win_size (int): neighbourhood window size\n            dim_head (int): attention head dim\n            scale (float): scale factor (for init)\n        \"\"\"\n        super().__init__()\n        self.block_size = block_size\n        self.dim_head = dim_head\n        self.height_rel = nn.Parameter(torch.randn(win_size * 2 - 1, dim_head) * scale)\n        self.width_rel = nn.Parameter(torch.randn(win_size * 2 - 1, dim_head) * scale)\n\n    def forward(self, q):\n        B, BB, HW, _ = q.shape\n\n        # relative logits in width dimension.\n        q = q.reshape(-1, self.block_size, self.block_size, self.dim_head)\n        rel_logits_w = rel_logits_1d(q, self.width_rel, permute_mask=(0, 1, 3, 2, 4))\n\n        # relative logits in height dimension.\n        q = q.transpose(1, 2)\n        rel_logits_h = rel_logits_1d(q, self.height_rel, permute_mask=(0, 3, 1, 4, 2))\n\n        rel_logits = rel_logits_h + rel_logits_w\n        rel_logits = rel_logits.reshape(B, BB, HW, -1)\n        return rel_logits\n\n\nclass HaloAttn(nn.Module):\n    \"\"\" Halo Attention\n\n    Paper: `Scaling Local Self-Attention for Parameter Efficient Visual Backbones`\n        - https://arxiv.org/abs/2103.12731\n\n    The internal dimensions of the attention module are controlled by the interaction of several arguments.\n      * the output dimension of the module is specified by dim_out, which falls back to input dim if not set\n      * the value (v) dimension is set to dim_out // num_heads, the v projection determines the output dim\n      * the query and key (qk) dimensions are determined by\n        * num_heads * dim_head if dim_head is not None\n        * num_heads * (dim_out * attn_ratio // num_heads) if dim_head is None\n      * as seen above, attn_ratio determines the ratio of q and k relative to the output if dim_head not used\n\n    Args:\n        dim (int): input dimension to the module\n        dim_out (int): output dimension of the module, same as dim if not set\n        feat_size (Tuple[int, int]): size of input feature_map (not used, for arg compat with bottle/lambda)\n        stride: output stride of the module, query downscaled if > 1 (default: 1).\n        num_heads: parallel attention heads (default: 8).\n        dim_head: dimension of query and key heads, calculated from dim_out * attn_ratio // num_heads if not set\n        block_size (int): size of blocks. (default: 8)\n        halo_size (int): size of halo overlap. (default: 3)\n        qk_ratio (float): ratio of q and k dimensions to output dimension when dim_head not set. (default: 1.0)\n        qkv_bias (bool) : add bias to q, k, and v projections\n        avg_down (bool): use average pool downsample instead of strided query blocks\n        scale_pos_embed (bool): scale the position embedding as well as Q @ K\n    \"\"\"\n    def __init__(\n            self, dim, dim_out=None, feat_size=None, stride=1, num_heads=8, dim_head=None, block_size=8, halo_size=3,\n            qk_ratio=1.0, qkv_bias=False, avg_down=False, scale_pos_embed=False):\n        super().__init__()\n        dim_out = dim_out or dim\n        assert dim_out % num_heads == 0\n        assert stride in (1, 2)\n        self.num_heads = num_heads\n        self.dim_head_qk = dim_head or make_divisible(dim_out * qk_ratio, divisor=8) // num_heads\n        self.dim_head_v = dim_out // self.num_heads\n        self.dim_out_qk = num_heads * self.dim_head_qk\n        self.dim_out_v = num_heads * self.dim_head_v\n        self.scale = self.dim_head_qk ** -0.5\n        self.scale_pos_embed = scale_pos_embed\n        self.block_size = self.block_size_ds = block_size\n        self.halo_size = halo_size\n        self.win_size = block_size + halo_size * 2  # neighbourhood window size\n        self.block_stride = 1\n        use_avg_pool = False\n        if stride > 1:\n            use_avg_pool = avg_down or block_size % stride != 0\n            self.block_stride = 1 if use_avg_pool else stride\n            self.block_size_ds = self.block_size // self.block_stride\n\n        # FIXME not clear if this stride behaviour is what the paper intended\n        # Also, the paper mentions using a 3D conv for dealing with the blocking/gather, and leaving\n        # data in unfolded block form. I haven't wrapped my head around how that'd look.\n        self.q = nn.Conv2d(dim, self.dim_out_qk, 1, stride=self.block_stride, bias=qkv_bias)\n        self.kv = nn.Conv2d(dim, self.dim_out_qk + self.dim_out_v, 1, bias=qkv_bias)\n\n        self.pos_embed = PosEmbedRel(\n            block_size=self.block_size_ds, win_size=self.win_size, dim_head=self.dim_head_qk, scale=self.scale)\n\n        self.pool = nn.AvgPool2d(2, 2) if use_avg_pool else nn.Identity()\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        std = self.q.weight.shape[1] ** -0.5  # fan-in\n        trunc_normal_(self.q.weight, std=std)\n        trunc_normal_(self.kv.weight, std=std)\n        trunc_normal_(self.pos_embed.height_rel, std=self.scale)\n        trunc_normal_(self.pos_embed.width_rel, std=self.scale)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        _assert(H % self.block_size == 0, '')\n        _assert(W % self.block_size == 0, '')\n        num_h_blocks = H // self.block_size\n        num_w_blocks = W // self.block_size\n        num_blocks = num_h_blocks * num_w_blocks\n\n        q = self.q(x)\n        # unfold\n        q = q.reshape(\n            -1, self.dim_head_qk,\n            num_h_blocks, self.block_size_ds, num_w_blocks, self.block_size_ds).permute(0, 1, 3, 5, 2, 4)\n        # B, num_heads * dim_head * block_size ** 2, num_blocks\n        q = q.reshape(B * self.num_heads, self.dim_head_qk, -1, num_blocks).transpose(1, 3)\n        # B * num_heads, num_blocks, block_size ** 2, dim_head\n\n        kv = self.kv(x)\n        # Generate overlapping windows for kv. This approach is good for GPU and CPU. However, unfold() is not\n        # lowered for PyTorch XLA so it will be very slow. See code at bottom of file for XLA friendly approach.\n        # FIXME figure out how to switch impl between this and conv2d if XLA being used.\n        kv = F.pad(kv, [self.halo_size, self.halo_size, self.halo_size, self.halo_size])\n        kv = kv.unfold(2, self.win_size, self.block_size).unfold(3, self.win_size, self.block_size).reshape(\n            B * self.num_heads, self.dim_head_qk + self.dim_head_v, num_blocks, -1).permute(0, 2, 3, 1)\n        k, v = torch.split(kv, [self.dim_head_qk, self.dim_head_v], dim=-1)\n        # B * num_heads, num_blocks, win_size ** 2, dim_head_qk or dim_head_v\n\n        if self.scale_pos_embed:\n            attn = (q @ k.transpose(-1, -2) + self.pos_embed(q)) * self.scale\n        else:\n            attn = (q @ k.transpose(-1, -2)) * self.scale + self.pos_embed(q)\n        # B * num_heads, num_blocks, block_size ** 2, win_size ** 2\n        attn = attn.softmax(dim=-1)\n\n        out = (attn @ v).transpose(1, 3)  # B * num_heads, dim_head_v, block_size ** 2, num_blocks\n        # fold\n        out = out.reshape(-1, self.block_size_ds, self.block_size_ds, num_h_blocks, num_w_blocks)\n        out = out.permute(0, 3, 1, 4, 2).contiguous().view(\n            B, self.dim_out_v, H // self.block_stride, W // self.block_stride)\n        # B, dim_out, H // block_stride, W // block_stride\n        out = self.pool(out)\n        return out\n\n\n\"\"\" Three alternatives for overlapping windows.\n\n`.unfold().unfold()` is same speed as stride tricks with similar clarity as F.unfold()\n\n    if is_xla:\n        # This code achieves haloing on PyTorch XLA with reasonable runtime trade-off, it is\n        # EXTREMELY slow for backward on a GPU though so I need a way of selecting based on environment.\n        WW = self.win_size ** 2\n        pw = torch.eye(WW, dtype=x.dtype, device=x.device).reshape(WW, 1, self.win_size, self.win_size)\n        kv = F.conv2d(kv.reshape(-1, 1, H, W), pw, stride=self.block_size, padding=self.halo_size)\n    elif self.stride_tricks:\n        kv = F.pad(kv, [self.halo_size, self.halo_size, self.halo_size, self.halo_size]).contiguous()\n        kv = kv.as_strided((\n            B, self.dim_out_qk + self.dim_out_v, self.win_size, self.win_size, num_h_blocks, num_w_blocks),\n            stride=(kv.stride(0), kv.stride(1), kv.shape[-1], 1, self.block_size * kv.shape[-1], self.block_size))\n    else:\n        kv = F.unfold(kv, kernel_size=self.win_size, stride=self.block_size, padding=self.halo_size)\n\n    kv = kv.reshape(\n       B * self.num_heads, self.dim_head_qk + self.dim_head_v, -1, num_blocks).transpose(1, 3)\n\"\"\"\n",
  "\"\"\" Bilinear-Attention-Transform and Non-Local Attention\n\nPaper: `Non-Local Neural Networks With Grouped Bilinear Attentional Transforms`\n    - https://openaccess.thecvf.com/content_CVPR_2020/html/Chi_Non-Local_Neural_Networks_With_Grouped_Bilinear_Attentional_Transforms_CVPR_2020_paper.html\nAdapted from original code: https://github.com/BA-Transform/BAT-Image-Classification\n\"\"\"\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .conv_bn_act import ConvNormAct\nfrom .helpers import make_divisible\nfrom .trace_utils import _assert\n\n\nclass NonLocalAttn(nn.Module):\n    \"\"\"Spatial NL block for image classification.\n\n    This was adapted from https://github.com/BA-Transform/BAT-Image-Classification\n    Their NonLocal impl inspired by https://github.com/facebookresearch/video-nonlocal-net.\n    \"\"\"\n\n    def __init__(self, in_channels, use_scale=True,  rd_ratio=1/8, rd_channels=None, rd_divisor=8, **kwargs):\n        super(NonLocalAttn, self).__init__()\n        if rd_channels is None:\n            rd_channels = make_divisible(in_channels * rd_ratio, divisor=rd_divisor)\n        self.scale = in_channels ** -0.5 if use_scale else 1.0\n        self.t = nn.Conv2d(in_channels, rd_channels, kernel_size=1, stride=1, bias=True)\n        self.p = nn.Conv2d(in_channels, rd_channels, kernel_size=1, stride=1, bias=True)\n        self.g = nn.Conv2d(in_channels, rd_channels, kernel_size=1, stride=1, bias=True)\n        self.z = nn.Conv2d(rd_channels, in_channels, kernel_size=1, stride=1, bias=True)\n        self.norm = nn.BatchNorm2d(in_channels)\n        self.reset_parameters()\n\n    def forward(self, x):\n        shortcut = x\n\n        t = self.t(x)\n        p = self.p(x)\n        g = self.g(x)\n\n        B, C, H, W = t.size()\n        t = t.view(B, C, -1).permute(0, 2, 1)\n        p = p.view(B, C, -1)\n        g = g.view(B, C, -1).permute(0, 2, 1)\n\n        att = torch.bmm(t, p) * self.scale\n        att = F.softmax(att, dim=2)\n        x = torch.bmm(att, g)\n\n        x = x.permute(0, 2, 1).reshape(B, C, H, W)\n        x = self.z(x)\n        x = self.norm(x) + shortcut\n\n        return x\n\n    def reset_parameters(self):\n        for name, m in self.named_modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode='fan_out', nonlinearity='relu')\n                if len(list(m.parameters())) > 1:\n                    nn.init.constant_(m.bias, 0.0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 0)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.GroupNorm):\n                nn.init.constant_(m.weight, 0)\n                nn.init.constant_(m.bias, 0)\n\n\nclass BilinearAttnTransform(nn.Module):\n\n    def __init__(self, in_channels, block_size, groups, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):\n        super(BilinearAttnTransform, self).__init__()\n\n        self.conv1 = ConvNormAct(in_channels, groups, 1, act_layer=act_layer, norm_layer=norm_layer)\n        self.conv_p = nn.Conv2d(groups, block_size * block_size * groups, kernel_size=(block_size, 1))\n        self.conv_q = nn.Conv2d(groups, block_size * block_size * groups, kernel_size=(1, block_size))\n        self.conv2 = ConvNormAct(in_channels, in_channels, 1, act_layer=act_layer, norm_layer=norm_layer)\n        self.block_size = block_size\n        self.groups = groups\n        self.in_channels = in_channels\n\n    def resize_mat(self, x, t: int):\n        B, C, block_size, block_size1 = x.shape\n        _assert(block_size == block_size1, '')\n        if t <= 1:\n            return x\n        x = x.view(B * C, -1, 1, 1)\n        x = x * torch.eye(t, t, dtype=x.dtype, device=x.device)\n        x = x.view(B * C, block_size, block_size, t, t)\n        x = torch.cat(torch.split(x, 1, dim=1), dim=3)\n        x = torch.cat(torch.split(x, 1, dim=2), dim=4)\n        x = x.view(B, C, block_size * t, block_size * t)\n        return x\n\n    def forward(self, x):\n        _assert(x.shape[-1] % self.block_size == 0, '')\n        _assert(x.shape[-2] % self.block_size == 0, '')\n        B, C, H, W = x.shape\n        out = self.conv1(x)\n        rp = F.adaptive_max_pool2d(out, (self.block_size, 1))\n        cp = F.adaptive_max_pool2d(out, (1, self.block_size))\n        p = self.conv_p(rp).view(B, self.groups, self.block_size, self.block_size).sigmoid()\n        q = self.conv_q(cp).view(B, self.groups, self.block_size, self.block_size).sigmoid()\n        p = p / p.sum(dim=3, keepdim=True)\n        q = q / q.sum(dim=2, keepdim=True)\n        p = p.view(B, self.groups, 1, self.block_size, self.block_size).expand(x.size(\n            0), self.groups, C // self.groups, self.block_size, self.block_size).contiguous()\n        p = p.view(B, C, self.block_size, self.block_size)\n        q = q.view(B, self.groups, 1, self.block_size, self.block_size).expand(x.size(\n            0), self.groups, C // self.groups, self.block_size, self.block_size).contiguous()\n        q = q.view(B, C, self.block_size, self.block_size)\n        p = self.resize_mat(p, H // self.block_size)\n        q = self.resize_mat(q, W // self.block_size)\n        y = p.matmul(x)\n        y = y.matmul(q)\n\n        y = self.conv2(y)\n        return y\n\n\nclass BatNonLocalAttn(nn.Module):\n    \"\"\" BAT\n    Adapted from: https://github.com/BA-Transform/BAT-Image-Classification\n    \"\"\"\n\n    def __init__(\n            self, in_channels, block_size=7, groups=2, rd_ratio=0.25, rd_channels=None, rd_divisor=8,\n            drop_rate=0.2, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d, **_):\n        super().__init__()\n        if rd_channels is None:\n            rd_channels = make_divisible(in_channels * rd_ratio, divisor=rd_divisor)\n        self.conv1 = ConvNormAct(in_channels, rd_channels, 1, act_layer=act_layer, norm_layer=norm_layer)\n        self.ba = BilinearAttnTransform(rd_channels, block_size, groups, act_layer=act_layer, norm_layer=norm_layer)\n        self.conv2 = ConvNormAct(rd_channels, in_channels, 1, act_layer=act_layer, norm_layer=norm_layer)\n        self.dropout = nn.Dropout2d(p=drop_rate)\n\n    def forward(self, x):\n        xl = self.conv1(x)\n        y = self.ba(xl)\n        y = self.conv2(y)\n        y = self.dropout(y)\n        return y + x\n",
  "\"\"\" Attention Pool 2D\n\nImplementations of 2D spatial feature pooling using multi-head attention instead of average pool.\n\nBased on idea in CLIP by OpenAI, licensed Apache 2.0\nhttps://github.com/openai/CLIP/blob/3b473b0e682c091a9e53623eebc1ca1657385717/clip/model.py\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nfrom typing import Union, Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom .helpers import to_2tuple\nfrom .pos_embed_sincos import apply_rot_embed, RotaryEmbedding\nfrom .weight_init import trunc_normal_\n\n\nclass RotAttentionPool2d(nn.Module):\n    \"\"\" Attention based 2D feature pooling w/ rotary (relative) pos embedding.\n    This is a multi-head attention based replacement for (spatial) average pooling in NN architectures.\n\n    Adapted from the AttentionPool2d in CLIP w/ rotary embedding instead of learned embed.\n    https://github.com/openai/CLIP/blob/3b473b0e682c091a9e53623eebc1ca1657385717/clip/model.py\n\n    NOTE: While this impl does not require a fixed feature size, performance at differeing resolutions from\n    train varies widely and falls off dramatically. I'm not sure if there is a way around this... -RW\n    \"\"\"\n    def __init__(\n            self,\n            in_features: int,\n            out_features: int = None,\n            embed_dim: int = None,\n            num_heads: int = 4,\n            qkv_bias: bool = True,\n    ):\n        super().__init__()\n        embed_dim = embed_dim or in_features\n        out_features = out_features or in_features\n        self.qkv = nn.Linear(in_features, embed_dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(embed_dim, out_features)\n        self.num_heads = num_heads\n        assert embed_dim % num_heads == 0\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.pos_embed = RotaryEmbedding(self.head_dim)\n\n        trunc_normal_(self.qkv.weight, std=in_features ** -0.5)\n        nn.init.zeros_(self.qkv.bias)\n\n    def forward(self, x):\n        B, _, H, W = x.shape\n        N = H * W\n        x = x.reshape(B, -1, N).permute(0, 2, 1)\n\n        x = torch.cat([x.mean(1, keepdim=True), x], dim=1)\n\n        x = self.qkv(x).reshape(B, N + 1, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = x[0], x[1], x[2]\n\n        qc, q = q[:, :, :1], q[:, :, 1:]\n        sin_emb, cos_emb = self.pos_embed.get_embed((H, W))\n        q = apply_rot_embed(q, sin_emb, cos_emb)\n        q = torch.cat([qc, q], dim=2)\n\n        kc, k = k[:, :, :1], k[:, :, 1:]\n        k = apply_rot_embed(k, sin_emb, cos_emb)\n        k = torch.cat([kc, k], dim=2)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N + 1, -1)\n        x = self.proj(x)\n        return x[:, 0]\n\n\nclass AttentionPool2d(nn.Module):\n    \"\"\" Attention based 2D feature pooling w/ learned (absolute) pos embedding.\n    This is a multi-head attention based replacement for (spatial) average pooling in NN architectures.\n\n    It was based on impl in CLIP by OpenAI\n    https://github.com/openai/CLIP/blob/3b473b0e682c091a9e53623eebc1ca1657385717/clip/model.py\n\n    NOTE: This requires feature size upon construction and well prevent adaptive sizing of the network.\n    \"\"\"\n    def __init__(\n            self,\n            in_features: int,\n            feat_size: Union[int, Tuple[int, int]],\n            out_features: int = None,\n            embed_dim: int = None,\n            num_heads: int = 4,\n            qkv_bias: bool = True,\n    ):\n        super().__init__()\n\n        embed_dim = embed_dim or in_features\n        out_features = out_features or in_features\n        assert embed_dim % num_heads == 0\n        self.feat_size = to_2tuple(feat_size)\n        self.qkv = nn.Linear(in_features, embed_dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(embed_dim, out_features)\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n\n        spatial_dim = self.feat_size[0] * self.feat_size[1]\n        self.pos_embed = nn.Parameter(torch.zeros(spatial_dim + 1, in_features))\n        trunc_normal_(self.pos_embed, std=in_features ** -0.5)\n        trunc_normal_(self.qkv.weight, std=in_features ** -0.5)\n        nn.init.zeros_(self.qkv.bias)\n\n    def forward(self, x):\n        B, _, H, W = x.shape\n        N = H * W\n        assert self.feat_size[0] == H\n        assert self.feat_size[1] == W\n        x = x.reshape(B, -1, N).permute(0, 2, 1)\n        x = torch.cat([x.mean(1, keepdim=True), x], dim=1)\n        x = x + self.pos_embed.unsqueeze(0).to(x.dtype)\n\n        x = self.qkv(x).reshape(B, N + 1, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = x[0], x[1], x[2]\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N + 1, -1)\n        x = self.proj(x)\n        return x[:, 0]\n",
  "\"\"\" Position Embedding Utilities\n\nHacked together by / Copyright 2022 Ross Wightman\n\"\"\"\nimport logging\nimport math\nfrom typing import List, Tuple, Optional, Union\n\nimport torch\nimport torch.nn.functional as F\n\nfrom .helpers import to_2tuple\n\n_logger = logging.getLogger(__name__)\n\n\ndef resample_abs_pos_embed(\n        posemb,\n        new_size: List[int],\n        old_size: Optional[List[int]] = None,\n        num_prefix_tokens: int = 1,\n        interpolation: str = 'bicubic',\n        antialias: bool = True,\n        verbose: bool = False,\n):\n    # sort out sizes, assume square if old size not provided\n    num_pos_tokens = posemb.shape[1]\n    num_new_tokens = new_size[0] * new_size[1] + num_prefix_tokens\n    if num_new_tokens == num_pos_tokens and new_size[0] == new_size[1]:\n        return posemb\n\n    if not old_size:\n        hw = int(math.sqrt(num_pos_tokens - num_prefix_tokens))\n        old_size = hw, hw\n\n    if num_prefix_tokens:\n        posemb_prefix, posemb = posemb[:, :num_prefix_tokens], posemb[:, num_prefix_tokens:]\n    else:\n        posemb_prefix, posemb = None, posemb\n\n    # do the interpolation\n    embed_dim = posemb.shape[-1]\n    posemb = posemb.reshape(1, old_size[0], old_size[1], -1).permute(0, 3, 1, 2)\n    posemb = F.interpolate(posemb, size=new_size, mode=interpolation, antialias=antialias)\n    posemb = posemb.permute(0, 2, 3, 1).reshape(1, -1, embed_dim)\n\n    # add back extra (class, etc) prefix tokens\n    if posemb_prefix is not None:\n        posemb = torch.cat([posemb_prefix, posemb], dim=1)\n\n    if not torch.jit.is_scripting() and verbose:\n        _logger.info(f'Resized position embedding: {old_size} to {new_size}.')\n\n    return posemb\n\n\ndef resample_abs_pos_embed_nhwc(\n        posemb,\n        new_size: List[int],\n        interpolation: str = 'bicubic',\n        antialias: bool = True,\n        verbose: bool = False,\n):\n    if new_size[0] == posemb.shape[-3] and new_size[1] == posemb.shape[-2]:\n        return posemb\n\n    # do the interpolation\n    posemb = posemb.reshape(1, posemb.shape[-3], posemb.shape[-2], posemb.shape[-1]).permute(0, 3, 1, 2)\n    posemb = F.interpolate(posemb, size=new_size, mode=interpolation, antialias=antialias)\n    posemb = posemb.permute(0, 2, 3, 1)\n\n    if not torch.jit.is_scripting() and verbose:\n        _logger.info(f'Resized position embedding: {posemb.shape[-3:-1]} to {new_size}.')\n\n    return posemb",
  "\"\"\"\nECA module from ECAnet\n\npaper: ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks\nhttps://arxiv.org/abs/1910.03151\n\nOriginal ECA model borrowed from https://github.com/BangguWu/ECANet\n\nModified circular ECA implementation and adaption for use in timm package\nby Chris Ha https://github.com/VRandme\n\nOriginal License:\n\nMIT License\n\nCopyright (c) 2019 BangguWu, Qilong Wang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\"\"\"\nimport math\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nfrom .create_act import create_act_layer\nfrom .helpers import make_divisible\n\n\nclass EcaModule(nn.Module):\n    \"\"\"Constructs an ECA module.\n\n    Args:\n        channels: Number of channels of the input feature map for use in adaptive kernel sizes\n            for actual calculations according to channel.\n            gamma, beta: when channel is given parameters of mapping function\n            refer to original paper https://arxiv.org/pdf/1910.03151.pdf\n            (default=None. if channel size not given, use k_size given for kernel size.)\n        kernel_size: Adaptive selection of kernel size (default=3)\n        gamm: used in kernel_size calc, see above\n        beta: used in kernel_size calc, see above\n        act_layer: optional non-linearity after conv, enables conv bias, this is an experiment\n        gate_layer: gating non-linearity to use\n    \"\"\"\n    def __init__(\n            self, channels=None, kernel_size=3, gamma=2, beta=1, act_layer=None, gate_layer='sigmoid',\n            rd_ratio=1/8, rd_channels=None, rd_divisor=8, use_mlp=False):\n        super(EcaModule, self).__init__()\n        if channels is not None:\n            t = int(abs(math.log(channels, 2) + beta) / gamma)\n            kernel_size = max(t if t % 2 else t + 1, 3)\n        assert kernel_size % 2 == 1\n        padding = (kernel_size - 1) // 2\n        if use_mlp:\n            # NOTE 'mlp' mode is a timm experiment, not in paper\n            assert channels is not None\n            if rd_channels is None:\n                rd_channels = make_divisible(channels * rd_ratio, divisor=rd_divisor)\n            act_layer = act_layer or nn.ReLU\n            self.conv = nn.Conv1d(1, rd_channels, kernel_size=1, padding=0, bias=True)\n            self.act = create_act_layer(act_layer)\n            self.conv2 = nn.Conv1d(rd_channels, 1, kernel_size=kernel_size, padding=padding, bias=True)\n        else:\n            self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=padding, bias=False)\n            self.act = None\n            self.conv2 = None\n        self.gate = create_act_layer(gate_layer)\n\n    def forward(self, x):\n        y = x.mean((2, 3)).view(x.shape[0], 1, -1)  # view for 1d conv\n        y = self.conv(y)\n        if self.conv2 is not None:\n            y = self.act(y)\n            y = self.conv2(y)\n        y = self.gate(y).view(x.shape[0], -1, 1, 1)\n        return x * y.expand_as(x)\n\n\nEfficientChannelAttn = EcaModule  # alias\n\n\nclass CecaModule(nn.Module):\n    \"\"\"Constructs a circular ECA module.\n\n    ECA module where the conv uses circular padding rather than zero padding.\n    Unlike the spatial dimension, the channels do not have inherent ordering nor\n    locality. Although this module in essence, applies such an assumption, it is unnecessary\n    to limit the channels on either \"edge\" from being circularly adapted to each other.\n    This will fundamentally increase connectivity and possibly increase performance metrics\n    (accuracy, robustness), without significantly impacting resource metrics\n    (parameter size, throughput,latency, etc)\n\n    Args:\n        channels: Number of channels of the input feature map for use in adaptive kernel sizes\n            for actual calculations according to channel.\n            gamma, beta: when channel is given parameters of mapping function\n            refer to original paper https://arxiv.org/pdf/1910.03151.pdf\n            (default=None. if channel size not given, use k_size given for kernel size.)\n        kernel_size: Adaptive selection of kernel size (default=3)\n        gamm: used in kernel_size calc, see above\n        beta: used in kernel_size calc, see above\n        act_layer: optional non-linearity after conv, enables conv bias, this is an experiment\n        gate_layer: gating non-linearity to use\n    \"\"\"\n\n    def __init__(self, channels=None, kernel_size=3, gamma=2, beta=1, act_layer=None, gate_layer='sigmoid'):\n        super(CecaModule, self).__init__()\n        if channels is not None:\n            t = int(abs(math.log(channels, 2) + beta) / gamma)\n            kernel_size = max(t if t % 2 else t + 1, 3)\n        has_act = act_layer is not None\n        assert kernel_size % 2 == 1\n\n        # PyTorch circular padding mode is buggy as of pytorch 1.4\n        # see https://github.com/pytorch/pytorch/pull/17240\n        # implement manual circular padding\n        self.padding = (kernel_size - 1) // 2\n        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=0, bias=has_act)\n        self.gate = create_act_layer(gate_layer)\n\n    def forward(self, x):\n        y = x.mean((2, 3)).view(x.shape[0], 1, -1)\n        # Manually implement circular padding, F.pad does not seemed to be bugged\n        y = F.pad(y, (self.padding, self.padding), mode='circular')\n        y = self.conv(y)\n        y = self.gate(y).view(x.shape[0], -1, 1, 1)\n        return x * y.expand_as(x)\n\n\nCircularEfficientChannelAttn = CecaModule\n",
  "\"\"\" Activations (memory-efficient w/ custom autograd)\n\nA collection of activations fn and modules with a common interface so that they can\neasily be swapped. All have an `inplace` arg even if not used.\n\nThese activations are not compatible with jit scripting or ONNX export of the model, please use either\nthe JIT or basic versions of the activations.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\n\n@torch.jit.script\ndef swish_jit_fwd(x):\n    return x.mul(torch.sigmoid(x))\n\n\n@torch.jit.script\ndef swish_jit_bwd(x, grad_output):\n    x_sigmoid = torch.sigmoid(x)\n    return grad_output * (x_sigmoid * (1 + x * (1 - x_sigmoid)))\n\n\nclass SwishJitAutoFn(torch.autograd.Function):\n    \"\"\" torch.jit.script optimised Swish w/ memory-efficient checkpoint\n    Inspired by conversation btw Jeremy Howard & Adam Pazske\n    https://twitter.com/jeremyphoward/status/1188251041835315200\n    \"\"\"\n    @staticmethod\n    def symbolic(g, x):\n        return g.op(\"Mul\", x, g.op(\"Sigmoid\", x))\n\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return swish_jit_fwd(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors[0]\n        return swish_jit_bwd(x, grad_output)\n\n\ndef swish_me(x, inplace=False):\n    return SwishJitAutoFn.apply(x)\n\n\nclass SwishMe(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(SwishMe, self).__init__()\n\n    def forward(self, x):\n        return SwishJitAutoFn.apply(x)\n\n\n@torch.jit.script\ndef mish_jit_fwd(x):\n    return x.mul(torch.tanh(F.softplus(x)))\n\n\n@torch.jit.script\ndef mish_jit_bwd(x, grad_output):\n    x_sigmoid = torch.sigmoid(x)\n    x_tanh_sp = F.softplus(x).tanh()\n    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\n\n\nclass MishJitAutoFn(torch.autograd.Function):\n    \"\"\" Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n    A memory efficient, jit scripted variant of Mish\n    \"\"\"\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return mish_jit_fwd(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors[0]\n        return mish_jit_bwd(x, grad_output)\n\n\ndef mish_me(x, inplace=False):\n    return MishJitAutoFn.apply(x)\n\n\nclass MishMe(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(MishMe, self).__init__()\n\n    def forward(self, x):\n        return MishJitAutoFn.apply(x)\n\n\n@torch.jit.script\ndef hard_sigmoid_jit_fwd(x, inplace: bool = False):\n    return (x + 3).clamp(min=0, max=6).div(6.)\n\n\n@torch.jit.script\ndef hard_sigmoid_jit_bwd(x, grad_output):\n    m = torch.ones_like(x) * ((x >= -3.) & (x <= 3.)) / 6.\n    return grad_output * m\n\n\nclass HardSigmoidJitAutoFn(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return hard_sigmoid_jit_fwd(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors[0]\n        return hard_sigmoid_jit_bwd(x, grad_output)\n\n\ndef hard_sigmoid_me(x, inplace: bool = False):\n    return HardSigmoidJitAutoFn.apply(x)\n\n\nclass HardSigmoidMe(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(HardSigmoidMe, self).__init__()\n\n    def forward(self, x):\n        return HardSigmoidJitAutoFn.apply(x)\n\n\n@torch.jit.script\ndef hard_swish_jit_fwd(x):\n    return x * (x + 3).clamp(min=0, max=6).div(6.)\n\n\n@torch.jit.script\ndef hard_swish_jit_bwd(x, grad_output):\n    m = torch.ones_like(x) * (x >= 3.)\n    m = torch.where((x >= -3.) & (x <= 3.),  x / 3. + .5, m)\n    return grad_output * m\n\n\nclass HardSwishJitAutoFn(torch.autograd.Function):\n    \"\"\"A memory efficient, jit-scripted HardSwish activation\"\"\"\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return hard_swish_jit_fwd(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors[0]\n        return hard_swish_jit_bwd(x, grad_output)\n\n    @staticmethod\n    def symbolic(g, self):\n        input = g.op(\"Add\", self, g.op('Constant', value_t=torch.tensor(3, dtype=torch.float)))\n        hardtanh_ = g.op(\"Clip\", input, g.op('Constant', value_t=torch.tensor(0, dtype=torch.float)), g.op('Constant', value_t=torch.tensor(6, dtype=torch.float)))\n        hardtanh_ = g.op(\"Div\", hardtanh_, g.op('Constant', value_t=torch.tensor(6, dtype=torch.float)))\n        return g.op(\"Mul\", self, hardtanh_)\n\n\ndef hard_swish_me(x, inplace=False):\n    return HardSwishJitAutoFn.apply(x)\n\n\nclass HardSwishMe(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(HardSwishMe, self).__init__()\n\n    def forward(self, x):\n        return HardSwishJitAutoFn.apply(x)\n\n\n@torch.jit.script\ndef hard_mish_jit_fwd(x):\n    return 0.5 * x * (x + 2).clamp(min=0, max=2)\n\n\n@torch.jit.script\ndef hard_mish_jit_bwd(x, grad_output):\n    m = torch.ones_like(x) * (x >= -2.)\n    m = torch.where((x >= -2.) & (x <= 0.), x + 1., m)\n    return grad_output * m\n\n\nclass HardMishJitAutoFn(torch.autograd.Function):\n    \"\"\" A memory efficient, jit scripted variant of Hard Mish\n    Experimental, based on notes by Mish author Diganta Misra at\n      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n    \"\"\"\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return hard_mish_jit_fwd(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors[0]\n        return hard_mish_jit_bwd(x, grad_output)\n\n\ndef hard_mish_me(x, inplace: bool = False):\n    return HardMishJitAutoFn.apply(x)\n\n\nclass HardMishMe(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(HardMishMe, self).__init__()\n\n    def forward(self, x):\n        return HardMishJitAutoFn.apply(x)\n\n\n\n",
  "\"\"\" Normalization layers and wrappers\n\nNorm layer definitions that support fast norm and consistent channel arg order (always first arg).\n\nHacked together by / Copyright 2022 Ross Wightman\n\"\"\"\nimport numbers\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .fast_norm import is_fast_norm, fast_group_norm, fast_layer_norm, fast_rms_norm\n\n\nclass GroupNorm(nn.GroupNorm):\n    def __init__(self, num_channels, num_groups=32, eps=1e-5, affine=True):\n        # NOTE num_channels is swapped to first arg for consistency in swapping norm layers with BN\n        super().__init__(num_groups, num_channels, eps=eps, affine=affine)\n        self.fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)\n\n    def forward(self, x):\n        if self.fast_norm:\n            return fast_group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n        else:\n            return F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n\n\nclass GroupNorm1(nn.GroupNorm):\n    \"\"\" Group Normalization with 1 group.\n    Input: tensor in shape [B, C, *]\n    \"\"\"\n\n    def __init__(self, num_channels, **kwargs):\n        super().__init__(1, num_channels, **kwargs)\n        self.fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.fast_norm:\n            return fast_group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n        else:\n            return F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\" LayerNorm w/ fast norm option\n    \"\"\"\n    def __init__(self, num_channels, eps=1e-6, affine=True):\n        super().__init__(num_channels, eps=eps, elementwise_affine=affine)\n        self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self._fast_norm:\n            x = fast_layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        else:\n            x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        return x\n\n\nclass LayerNorm2d(nn.LayerNorm):\n    \"\"\" LayerNorm for channels of '2D' spatial NCHW tensors \"\"\"\n    def __init__(self, num_channels, eps=1e-6, affine=True):\n        super().__init__(num_channels, eps=eps, elementwise_affine=affine)\n        self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.permute(0, 2, 3, 1)\n        if self._fast_norm:\n            x = fast_layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        else:\n            x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        x = x.permute(0, 3, 1, 2)\n        return x\n\n\ndef _is_contiguous(tensor: torch.Tensor) -> bool:\n    # jit is oh so lovely :/\n    if torch.jit.is_scripting():\n        return tensor.is_contiguous()\n    else:\n        return tensor.is_contiguous(memory_format=torch.contiguous_format)\n\n\n@torch.jit.script\ndef _layer_norm_cf(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float):\n    s, u = torch.var_mean(x, dim=1, unbiased=False, keepdim=True)\n    x = (x - u) * torch.rsqrt(s + eps)\n    x = x * weight[:, None, None] + bias[:, None, None]\n    return x\n\n\ndef _layer_norm_cf_sqm(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float):\n    u = x.mean(dim=1, keepdim=True)\n    s = ((x * x).mean(dim=1, keepdim=True) - (u * u)).clamp(0)\n    x = (x - u) * torch.rsqrt(s + eps)\n    x = x * weight.view(1, -1, 1, 1) + bias.view(1, -1, 1, 1)\n    return x\n\n\nclass LayerNormExp2d(nn.LayerNorm):\n    \"\"\" LayerNorm for channels_first tensors with 2d spatial dimensions (ie N, C, H, W).\n\n    Experimental implementation w/ manual norm for tensors non-contiguous tensors.\n\n    This improves throughput in some scenarios (tested on Ampere GPU), esp w/ channels_last\n    layout. However, benefits are not always clear and can perform worse on other GPUs.\n    \"\"\"\n\n    def __init__(self, num_channels, eps=1e-6):\n        super().__init__(num_channels, eps=eps)\n\n    def forward(self, x) -> torch.Tensor:\n        if _is_contiguous(x):\n            x = F.layer_norm(\n                x.permute(0, 2, 3, 1), self.normalized_shape, self.weight, self.bias, self.eps).permute(0, 3, 1, 2)\n        else:\n            x = _layer_norm_cf(x, self.weight, self.bias, self.eps)\n        return x\n\n\nclass RmsNorm(nn.Module):\n    \"\"\" RmsNorm w/ fast (apex) norm if available\n    \"\"\"\n    __constants__ = ['normalized_shape', 'eps', 'elementwise_affine']\n    normalized_shape: Tuple[int, ...]\n    eps: float\n    elementwise_affine: bool\n\n    def __init__(self, channels, eps=1e-6, affine=True, device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        normalized_shape = channels\n        if isinstance(normalized_shape, numbers.Integral):\n            # mypy error: incompatible types in assignment\n            normalized_shape = (normalized_shape,)  # type: ignore[assignment]\n        self.normalized_shape = tuple(normalized_shape)  # type: ignore[arg-type]\n        self.eps = eps\n        self.elementwise_affine = affine\n        if self.elementwise_affine:\n            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **factory_kwargs))\n        else:\n            self.register_parameter('weight', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        if self.elementwise_affine:\n            nn.init.ones_(self.weight)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # NOTE fast norm fallback needs our rms norm impl, so both paths through here.\n        # Since there is no built-in PyTorch impl, always use APEX RmsNorm if is installed.\n        x = fast_rms_norm(x, self.normalized_shape, self.weight, self.eps)\n        return x\n",
  "try:\n    from torch import _assert\nexcept ImportError:\n    def _assert(condition: bool, message: str):\n        assert condition, message\n\n\ndef _float_to_int(x: float) -> int:\n    \"\"\"\n    Symbolic tracing helper to substitute for inbuilt `int`.\n    Hint: Inbuilt `int` can't accept an argument of type `Proxy`\n    \"\"\"\n    return int(x)\n",
  "\"\"\" 'Fast' Normalization Functions\n\nFor GroupNorm and LayerNorm these functions bypass typical AMP upcast to float32.\n\nAdditionally, for LayerNorm, the APEX fused LN is used if available (which also does not upcast)\n\nHacked together by / Copyright 2022 Ross Wightman\n\"\"\"\nfrom typing import List, Optional\n\nimport torch\nfrom torch.nn import functional as F\n\ntry:\n    from apex.normalization.fused_layer_norm import fused_layer_norm_affine\n    has_apex = True\nexcept ImportError:\n    has_apex = False\n\ntry:\n    from apex.normalization.fused_layer_norm import fused_rms_norm_affine, fused_rms_norm\n    has_apex_rmsnorm = True\nexcept ImportError:\n    has_apex_rmsnorm = False\n\n\n# fast (ie lower precision LN) can be disabled with this flag if issues crop up\n_USE_FAST_NORM = False  # defaulting to False for now\n\n\ndef is_fast_norm():\n    return _USE_FAST_NORM\n\n\ndef set_fast_norm(enable=True):\n    global _USE_FAST_NORM\n    _USE_FAST_NORM = enable\n\n\ndef fast_group_norm(\n    x: torch.Tensor,\n    num_groups: int,\n    weight: Optional[torch.Tensor] = None,\n    bias: Optional[torch.Tensor] = None,\n    eps: float = 1e-5\n) -> torch.Tensor:\n    if torch.jit.is_scripting():\n        # currently cannot use is_autocast_enabled within torchscript\n        return F.group_norm(x, num_groups, weight, bias, eps)\n\n    if torch.is_autocast_enabled():\n        # normally native AMP casts GN inputs to float32\n        # here we use the low precision autocast dtype\n        # FIXME what to do re CPU autocast?\n        dt = torch.get_autocast_gpu_dtype()\n        x, weight, bias = x.to(dt), weight.to(dt), bias.to(dt) if bias is not None else None\n\n    with torch.cuda.amp.autocast(enabled=False):\n        return F.group_norm(x, num_groups, weight, bias, eps)\n\n\ndef fast_layer_norm(\n    x: torch.Tensor,\n    normalized_shape: List[int],\n    weight: Optional[torch.Tensor] = None,\n    bias: Optional[torch.Tensor] = None,\n    eps: float = 1e-5\n) -> torch.Tensor:\n    if torch.jit.is_scripting():\n        # currently cannot use is_autocast_enabled within torchscript\n        return F.layer_norm(x, normalized_shape, weight, bias, eps)\n\n    if has_apex:\n        return fused_layer_norm_affine(x, weight, bias, normalized_shape, eps)\n\n    if torch.is_autocast_enabled():\n        # normally native AMP casts LN inputs to float32\n        # apex LN does not, this is behaving like Apex\n        dt = torch.get_autocast_gpu_dtype()\n        # FIXME what to do re CPU autocast?\n        x, weight, bias = x.to(dt), weight.to(dt), bias.to(dt) if bias is not None else None\n\n    with torch.cuda.amp.autocast(enabled=False):\n        return F.layer_norm(x, normalized_shape, weight, bias, eps)\n\n\ndef rms_norm(\n    x: torch.Tensor,\n    normalized_shape: List[int],\n    weight: Optional[torch.Tensor] = None,\n    eps: float = 1e-5,\n):\n    norm_ndim = len(normalized_shape)\n    if torch.jit.is_scripting():\n        # ndim = len(x.shape)\n        # dims = list(range(ndim - norm_ndim, ndim))  # this doesn't work on pytorch <= 1.13.x\n        # NOTE -ve dims cause torchscript to crash in some cases, out of options to work around\n        assert norm_ndim == 1\n        v = torch.var(x, dim=-1).unsqueeze(-1)  # ts crashes with -ve dim + keepdim=True\n    else:\n        dims = tuple(range(-1, -norm_ndim - 1, -1))\n        v = torch.var(x, dim=dims, keepdim=True)\n    x = x * torch.rsqrt(v + eps)\n    if weight is not None:\n        x = x * weight\n    return x\n\n\ndef fast_rms_norm(\n    x: torch.Tensor,\n    normalized_shape: List[int],\n    weight: Optional[torch.Tensor] = None,\n    eps: float = 1e-5,\n) -> torch.Tensor:\n    if torch.jit.is_scripting():\n        # this must be by itself, cannot merge with has_apex_rmsnorm\n        return rms_norm(x, normalized_shape, weight, eps)\n\n    if has_apex_rmsnorm:\n        if weight is None:\n            return fused_rms_norm(x, normalized_shape, eps)\n        else:\n            return fused_rms_norm_affine(x, weight, normalized_shape, eps)\n\n    # fallback\n    return rms_norm(x, normalized_shape, weight, eps)\n",
  "\"\"\" Classifier head and layer factory\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom collections import OrderedDict\nfrom functools import partial\nfrom typing import Optional, Union, Callable\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nfrom .adaptive_avgmax_pool import SelectAdaptivePool2d\nfrom .create_act import get_act_layer\nfrom .create_norm import get_norm_layer\n\n\ndef _create_pool(\n        num_features: int,\n        num_classes: int,\n        pool_type: str = 'avg',\n        use_conv: bool = False,\n        input_fmt: Optional[str] = None,\n):\n    flatten_in_pool = not use_conv  # flatten when we use a Linear layer after pooling\n    if not pool_type:\n        assert num_classes == 0 or use_conv,\\\n            'Pooling can only be disabled if classifier is also removed or conv classifier is used'\n        flatten_in_pool = False  # disable flattening if pooling is pass-through (no pooling)\n    global_pool = SelectAdaptivePool2d(\n        pool_type=pool_type,\n        flatten=flatten_in_pool,\n        input_fmt=input_fmt,\n    )\n    num_pooled_features = num_features * global_pool.feat_mult()\n    return global_pool, num_pooled_features\n\n\ndef _create_fc(num_features, num_classes, use_conv=False):\n    if num_classes <= 0:\n        fc = nn.Identity()  # pass-through (no classifier)\n    elif use_conv:\n        fc = nn.Conv2d(num_features, num_classes, 1, bias=True)\n    else:\n        fc = nn.Linear(num_features, num_classes, bias=True)\n    return fc\n\n\ndef create_classifier(\n        num_features: int,\n        num_classes: int,\n        pool_type: str = 'avg',\n        use_conv: bool = False,\n        input_fmt: str = 'NCHW',\n        drop_rate: Optional[float] = None,\n):\n    global_pool, num_pooled_features = _create_pool(\n        num_features,\n        num_classes,\n        pool_type,\n        use_conv=use_conv,\n        input_fmt=input_fmt,\n    )\n    fc = _create_fc(\n        num_pooled_features,\n        num_classes,\n        use_conv=use_conv,\n    )\n    if drop_rate is not None:\n        dropout = nn.Dropout(drop_rate)\n        return global_pool, dropout, fc\n    return global_pool, fc\n\n\nclass ClassifierHead(nn.Module):\n    \"\"\"Classifier head w/ configurable global pooling and dropout.\"\"\"\n\n    def __init__(\n            self,\n            in_features: int,\n            num_classes: int,\n            pool_type: str = 'avg',\n            drop_rate: float = 0.,\n            use_conv: bool = False,\n            input_fmt: str = 'NCHW',\n    ):\n        \"\"\"\n        Args:\n            in_features: The number of input features.\n            num_classes:  The number of classes for the final classifier layer (output).\n            pool_type: Global pooling type, pooling disabled if empty string ('').\n            drop_rate: Pre-classifier dropout rate.\n        \"\"\"\n        super(ClassifierHead, self).__init__()\n        self.in_features = in_features\n        self.use_conv = use_conv\n        self.input_fmt = input_fmt\n\n        global_pool, fc = create_classifier(\n            in_features,\n            num_classes,\n            pool_type,\n            use_conv=use_conv,\n            input_fmt=input_fmt,\n        )\n        self.global_pool = global_pool\n        self.drop = nn.Dropout(drop_rate)\n        self.fc = fc\n        self.flatten = nn.Flatten(1) if use_conv and pool_type else nn.Identity()\n\n    def reset(self, num_classes, pool_type=None):\n        if pool_type is not None and pool_type != self.global_pool.pool_type:\n            self.global_pool, self.fc = create_classifier(\n                self.in_features,\n                num_classes,\n                pool_type=pool_type,\n                use_conv=self.use_conv,\n                input_fmt=self.input_fmt,\n            )\n            self.flatten = nn.Flatten(1) if self.use_conv and pool_type else nn.Identity()\n        else:\n            num_pooled_features = self.in_features * self.global_pool.feat_mult()\n            self.fc = _create_fc(\n                num_pooled_features,\n                num_classes,\n                use_conv=self.use_conv,\n            )\n\n    def forward(self, x, pre_logits: bool = False):\n        x = self.global_pool(x)\n        x = self.drop(x)\n        if pre_logits:\n            return self.flatten(x)\n        x = self.fc(x)\n        return self.flatten(x)\n\n\nclass NormMlpClassifierHead(nn.Module):\n\n    def __init__(\n            self,\n            in_features: int,\n            num_classes: int,\n            hidden_size: Optional[int] = None,\n            pool_type: str = 'avg',\n            drop_rate: float = 0.,\n            norm_layer: Union[str, Callable] = 'layernorm2d',\n            act_layer: Union[str, Callable] = 'tanh',\n    ):\n        \"\"\"\n        Args:\n            in_features: The number of input features.\n            num_classes:  The number of classes for the final classifier layer (output).\n            hidden_size: The hidden size of the MLP (pre-logits FC layer) if not None.\n            pool_type: Global pooling type, pooling disabled if empty string ('').\n            drop_rate: Pre-classifier dropout rate.\n            norm_layer: Normalization layer type.\n            act_layer: MLP activation layer type (only used if hidden_size is not None).\n        \"\"\"\n        super().__init__()\n        self.in_features = in_features\n        self.hidden_size = hidden_size\n        self.num_features = in_features\n        self.use_conv = not pool_type\n        norm_layer = get_norm_layer(norm_layer)\n        act_layer = get_act_layer(act_layer)\n        linear_layer = partial(nn.Conv2d, kernel_size=1) if self.use_conv else nn.Linear\n\n        self.global_pool = SelectAdaptivePool2d(pool_type=pool_type)\n        self.norm = norm_layer(in_features)\n        self.flatten = nn.Flatten(1) if pool_type else nn.Identity()\n        if hidden_size:\n            self.pre_logits = nn.Sequential(OrderedDict([\n                ('fc', linear_layer(in_features, hidden_size)),\n                ('act', act_layer()),\n            ]))\n            self.num_features = hidden_size\n        else:\n            self.pre_logits = nn.Identity()\n        self.drop = nn.Dropout(drop_rate)\n        self.fc = linear_layer(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    def reset(self, num_classes, global_pool=None):\n        if global_pool is not None:\n            self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n            self.flatten = nn.Flatten(1) if global_pool else nn.Identity()\n        self.use_conv = self.global_pool.is_identity()\n        linear_layer = partial(nn.Conv2d, kernel_size=1) if self.use_conv else nn.Linear\n        if self.hidden_size:\n            if ((isinstance(self.pre_logits.fc, nn.Conv2d) and not self.use_conv) or\n                    (isinstance(self.pre_logits.fc, nn.Linear) and self.use_conv)):\n                with torch.no_grad():\n                    new_fc = linear_layer(self.in_features, self.hidden_size)\n                    new_fc.weight.copy_(self.pre_logits.fc.weight.reshape(new_fc.weight.shape))\n                    new_fc.bias.copy_(self.pre_logits.fc.bias)\n                    self.pre_logits.fc = new_fc\n        self.fc = linear_layer(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward(self, x, pre_logits: bool = False):\n        x = self.global_pool(x)\n        x = self.norm(x)\n        x = self.flatten(x)\n        x = self.pre_logits(x)\n        x = self.drop(x)\n        if pre_logits:\n            return x\n        x = self.fc(x)\n        return x\n",
  "\"\"\" AvgPool2d w/ Same Padding\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import List, Tuple, Optional\n\nfrom .helpers import to_2tuple\nfrom .padding import pad_same, get_padding_value\n\n\ndef avg_pool2d_same(x, kernel_size: List[int], stride: List[int], padding: List[int] = (0, 0),\n                    ceil_mode: bool = False, count_include_pad: bool = True):\n    # FIXME how to deal with count_include_pad vs not for external padding?\n    x = pad_same(x, kernel_size, stride)\n    return F.avg_pool2d(x, kernel_size, stride, (0, 0), ceil_mode, count_include_pad)\n\n\nclass AvgPool2dSame(nn.AvgPool2d):\n    \"\"\" Tensorflow like 'SAME' wrapper for 2D average pooling\n    \"\"\"\n    def __init__(self, kernel_size: int, stride=None, padding=0, ceil_mode=False, count_include_pad=True):\n        kernel_size = to_2tuple(kernel_size)\n        stride = to_2tuple(stride)\n        super(AvgPool2dSame, self).__init__(kernel_size, stride, (0, 0), ceil_mode, count_include_pad)\n\n    def forward(self, x):\n        x = pad_same(x, self.kernel_size, self.stride)\n        return F.avg_pool2d(\n            x, self.kernel_size, self.stride, self.padding, self.ceil_mode, self.count_include_pad)\n\n\ndef max_pool2d_same(\n        x, kernel_size: List[int], stride: List[int], padding: List[int] = (0, 0),\n        dilation: List[int] = (1, 1), ceil_mode: bool = False):\n    x = pad_same(x, kernel_size, stride, value=-float('inf'))\n    return F.max_pool2d(x, kernel_size, stride, (0, 0), dilation, ceil_mode)\n\n\nclass MaxPool2dSame(nn.MaxPool2d):\n    \"\"\" Tensorflow like 'SAME' wrapper for 2D max pooling\n    \"\"\"\n    def __init__(self, kernel_size: int, stride=None, padding=0, dilation=1, ceil_mode=False):\n        kernel_size = to_2tuple(kernel_size)\n        stride = to_2tuple(stride)\n        dilation = to_2tuple(dilation)\n        super(MaxPool2dSame, self).__init__(kernel_size, stride, (0, 0), dilation, ceil_mode)\n\n    def forward(self, x):\n        x = pad_same(x, self.kernel_size, self.stride, value=-float('inf'))\n        return F.max_pool2d(x, self.kernel_size, self.stride, (0, 0), self.dilation, self.ceil_mode)\n\n\ndef create_pool2d(pool_type, kernel_size, stride=None, **kwargs):\n    stride = stride or kernel_size\n    padding = kwargs.pop('padding', '')\n    padding, is_dynamic = get_padding_value(padding, kernel_size, stride=stride, **kwargs)\n    if is_dynamic:\n        if pool_type == 'avg':\n            return AvgPool2dSame(kernel_size, stride=stride, **kwargs)\n        elif pool_type == 'max':\n            return MaxPool2dSame(kernel_size, stride=stride, **kwargs)\n        else:\n            assert False, f'Unsupported pool type {pool_type}'\n    else:\n        if pool_type == 'avg':\n            return nn.AvgPool2d(kernel_size, stride=stride, padding=padding, **kwargs)\n        elif pool_type == 'max':\n            return nn.MaxPool2d(kernel_size, stride=stride, padding=padding, **kwargs)\n        else:\n            assert False, f'Unsupported pool type {pool_type}'\n",
  "\"\"\" Convolution with Weight Standardization (StdConv and ScaledStdConv)\n\nStdConv:\n@article{weightstandardization,\n  author    = {Siyuan Qiao and Huiyu Wang and Chenxi Liu and Wei Shen and Alan Yuille},\n  title     = {Weight Standardization},\n  journal   = {arXiv preprint arXiv:1903.10520},\n  year      = {2019},\n}\nCode: https://github.com/joe-siyuan-qiao/WeightStandardization\n\nScaledStdConv:\nPaper: `Characterizing signal propagation to close the performance gap in unnormalized ResNets`\n    - https://arxiv.org/abs/2101.08692\nOfficial Deepmind JAX code: https://github.com/deepmind/deepmind-research/tree/master/nfnets\n\nHacked together by / copyright Ross Wightman, 2021.\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .padding import get_padding, get_padding_value, pad_same\n\n\nclass StdConv2d(nn.Conv2d):\n    \"\"\"Conv2d with Weight Standardization. Used for BiT ResNet-V2 models.\n\n    Paper: `Micro-Batch Training with Batch-Channel Normalization and Weight Standardization` -\n        https://arxiv.org/abs/1903.10520v2\n    \"\"\"\n    def __init__(\n            self, in_channel, out_channels, kernel_size, stride=1, padding=None,\n            dilation=1, groups=1, bias=False, eps=1e-6):\n        if padding is None:\n            padding = get_padding(kernel_size, stride, dilation)\n        super().__init__(\n            in_channel, out_channels, kernel_size, stride=stride,\n            padding=padding, dilation=dilation, groups=groups, bias=bias)\n        self.eps = eps\n\n    def forward(self, x):\n        weight = F.batch_norm(\n            self.weight.reshape(1, self.out_channels, -1), None, None,\n            training=True, momentum=0., eps=self.eps).reshape_as(self.weight)\n        x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n        return x\n\n\nclass StdConv2dSame(nn.Conv2d):\n    \"\"\"Conv2d with Weight Standardization. TF compatible SAME padding. Used for ViT Hybrid model.\n\n    Paper: `Micro-Batch Training with Batch-Channel Normalization and Weight Standardization` -\n        https://arxiv.org/abs/1903.10520v2\n    \"\"\"\n    def __init__(\n            self, in_channel, out_channels, kernel_size, stride=1, padding='SAME',\n            dilation=1, groups=1, bias=False, eps=1e-6):\n        padding, is_dynamic = get_padding_value(padding, kernel_size, stride=stride, dilation=dilation)\n        super().__init__(\n            in_channel, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation,\n            groups=groups, bias=bias)\n        self.same_pad = is_dynamic\n        self.eps = eps\n\n    def forward(self, x):\n        if self.same_pad:\n            x = pad_same(x, self.kernel_size, self.stride, self.dilation)\n        weight = F.batch_norm(\n            self.weight.reshape(1, self.out_channels, -1), None, None,\n            training=True, momentum=0., eps=self.eps).reshape_as(self.weight)\n        x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n        return x\n\n\nclass ScaledStdConv2d(nn.Conv2d):\n    \"\"\"Conv2d layer with Scaled Weight Standardization.\n\n    Paper: `Characterizing signal propagation to close the performance gap in unnormalized ResNets` -\n        https://arxiv.org/abs/2101.08692\n\n    NOTE: the operations used in this impl differ slightly from the DeepMind Haiku impl. The impact is minor.\n    \"\"\"\n\n    def __init__(\n            self, in_channels, out_channels, kernel_size, stride=1, padding=None,\n            dilation=1, groups=1, bias=True, gamma=1.0, eps=1e-6, gain_init=1.0):\n        if padding is None:\n            padding = get_padding(kernel_size, stride, dilation)\n        super().__init__(\n            in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation,\n            groups=groups, bias=bias)\n        self.gain = nn.Parameter(torch.full((self.out_channels, 1, 1, 1), gain_init))\n        self.scale = gamma * self.weight[0].numel() ** -0.5  # gamma * 1 / sqrt(fan-in)\n        self.eps = eps\n\n    def forward(self, x):\n        weight = F.batch_norm(\n            self.weight.reshape(1, self.out_channels, -1), None, None,\n            weight=(self.gain * self.scale).view(-1),\n            training=True, momentum=0., eps=self.eps).reshape_as(self.weight)\n        return F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\n\nclass ScaledStdConv2dSame(nn.Conv2d):\n    \"\"\"Conv2d layer with Scaled Weight Standardization and Tensorflow-like SAME padding support\n\n    Paper: `Characterizing signal propagation to close the performance gap in unnormalized ResNets` -\n        https://arxiv.org/abs/2101.08692\n\n    NOTE: the operations used in this impl differ slightly from the DeepMind Haiku impl. The impact is minor.\n    \"\"\"\n\n    def __init__(\n            self, in_channels, out_channels, kernel_size, stride=1, padding='SAME',\n            dilation=1, groups=1, bias=True, gamma=1.0, eps=1e-6, gain_init=1.0):\n        padding, is_dynamic = get_padding_value(padding, kernel_size, stride=stride, dilation=dilation)\n        super().__init__(\n            in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation,\n            groups=groups, bias=bias)\n        self.gain = nn.Parameter(torch.full((self.out_channels, 1, 1, 1), gain_init))\n        self.scale = gamma * self.weight[0].numel() ** -0.5\n        self.same_pad = is_dynamic\n        self.eps = eps\n\n    def forward(self, x):\n        if self.same_pad:\n            x = pad_same(x, self.kernel_size, self.stride, self.dilation)\n        weight = F.batch_norm(\n            self.weight.reshape(1, self.out_channels, -1), None, None,\n            weight=(self.gain * self.scale).view(-1),\n            training=True, momentum=0., eps=self.eps).reshape_as(self.weight)\n        return F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
  "\"\"\" Activations\n\nA collection of activations fn and modules with a common interface so that they can\neasily be swapped. All have an `inplace` arg even if not used.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\n\ndef swish(x, inplace: bool = False):\n    \"\"\"Swish - Described in: https://arxiv.org/abs/1710.05941\n    \"\"\"\n    return x.mul_(x.sigmoid()) if inplace else x.mul(x.sigmoid())\n\n\nclass Swish(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(Swish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return swish(x, self.inplace)\n\n\ndef mish(x, inplace: bool = False):\n    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n    NOTE: I don't have a working inplace variant\n    \"\"\"\n    return x.mul(F.softplus(x).tanh())\n\n\nclass Mish(nn.Module):\n    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n    \"\"\"\n    def __init__(self, inplace: bool = False):\n        super(Mish, self).__init__()\n\n    def forward(self, x):\n        return mish(x)\n\n\ndef sigmoid(x, inplace: bool = False):\n    return x.sigmoid_() if inplace else x.sigmoid()\n\n\n# PyTorch has this, but not with a consistent inplace argmument interface\nclass Sigmoid(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(Sigmoid, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return x.sigmoid_() if self.inplace else x.sigmoid()\n\n\ndef tanh(x, inplace: bool = False):\n    return x.tanh_() if inplace else x.tanh()\n\n\n# PyTorch has this, but not with a consistent inplace argmument interface\nclass Tanh(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(Tanh, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return x.tanh_() if self.inplace else x.tanh()\n\n\ndef hard_swish(x, inplace: bool = False):\n    inner = F.relu6(x + 3.).div_(6.)\n    return x.mul_(inner) if inplace else x.mul(inner)\n\n\nclass HardSwish(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(HardSwish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return hard_swish(x, self.inplace)\n\n\ndef hard_sigmoid(x, inplace: bool = False):\n    if inplace:\n        return x.add_(3.).clamp_(0., 6.).div_(6.)\n    else:\n        return F.relu6(x + 3.) / 6.\n\n\nclass HardSigmoid(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(HardSigmoid, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return hard_sigmoid(x, self.inplace)\n\n\ndef hard_mish(x, inplace: bool = False):\n    \"\"\" Hard Mish\n    Experimental, based on notes by Mish author Diganta Misra at\n      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n    \"\"\"\n    if inplace:\n        return x.mul_(0.5 * (x + 2).clamp(min=0, max=2))\n    else:\n        return 0.5 * x * (x + 2).clamp(min=0, max=2)\n\n\nclass HardMish(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(HardMish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return hard_mish(x, self.inplace)\n\n\nclass PReLU(nn.PReLU):\n    \"\"\"Applies PReLU (w/ dummy inplace arg)\n    \"\"\"\n    def __init__(self, num_parameters: int = 1, init: float = 0.25, inplace: bool = False) -> None:\n        super(PReLU, self).__init__(num_parameters=num_parameters, init=init)\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        return F.prelu(input, self.weight)\n\n\ndef gelu(x: torch.Tensor, inplace: bool = False) -> torch.Tensor:\n    return F.gelu(x)\n\n\nclass GELU(nn.Module):\n    \"\"\"Applies the Gaussian Error Linear Units function (w/ dummy inplace arg)\n    \"\"\"\n    def __init__(self, inplace: bool = False):\n        super(GELU, self).__init__()\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        return F.gelu(input)\n\n\ndef gelu_tanh(x: torch.Tensor, inplace: bool = False) -> torch.Tensor:\n    return F.gelu(x, approximate='tanh')\n\n\nclass GELUTanh(nn.Module):\n    \"\"\"Applies the Gaussian Error Linear Units function (w/ dummy inplace arg)\n    \"\"\"\n    def __init__(self, inplace: bool = False):\n        super(GELUTanh, self).__init__()\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        return F.gelu(input, approximate='tanh')\n",
  "\"\"\" Conv2d + BN + Act\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport functools\nfrom torch import nn as nn\n\nfrom .create_conv2d import create_conv2d\nfrom .create_norm_act import get_norm_act_layer\n\n\nclass ConvNormAct(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size=1,\n            stride=1,\n            padding='',\n            dilation=1,\n            groups=1,\n            bias=False,\n            apply_act=True,\n            norm_layer=nn.BatchNorm2d,\n            norm_kwargs=None,\n            act_layer=nn.ReLU,\n            act_kwargs=None,\n            drop_layer=None,\n    ):\n        super(ConvNormAct, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n        act_kwargs = act_kwargs or {}\n\n        self.conv = create_conv2d(\n            in_channels, out_channels, kernel_size, stride=stride,\n            padding=padding, dilation=dilation, groups=groups, bias=bias)\n\n        # NOTE for backwards compatibility with models that use separate norm and act layer definitions\n        norm_act_layer = get_norm_act_layer(norm_layer, act_layer)\n        # NOTE for backwards (weight) compatibility, norm layer name remains `.bn`\n        if drop_layer:\n            norm_kwargs['drop_layer'] = drop_layer\n        self.bn = norm_act_layer(\n            out_channels,\n            apply_act=apply_act,\n            act_kwargs=act_kwargs,\n            **norm_kwargs,\n        )\n\n    @property\n    def in_channels(self):\n        return self.conv.in_channels\n\n    @property\n    def out_channels(self):\n        return self.conv.out_channels\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nConvBnAct = ConvNormAct\n\n\ndef create_aa(aa_layer, channels, stride=2, enable=True):\n    if not aa_layer or not enable:\n        return nn.Identity()\n    if isinstance(aa_layer, functools.partial):\n        if issubclass(aa_layer.func, nn.AvgPool2d):\n            return aa_layer()\n        else:\n            return aa_layer(channels)\n    elif issubclass(aa_layer, nn.AvgPool2d):\n        return aa_layer(stride)\n    else:\n        return aa_layer(channels=channels, stride=stride)\n\n\nclass ConvNormActAa(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size=1,\n            stride=1,\n            padding='',\n            dilation=1,\n            groups=1,\n            bias=False,\n            apply_act=True,\n            norm_layer=nn.BatchNorm2d,\n            norm_kwargs=None,\n            act_layer=nn.ReLU,\n            act_kwargs=None,\n            aa_layer=None,\n            drop_layer=None,\n    ):\n        super(ConvNormActAa, self).__init__()\n        use_aa = aa_layer is not None and stride == 2\n        norm_kwargs = norm_kwargs or {}\n        act_kwargs = act_kwargs or {}\n\n        self.conv = create_conv2d(\n            in_channels, out_channels, kernel_size, stride=1 if use_aa else stride,\n            padding=padding, dilation=dilation, groups=groups, bias=bias)\n\n        # NOTE for backwards compatibility with models that use separate norm and act layer definitions\n        norm_act_layer = get_norm_act_layer(norm_layer, act_layer)\n        # NOTE for backwards (weight) compatibility, norm layer name remains `.bn`\n        if drop_layer:\n            norm_kwargs['drop_layer'] = drop_layer\n        self.bn = norm_act_layer(out_channels, apply_act=apply_act, act_kwargs=act_kwargs, **norm_kwargs)\n        self.aa = create_aa(aa_layer, out_channels, stride=stride, enable=use_aa)\n\n    @property\n    def in_channels(self):\n        return self.conv.in_channels\n\n    @property\n    def out_channels(self):\n        return self.conv.out_channels\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.aa(x)\n        return x\n",
  "\"\"\" Padding Helpers\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn.functional as F\n\n\n# Calculate symmetric padding for a convolution\ndef get_padding(kernel_size: int, stride: int = 1, dilation: int = 1, **_) -> int:\n    padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n    return padding\n\n\n# Calculate asymmetric TensorFlow-like 'SAME' padding for a convolution\ndef get_same_padding(x: int, kernel_size: int, stride: int, dilation: int):\n    if isinstance(x, torch.Tensor):\n        return torch.clamp(((x / stride).ceil() - 1) * stride + (kernel_size - 1) * dilation + 1 - x, min=0)\n    else:\n        return max((math.ceil(x / stride) - 1) * stride + (kernel_size - 1) * dilation + 1 - x, 0)\n\n\n# Can SAME padding for given args be done statically?\ndef is_static_pad(kernel_size: int, stride: int = 1, dilation: int = 1, **_):\n    return stride == 1 and (dilation * (kernel_size - 1)) % 2 == 0\n\n\ndef pad_same_arg(\n        input_size: List[int],\n        kernel_size: List[int],\n        stride: List[int],\n        dilation: List[int] = (1, 1),\n) -> List[int]:\n    ih, iw = input_size\n    kh, kw = kernel_size\n    pad_h = get_same_padding(ih, kh, stride[0], dilation[0])\n    pad_w = get_same_padding(iw, kw, stride[1], dilation[1])\n    return [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2]\n\n\n# Dynamically pad input x with 'SAME' padding for conv with specified args\ndef pad_same(\n        x,\n        kernel_size: List[int],\n        stride: List[int],\n        dilation: List[int] = (1, 1),\n        value: float = 0,\n):\n    ih, iw = x.size()[-2:]\n    pad_h = get_same_padding(ih, kernel_size[0], stride[0], dilation[0])\n    pad_w = get_same_padding(iw, kernel_size[1], stride[1], dilation[1])\n    x = F.pad(x, (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2), value=value)\n    return x\n\n\ndef get_padding_value(padding, kernel_size, **kwargs) -> Tuple[Tuple, bool]:\n    dynamic = False\n    if isinstance(padding, str):\n        # for any string padding, the padding will be calculated for you, one of three ways\n        padding = padding.lower()\n        if padding == 'same':\n            # TF compatible 'SAME' padding, has a performance and GPU memory allocation impact\n            if is_static_pad(kernel_size, **kwargs):\n                # static case, no extra overhead\n                padding = get_padding(kernel_size, **kwargs)\n            else:\n                # dynamic 'SAME' padding, has runtime/GPU memory overhead\n                padding = 0\n                dynamic = True\n        elif padding == 'valid':\n            # 'VALID' padding, same as padding=0\n            padding = 0\n        else:\n            # Default to PyTorch style 'same'-ish symmetric padding\n            padding = get_padding(kernel_size, **kwargs)\n    return padding, dynamic\n",
  "\"\"\" Median Pool\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .helpers import to_2tuple, to_4tuple\n\n\nclass MedianPool2d(nn.Module):\n    \"\"\" Median pool (usable as median filter when stride=1) module.\n\n    Args:\n         kernel_size: size of pooling kernel, int or 2-tuple\n         stride: pool stride, int or 2-tuple\n         padding: pool padding, int or 4-tuple (l, r, t, b) as in pytorch F.pad\n         same: override padding and enforce same padding, boolean\n    \"\"\"\n    def __init__(self, kernel_size=3, stride=1, padding=0, same=False):\n        super(MedianPool2d, self).__init__()\n        self.k = to_2tuple(kernel_size)\n        self.stride = to_2tuple(stride)\n        self.padding = to_4tuple(padding)  # convert to l, r, t, b\n        self.same = same\n\n    def _padding(self, x):\n        if self.same:\n            ih, iw = x.size()[2:]\n            if ih % self.stride[0] == 0:\n                ph = max(self.k[0] - self.stride[0], 0)\n            else:\n                ph = max(self.k[0] - (ih % self.stride[0]), 0)\n            if iw % self.stride[1] == 0:\n                pw = max(self.k[1] - self.stride[1], 0)\n            else:\n                pw = max(self.k[1] - (iw % self.stride[1]), 0)\n            pl = pw // 2\n            pr = pw - pl\n            pt = ph // 2\n            pb = ph - pt\n            padding = (pl, pr, pt, pb)\n        else:\n            padding = self.padding\n        return padding\n\n    def forward(self, x):\n        x = F.pad(x, self._padding(x), mode='reflect')\n        x = x.unfold(2, self.k[0], self.stride[0]).unfold(3, self.k[1], self.stride[1])\n        x = x.contiguous().view(x.size()[:4] + (-1,)).median(dim=-1)[0]\n        return x\n",
  "\"\"\" Gather-Excite Attention Block\n\nPaper: `Gather-Excite: Exploiting Feature Context in CNNs` - https://arxiv.org/abs/1810.12348\n\nOfficial code here, but it's only partial impl in Caffe: https://github.com/hujie-frank/GENet\n\nI've tried to support all of the extent both w/ and w/o params. I don't believe I've seen another\nimpl that covers all of the cases.\n\nNOTE: extent=0 + extra_params=False is equivalent to Squeeze-and-Excitation\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport math\n\nfrom torch import nn as nn\nimport torch.nn.functional as F\n\nfrom .create_act import create_act_layer, get_act_layer\nfrom .create_conv2d import create_conv2d\nfrom .helpers import make_divisible\nfrom .mlp import ConvMlp\n\n\nclass GatherExcite(nn.Module):\n    \"\"\" Gather-Excite Attention Module\n    \"\"\"\n    def __init__(\n            self, channels, feat_size=None, extra_params=False, extent=0, use_mlp=True,\n            rd_ratio=1./16, rd_channels=None,  rd_divisor=1, add_maxpool=False,\n            act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d, gate_layer='sigmoid'):\n        super(GatherExcite, self).__init__()\n        self.add_maxpool = add_maxpool\n        act_layer = get_act_layer(act_layer)\n        self.extent = extent\n        if extra_params:\n            self.gather = nn.Sequential()\n            if extent == 0:\n                assert feat_size is not None, 'spatial feature size must be specified for global extent w/ params'\n                self.gather.add_module(\n                    'conv1', create_conv2d(channels, channels, kernel_size=feat_size, stride=1, depthwise=True))\n                if norm_layer:\n                    self.gather.add_module(f'norm1', nn.BatchNorm2d(channels))\n            else:\n                assert extent % 2 == 0\n                num_conv = int(math.log2(extent))\n                for i in range(num_conv):\n                    self.gather.add_module(\n                        f'conv{i + 1}',\n                        create_conv2d(channels, channels, kernel_size=3, stride=2, depthwise=True))\n                    if norm_layer:\n                        self.gather.add_module(f'norm{i + 1}', nn.BatchNorm2d(channels))\n                    if i != num_conv - 1:\n                        self.gather.add_module(f'act{i + 1}', act_layer(inplace=True))\n        else:\n            self.gather = None\n            if self.extent == 0:\n                self.gk = 0\n                self.gs = 0\n            else:\n                assert extent % 2 == 0\n                self.gk = self.extent * 2 - 1\n                self.gs = self.extent\n\n        if not rd_channels:\n            rd_channels = make_divisible(channels * rd_ratio, rd_divisor, round_limit=0.)\n        self.mlp = ConvMlp(channels, rd_channels, act_layer=act_layer) if use_mlp else nn.Identity()\n        self.gate = create_act_layer(gate_layer)\n\n    def forward(self, x):\n        size = x.shape[-2:]\n        if self.gather is not None:\n            x_ge = self.gather(x)\n        else:\n            if self.extent == 0:\n                # global extent\n                x_ge = x.mean(dim=(2, 3), keepdims=True)\n                if self.add_maxpool:\n                    # experimental codepath, may remove or change\n                    x_ge = 0.5 * x_ge + 0.5 * x.amax((2, 3), keepdim=True)\n            else:\n                x_ge = F.avg_pool2d(\n                    x, kernel_size=self.gk, stride=self.gs, padding=self.gk // 2, count_include_pad=False)\n                if self.add_maxpool:\n                    # experimental codepath, may remove or change\n                    x_ge = 0.5 * x_ge + 0.5 * F.max_pool2d(x, kernel_size=self.gk, stride=self.gs, padding=self.gk // 2)\n        x_ge = self.mlp(x_ge)\n        if x_ge.shape[-1] != 1 or x_ge.shape[-2] != 1:\n            x_ge = F.interpolate(x_ge, size=size)\n        return x * self.gate(x_ge)\n",
  "\"\"\" Norm Layer Factory\n\nCreate norm modules by string (to mirror create_act and creat_norm-act fns)\n\nCopyright 2022 Ross Wightman\n\"\"\"\nimport types\nimport functools\n\nimport torch.nn as nn\n\nfrom .norm import GroupNorm, GroupNorm1, LayerNorm, LayerNorm2d\n\n_NORM_MAP = dict(\n    batchnorm=nn.BatchNorm2d,\n    batchnorm2d=nn.BatchNorm2d,\n    batchnorm1d=nn.BatchNorm1d,\n    groupnorm=GroupNorm,\n    groupnorm1=GroupNorm1,\n    layernorm=LayerNorm,\n    layernorm2d=LayerNorm2d,\n)\n_NORM_TYPES = {m for n, m in _NORM_MAP.items()}\n\n\ndef create_norm_layer(layer_name, num_features, **kwargs):\n    layer = get_norm_layer(layer_name)\n    layer_instance = layer(num_features, **kwargs)\n    return layer_instance\n\n\ndef get_norm_layer(norm_layer):\n    assert isinstance(norm_layer, (type, str,  types.FunctionType, functools.partial))\n    norm_kwargs = {}\n\n    # unbind partial fn, so args can be rebound later\n    if isinstance(norm_layer, functools.partial):\n        norm_kwargs.update(norm_layer.keywords)\n        norm_layer = norm_layer.func\n\n    if isinstance(norm_layer, str):\n        layer_name = norm_layer.replace('_', '')\n        norm_layer = _NORM_MAP.get(layer_name, None)\n    elif norm_layer in _NORM_TYPES:\n        norm_layer = norm_layer\n    elif isinstance(norm_layer, types.FunctionType):\n        # if function type, assume it is a lambda/fn that creates a norm layer\n        norm_layer = norm_layer\n    else:\n        type_name = norm_layer.__name__.lower().replace('_', '')\n        norm_layer = _NORM_MAP.get(type_name, None)\n        assert norm_layer is not None, f\"No equivalent norm layer for {type_name}\"\n\n    if norm_kwargs:\n        norm_layer = functools.partial(norm_layer, **norm_kwargs)  # bind/rebind args\n    return norm_layer\n",
  "from typing import Optional\n\nimport torch\nfrom torch import nn\nfrom torch import nn, Tensor\nfrom torch.nn.modules.transformer import _get_activation_fn\n\n\ndef add_ml_decoder_head(model):\n    if hasattr(model, 'global_pool') and hasattr(model, 'fc'):  # most CNN models, like Resnet50\n        model.global_pool = nn.Identity()\n        del model.fc\n        num_classes = model.num_classes\n        num_features = model.num_features\n        model.fc = MLDecoder(num_classes=num_classes, initial_num_features=num_features)\n    elif hasattr(model, 'global_pool') and hasattr(model, 'classifier'):  # EfficientNet\n        model.global_pool = nn.Identity()\n        del model.classifier\n        num_classes = model.num_classes\n        num_features = model.num_features\n        model.classifier = MLDecoder(num_classes=num_classes, initial_num_features=num_features)\n    elif 'RegNet' in model._get_name() or 'TResNet' in model._get_name():  # hasattr(model, 'head')\n        del model.head\n        num_classes = model.num_classes\n        num_features = model.num_features\n        model.head = MLDecoder(num_classes=num_classes, initial_num_features=num_features)\n    else:\n        print(\"Model code-writing is not aligned currently with ml-decoder\")\n        exit(-1)\n    if hasattr(model, 'drop_rate'):  # Ml-Decoder has inner dropout\n        model.drop_rate = 0\n    return model\n\n\nclass TransformerDecoderLayerOptimal(nn.Module):\n    def __init__(self, d_model, nhead=8, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n                 layer_norm_eps=1e-5) -> None:\n        super(TransformerDecoderLayerOptimal, self).__init__()\n        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        self.dropout = nn.Dropout(dropout)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n\n        self.activation = _get_activation_fn(activation)\n\n    def __setstate__(self, state):\n        if 'activation' not in state:\n            state['activation'] = torch.nn.functional.relu\n        super(TransformerDecoderLayerOptimal, self).__setstate__(state)\n\n    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n                memory_mask: Optional[Tensor] = None,\n                tgt_key_padding_mask: Optional[Tensor] = None,\n                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n        tgt = tgt + self.dropout1(tgt)\n        tgt = self.norm1(tgt)\n        tgt2 = self.multihead_attn(tgt, memory, memory)[0]\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n        return tgt\n\n\n# @torch.jit.script\n# class ExtrapClasses(object):\n#     def __init__(self, num_queries: int, group_size: int):\n#         self.num_queries = num_queries\n#         self.group_size = group_size\n#\n#     def __call__(self, h: torch.Tensor, class_embed_w: torch.Tensor, class_embed_b: torch.Tensor, out_extrap:\n#     torch.Tensor):\n#         # h = h.unsqueeze(-1).expand(-1, -1, -1, self.group_size)\n#         h = h[..., None].repeat(1, 1, 1, self.group_size) # torch.Size([bs, 5, 768, groups])\n#         w = class_embed_w.view((self.num_queries, h.shape[2], self.group_size))\n#         out = (h * w).sum(dim=2) + class_embed_b\n#         out = out.view((h.shape[0], self.group_size * self.num_queries))\n#         return out\n\n@torch.jit.script\nclass GroupFC(object):\n    def __init__(self, embed_len_decoder: int):\n        self.embed_len_decoder = embed_len_decoder\n\n    def __call__(self, h: torch.Tensor, duplicate_pooling: torch.Tensor, out_extrap: torch.Tensor):\n        for i in range(self.embed_len_decoder):\n            h_i = h[:, i, :]\n            w_i = duplicate_pooling[i, :, :]\n            out_extrap[:, i, :] = torch.matmul(h_i, w_i)\n\n\nclass MLDecoder(nn.Module):\n    def __init__(self, num_classes, num_of_groups=-1, decoder_embedding=768, initial_num_features=2048):\n        super(MLDecoder, self).__init__()\n        embed_len_decoder = 100 if num_of_groups < 0 else num_of_groups\n        if embed_len_decoder > num_classes:\n            embed_len_decoder = num_classes\n\n        # switching to 768 initial embeddings\n        decoder_embedding = 768 if decoder_embedding < 0 else decoder_embedding\n        self.embed_standart = nn.Linear(initial_num_features, decoder_embedding)\n\n        # decoder\n        decoder_dropout = 0.1\n        num_layers_decoder = 1\n        dim_feedforward = 2048\n        layer_decode = TransformerDecoderLayerOptimal(d_model=decoder_embedding,\n                                                      dim_feedforward=dim_feedforward, dropout=decoder_dropout)\n        self.decoder = nn.TransformerDecoder(layer_decode, num_layers=num_layers_decoder)\n\n        # non-learnable queries\n        self.query_embed = nn.Embedding(embed_len_decoder, decoder_embedding)\n        self.query_embed.requires_grad_(False)\n\n        # group fully-connected\n        self.num_classes = num_classes\n        self.duplicate_factor = int(num_classes / embed_len_decoder + 0.999)\n        self.duplicate_pooling = torch.nn.Parameter(\n            torch.Tensor(embed_len_decoder, decoder_embedding, self.duplicate_factor))\n        self.duplicate_pooling_bias = torch.nn.Parameter(torch.Tensor(num_classes))\n        torch.nn.init.xavier_normal_(self.duplicate_pooling)\n        torch.nn.init.constant_(self.duplicate_pooling_bias, 0)\n        self.group_fc = GroupFC(embed_len_decoder)\n\n    def forward(self, x):\n        if len(x.shape) == 4:  # [bs,2048, 7,7]\n            embedding_spatial = x.flatten(2).transpose(1, 2)\n        else:  # [bs, 197,468]\n            embedding_spatial = x\n        embedding_spatial_786 = self.embed_standart(embedding_spatial)\n        embedding_spatial_786 = torch.nn.functional.relu(embedding_spatial_786, inplace=True)\n\n        bs = embedding_spatial_786.shape[0]\n        query_embed = self.query_embed.weight\n        # tgt = query_embed.unsqueeze(1).repeat(1, bs, 1)\n        tgt = query_embed.unsqueeze(1).expand(-1, bs, -1)  # no allocation of memory with expand\n        h = self.decoder(tgt, embedding_spatial_786.transpose(0, 1))  # [embed_len_decoder, batch, 768]\n        h = h.transpose(0, 1)\n\n        out_extrap = torch.zeros(h.shape[0], h.shape[1], self.duplicate_factor, device=h.device, dtype=h.dtype)\n        self.group_fc(h, self.duplicate_pooling, out_extrap)\n        h_out = out_extrap.flatten(1)[:, :self.num_classes]\n        h_out += self.duplicate_pooling_bias\n        logits = h_out\n        return logits\n",
  "import torch\nimport torch.nn as nn\n\n\nclass AsymmetricLossMultiLabel(nn.Module):\n    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False):\n        super(AsymmetricLossMultiLabel, self).__init__()\n\n        self.gamma_neg = gamma_neg\n        self.gamma_pos = gamma_pos\n        self.clip = clip\n        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n        self.eps = eps\n\n    def forward(self, x, y):\n        \"\"\"\"\n        Parameters\n        ----------\n        x: input logits\n        y: targets (multi-label binarized vector)\n        \"\"\"\n\n        # Calculating Probabilities\n        x_sigmoid = torch.sigmoid(x)\n        xs_pos = x_sigmoid\n        xs_neg = 1 - x_sigmoid\n\n        # Asymmetric Clipping\n        if self.clip is not None and self.clip > 0:\n            xs_neg = (xs_neg + self.clip).clamp(max=1)\n\n        # Basic CE calculation\n        los_pos = y * torch.log(xs_pos.clamp(min=self.eps))\n        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=self.eps))\n        loss = los_pos + los_neg\n\n        # Asymmetric Focusing\n        if self.gamma_neg > 0 or self.gamma_pos > 0:\n            if self.disable_torch_grad_focal_loss:\n                torch._C.set_grad_enabled(False)\n            pt0 = xs_pos * y\n            pt1 = xs_neg * (1 - y)  # pt = p if t > 0 else 1-p\n            pt = pt0 + pt1\n            one_sided_gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)\n            one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n            if self.disable_torch_grad_focal_loss:\n                torch._C.set_grad_enabled(True)\n            loss *= one_sided_w\n\n        return -loss.sum()\n\n\nclass AsymmetricLossSingleLabel(nn.Module):\n    def __init__(self, gamma_pos=1, gamma_neg=4, eps: float = 0.1, reduction='mean'):\n        super(AsymmetricLossSingleLabel, self).__init__()\n\n        self.eps = eps\n        self.logsoftmax = nn.LogSoftmax(dim=-1)\n        self.targets_classes = []  # prevent gpu repeated memory allocation\n        self.gamma_pos = gamma_pos\n        self.gamma_neg = gamma_neg\n        self.reduction = reduction\n\n    def forward(self, inputs, target, reduction=None):\n        \"\"\"\"\n        Parameters\n        ----------\n        x: input logits\n        y: targets (1-hot vector)\n        \"\"\"\n\n        num_classes = inputs.size()[-1]\n        log_preds = self.logsoftmax(inputs)\n        self.targets_classes = torch.zeros_like(inputs).scatter_(1, target.long().unsqueeze(1), 1)\n\n        # ASL weights\n        targets = self.targets_classes\n        anti_targets = 1 - targets\n        xs_pos = torch.exp(log_preds)\n        xs_neg = 1 - xs_pos\n        xs_pos = xs_pos * targets\n        xs_neg = xs_neg * anti_targets\n        asymmetric_w = torch.pow(1 - xs_pos - xs_neg,\n                                 self.gamma_pos * targets + self.gamma_neg * anti_targets)\n        log_preds = log_preds * asymmetric_w\n\n        if self.eps > 0:  # label smoothing\n            self.targets_classes = self.targets_classes.mul(1 - self.eps).add(self.eps / num_classes)\n\n        # loss calculation\n        loss = - self.targets_classes.mul(log_preds)\n\n        loss = loss.sum(dim=-1)\n        if self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n",
  "from .asymmetric_loss import AsymmetricLossMultiLabel, AsymmetricLossSingleLabel\nfrom .binary_cross_entropy import BinaryCrossEntropy\nfrom .cross_entropy import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\nfrom .jsd import JsdCrossEntropy\n",
  "\"\"\" Binary Cross Entropy w/ a few extras\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BinaryCrossEntropy(nn.Module):\n    \"\"\" BCE with optional one-hot from dense targets, label smoothing, thresholding\n    NOTE for experiments comparing CE to BCE /w label smoothing, may remove\n    \"\"\"\n    def __init__(\n            self, smoothing=0.1, target_threshold: Optional[float] = None, weight: Optional[torch.Tensor] = None,\n            reduction: str = 'mean', pos_weight: Optional[torch.Tensor] = None):\n        super(BinaryCrossEntropy, self).__init__()\n        assert 0. <= smoothing < 1.0\n        self.smoothing = smoothing\n        self.target_threshold = target_threshold\n        self.reduction = reduction\n        self.register_buffer('weight', weight)\n        self.register_buffer('pos_weight', pos_weight)\n\n    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        assert x.shape[0] == target.shape[0]\n        if target.shape != x.shape:\n            # NOTE currently assume smoothing or other label softening is applied upstream if targets are already sparse\n            num_classes = x.shape[-1]\n            # FIXME should off/on be different for smoothing w/ BCE? Other impl out there differ\n            off_value = self.smoothing / num_classes\n            on_value = 1. - self.smoothing + off_value\n            target = target.long().view(-1, 1)\n            target = torch.full(\n                (target.size()[0], num_classes),\n                off_value,\n                device=x.device, dtype=x.dtype).scatter_(1, target, on_value)\n        if self.target_threshold is not None:\n            # Make target 0, or 1 if threshold set\n            target = target.gt(self.target_threshold).to(dtype=target.dtype)\n        return F.binary_cross_entropy_with_logits(\n            x, target,\n            self.weight,\n            pos_weight=self.pos_weight,\n            reduction=self.reduction)\n",
  "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .cross_entropy import LabelSmoothingCrossEntropy\n\n\nclass JsdCrossEntropy(nn.Module):\n    \"\"\" Jensen-Shannon Divergence + Cross-Entropy Loss\n\n    Based on impl here: https://github.com/google-research/augmix/blob/master/imagenet.py\n    From paper: 'AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty -\n    https://arxiv.org/abs/1912.02781\n\n    Hacked together by / Copyright 2020 Ross Wightman\n    \"\"\"\n    def __init__(self, num_splits=3, alpha=12, smoothing=0.1):\n        super().__init__()\n        self.num_splits = num_splits\n        self.alpha = alpha\n        if smoothing is not None and smoothing > 0:\n            self.cross_entropy_loss = LabelSmoothingCrossEntropy(smoothing)\n        else:\n            self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n\n    def __call__(self, output, target):\n        split_size = output.shape[0] // self.num_splits\n        assert split_size * self.num_splits == output.shape[0]\n        logits_split = torch.split(output, split_size)\n\n        # Cross-entropy is only computed on clean images\n        loss = self.cross_entropy_loss(logits_split[0], target[:split_size])\n        probs = [F.softmax(logits, dim=1) for logits in logits_split]\n\n        # Clamp mixture distribution to avoid exploding KL divergence\n        logp_mixture = torch.clamp(torch.stack(probs).mean(axis=0), 1e-7, 1).log()\n        loss += self.alpha * sum([F.kl_div(\n            logp_mixture, p_split, reduction='batchmean') for p_split in probs]) / len(probs)\n        return loss\n",
  "\"\"\" Cross Entropy w/ smoothing or soft targets\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass LabelSmoothingCrossEntropy(nn.Module):\n    \"\"\" NLL loss with label smoothing.\n    \"\"\"\n    def __init__(self, smoothing=0.1):\n        super(LabelSmoothingCrossEntropy, self).__init__()\n        assert smoothing < 1.0\n        self.smoothing = smoothing\n        self.confidence = 1. - smoothing\n\n    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        logprobs = F.log_softmax(x, dim=-1)\n        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        nll_loss = nll_loss.squeeze(1)\n        smooth_loss = -logprobs.mean(dim=-1)\n        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        return loss.mean()\n\n\nclass SoftTargetCrossEntropy(nn.Module):\n\n    def __init__(self):\n        super(SoftTargetCrossEntropy, self).__init__()\n\n    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        loss = torch.sum(-target * F.log_softmax(x, dim=-1), dim=-1)\n        return loss.mean()\n",
  "\"\"\" Distributed training/validation utils\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport os\n\nimport torch\nfrom torch import distributed as dist\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    hvd = None\n\nfrom .model import unwrap_model\n\n\ndef reduce_tensor(tensor, n):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    rt /= n\n    return rt\n\n\ndef distribute_bn(model, world_size, reduce=False):\n    # ensure every node has the same running bn stats\n    for bn_name, bn_buf in unwrap_model(model).named_buffers(recurse=True):\n        if ('running_mean' in bn_name) or ('running_var' in bn_name):\n            if reduce:\n                # average bn stats across whole group\n                torch.distributed.all_reduce(bn_buf, op=dist.ReduceOp.SUM)\n                bn_buf /= float(world_size)\n            else:\n                # broadcast bn stats from rank 0 to whole group\n                torch.distributed.broadcast(bn_buf, 0)\n\n\ndef is_global_primary(args):\n    return args.rank == 0\n\n\ndef is_local_primary(args):\n    return args.local_rank == 0\n\n\ndef is_primary(args, local=False):\n    return is_local_primary(args) if local else is_global_primary(args)\n\n\ndef is_distributed_env():\n    if 'WORLD_SIZE' in os.environ:\n        return int(os.environ['WORLD_SIZE']) > 1\n    if 'SLURM_NTASKS' in os.environ:\n        return int(os.environ['SLURM_NTASKS']) > 1\n    return False\n\n\ndef world_info_from_env():\n    local_rank = 0\n    for v in ('LOCAL_RANK', 'MPI_LOCALRANKID', 'SLURM_LOCALID', 'OMPI_COMM_WORLD_LOCAL_RANK'):\n        if v in os.environ:\n            local_rank = int(os.environ[v])\n            break\n\n    global_rank = 0\n    for v in ('RANK', 'PMI_RANK', 'SLURM_PROCID', 'OMPI_COMM_WORLD_RANK'):\n        if v in os.environ:\n            global_rank = int(os.environ[v])\n            break\n\n    world_size = 1\n    for v in ('WORLD_SIZE', 'PMI_SIZE', 'SLURM_NTASKS', 'OMPI_COMM_WORLD_SIZE'):\n        if v in os.environ:\n            world_size = int(os.environ[v])\n            break\n\n    return local_rank, global_rank, world_size\n\n\ndef init_distributed_device(args):\n    # Distributed training = training on more than one GPU.\n    # Works in both single and multi-node scenarios.\n    args.distributed = False\n    args.world_size = 1\n    args.rank = 0  # global rank\n    args.local_rank = 0\n\n    # TBD, support horovod?\n    # if args.horovod:\n    #     assert hvd is not None, \"Horovod is not installed\"\n    #     hvd.init()\n    #     args.local_rank = int(hvd.local_rank())\n    #     args.rank = hvd.rank()\n    #     args.world_size = hvd.size()\n    #     args.distributed = True\n    #     os.environ['LOCAL_RANK'] = str(args.local_rank)\n    #     os.environ['RANK'] = str(args.rank)\n    #     os.environ['WORLD_SIZE'] = str(args.world_size)\n    dist_backend = getattr(args, 'dist_backend', 'nccl')\n    dist_url = getattr(args, 'dist_url', 'env://')\n    if is_distributed_env():\n        if 'SLURM_PROCID' in os.environ:\n            # DDP via SLURM\n            args.local_rank, args.rank, args.world_size = world_info_from_env()\n            # SLURM var -> torch.distributed vars in case needed\n            os.environ['LOCAL_RANK'] = str(args.local_rank)\n            os.environ['RANK'] = str(args.rank)\n            os.environ['WORLD_SIZE'] = str(args.world_size)\n            torch.distributed.init_process_group(\n                backend=dist_backend,\n                init_method=dist_url,\n                world_size=args.world_size,\n                rank=args.rank,\n            )\n        else:\n            # DDP via torchrun, torch.distributed.launch\n            args.local_rank, _, _ = world_info_from_env()\n            torch.distributed.init_process_group(\n                backend=dist_backend,\n                init_method=dist_url,\n            )\n            args.world_size = torch.distributed.get_world_size()\n            args.rank = torch.distributed.get_rank()\n        args.distributed = True\n\n    if torch.cuda.is_available():\n        if args.distributed:\n            device = 'cuda:%d' % args.local_rank\n        else:\n            device = 'cuda:0'\n        torch.cuda.set_device(device)\n    else:\n        device = 'cpu'\n\n    args.device = device\n    device = torch.device(device)\n    return device\n",
  "from .agc import adaptive_clip_grad\nfrom .checkpoint_saver import CheckpointSaver\nfrom .clip_grad import dispatch_clip_grad\nfrom .cuda import ApexScaler, NativeScaler\nfrom .decay_batch import decay_batch_step, check_batch_size_retry\nfrom .distributed import distribute_bn, reduce_tensor, init_distributed_device,\\\n    world_info_from_env, is_distributed_env, is_primary\nfrom .jit import set_jit_legacy, set_jit_fuser\nfrom .log import setup_default_logging, FormatterNoInfo\nfrom .metrics import AverageMeter, accuracy\nfrom .misc import natural_key, add_bool_arg, ParseKwargs\nfrom .model import unwrap_model, get_state_dict, freeze, unfreeze\nfrom .model_ema import ModelEma, ModelEmaV2\nfrom .random import random_seed\nfrom .summary import update_summary, get_outdir\n",
  "\"\"\" Model / state_dict utils\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport fnmatch\n\nimport torch\nfrom torchvision.ops.misc import FrozenBatchNorm2d\n\nfrom timm.layers import BatchNormAct2d, SyncBatchNormAct, FrozenBatchNormAct2d,\\\n    freeze_batch_norm_2d, unfreeze_batch_norm_2d\nfrom .model_ema import ModelEma\n\n\ndef unwrap_model(model):\n    if isinstance(model, ModelEma):\n        return unwrap_model(model.ema)\n    else:\n        return model.module if hasattr(model, 'module') else model\n\n\ndef get_state_dict(model, unwrap_fn=unwrap_model):\n    return unwrap_fn(model).state_dict()\n\n\ndef avg_sq_ch_mean(model, input, output):\n    \"\"\" calculate average channel square mean of output activations\n    \"\"\"\n    return torch.mean(output.mean(axis=[0, 2, 3]) ** 2).item()\n\n\ndef avg_ch_var(model, input, output):\n    \"\"\" calculate average channel variance of output activations\n    \"\"\"\n    return torch.mean(output.var(axis=[0, 2, 3])).item()\n\n\ndef avg_ch_var_residual(model, input, output):\n    \"\"\" calculate average channel variance of output activations\n    \"\"\"\n    return torch.mean(output.var(axis=[0, 2, 3])).item()\n\n\nclass ActivationStatsHook:\n    \"\"\"Iterates through each of `model`'s modules and matches modules using unix pattern \n    matching based on `hook_fn_locs` and registers `hook_fn` to the module if there is \n    a match. \n\n    Arguments:\n        model (nn.Module): model from which we will extract the activation stats\n        hook_fn_locs (List[str]): List of `hook_fn` locations based on Unix type string \n            matching with the name of model's modules. \n        hook_fns (List[Callable]): List of hook functions to be registered at every\n            module in `layer_names`.\n    \n    Inspiration from https://docs.fast.ai/callback.hook.html.\n\n    Refer to https://gist.github.com/amaarora/6e56942fcb46e67ba203f3009b30d950 for an example \n    on how to plot Signal Propogation Plots using `ActivationStatsHook`.\n    \"\"\"\n\n    def __init__(self, model, hook_fn_locs, hook_fns):\n        self.model = model\n        self.hook_fn_locs = hook_fn_locs\n        self.hook_fns = hook_fns\n        if len(hook_fn_locs) != len(hook_fns):\n            raise ValueError(\"Please provide `hook_fns` for each `hook_fn_locs`, \\\n                their lengths are different.\")\n        self.stats = dict((hook_fn.__name__, []) for hook_fn in hook_fns)\n        for hook_fn_loc, hook_fn in zip(hook_fn_locs, hook_fns):\n            self.register_hook(hook_fn_loc, hook_fn)\n\n    def _create_hook(self, hook_fn):\n        def append_activation_stats(module, input, output):\n            out = hook_fn(module, input, output)\n            self.stats[hook_fn.__name__].append(out)\n\n        return append_activation_stats\n\n    def register_hook(self, hook_fn_loc, hook_fn):\n        for name, module in self.model.named_modules():\n            if not fnmatch.fnmatch(name, hook_fn_loc):\n                continue\n            module.register_forward_hook(self._create_hook(hook_fn))\n\n\ndef extract_spp_stats(\n        model,\n        hook_fn_locs,\n        hook_fns,\n        input_shape=[8, 3, 224, 224]):\n    \"\"\"Extract average square channel mean and variance of activations during \n    forward pass to plot Signal Propogation Plots (SPP).\n    \n    Paper: https://arxiv.org/abs/2101.08692\n\n    Example Usage: https://gist.github.com/amaarora/6e56942fcb46e67ba203f3009b30d950\n    \"\"\"\n    x = torch.normal(0., 1., input_shape)\n    hook = ActivationStatsHook(model, hook_fn_locs=hook_fn_locs, hook_fns=hook_fns)\n    _ = model(x)\n    return hook.stats\n\n\ndef _freeze_unfreeze(root_module, submodules=[], include_bn_running_stats=True, mode='freeze'):\n    \"\"\"\n    Freeze or unfreeze parameters of the specified modules and those of all their hierarchical descendants. This is\n    done in place.\n    Args:\n        root_module (nn.Module, optional): Root module relative to which the `submodules` are referenced.\n        submodules (list[str]): List of modules for which the parameters will be (un)frozen. They are to be provided as\n            named modules relative to the root module (accessible via `root_module.named_modules()`). An empty list\n            means that the whole root module will be (un)frozen. Defaults to []\n        include_bn_running_stats (bool): Whether to also (un)freeze the running statistics of batch norm 2d layers.\n            Defaults to `True`.\n        mode (bool): Whether to freeze (\"freeze\") or unfreeze (\"unfreeze\"). Defaults to `\"freeze\"`.\n    \"\"\"\n    assert mode in [\"freeze\", \"unfreeze\"], '`mode` must be one of \"freeze\" or \"unfreeze\"'\n\n    if isinstance(root_module, (\n            torch.nn.modules.batchnorm.BatchNorm2d,\n            torch.nn.modules.batchnorm.SyncBatchNorm,\n            BatchNormAct2d,\n            SyncBatchNormAct,\n    )):\n        # Raise assertion here because we can't convert it in place\n        raise AssertionError(\n            \"You have provided a batch norm layer as the `root module`. Please use \"\n            \"`timm.utils.model.freeze_batch_norm_2d` or `timm.utils.model.unfreeze_batch_norm_2d` instead.\")\n\n    if isinstance(submodules, str):\n        submodules = [submodules]\n\n    named_modules = submodules\n    submodules = [root_module.get_submodule(m) for m in submodules]\n\n    if not len(submodules):\n        named_modules, submodules = list(zip(*root_module.named_children()))\n\n    for n, m in zip(named_modules, submodules):\n        # (Un)freeze parameters\n        for p in m.parameters():\n            p.requires_grad = False if mode == 'freeze' else True\n        if include_bn_running_stats:\n            # Helper to add submodule specified as a named_module\n            def _add_submodule(module, name, submodule):\n                split = name.rsplit('.', 1)\n                if len(split) > 1:\n                    module.get_submodule(split[0]).add_module(split[1], submodule)\n                else:\n                    module.add_module(name, submodule)\n\n            # Freeze batch norm\n            if mode == 'freeze':\n                res = freeze_batch_norm_2d(m)\n                # It's possible that `m` is a type of BatchNorm in itself, in which case `unfreeze_batch_norm_2d` won't\n                # convert it in place, but will return the converted result. In this case `res` holds the converted\n                # result and we may try to re-assign the named module\n                if isinstance(m, (\n                        torch.nn.modules.batchnorm.BatchNorm2d,\n                        torch.nn.modules.batchnorm.SyncBatchNorm,\n                        BatchNormAct2d,\n                        SyncBatchNormAct,\n                )):\n                    _add_submodule(root_module, n, res)\n            # Unfreeze batch norm\n            else:\n                res = unfreeze_batch_norm_2d(m)\n                # Ditto. See note above in mode == 'freeze' branch\n                if isinstance(m, (FrozenBatchNorm2d, FrozenBatchNormAct2d)):\n                    _add_submodule(root_module, n, res)\n\n\ndef freeze(root_module, submodules=[], include_bn_running_stats=True):\n    \"\"\"\n    Freeze parameters of the specified modules and those of all their hierarchical descendants. This is done in place.\n    Args:\n        root_module (nn.Module): Root module relative to which `submodules` are referenced.\n        submodules (list[str]): List of modules for which the parameters will be frozen. They are to be provided as\n            named modules relative to the root module (accessible via `root_module.named_modules()`). An empty list\n            means that the whole root module will be frozen. Defaults to `[]`.\n        include_bn_running_stats (bool): Whether to also freeze the running statistics of `BatchNorm2d` and\n            `SyncBatchNorm` layers. These will be converted to `FrozenBatchNorm2d` in place. Hint: During fine tuning,\n            it's good practice to freeze batch norm stats. And note that these are different to the affine parameters\n            which are just normal PyTorch parameters. Defaults to `True`.\n\n    Hint: If you want to freeze batch norm ONLY, use `timm.utils.model.freeze_batch_norm_2d`.\n\n    Examples::\n\n        >>> model = timm.create_model('resnet18')\n        >>> # Freeze up to and including layer2\n        >>> submodules = [n for n, _ in model.named_children()]\n        >>> print(submodules)\n        ['conv1', 'bn1', 'act1', 'maxpool', 'layer1', 'layer2', 'layer3', 'layer4', 'global_pool', 'fc']\n        >>> freeze(model, submodules[:submodules.index('layer2') + 1])\n        >>> # Check for yourself that it works as expected\n        >>> print(model.layer2[0].conv1.weight.requires_grad)\n        False\n        >>> print(model.layer3[0].conv1.weight.requires_grad)\n        True\n        >>> # Unfreeze\n        >>> unfreeze(model)\n    \"\"\"\n    _freeze_unfreeze(root_module, submodules, include_bn_running_stats=include_bn_running_stats, mode=\"freeze\")\n\n\ndef unfreeze(root_module, submodules=[], include_bn_running_stats=True):\n    \"\"\"\n    Unfreeze parameters of the specified modules and those of all their hierarchical descendants. This is done in place.\n    Args:\n        root_module (nn.Module): Root module relative to which `submodules` are referenced.\n        submodules (list[str]): List of submodules for which the parameters will be (un)frozen. They are to be provided\n            as named modules relative to the root module (accessible via `root_module.named_modules()`). An empty\n            list means that the whole root module will be unfrozen. Defaults to `[]`.\n        include_bn_running_stats (bool): Whether to also unfreeze the running statistics of `FrozenBatchNorm2d` layers.\n            These will be converted to `BatchNorm2d` in place. Defaults to `True`.\n\n    See example in docstring for `freeze`.\n    \"\"\"\n    _freeze_unfreeze(root_module, submodules, include_bn_running_stats=include_bn_running_stats, mode=\"unfreeze\")\n",
  "\"\"\" JIT scripting/tracing utils\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport os\n\nimport torch\n\n\ndef set_jit_legacy():\n    \"\"\" Set JIT executor to legacy w/ support for op fusion\n    This is hopefully a temporary need in 1.5/1.5.1/1.6 to restore performance due to changes\n    in the JIT exectutor. These API are not supported so could change.\n    \"\"\"\n    #\n    assert hasattr(torch._C, '_jit_set_profiling_executor'), \"Old JIT behavior doesn't exist!\"\n    torch._C._jit_set_profiling_executor(False)\n    torch._C._jit_set_profiling_mode(False)\n    torch._C._jit_override_can_fuse_on_gpu(True)\n    #torch._C._jit_set_texpr_fuser_enabled(True)\n\n\ndef set_jit_fuser(fuser):\n    if fuser == \"te\":\n        # default fuser should be == 'te'\n        torch._C._jit_set_profiling_executor(True)\n        torch._C._jit_set_profiling_mode(True)\n        torch._C._jit_override_can_fuse_on_cpu(False)\n        torch._C._jit_override_can_fuse_on_gpu(True)\n        torch._C._jit_set_texpr_fuser_enabled(True)\n        try:\n            torch._C._jit_set_nvfuser_enabled(False)\n        except Exception:\n            pass\n    elif fuser == \"old\" or fuser == \"legacy\":\n        torch._C._jit_set_profiling_executor(False)\n        torch._C._jit_set_profiling_mode(False)\n        torch._C._jit_override_can_fuse_on_gpu(True)\n        torch._C._jit_set_texpr_fuser_enabled(False)\n        try:\n            torch._C._jit_set_nvfuser_enabled(False)\n        except Exception:\n            pass\n    elif fuser == \"nvfuser\" or fuser == \"nvf\":\n        os.environ['PYTORCH_NVFUSER_DISABLE_FALLBACK'] = '1'\n        #os.environ['PYTORCH_NVFUSER_DISABLE_FMA'] = '1'\n        #os.environ['PYTORCH_NVFUSER_JIT_OPT_LEVEL'] = '0'\n        torch._C._jit_set_texpr_fuser_enabled(False)\n        torch._C._jit_set_profiling_executor(True)\n        torch._C._jit_set_profiling_mode(True)\n        torch._C._jit_can_fuse_on_cpu()\n        torch._C._jit_can_fuse_on_gpu()\n        torch._C._jit_override_can_fuse_on_cpu(False)\n        torch._C._jit_override_can_fuse_on_gpu(False)\n        torch._C._jit_set_nvfuser_guard_mode(True)\n        torch._C._jit_set_nvfuser_enabled(True)\n    else:\n        assert False, f\"Invalid jit fuser ({fuser})\"\n",
  "\"\"\" Eval metrics and related\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\n\nclass AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    maxk = min(max(topk), output.size()[1])\n    batch_size = target.size(0)\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n    return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]\n",
  "import torch\n\nfrom timm.utils.agc import adaptive_clip_grad\n\n\ndef dispatch_clip_grad(parameters, value: float, mode: str = 'norm', norm_type: float = 2.0):\n    \"\"\" Dispatch to gradient clipping method\n\n    Args:\n        parameters (Iterable): model parameters to clip\n        value (float): clipping value/factor/norm, mode dependant\n        mode (str): clipping mode, one of 'norm', 'value', 'agc'\n        norm_type (float): p-norm, default 2.0\n    \"\"\"\n    if mode == 'norm':\n        torch.nn.utils.clip_grad_norm_(parameters, value, norm_type=norm_type)\n    elif mode == 'value':\n        torch.nn.utils.clip_grad_value_(parameters, value)\n    elif mode == 'agc':\n        adaptive_clip_grad(parameters, value, norm_type=norm_type)\n    else:\n        assert False, f\"Unknown clip mode ({mode}).\"\n\n",
  "\"\"\" Summary utilities\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport csv\nimport os\nfrom collections import OrderedDict\ntry: \n    import wandb\nexcept ImportError:\n    pass\n\n\ndef get_outdir(path, *paths, inc=False):\n    outdir = os.path.join(path, *paths)\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n    elif inc:\n        count = 1\n        outdir_inc = outdir + '-' + str(count)\n        while os.path.exists(outdir_inc):\n            count = count + 1\n            outdir_inc = outdir + '-' + str(count)\n            assert count < 100\n        outdir = outdir_inc\n        os.makedirs(outdir)\n    return outdir\n\n\ndef update_summary(\n        epoch,\n        train_metrics,\n        eval_metrics,\n        filename,\n        lr=None,\n        write_header=False,\n        log_wandb=False,\n):\n    rowd = OrderedDict(epoch=epoch)\n    rowd.update([('train_' + k, v) for k, v in train_metrics.items()])\n    rowd.update([('eval_' + k, v) for k, v in eval_metrics.items()])\n    if lr is not None:\n        rowd['lr'] = lr\n    if log_wandb:\n        wandb.log(rowd)\n    with open(filename, mode='a') as cf:\n        dw = csv.DictWriter(cf, fieldnames=rowd.keys())\n        if write_header:  # first iteration (epoch == 1 can't be used)\n            dw.writeheader()\n        dw.writerow(rowd)\n",
  "\"\"\" Exponential Moving Average (EMA) of model updates\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport logging\nfrom collections import OrderedDict\nfrom copy import deepcopy\n\nimport torch\nimport torch.nn as nn\n\n_logger = logging.getLogger(__name__)\n\n\nclass ModelEma:\n    \"\"\" Model Exponential Moving Average (DEPRECATED)\n\n    Keep a moving average of everything in the model state_dict (parameters and buffers).\n    This version is deprecated, it does not work with scripted models. Will be removed eventually.\n\n    This is intended to allow functionality like\n    https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n\n    A smoothed version of the weights is necessary for some training schemes to perform well.\n    E.g. Google's hyper-params for training MNASNet, MobileNet-V3, EfficientNet, etc that use\n    RMSprop with a short 2.4-3 epoch decay period and slow LR decay rate of .96-.99 requires EMA\n    smoothing of weights to match results. Pay attention to the decay constant you are using\n    relative to your update count per epoch.\n\n    To keep EMA from using GPU resources, set device='cpu'. This will save a bit of memory but\n    disable validation of the EMA weights. Validation will have to be done manually in a separate\n    process, or after the training stops converging.\n\n    This class is sensitive where it is initialized in the sequence of model init,\n    GPU assignment and distributed training wrappers.\n    \"\"\"\n    def __init__(self, model, decay=0.9999, device='', resume=''):\n        # make a copy of the model for accumulating moving average of weights\n        self.ema = deepcopy(model)\n        self.ema.eval()\n        self.decay = decay\n        self.device = device  # perform ema on different device from model if set\n        if device:\n            self.ema.to(device=device)\n        self.ema_has_module = hasattr(self.ema, 'module')\n        if resume:\n            self._load_checkpoint(resume)\n        for p in self.ema.parameters():\n            p.requires_grad_(False)\n\n    def _load_checkpoint(self, checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n        assert isinstance(checkpoint, dict)\n        if 'state_dict_ema' in checkpoint:\n            new_state_dict = OrderedDict()\n            for k, v in checkpoint['state_dict_ema'].items():\n                # ema model may have been wrapped by DataParallel, and need module prefix\n                if self.ema_has_module:\n                    name = 'module.' + k if not k.startswith('module') else k\n                else:\n                    name = k\n                new_state_dict[name] = v\n            self.ema.load_state_dict(new_state_dict)\n            _logger.info(\"Loaded state_dict_ema\")\n        else:\n            _logger.warning(\"Failed to find state_dict_ema, starting from loaded model weights\")\n\n    def update(self, model):\n        # correct a mismatch in state dict keys\n        needs_module = hasattr(model, 'module') and not self.ema_has_module\n        with torch.no_grad():\n            msd = model.state_dict()\n            for k, ema_v in self.ema.state_dict().items():\n                if needs_module:\n                    k = 'module.' + k\n                model_v = msd[k].detach()\n                if self.device:\n                    model_v = model_v.to(device=self.device)\n                ema_v.copy_(ema_v * self.decay + (1. - self.decay) * model_v)\n\n\nclass ModelEmaV2(nn.Module):\n    \"\"\" Model Exponential Moving Average V2\n\n    Keep a moving average of everything in the model state_dict (parameters and buffers).\n    V2 of this module is simpler, it does not match params/buffers based on name but simply\n    iterates in order. It works with torchscript (JIT of full model).\n\n    This is intended to allow functionality like\n    https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n\n    A smoothed version of the weights is necessary for some training schemes to perform well.\n    E.g. Google's hyper-params for training MNASNet, MobileNet-V3, EfficientNet, etc that use\n    RMSprop with a short 2.4-3 epoch decay period and slow LR decay rate of .96-.99 requires EMA\n    smoothing of weights to match results. Pay attention to the decay constant you are using\n    relative to your update count per epoch.\n\n    To keep EMA from using GPU resources, set device='cpu'. This will save a bit of memory but\n    disable validation of the EMA weights. Validation will have to be done manually in a separate\n    process, or after the training stops converging.\n\n    This class is sensitive where it is initialized in the sequence of model init,\n    GPU assignment and distributed training wrappers.\n    \"\"\"\n    def __init__(self, model, decay=0.9999, device=None):\n        super(ModelEmaV2, self).__init__()\n        # make a copy of the model for accumulating moving average of weights\n        self.module = deepcopy(model)\n        self.module.eval()\n        self.decay = decay\n        self.device = device  # perform ema on different device from model if set\n        if self.device is not None:\n            self.module.to(device=device)\n\n    def _update(self, model, update_fn):\n        with torch.no_grad():\n            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n                if self.device is not None:\n                    model_v = model_v.to(device=self.device)\n                ema_v.copy_(update_fn(ema_v, model_v))\n\n    def update(self, model):\n        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n\n    def set(self, model):\n        self._update(model, update_fn=lambda e, m: m)\n",
  "\"\"\" Batch size decay and retry helpers.\n\nCopyright 2022 Ross Wightman\n\"\"\"\nimport math\n\n\ndef decay_batch_step(batch_size, num_intra_steps=2, no_odd=False):\n    \"\"\" power of two batch-size decay with intra steps\n\n    Decay by stepping between powers of 2:\n    * determine power-of-2 floor of current batch size (base batch size)\n    * divide above value by num_intra_steps to determine step size\n    * floor batch_size to nearest multiple of step_size (from base batch size)\n    Examples:\n     num_steps == 4 --> 64, 56, 48, 40, 32, 28, 24, 20, 16, 14, 12, 10, 8, 7, 6, 5, 4, 3, 2, 1\n     num_steps (no_odd=True) == 4 --> 64, 56, 48, 40, 32, 28, 24, 20, 16, 14, 12, 10, 8, 6, 4, 2\n     num_steps == 2 --> 64, 48, 32, 24, 16, 12, 8, 6, 4, 3, 2, 1\n     num_steps == 1 --> 64, 32, 16, 8, 4, 2, 1\n    \"\"\"\n    if batch_size <= 1:\n        # return 0 for stopping value so easy to use in loop\n        return 0\n    base_batch_size = int(2 ** (math.log(batch_size - 1) // math.log(2)))\n    step_size = max(base_batch_size // num_intra_steps, 1)\n    batch_size = base_batch_size + ((batch_size - base_batch_size - 1) // step_size) * step_size\n    if no_odd and batch_size % 2:\n        batch_size -= 1\n    return batch_size\n\n\ndef check_batch_size_retry(error_str):\n    \"\"\" check failure error string for conditions where batch decay retry should not be attempted\n    \"\"\"\n    error_str = error_str.lower()\n    if 'required rank' in error_str:\n        # Errors involving phrase 'required rank' typically happen when a conv is used that's\n        # not compatible with channels_last memory format.\n        return False\n    if 'illegal' in error_str:\n        # 'Illegal memory access' errors in CUDA typically leave process in unusable state\n        return False\n    return True\n",
  "\"\"\" Adaptive Gradient Clipping\n\nAn impl of AGC, as per (https://arxiv.org/abs/2102.06171):\n\n@article{brock2021high,\n  author={Andrew Brock and Soham De and Samuel L. Smith and Karen Simonyan},\n  title={High-Performance Large-Scale Image Recognition Without Normalization},\n  journal={arXiv preprint arXiv:},\n  year={2021}\n}\n\nCode references:\n  * Official JAX impl (paper authors): https://github.com/deepmind/deepmind-research/tree/master/nfnets\n  * Phil Wang's PyTorch gist: https://gist.github.com/lucidrains/0d6560077edac419ab5d3aa29e674d5c\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport torch\n\n\ndef unitwise_norm(x, norm_type=2.0):\n    if x.ndim <= 1:\n        return x.norm(norm_type)\n    else:\n        # works for nn.ConvNd and nn,Linear where output dim is first in the kernel/weight tensor\n        # might need special cases for other weights (possibly MHA) where this may not be true\n        return x.norm(norm_type, dim=tuple(range(1, x.ndim)), keepdim=True)\n\n\ndef adaptive_clip_grad(parameters, clip_factor=0.01, eps=1e-3, norm_type=2.0):\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    for p in parameters:\n        if p.grad is None:\n            continue\n        p_data = p.detach()\n        g_data = p.grad.detach()\n        max_norm = unitwise_norm(p_data, norm_type=norm_type).clamp_(min=eps).mul_(clip_factor)\n        grad_norm = unitwise_norm(g_data, norm_type=norm_type)\n        clipped_grad = g_data * (max_norm / grad_norm.clamp(min=1e-6))\n        new_grads = torch.where(grad_norm < max_norm, g_data, clipped_grad)\n        p.grad.detach().copy_(new_grads)\n",
  "from typing import Optional, Tuple, List\n\nimport torch\n\n\ndef onnx_forward(onnx_file, example_input):\n    import onnxruntime\n\n    sess_options = onnxruntime.SessionOptions()\n    session = onnxruntime.InferenceSession(onnx_file, sess_options)\n    input_name = session.get_inputs()[0].name\n    output = session.run([], {input_name: example_input.numpy()})\n    output = output[0]\n    return output\n\n\ndef onnx_export(\n        model: torch.nn.Module,\n        output_file: str,\n        example_input: Optional[torch.Tensor] = None,\n        training: bool = False,\n        verbose: bool = False,\n        check: bool = True,\n        check_forward: bool = False,\n        batch_size: int = 64,\n        input_size: Tuple[int, int, int] = None,\n        opset: Optional[int] = None,\n        dynamic_size: bool = False,\n        aten_fallback: bool = False,\n        keep_initializers: Optional[bool] = None,\n        input_names: List[str] = None,\n        output_names: List[str] = None,\n):\n    import onnx\n\n    if training:\n        training_mode = torch.onnx.TrainingMode.TRAINING\n        model.train()\n    else:\n        training_mode = torch.onnx.TrainingMode.EVAL\n        model.eval()\n\n    if example_input is None:\n        if not input_size:\n            assert hasattr(model, 'default_cfg')\n            input_size = model.default_cfg.get('input_size')\n        example_input = torch.randn((batch_size,) + input_size, requires_grad=training)\n\n    # Run model once before export trace, sets padding for models with Conv2dSameExport. This means\n    # that the padding for models with Conv2dSameExport (most models with tf_ prefix) is fixed for\n    # the input img_size specified in this script.\n\n    # Opset >= 11 should allow for dynamic padding, however I cannot get it to work due to\n    # issues in the tracing of the dynamic padding or errors attempting to export the model after jit\n    # scripting it (an approach that should work). Perhaps in a future PyTorch or ONNX versions...\n    original_out = model(example_input)\n\n    input_names = input_names or [\"input0\"]\n    output_names = output_names or [\"output0\"]\n\n    dynamic_axes = {'input0': {0: 'batch'}, 'output0': {0: 'batch'}}\n    if dynamic_size:\n        dynamic_axes['input0'][2] = 'height'\n        dynamic_axes['input0'][3] = 'width'\n\n    if aten_fallback:\n        export_type = torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK\n    else:\n        export_type = torch.onnx.OperatorExportTypes.ONNX\n\n    torch_out = torch.onnx._export(\n        model,\n        example_input,\n        output_file,\n        training=training_mode,\n        export_params=True,\n        verbose=verbose,\n        input_names=input_names,\n        output_names=output_names,\n        keep_initializers_as_inputs=keep_initializers,\n        dynamic_axes=dynamic_axes,\n        opset_version=opset,\n        operator_export_type=export_type\n    )\n\n    if check:\n        onnx_model = onnx.load(output_file)\n        onnx.checker.check_model(onnx_model, full_check=True)  # assuming throw on error\n        if check_forward and not training:\n            import numpy as np\n            onnx_out = onnx_forward(output_file, example_input)\n            np.testing.assert_almost_equal(torch_out.data.numpy(), onnx_out, decimal=3)\n            np.testing.assert_almost_equal(original_out.data.numpy(), torch_out.data.numpy(), decimal=5)\n\n\n",
  "\"\"\" Checkpoint Saver\n\nTrack top-n training checkpoints and maintain recovery checkpoints on specified intervals.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\nimport glob\nimport operator\nimport os\nimport logging\n\nimport torch\n\nfrom .model import unwrap_model, get_state_dict\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass CheckpointSaver:\n    def __init__(\n            self,\n            model,\n            optimizer,\n            args=None,\n            model_ema=None,\n            amp_scaler=None,\n            checkpoint_prefix='checkpoint',\n            recovery_prefix='recovery',\n            checkpoint_dir='',\n            recovery_dir='',\n            decreasing=False,\n            max_history=10,\n            unwrap_fn=unwrap_model):\n\n        # objects to save state_dicts of\n        self.model = model\n        self.optimizer = optimizer\n        self.args = args\n        self.model_ema = model_ema\n        self.amp_scaler = amp_scaler\n\n        # state\n        self.checkpoint_files = []  # (filename, metric) tuples in order of decreasing betterness\n        self.best_epoch = None\n        self.best_metric = None\n        self.curr_recovery_file = ''\n        self.last_recovery_file = ''\n\n        # config\n        self.checkpoint_dir = checkpoint_dir\n        self.recovery_dir = recovery_dir\n        self.save_prefix = checkpoint_prefix\n        self.recovery_prefix = recovery_prefix\n        self.extension = '.pth.tar'\n        self.decreasing = decreasing  # a lower metric is better if True\n        self.cmp = operator.lt if decreasing else operator.gt  # True if lhs better than rhs\n        self.max_history = max_history\n        self.unwrap_fn = unwrap_fn\n        assert self.max_history >= 1\n\n    def save_checkpoint(self, epoch, metric=None):\n        assert epoch >= 0\n        tmp_save_path = os.path.join(self.checkpoint_dir, 'tmp' + self.extension)\n        last_save_path = os.path.join(self.checkpoint_dir, 'last' + self.extension)\n        self._save(tmp_save_path, epoch, metric)\n        if os.path.exists(last_save_path):\n            os.unlink(last_save_path)  # required for Windows support.\n        os.rename(tmp_save_path, last_save_path)\n        worst_file = self.checkpoint_files[-1] if self.checkpoint_files else None\n        if (len(self.checkpoint_files) < self.max_history\n                or metric is None or self.cmp(metric, worst_file[1])):\n            if len(self.checkpoint_files) >= self.max_history:\n                self._cleanup_checkpoints(1)\n            filename = '-'.join([self.save_prefix, str(epoch)]) + self.extension\n            save_path = os.path.join(self.checkpoint_dir, filename)\n            os.link(last_save_path, save_path)\n            self.checkpoint_files.append((save_path, metric))\n            self.checkpoint_files = sorted(\n                self.checkpoint_files, key=lambda x: x[1],\n                reverse=not self.decreasing)  # sort in descending order if a lower metric is not better\n\n            checkpoints_str = \"Current checkpoints:\\n\"\n            for c in self.checkpoint_files:\n                checkpoints_str += ' {}\\n'.format(c)\n            _logger.info(checkpoints_str)\n\n            if metric is not None and (self.best_metric is None or self.cmp(metric, self.best_metric)):\n                self.best_epoch = epoch\n                self.best_metric = metric\n                best_save_path = os.path.join(self.checkpoint_dir, 'model_best' + self.extension)\n                if os.path.exists(best_save_path):\n                    os.unlink(best_save_path)\n                os.link(last_save_path, best_save_path)\n\n        return (None, None) if self.best_metric is None else (self.best_metric, self.best_epoch)\n\n    def _save(self, save_path, epoch, metric=None):\n        save_state = {\n            'epoch': epoch,\n            'arch': type(self.model).__name__.lower(),\n            'state_dict': get_state_dict(self.model, self.unwrap_fn),\n            'optimizer': self.optimizer.state_dict(),\n            'version': 2,  # version < 2 increments epoch before save\n        }\n        if self.args is not None:\n            save_state['arch'] = self.args.model\n            save_state['args'] = self.args\n        if self.amp_scaler is not None:\n            save_state[self.amp_scaler.state_dict_key] = self.amp_scaler.state_dict()\n        if self.model_ema is not None:\n            save_state['state_dict_ema'] = get_state_dict(self.model_ema, self.unwrap_fn)\n        if metric is not None:\n            save_state['metric'] = metric\n        torch.save(save_state, save_path)\n\n    def _cleanup_checkpoints(self, trim=0):\n        trim = min(len(self.checkpoint_files), trim)\n        delete_index = self.max_history - trim\n        if delete_index < 0 or len(self.checkpoint_files) <= delete_index:\n            return\n        to_delete = self.checkpoint_files[delete_index:]\n        for d in to_delete:\n            try:\n                _logger.debug(\"Cleaning checkpoint: {}\".format(d))\n                os.remove(d[0])\n            except Exception as e:\n                _logger.error(\"Exception '{}' while deleting checkpoint\".format(e))\n        self.checkpoint_files = self.checkpoint_files[:delete_index]\n\n    def save_recovery(self, epoch, batch_idx=0):\n        assert epoch >= 0\n        filename = '-'.join([self.recovery_prefix, str(epoch), str(batch_idx)]) + self.extension\n        save_path = os.path.join(self.recovery_dir, filename)\n        self._save(save_path, epoch)\n        if os.path.exists(self.last_recovery_file):\n            try:\n                _logger.debug(\"Cleaning recovery: {}\".format(self.last_recovery_file))\n                os.remove(self.last_recovery_file)\n            except Exception as e:\n                _logger.error(\"Exception '{}' while removing {}\".format(e, self.last_recovery_file))\n        self.last_recovery_file = self.curr_recovery_file\n        self.curr_recovery_file = save_path\n\n    def find_recovery(self):\n        recovery_path = os.path.join(self.recovery_dir, self.recovery_prefix)\n        files = glob.glob(recovery_path + '*' + self.extension)\n        files = sorted(files)\n        return files[0] if len(files) else ''\n",
  "import random\nimport numpy as np\nimport torch\n\n\ndef random_seed(seed=42, rank=0):\n    torch.manual_seed(seed + rank)\n    np.random.seed(seed + rank)\n    random.seed(seed + rank)\n",
  "\"\"\" CUDA / AMP utils\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\n\ntry:\n    from apex import amp\n    has_apex = True\nexcept ImportError:\n    amp = None\n    has_apex = False\n\nfrom .clip_grad import dispatch_clip_grad\n\n\nclass ApexScaler:\n    state_dict_key = \"amp\"\n\n    def __call__(\n            self,\n            loss,\n            optimizer,\n            clip_grad=None,\n            clip_mode='norm',\n            parameters=None,\n            create_graph=False,\n            need_update=True,\n    ):\n        with amp.scale_loss(loss, optimizer) as scaled_loss:\n            scaled_loss.backward(create_graph=create_graph)\n        if need_update:\n            if clip_grad is not None:\n                dispatch_clip_grad(amp.master_params(optimizer), clip_grad, mode=clip_mode)\n            optimizer.step()\n\n    def state_dict(self):\n        if 'state_dict' in amp.__dict__:\n            return amp.state_dict()\n\n    def load_state_dict(self, state_dict):\n        if 'load_state_dict' in amp.__dict__:\n            amp.load_state_dict(state_dict)\n\n\nclass NativeScaler:\n    state_dict_key = \"amp_scaler\"\n\n    def __init__(self):\n        self._scaler = torch.cuda.amp.GradScaler()\n\n    def __call__(\n            self,\n            loss,\n            optimizer,\n            clip_grad=None,\n            clip_mode='norm',\n            parameters=None,\n            create_graph=False,\n            need_update=True,\n    ):\n        self._scaler.scale(loss).backward(create_graph=create_graph)\n        if need_update:\n            if clip_grad is not None:\n                assert parameters is not None\n                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n                dispatch_clip_grad(parameters, clip_grad, mode=clip_mode)\n            self._scaler.step(optimizer)\n            self._scaler.update()\n\n    def state_dict(self):\n        return self._scaler.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self._scaler.load_state_dict(state_dict)\n",
  "\"\"\" Misc utils\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport argparse\nimport ast\nimport re\n\n\ndef natural_key(string_):\n    \"\"\"See http://www.codinghorror.com/blog/archives/001018.html\"\"\"\n    return [int(s) if s.isdigit() else s for s in re.split(r'(\\d+)', string_.lower())]\n\n\ndef add_bool_arg(parser, name, default=False, help=''):\n    dest_name = name.replace('-', '_')\n    group = parser.add_mutually_exclusive_group(required=False)\n    group.add_argument('--' + name, dest=dest_name, action='store_true', help=help)\n    group.add_argument('--no-' + name, dest=dest_name, action='store_false', help=help)\n    parser.set_defaults(**{dest_name: default})\n\n\nclass ParseKwargs(argparse.Action):\n    def __call__(self, parser, namespace, values, option_string=None):\n        kw = {}\n        for value in values:\n            key, value = value.split('=')\n            try:\n                kw[key] = ast.literal_eval(value)\n            except ValueError:\n                kw[key] = str(value)  # fallback to string (avoid need to escape on command line)\n        setattr(namespace, self.dest, kw)\n",
  "\"\"\" Logging helpers\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport logging\nimport logging.handlers\n\n\nclass FormatterNoInfo(logging.Formatter):\n    def __init__(self, fmt='%(levelname)s: %(message)s'):\n        logging.Formatter.__init__(self, fmt)\n\n    def format(self, record):\n        if record.levelno == logging.INFO:\n            return str(record.getMessage())\n        return logging.Formatter.format(self, record)\n\n\ndef setup_default_logging(default_level=logging.INFO, log_path=''):\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(FormatterNoInfo())\n    logging.root.addHandler(console_handler)\n    logging.root.setLevel(default_level)\n    if log_path:\n        file_handler = logging.handlers.RotatingFileHandler(log_path, maxBytes=(1024 ** 2 * 2), backupCount=3)\n        file_formatter = logging.Formatter(\"%(asctime)s - %(name)20s: [%(levelname)8s] - %(message)s\")\n        file_handler.setFormatter(file_formatter)\n        logging.root.addHandler(file_handler)\n",
  "\"\"\" Polynomial Scheduler\n\nPolynomial LR schedule with warmup, noise.\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport math\nimport logging\n\nimport torch\n\nfrom .scheduler import Scheduler\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass PolyLRScheduler(Scheduler):\n    \"\"\" Polynomial LR Scheduler w/ warmup, noise, and k-decay\n\n    k-decay option based on `k-decay: A New Method For Learning Rate Schedule` - https://arxiv.org/abs/2004.05909\n    \"\"\"\n\n    def __init__(\n            self,\n            optimizer: torch.optim.Optimizer,\n            t_initial: int,\n            power: float = 0.5,\n            lr_min: float = 0.,\n            cycle_mul: float = 1.,\n            cycle_decay: float = 1.,\n            cycle_limit: int = 1,\n            warmup_t=0,\n            warmup_lr_init=0,\n            warmup_prefix=False,\n            t_in_epochs=True,\n            noise_range_t=None,\n            noise_pct=0.67,\n            noise_std=1.0,\n            noise_seed=42,\n            k_decay=1.0,\n            initialize=True,\n    ) -> None:\n        super().__init__(\n            optimizer,\n            param_group_field=\"lr\",\n            t_in_epochs=t_in_epochs,\n            noise_range_t=noise_range_t,\n            noise_pct=noise_pct,\n            noise_std=noise_std,\n            noise_seed=noise_seed,\n            initialize=initialize\n        )\n\n        assert t_initial > 0\n        assert lr_min >= 0\n        if t_initial == 1 and cycle_mul == 1 and cycle_decay == 1:\n            _logger.warning(\"Cosine annealing scheduler will have no effect on the learning \"\n                            \"rate since t_initial = t_mul = eta_mul = 1.\")\n        self.t_initial = t_initial\n        self.power = power\n        self.lr_min = lr_min\n        self.cycle_mul = cycle_mul\n        self.cycle_decay = cycle_decay\n        self.cycle_limit = cycle_limit\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.warmup_prefix = warmup_prefix\n        self.k_decay = k_decay\n        if self.warmup_t:\n            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in self.base_values]\n            super().update_groups(self.warmup_lr_init)\n        else:\n            self.warmup_steps = [1 for _ in self.base_values]\n\n    def _get_lr(self, t):\n        if t < self.warmup_t:\n            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n        else:\n            if self.warmup_prefix:\n                t = t - self.warmup_t\n\n            if self.cycle_mul != 1:\n                i = math.floor(math.log(1 - t / self.t_initial * (1 - self.cycle_mul), self.cycle_mul))\n                t_i = self.cycle_mul ** i * self.t_initial\n                t_curr = t - (1 - self.cycle_mul ** i) / (1 - self.cycle_mul) * self.t_initial\n            else:\n                i = t // self.t_initial\n                t_i = self.t_initial\n                t_curr = t - (self.t_initial * i)\n\n            gamma = self.cycle_decay ** i\n            lr_max_values = [v * gamma for v in self.base_values]\n            k = self.k_decay\n\n            if i < self.cycle_limit:\n                lrs = [\n                    self.lr_min + (lr_max - self.lr_min) * (1 - t_curr ** k / t_i ** k) ** self.power\n                    for lr_max in lr_max_values\n                ]\n            else:\n                lrs = [self.lr_min for _ in self.base_values]\n\n        return lrs\n\n    def get_cycle_length(self, cycles=0):\n        cycles = max(1, cycles or self.cycle_limit)\n        if self.cycle_mul == 1.0:\n            return self.t_initial * cycles\n        else:\n            return int(math.floor(-self.t_initial * (self.cycle_mul ** cycles - 1) / (1 - self.cycle_mul)))\n",
  "from .cosine_lr import CosineLRScheduler\nfrom .multistep_lr import MultiStepLRScheduler\nfrom .plateau_lr import PlateauLRScheduler\nfrom .poly_lr import PolyLRScheduler\nfrom .step_lr import StepLRScheduler\nfrom .tanh_lr import TanhLRScheduler\n\nfrom .scheduler_factory import create_scheduler, create_scheduler_v2, scheduler_kwargs\n",
  "\"\"\" Plateau Scheduler\n\nAdapts PyTorch plateau scheduler and allows application of noise, warmup.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch\n\nfrom .scheduler import Scheduler\n\n\nclass PlateauLRScheduler(Scheduler):\n    \"\"\"Decay the LR by a factor every time the validation loss plateaus.\"\"\"\n\n    def __init__(\n            self,\n            optimizer,\n            decay_rate=0.1,\n            patience_t=10,\n            verbose=True,\n            threshold=1e-4,\n            cooldown_t=0,\n            warmup_t=0,\n            warmup_lr_init=0,\n            lr_min=0,\n            mode='max',\n            noise_range_t=None,\n            noise_type='normal',\n            noise_pct=0.67,\n            noise_std=1.0,\n            noise_seed=None,\n            initialize=True,\n    ):\n        super().__init__(\n            optimizer,\n            'lr',\n            noise_range_t=noise_range_t,\n            noise_type=noise_type,\n            noise_pct=noise_pct,\n            noise_std=noise_std,\n            noise_seed=noise_seed,\n            initialize=initialize,\n        )\n\n        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            self.optimizer,\n            patience=patience_t,\n            factor=decay_rate,\n            verbose=verbose,\n            threshold=threshold,\n            cooldown=cooldown_t,\n            mode=mode,\n            min_lr=lr_min\n        )\n\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        if self.warmup_t:\n            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in self.base_values]\n            super().update_groups(self.warmup_lr_init)\n        else:\n            self.warmup_steps = [1 for _ in self.base_values]\n        self.restore_lr = None\n\n    def state_dict(self):\n        return {\n            'best': self.lr_scheduler.best,\n            'last_epoch': self.lr_scheduler.last_epoch,\n        }\n\n    def load_state_dict(self, state_dict):\n        self.lr_scheduler.best = state_dict['best']\n        if 'last_epoch' in state_dict:\n            self.lr_scheduler.last_epoch = state_dict['last_epoch']\n\n    # override the base class step fn completely\n    def step(self, epoch, metric=None):\n        if epoch <= self.warmup_t:\n            lrs = [self.warmup_lr_init + epoch * s for s in self.warmup_steps]\n            super().update_groups(lrs)\n        else:\n            if self.restore_lr is not None:\n                # restore actual LR from before our last noise perturbation before stepping base\n                for i, param_group in enumerate(self.optimizer.param_groups):\n                    param_group['lr'] = self.restore_lr[i]\n                self.restore_lr = None\n\n            self.lr_scheduler.step(metric, epoch)  # step the base scheduler\n\n            if self._is_apply_noise(epoch):\n                self._apply_noise(epoch)\n\n    def step_update(self, num_updates: int, metric: float = None):\n        return None\n\n    def _apply_noise(self, epoch):\n        noise = self._calculate_noise(epoch)\n\n        # apply the noise on top of previous LR, cache the old value so we can restore for normal\n        # stepping of base scheduler\n        restore_lr = []\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            old_lr = float(param_group['lr'])\n            restore_lr.append(old_lr)\n            new_lr = old_lr + old_lr * noise\n            param_group['lr'] = new_lr\n        self.restore_lr = restore_lr\n\n    def _get_lr(self, t: int) -> float:\n        assert False, 'should not be called as step is overridden'\n",
  "\"\"\" Scheduler Factory\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nfrom typing import List, Union\n\nfrom torch.optim import Optimizer\n\nfrom .cosine_lr import CosineLRScheduler\nfrom .multistep_lr import MultiStepLRScheduler\nfrom .plateau_lr import PlateauLRScheduler\nfrom .poly_lr import PolyLRScheduler\nfrom .step_lr import StepLRScheduler\nfrom .tanh_lr import TanhLRScheduler\n\n\ndef scheduler_kwargs(cfg):\n    \"\"\" cfg/argparse to kwargs helper\n    Convert scheduler args in argparse args or cfg (.dot) like object to keyword args.\n    \"\"\"\n    eval_metric = getattr(cfg, 'eval_metric', 'top1')\n    plateau_mode = 'min' if 'loss' in eval_metric else 'max'\n    kwargs = dict(\n        sched=cfg.sched,\n        num_epochs=getattr(cfg, 'epochs', 100),\n        decay_epochs=getattr(cfg, 'decay_epochs', 30),\n        decay_milestones=getattr(cfg, 'decay_milestones', [30, 60]),\n        warmup_epochs=getattr(cfg, 'warmup_epochs', 5),\n        cooldown_epochs=getattr(cfg, 'cooldown_epochs', 0),\n        patience_epochs=getattr(cfg, 'patience_epochs', 10),\n        decay_rate=getattr(cfg, 'decay_rate', 0.1),\n        min_lr=getattr(cfg, 'min_lr', 0.),\n        warmup_lr=getattr(cfg, 'warmup_lr', 1e-5),\n        warmup_prefix=getattr(cfg, 'warmup_prefix', False),\n        noise=getattr(cfg, 'lr_noise', None),\n        noise_pct=getattr(cfg, 'lr_noise_pct', 0.67),\n        noise_std=getattr(cfg, 'lr_noise_std', 1.),\n        noise_seed=getattr(cfg, 'seed', 42),\n        cycle_mul=getattr(cfg, 'lr_cycle_mul', 1.),\n        cycle_decay=getattr(cfg, 'lr_cycle_decay', 0.1),\n        cycle_limit=getattr(cfg, 'lr_cycle_limit', 1),\n        k_decay=getattr(cfg, 'lr_k_decay', 1.0),\n        plateau_mode=plateau_mode,\n        step_on_epochs=not getattr(cfg, 'sched_on_updates', False),\n    )\n    return kwargs\n\n\ndef create_scheduler(\n        args,\n        optimizer: Optimizer,\n        updates_per_epoch: int = 0,\n):\n    return create_scheduler_v2(\n        optimizer=optimizer,\n        **scheduler_kwargs(args),\n        updates_per_epoch=updates_per_epoch,\n    )\n\n\ndef create_scheduler_v2(\n        optimizer: Optimizer,\n        sched: str = 'cosine',\n        num_epochs: int = 300,\n        decay_epochs: int = 90,\n        decay_milestones: List[int] = (90, 180, 270),\n        cooldown_epochs: int = 0,\n        patience_epochs: int = 10,\n        decay_rate: float = 0.1,\n        min_lr: float = 0,\n        warmup_lr: float = 1e-5,\n        warmup_epochs: int = 0,\n        warmup_prefix: bool = False,\n        noise: Union[float, List[float]] = None,\n        noise_pct: float = 0.67,\n        noise_std: float = 1.,\n        noise_seed: int = 42,\n        cycle_mul: float = 1.,\n        cycle_decay: float = 0.1,\n        cycle_limit: int = 1,\n        k_decay: float = 1.0,\n        plateau_mode: str = 'max',\n        step_on_epochs: bool = True,\n        updates_per_epoch: int = 0,\n):\n    t_initial = num_epochs\n    warmup_t = warmup_epochs\n    decay_t = decay_epochs\n    cooldown_t = cooldown_epochs\n\n    if not step_on_epochs:\n        assert updates_per_epoch > 0, 'updates_per_epoch must be set to number of dataloader batches'\n        t_initial = t_initial * updates_per_epoch\n        warmup_t = warmup_t * updates_per_epoch\n        decay_t = decay_t * updates_per_epoch\n        decay_milestones = [d * updates_per_epoch for d in decay_milestones]\n        cooldown_t = cooldown_t * updates_per_epoch\n\n    # warmup args\n    warmup_args = dict(\n        warmup_lr_init=warmup_lr,\n        warmup_t=warmup_t,\n        warmup_prefix=warmup_prefix,\n    )\n\n    # setup noise args for supporting schedulers\n    if noise is not None:\n        if isinstance(noise, (list, tuple)):\n            noise_range = [n * t_initial for n in noise]\n            if len(noise_range) == 1:\n                noise_range = noise_range[0]\n        else:\n            noise_range = noise * t_initial\n    else:\n        noise_range = None\n    noise_args = dict(\n        noise_range_t=noise_range,\n        noise_pct=noise_pct,\n        noise_std=noise_std,\n        noise_seed=noise_seed,\n    )\n\n    # setup cycle args for supporting schedulers\n    cycle_args = dict(\n        cycle_mul=cycle_mul,\n        cycle_decay=cycle_decay,\n        cycle_limit=cycle_limit,\n    )\n\n    lr_scheduler = None\n    if sched == 'cosine':\n        lr_scheduler = CosineLRScheduler(\n            optimizer,\n            t_initial=t_initial,\n            lr_min=min_lr,\n            t_in_epochs=step_on_epochs,\n            **cycle_args,\n            **warmup_args,\n            **noise_args,\n            k_decay=k_decay,\n        )\n    elif sched == 'tanh':\n        lr_scheduler = TanhLRScheduler(\n            optimizer,\n            t_initial=t_initial,\n            lr_min=min_lr,\n            t_in_epochs=step_on_epochs,\n            **cycle_args,\n            **warmup_args,\n            **noise_args,\n        )\n    elif sched == 'step':\n        lr_scheduler = StepLRScheduler(\n            optimizer,\n            decay_t=decay_t,\n            decay_rate=decay_rate,\n            t_in_epochs=step_on_epochs,\n            **warmup_args,\n            **noise_args,\n        )\n    elif sched == 'multistep':\n        lr_scheduler = MultiStepLRScheduler(\n            optimizer,\n            decay_t=decay_milestones,\n            decay_rate=decay_rate,\n            t_in_epochs=step_on_epochs,\n            **warmup_args,\n            **noise_args,\n        )\n    elif sched == 'plateau':\n        assert step_on_epochs, 'Plateau LR only supports step per epoch.'\n        warmup_args.pop('warmup_prefix', False)\n        lr_scheduler = PlateauLRScheduler(\n            optimizer,\n            decay_rate=decay_rate,\n            patience_t=patience_epochs,\n            cooldown_t=0,\n            **warmup_args,\n            lr_min=min_lr,\n            mode=plateau_mode,\n            **noise_args,\n        )\n    elif sched == 'poly':\n        lr_scheduler = PolyLRScheduler(\n            optimizer,\n            power=decay_rate,  # overloading 'decay_rate' as polynomial power\n            t_initial=t_initial,\n            lr_min=min_lr,\n            t_in_epochs=step_on_epochs,\n            k_decay=k_decay,\n            **cycle_args,\n            **warmup_args,\n            **noise_args,\n        )\n\n    if hasattr(lr_scheduler, 'get_cycle_length'):\n        # for cycle based schedulers (cosine, tanh, poly) recalculate total epochs w/ cycles & cooldown\n        t_with_cycles_and_cooldown = lr_scheduler.get_cycle_length() + cooldown_t\n        if step_on_epochs:\n            num_epochs = t_with_cycles_and_cooldown\n        else:\n            num_epochs = t_with_cycles_and_cooldown // updates_per_epoch\n\n    return lr_scheduler, num_epochs\n",
  "\"\"\" Cosine Scheduler\n\nCosine LR schedule with warmup, cycle/restarts, noise, k-decay.\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport logging\nimport math\nimport numpy as np\nimport torch\n\nfrom .scheduler import Scheduler\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass CosineLRScheduler(Scheduler):\n    \"\"\"\n    Cosine decay with restarts.\n    This is described in the paper https://arxiv.org/abs/1608.03983.\n\n    Inspiration from\n    https://github.com/allenai/allennlp/blob/master/allennlp/training/learning_rate_schedulers/cosine.py\n\n    k-decay option based on `k-decay: A New Method For Learning Rate Schedule` - https://arxiv.org/abs/2004.05909\n    \"\"\"\n\n    def __init__(\n            self,\n            optimizer: torch.optim.Optimizer,\n            t_initial: int,\n            lr_min: float = 0.,\n            cycle_mul: float = 1.,\n            cycle_decay: float = 1.,\n            cycle_limit: int = 1,\n            warmup_t=0,\n            warmup_lr_init=0,\n            warmup_prefix=False,\n            t_in_epochs=True,\n            noise_range_t=None,\n            noise_pct=0.67,\n            noise_std=1.0,\n            noise_seed=42,\n            k_decay=1.0,\n            initialize=True,\n    ) -> None:\n        super().__init__(\n            optimizer,\n            param_group_field=\"lr\",\n            t_in_epochs=t_in_epochs,\n            noise_range_t=noise_range_t,\n            noise_pct=noise_pct,\n            noise_std=noise_std,\n            noise_seed=noise_seed,\n            initialize=initialize,\n        )\n\n        assert t_initial > 0\n        assert lr_min >= 0\n        if t_initial == 1 and cycle_mul == 1 and cycle_decay == 1:\n            _logger.warning(\n                \"Cosine annealing scheduler will have no effect on the learning \"\n                \"rate since t_initial = t_mul = eta_mul = 1.\")\n        self.t_initial = t_initial\n        self.lr_min = lr_min\n        self.cycle_mul = cycle_mul\n        self.cycle_decay = cycle_decay\n        self.cycle_limit = cycle_limit\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.warmup_prefix = warmup_prefix\n        self.k_decay = k_decay\n        if self.warmup_t:\n            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in self.base_values]\n            super().update_groups(self.warmup_lr_init)\n        else:\n            self.warmup_steps = [1 for _ in self.base_values]\n\n    def _get_lr(self, t):\n        if t < self.warmup_t:\n            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n        else:\n            if self.warmup_prefix:\n                t = t - self.warmup_t\n\n            if self.cycle_mul != 1:\n                i = math.floor(math.log(1 - t / self.t_initial * (1 - self.cycle_mul), self.cycle_mul))\n                t_i = self.cycle_mul ** i * self.t_initial\n                t_curr = t - (1 - self.cycle_mul ** i) / (1 - self.cycle_mul) * self.t_initial\n            else:\n                i = t // self.t_initial\n                t_i = self.t_initial\n                t_curr = t - (self.t_initial * i)\n\n            gamma = self.cycle_decay ** i\n            lr_max_values = [v * gamma for v in self.base_values]\n            k = self.k_decay\n\n            if i < self.cycle_limit:\n                lrs = [\n                    self.lr_min + 0.5 * (lr_max - self.lr_min) * (1 + math.cos(math.pi * t_curr ** k / t_i ** k))\n                    for lr_max in lr_max_values\n                ]\n            else:\n                lrs = [self.lr_min for _ in self.base_values]\n\n        return lrs\n\n    def get_cycle_length(self, cycles=0):\n        cycles = max(1, cycles or self.cycle_limit)\n        if self.cycle_mul == 1.0:\n            return self.t_initial * cycles\n        else:\n            return int(math.floor(-self.t_initial * (self.cycle_mul ** cycles - 1) / (1 - self.cycle_mul)))\n",
  "\"\"\" MultiStep LR Scheduler\n\nBasic multi step LR schedule with warmup, noise.\n\"\"\"\nimport torch\nimport bisect\nfrom timm.scheduler.scheduler import Scheduler\nfrom typing import List\n\nclass MultiStepLRScheduler(Scheduler):\n    \"\"\"\n    \"\"\"\n\n    def __init__(\n            self,\n            optimizer: torch.optim.Optimizer,\n            decay_t: List[int],\n            decay_rate: float = 1.,\n            warmup_t=0,\n            warmup_lr_init=0,\n            warmup_prefix=True,\n            t_in_epochs=True,\n            noise_range_t=None,\n            noise_pct=0.67,\n            noise_std=1.0,\n            noise_seed=42,\n            initialize=True,\n    ) -> None:\n        super().__init__(\n            optimizer,\n            param_group_field=\"lr\",\n            t_in_epochs=t_in_epochs,\n            noise_range_t=noise_range_t,\n            noise_pct=noise_pct,\n            noise_std=noise_std,\n            noise_seed=noise_seed,\n            initialize=initialize,\n        )\n\n        self.decay_t = decay_t\n        self.decay_rate = decay_rate\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.warmup_prefix = warmup_prefix\n        if self.warmup_t:\n            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in self.base_values]\n            super().update_groups(self.warmup_lr_init)\n        else:\n            self.warmup_steps = [1 for _ in self.base_values]\n\n    def get_curr_decay_steps(self, t):\n        # find where in the array t goes,\n        # assumes self.decay_t is sorted\n        return bisect.bisect_right(self.decay_t, t + 1)\n\n    def _get_lr(self, t):\n        if t < self.warmup_t:\n            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n        else:\n            if self.warmup_prefix:\n                t = t - self.warmup_t\n            lrs = [v * (self.decay_rate ** self.get_curr_decay_steps(t)) for v in self.base_values]\n        return lrs\n",
  "import abc\nfrom abc import ABC\nfrom typing import Any, Dict, Optional\n\nimport torch\n\n\nclass Scheduler(ABC):\n    \"\"\" Parameter Scheduler Base Class\n    A scheduler base class that can be used to schedule any optimizer parameter groups.\n\n    Unlike the builtin PyTorch schedulers, this is intended to be consistently called\n    * At the END of each epoch, before incrementing the epoch count, to calculate next epoch's value\n    * At the END of each optimizer update, after incrementing the update count, to calculate next update's value\n\n    The schedulers built on this should try to remain as stateless as possible (for simplicity).\n\n    This family of schedulers is attempting to avoid the confusion of the meaning of 'last_epoch'\n    and -1 values for special behaviour. All epoch and update counts must be tracked in the training\n    code and explicitly passed in to the schedulers on the corresponding step or step_update call.\n\n    Based on ideas from:\n     * https://github.com/pytorch/fairseq/tree/master/fairseq/optim/lr_scheduler\n     * https://github.com/allenai/allennlp/tree/master/allennlp/training/learning_rate_schedulers\n    \"\"\"\n\n    def __init__(\n            self,\n            optimizer: torch.optim.Optimizer,\n            param_group_field: str,\n            t_in_epochs: bool = True,\n            noise_range_t=None,\n            noise_type='normal',\n            noise_pct=0.67,\n            noise_std=1.0,\n            noise_seed=None,\n            initialize: bool = True,\n    ) -> None:\n        self.optimizer = optimizer\n        self.param_group_field = param_group_field\n        self._initial_param_group_field = f\"initial_{param_group_field}\"\n        if initialize:\n            for i, group in enumerate(self.optimizer.param_groups):\n                if param_group_field not in group:\n                    raise KeyError(f\"{param_group_field} missing from param_groups[{i}]\")\n                group.setdefault(self._initial_param_group_field, group[param_group_field])\n        else:\n            for i, group in enumerate(self.optimizer.param_groups):\n                if self._initial_param_group_field not in group:\n                    raise KeyError(f\"{self._initial_param_group_field} missing from param_groups[{i}]\")\n        self.base_values = [group[self._initial_param_group_field] for group in self.optimizer.param_groups]\n        self.metric = None  # any point to having this for all?\n        self.t_in_epochs = t_in_epochs\n        self.noise_range_t = noise_range_t\n        self.noise_pct = noise_pct\n        self.noise_type = noise_type\n        self.noise_std = noise_std\n        self.noise_seed = noise_seed if noise_seed is not None else 42\n        self.update_groups(self.base_values)\n\n    def state_dict(self) -> Dict[str, Any]:\n        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n        self.__dict__.update(state_dict)\n\n    @abc.abstractmethod\n    def _get_lr(self, t: int) -> float:\n        pass\n\n    def _get_values(self, t: int, on_epoch: bool = True) -> Optional[float]:\n        proceed = (on_epoch and self.t_in_epochs) or (not on_epoch and not self.t_in_epochs)\n        if not proceed:\n            return None\n        return self._get_lr(t)\n\n    def step(self, epoch: int, metric: float = None) -> None:\n        self.metric = metric\n        values = self._get_values(epoch, on_epoch=True)\n        if values is not None:\n            values = self._add_noise(values, epoch)\n            self.update_groups(values)\n\n    def step_update(self, num_updates: int, metric: float = None):\n        self.metric = metric\n        values = self._get_values(num_updates, on_epoch=False)\n        if values is not None:\n            values = self._add_noise(values, num_updates)\n            self.update_groups(values)\n\n    def update_groups(self, values):\n        if not isinstance(values, (list, tuple)):\n            values = [values] * len(self.optimizer.param_groups)\n        for param_group, value in zip(self.optimizer.param_groups, values):\n            if 'lr_scale' in param_group:\n                param_group[self.param_group_field] = value * param_group['lr_scale']\n            else:\n                param_group[self.param_group_field] = value\n\n    def _add_noise(self, lrs, t):\n        if self._is_apply_noise(t):\n            noise = self._calculate_noise(t)\n            lrs = [v + v * noise for v in lrs]\n        return lrs\n\n    def _is_apply_noise(self, t) -> bool:\n        \"\"\"Return True if scheduler in noise range.\"\"\"\n        apply_noise = False\n        if self.noise_range_t is not None:\n            if isinstance(self.noise_range_t, (list, tuple)):\n                apply_noise = self.noise_range_t[0] <= t < self.noise_range_t[1]\n            else:\n                apply_noise = t >= self.noise_range_t\n        return apply_noise\n\n    def _calculate_noise(self, t) -> float:\n        g = torch.Generator()\n        g.manual_seed(self.noise_seed + t)\n        if self.noise_type == 'normal':\n            while True:\n                # resample if noise out of percent limit, brute force but shouldn't spin much\n                noise = torch.randn(1, generator=g).item()\n                if abs(noise) < self.noise_pct:\n                    return noise\n        else:\n            noise = 2 * (torch.rand(1, generator=g).item() - 0.5) * self.noise_pct\n        return noise\n",
  "\"\"\" TanH Scheduler\n\nTanH schedule with warmup, cycle/restarts, noise.\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport logging\nimport math\nimport numpy as np\nimport torch\n\nfrom .scheduler import Scheduler\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass TanhLRScheduler(Scheduler):\n    \"\"\"\n    Hyberbolic-Tangent decay with restarts.\n    This is described in the paper https://arxiv.org/abs/1806.01593\n    \"\"\"\n\n    def __init__(\n            self,\n            optimizer: torch.optim.Optimizer,\n            t_initial: int,\n            lb: float = -7.,\n            ub: float = 3.,\n            lr_min: float = 0.,\n            cycle_mul: float = 1.,\n            cycle_decay: float = 1.,\n            cycle_limit: int = 1,\n            warmup_t=0,\n            warmup_lr_init=0,\n            warmup_prefix=False,\n            t_in_epochs=True,\n            noise_range_t=None,\n            noise_pct=0.67,\n            noise_std=1.0,\n            noise_seed=42,\n            initialize=True,\n    ) -> None:\n        super().__init__(\n            optimizer,\n            param_group_field=\"lr\",\n            t_in_epochs=t_in_epochs,\n            noise_range_t=noise_range_t,\n            noise_pct=noise_pct,\n            noise_std=noise_std,\n            noise_seed=noise_seed,\n            initialize=initialize,\n        )\n\n        assert t_initial > 0\n        assert lr_min >= 0\n        assert lb < ub\n        assert cycle_limit >= 0\n        assert warmup_t >= 0\n        assert warmup_lr_init >= 0\n        self.lb = lb\n        self.ub = ub\n        self.t_initial = t_initial\n        self.lr_min = lr_min\n        self.cycle_mul = cycle_mul\n        self.cycle_decay = cycle_decay\n        self.cycle_limit = cycle_limit\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.warmup_prefix = warmup_prefix\n        if self.warmup_t:\n            t_v = self.base_values if self.warmup_prefix else self._get_lr(self.warmup_t)\n            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in t_v]\n            super().update_groups(self.warmup_lr_init)\n        else:\n            self.warmup_steps = [1 for _ in self.base_values]\n\n    def _get_lr(self, t):\n        if t < self.warmup_t:\n            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n        else:\n            if self.warmup_prefix:\n                t = t - self.warmup_t\n\n            if self.cycle_mul != 1:\n                i = math.floor(math.log(1 - t / self.t_initial * (1 - self.cycle_mul), self.cycle_mul))\n                t_i = self.cycle_mul ** i * self.t_initial\n                t_curr = t - (1 - self.cycle_mul ** i) / (1 - self.cycle_mul) * self.t_initial\n            else:\n                i = t // self.t_initial\n                t_i = self.t_initial\n                t_curr = t - (self.t_initial * i)\n\n            if i < self.cycle_limit:\n                gamma = self.cycle_decay ** i\n                lr_max_values = [v * gamma for v in self.base_values]\n\n                tr = t_curr / t_i\n                lrs = [\n                    self.lr_min + 0.5 * (lr_max - self.lr_min) * (1 - math.tanh(self.lb * (1. - tr) + self.ub * tr))\n                    for lr_max in lr_max_values\n                ]\n            else:\n                lrs = [self.lr_min for _ in self.base_values]\n        return lrs\n\n    def get_cycle_length(self, cycles=0):\n        cycles = max(1, cycles or self.cycle_limit)\n        if self.cycle_mul == 1.0:\n            return self.t_initial * cycles\n        else:\n            return int(math.floor(-self.t_initial * (self.cycle_mul ** cycles - 1) / (1 - self.cycle_mul)))\n",
  "\"\"\" Step Scheduler\n\nBasic step LR schedule with warmup, noise.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nimport torch\n\nfrom .scheduler import Scheduler\n\n\nclass StepLRScheduler(Scheduler):\n    \"\"\"\n    \"\"\"\n\n    def __init__(\n            self,\n            optimizer: torch.optim.Optimizer,\n            decay_t: float,\n            decay_rate: float = 1.,\n            warmup_t=0,\n            warmup_lr_init=0,\n            warmup_prefix=True,\n            t_in_epochs=True,\n            noise_range_t=None,\n            noise_pct=0.67,\n            noise_std=1.0,\n            noise_seed=42,\n            initialize=True,\n    ) -> None:\n        super().__init__(\n            optimizer,\n            param_group_field=\"lr\",\n            t_in_epochs=t_in_epochs,\n            noise_range_t=noise_range_t,\n            noise_pct=noise_pct,\n            noise_std=noise_std,\n            noise_seed=noise_seed,\n            initialize=initialize,\n        )\n\n        self.decay_t = decay_t\n        self.decay_rate = decay_rate\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.warmup_prefix = warmup_prefix\n        if self.warmup_t:\n            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in self.base_values]\n            super().update_groups(self.warmup_lr_init)\n        else:\n            self.warmup_steps = [1 for _ in self.base_values]\n\n    def _get_lr(self, t):\n        if t < self.warmup_t:\n            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n        else:\n            if self.warmup_prefix:\n                t = t - self.warmup_t\n            lrs = [v * (self.decay_rate ** (t // self.decay_t)) for v in self.base_values]\n        return lrs\n",
  "from .auto_augment import RandAugment, AutoAugment, rand_augment_ops, auto_augment_policy,\\\n    rand_augment_transform, auto_augment_transform\nfrom .config import resolve_data_config, resolve_model_data_config\nfrom .constants import *\nfrom .dataset import ImageDataset, IterableImageDataset, AugMixDataset\nfrom .dataset_factory import create_dataset\nfrom .dataset_info import DatasetInfo, CustomDatasetInfo\nfrom .imagenet_info import ImageNetInfo, infer_imagenet_subset\nfrom .loader import create_loader\nfrom .mixup import Mixup, FastCollateMixup\nfrom .readers import create_reader\nfrom .readers import get_img_extensions, is_img_extension, set_img_extensions, add_img_extensions, del_img_extensions\nfrom .real_labels import RealLabelsImagenet\nfrom .transforms import *\nfrom .transforms_factory import create_transform\n",
  "import logging\nfrom .constants import *\n\n\n_logger = logging.getLogger(__name__)\n\n\ndef resolve_data_config(\n        args=None,\n        pretrained_cfg=None,\n        model=None,\n        use_test_size=False,\n        verbose=False\n):\n    assert model or args or pretrained_cfg, \"At least one of model, args, or pretrained_cfg required for data config.\"\n    args = args or {}\n    pretrained_cfg = pretrained_cfg or {}\n    if not pretrained_cfg and model is not None and hasattr(model, 'pretrained_cfg'):\n        pretrained_cfg = model.pretrained_cfg\n    data_config = {}\n\n    # Resolve input/image size\n    in_chans = 3\n    if args.get('chans', None) is not None:\n        in_chans = args['chans']\n\n    input_size = (in_chans, 224, 224)\n    if args.get('input_size', None) is not None:\n        assert isinstance(args['input_size'], (tuple, list))\n        assert len(args['input_size']) == 3\n        input_size = tuple(args['input_size'])\n        in_chans = input_size[0]  # input_size overrides in_chans\n    elif args.get('img_size', None) is not None:\n        assert isinstance(args['img_size'], int)\n        input_size = (in_chans, args['img_size'], args['img_size'])\n    else:\n        if use_test_size and pretrained_cfg.get('test_input_size', None) is not None:\n            input_size = pretrained_cfg['test_input_size']\n        elif pretrained_cfg.get('input_size', None) is not None:\n            input_size = pretrained_cfg['input_size']\n    data_config['input_size'] = input_size\n\n    # resolve interpolation method\n    data_config['interpolation'] = 'bicubic'\n    if args.get('interpolation', None):\n        data_config['interpolation'] = args['interpolation']\n    elif pretrained_cfg.get('interpolation', None):\n        data_config['interpolation'] = pretrained_cfg['interpolation']\n\n    # resolve dataset + model mean for normalization\n    data_config['mean'] = IMAGENET_DEFAULT_MEAN\n    if args.get('mean', None) is not None:\n        mean = tuple(args['mean'])\n        if len(mean) == 1:\n            mean = tuple(list(mean) * in_chans)\n        else:\n            assert len(mean) == in_chans\n        data_config['mean'] = mean\n    elif pretrained_cfg.get('mean', None):\n        data_config['mean'] = pretrained_cfg['mean']\n\n    # resolve dataset + model std deviation for normalization\n    data_config['std'] = IMAGENET_DEFAULT_STD\n    if args.get('std', None) is not None:\n        std = tuple(args['std'])\n        if len(std) == 1:\n            std = tuple(list(std) * in_chans)\n        else:\n            assert len(std) == in_chans\n        data_config['std'] = std\n    elif pretrained_cfg.get('std', None):\n        data_config['std'] = pretrained_cfg['std']\n\n    # resolve default inference crop\n    crop_pct = DEFAULT_CROP_PCT\n    if args.get('crop_pct', None):\n        crop_pct = args['crop_pct']\n    else:\n        if use_test_size and pretrained_cfg.get('test_crop_pct', None):\n            crop_pct = pretrained_cfg['test_crop_pct']\n        elif pretrained_cfg.get('crop_pct', None):\n            crop_pct = pretrained_cfg['crop_pct']\n    data_config['crop_pct'] = crop_pct\n\n    # resolve default crop percentage\n    crop_mode = DEFAULT_CROP_MODE\n    if args.get('crop_mode', None):\n        crop_mode = args['crop_mode']\n    elif pretrained_cfg.get('crop_mode', None):\n        crop_mode = pretrained_cfg['crop_mode']\n    data_config['crop_mode'] = crop_mode\n\n    if verbose:\n        _logger.info('Data processing configuration for current model + dataset:')\n        for n, v in data_config.items():\n            _logger.info('\\t%s: %s' % (n, str(v)))\n\n    return data_config\n\n\ndef resolve_model_data_config(\n        model,\n        args=None,\n        pretrained_cfg=None,\n        use_test_size=False,\n        verbose=False,\n):\n    \"\"\" Resolve Model Data Config\n    This is equivalent to resolve_data_config() but with arguments re-ordered to put model first.\n\n    Args:\n        model (nn.Module): the model instance\n        args (dict): command line arguments / configuration in dict form (overrides pretrained_cfg)\n        pretrained_cfg (dict): pretrained model config (overrides pretrained_cfg attached to model)\n        use_test_size (bool): use the test time input resolution (if one exists) instead of default train resolution\n        verbose (bool): enable extra logging of resolved values\n\n    Returns:\n        dictionary of config\n    \"\"\"\n    return resolve_data_config(\n        args=args,\n        pretrained_cfg=pretrained_cfg,\n        model=model,\n        use_test_size=use_test_size,\n        verbose=verbose,\n    )\n",
  "\"\"\" Transforms Factory\nFactory methods for building image transforms for use with TIMM (PyTorch Image Models)\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\nimport math\n\nimport torch\nfrom torchvision import transforms\n\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, DEFAULT_CROP_PCT\nfrom timm.data.auto_augment import rand_augment_transform, augment_and_mix_transform, auto_augment_transform\nfrom timm.data.transforms import str_to_interp_mode, str_to_pil_interp, RandomResizedCropAndInterpolation,\\\n    ResizeKeepRatio, CenterCropOrPad, ToNumpy\nfrom timm.data.random_erasing import RandomErasing\n\n\ndef transforms_noaug_train(\n        img_size=224,\n        interpolation='bilinear',\n        use_prefetcher=False,\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD,\n):\n    if interpolation == 'random':\n        # random interpolation not supported with no-aug\n        interpolation = 'bilinear'\n    tfl = [\n        transforms.Resize(img_size, interpolation=str_to_interp_mode(interpolation)),\n        transforms.CenterCrop(img_size)\n    ]\n    if use_prefetcher:\n        # prefetcher and collate will handle tensor conversion and norm\n        tfl += [ToNumpy()]\n    else:\n        tfl += [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=torch.tensor(mean),\n                std=torch.tensor(std))\n        ]\n    return transforms.Compose(tfl)\n\n\ndef transforms_imagenet_train(\n        img_size=224,\n        scale=None,\n        ratio=None,\n        hflip=0.5,\n        vflip=0.,\n        color_jitter=0.4,\n        auto_augment=None,\n        interpolation='random',\n        use_prefetcher=False,\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD,\n        re_prob=0.,\n        re_mode='const',\n        re_count=1,\n        re_num_splits=0,\n        separate=False,\n        force_color_jitter=False,\n):\n    \"\"\"\n    If separate==True, the transforms are returned as a tuple of 3 separate transforms\n    for use in a mixing dataset that passes\n     * all data through the first (primary) transform, called the 'clean' data\n     * a portion of the data through the secondary transform\n     * normalizes and converts the branches above with the third, final transform\n    \"\"\"\n    scale = tuple(scale or (0.08, 1.0))  # default imagenet scale range\n    ratio = tuple(ratio or (3./4., 4./3.))  # default imagenet ratio range\n    primary_tfl = [\n        RandomResizedCropAndInterpolation(img_size, scale=scale, ratio=ratio, interpolation=interpolation)]\n    if hflip > 0.:\n        primary_tfl += [transforms.RandomHorizontalFlip(p=hflip)]\n    if vflip > 0.:\n        primary_tfl += [transforms.RandomVerticalFlip(p=vflip)]\n\n    secondary_tfl = []\n    disable_color_jitter = False\n    if auto_augment:\n        assert isinstance(auto_augment, str)\n        # color jitter is typically disabled if AA/RA on,\n        # this allows override without breaking old hparm cfgs\n        disable_color_jitter = not (force_color_jitter or '3a' in auto_augment)\n        if isinstance(img_size, (tuple, list)):\n            img_size_min = min(img_size)\n        else:\n            img_size_min = img_size\n        aa_params = dict(\n            translate_const=int(img_size_min * 0.45),\n            img_mean=tuple([min(255, round(255 * x)) for x in mean]),\n        )\n        if interpolation and interpolation != 'random':\n            aa_params['interpolation'] = str_to_pil_interp(interpolation)\n        if auto_augment.startswith('rand'):\n            secondary_tfl += [rand_augment_transform(auto_augment, aa_params)]\n        elif auto_augment.startswith('augmix'):\n            aa_params['translate_pct'] = 0.3\n            secondary_tfl += [augment_and_mix_transform(auto_augment, aa_params)]\n        else:\n            secondary_tfl += [auto_augment_transform(auto_augment, aa_params)]\n\n    if color_jitter is not None and not disable_color_jitter:\n        # color jitter is enabled when not using AA or when forced\n        if isinstance(color_jitter, (list, tuple)):\n            # color jitter should be a 3-tuple/list if spec brightness/contrast/saturation\n            # or 4 if also augmenting hue\n            assert len(color_jitter) in (3, 4)\n        else:\n            # if it's a scalar, duplicate for brightness, contrast, and saturation, no hue\n            color_jitter = (float(color_jitter),) * 3\n        secondary_tfl += [transforms.ColorJitter(*color_jitter)]\n\n    final_tfl = []\n    if use_prefetcher:\n        # prefetcher and collate will handle tensor conversion and norm\n        final_tfl += [ToNumpy()]\n    else:\n        final_tfl += [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=torch.tensor(mean),\n                std=torch.tensor(std))\n        ]\n        if re_prob > 0.:\n            final_tfl.append(\n                RandomErasing(re_prob, mode=re_mode, max_count=re_count, num_splits=re_num_splits, device='cpu'))\n\n    if separate:\n        return transforms.Compose(primary_tfl), transforms.Compose(secondary_tfl), transforms.Compose(final_tfl)\n    else:\n        return transforms.Compose(primary_tfl + secondary_tfl + final_tfl)\n\n\ndef transforms_imagenet_eval(\n        img_size=224,\n        crop_pct=None,\n        crop_mode=None,\n        interpolation='bilinear',\n        use_prefetcher=False,\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD\n):\n    crop_pct = crop_pct or DEFAULT_CROP_PCT\n\n    if isinstance(img_size, (tuple, list)):\n        assert len(img_size) == 2\n        scale_size = tuple([math.floor(x / crop_pct) for x in img_size])\n    else:\n        scale_size = math.floor(img_size / crop_pct)\n        scale_size = (scale_size, scale_size)\n\n    if crop_mode == 'squash':\n        # squash mode scales each edge to 1/pct of target, then crops\n        # aspect ratio is not preserved, no img lost if crop_pct == 1.0\n        tfl = [\n            transforms.Resize(scale_size, interpolation=str_to_interp_mode(interpolation)),\n            transforms.CenterCrop(img_size),\n        ]\n    elif crop_mode == 'border':\n        # scale the longest edge of image to 1/pct of target edge, add borders to pad, then crop\n        # no image lost if crop_pct == 1.0\n        fill = [round(255 * v) for v in mean]\n        tfl = [\n            ResizeKeepRatio(scale_size, interpolation=interpolation, longest=1.0),\n            CenterCropOrPad(img_size, fill=fill),\n        ]\n    else:\n        # default crop model is center\n        # aspect ratio is preserved, crops center within image, no borders are added, image is lost\n        if scale_size[0] == scale_size[1]:\n            # simple case, use torchvision built-in Resize w/ shortest edge mode (scalar size arg)\n            tfl = [\n                transforms.Resize(scale_size[0], interpolation=str_to_interp_mode(interpolation))\n            ]\n        else:\n            # resize shortest edge to matching target dim for non-square target\n            tfl = [ResizeKeepRatio(scale_size)]\n        tfl += [transforms.CenterCrop(img_size)]\n\n    if use_prefetcher:\n        # prefetcher and collate will handle tensor conversion and norm\n        tfl += [ToNumpy()]\n    else:\n        tfl += [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=torch.tensor(mean),\n                std=torch.tensor(std),\n            )\n        ]\n\n    return transforms.Compose(tfl)\n\n\ndef create_transform(\n        input_size,\n        is_training=False,\n        use_prefetcher=False,\n        no_aug=False,\n        scale=None,\n        ratio=None,\n        hflip=0.5,\n        vflip=0.,\n        color_jitter=0.4,\n        auto_augment=None,\n        interpolation='bilinear',\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD,\n        re_prob=0.,\n        re_mode='const',\n        re_count=1,\n        re_num_splits=0,\n        crop_pct=None,\n        crop_mode=None,\n        tf_preprocessing=False,\n        separate=False):\n\n    if isinstance(input_size, (tuple, list)):\n        img_size = input_size[-2:]\n    else:\n        img_size = input_size\n\n    if tf_preprocessing and use_prefetcher:\n        assert not separate, \"Separate transforms not supported for TF preprocessing\"\n        from timm.data.tf_preprocessing import TfPreprocessTransform\n        transform = TfPreprocessTransform(\n            is_training=is_training, size=img_size, interpolation=interpolation)\n    else:\n        if is_training and no_aug:\n            assert not separate, \"Cannot perform split augmentation with no_aug\"\n            transform = transforms_noaug_train(\n                img_size,\n                interpolation=interpolation,\n                use_prefetcher=use_prefetcher,\n                mean=mean,\n                std=std,\n            )\n        elif is_training:\n            transform = transforms_imagenet_train(\n                img_size,\n                scale=scale,\n                ratio=ratio,\n                hflip=hflip,\n                vflip=vflip,\n                color_jitter=color_jitter,\n                auto_augment=auto_augment,\n                interpolation=interpolation,\n                use_prefetcher=use_prefetcher,\n                mean=mean,\n                std=std,\n                re_prob=re_prob,\n                re_mode=re_mode,\n                re_count=re_count,\n                re_num_splits=re_num_splits,\n                separate=separate,\n            )\n        else:\n            assert not separate, \"Separate transforms not supported for validation preprocessing\"\n            transform = transforms_imagenet_eval(\n                img_size,\n                interpolation=interpolation,\n                use_prefetcher=use_prefetcher,\n                mean=mean,\n                std=std,\n                crop_pct=crop_pct,\n                crop_mode=crop_mode,\n            )\n\n    return transform\n",
  "\"\"\" Loader Factory, Fast Collate, CUDA Prefetcher\n\nPrefetcher and Fast Collate inspired by NVIDIA APEX example at\nhttps://github.com/NVIDIA/apex/commit/d5e2bb4bdeedd27b1dfaf5bb2b24d6c000dee9be#diff-cf86c282ff7fba81fad27a559379d5bf\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\nimport logging\nimport random\nfrom contextlib import suppress\nfrom functools import partial\nfrom itertools import repeat\nfrom typing import Callable\n\nimport torch\nimport torch.utils.data\nimport numpy as np\n\nfrom .constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom .dataset import IterableImageDataset\nfrom .distributed_sampler import OrderedDistributedSampler, RepeatAugSampler\nfrom .random_erasing import RandomErasing\nfrom .mixup import FastCollateMixup\nfrom .transforms_factory import create_transform\n\n_logger = logging.getLogger(__name__)\n\n\ndef fast_collate(batch):\n    \"\"\" A fast collation function optimized for uint8 images (np array or torch) and int64 targets (labels)\"\"\"\n    assert isinstance(batch[0], tuple)\n    batch_size = len(batch)\n    if isinstance(batch[0][0], tuple):\n        # This branch 'deinterleaves' and flattens tuples of input tensors into one tensor ordered by position\n        # such that all tuple of position n will end up in a torch.split(tensor, batch_size) in nth position\n        inner_tuple_size = len(batch[0][0])\n        flattened_batch_size = batch_size * inner_tuple_size\n        targets = torch.zeros(flattened_batch_size, dtype=torch.int64)\n        tensor = torch.zeros((flattened_batch_size, *batch[0][0][0].shape), dtype=torch.uint8)\n        for i in range(batch_size):\n            assert len(batch[i][0]) == inner_tuple_size  # all input tensor tuples must be same length\n            for j in range(inner_tuple_size):\n                targets[i + j * batch_size] = batch[i][1]\n                tensor[i + j * batch_size] += torch.from_numpy(batch[i][0][j])\n        return tensor, targets\n    elif isinstance(batch[0][0], np.ndarray):\n        targets = torch.tensor([b[1] for b in batch], dtype=torch.int64)\n        assert len(targets) == batch_size\n        tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n        for i in range(batch_size):\n            tensor[i] += torch.from_numpy(batch[i][0])\n        return tensor, targets\n    elif isinstance(batch[0][0], torch.Tensor):\n        targets = torch.tensor([b[1] for b in batch], dtype=torch.int64)\n        assert len(targets) == batch_size\n        tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n        for i in range(batch_size):\n            tensor[i].copy_(batch[i][0])\n        return tensor, targets\n    else:\n        assert False\n\n\ndef adapt_to_chs(x, n):\n    if not isinstance(x, (tuple, list)):\n        x = tuple(repeat(x, n))\n    elif len(x) != n:\n        x_mean = np.mean(x).item()\n        x = (x_mean,) * n\n        _logger.warning(f'Pretrained mean/std different shape than model, using avg value {x}.')\n    else:\n        assert len(x) == n, 'normalization stats must match image channels'\n    return x\n\n\nclass PrefetchLoader:\n\n    def __init__(\n            self,\n            loader,\n            mean=IMAGENET_DEFAULT_MEAN,\n            std=IMAGENET_DEFAULT_STD,\n            channels=3,\n            device=torch.device('cuda'),\n            img_dtype=torch.float32,\n            fp16=False,\n            re_prob=0.,\n            re_mode='const',\n            re_count=1,\n            re_num_splits=0):\n\n        mean = adapt_to_chs(mean, channels)\n        std = adapt_to_chs(std, channels)\n        normalization_shape = (1, channels, 1, 1)\n\n        self.loader = loader\n        self.device = device\n        if fp16:\n            # fp16 arg is deprecated, but will override dtype arg if set for bwd compat\n            img_dtype = torch.float16\n        self.img_dtype = img_dtype\n        self.mean = torch.tensor(\n            [x * 255 for x in mean], device=device, dtype=img_dtype).view(normalization_shape)\n        self.std = torch.tensor(\n            [x * 255 for x in std], device=device, dtype=img_dtype).view(normalization_shape)\n        if re_prob > 0.:\n            self.random_erasing = RandomErasing(\n                probability=re_prob,\n                mode=re_mode,\n                max_count=re_count,\n                num_splits=re_num_splits,\n                device=device,\n            )\n        else:\n            self.random_erasing = None\n        self.is_cuda = torch.cuda.is_available() and device.type == 'cuda'\n\n    def __iter__(self):\n        first = True\n        if self.is_cuda:\n            stream = torch.cuda.Stream()\n            stream_context = partial(torch.cuda.stream, stream=stream)\n        else:\n            stream = None\n            stream_context = suppress\n\n        for next_input, next_target in self.loader:\n\n            with stream_context():\n                next_input = next_input.to(device=self.device, non_blocking=True)\n                next_target = next_target.to(device=self.device, non_blocking=True)\n                next_input = next_input.to(self.img_dtype).sub_(self.mean).div_(self.std)\n                if self.random_erasing is not None:\n                    next_input = self.random_erasing(next_input)\n\n            if not first:\n                yield input, target\n            else:\n                first = False\n\n            if stream is not None:\n                torch.cuda.current_stream().wait_stream(stream)\n\n            input = next_input\n            target = next_target\n\n        yield input, target\n\n    def __len__(self):\n        return len(self.loader)\n\n    @property\n    def sampler(self):\n        return self.loader.sampler\n\n    @property\n    def dataset(self):\n        return self.loader.dataset\n\n    @property\n    def mixup_enabled(self):\n        if isinstance(self.loader.collate_fn, FastCollateMixup):\n            return self.loader.collate_fn.mixup_enabled\n        else:\n            return False\n\n    @mixup_enabled.setter\n    def mixup_enabled(self, x):\n        if isinstance(self.loader.collate_fn, FastCollateMixup):\n            self.loader.collate_fn.mixup_enabled = x\n\n\ndef _worker_init(worker_id, worker_seeding='all'):\n    worker_info = torch.utils.data.get_worker_info()\n    assert worker_info.id == worker_id\n    if isinstance(worker_seeding, Callable):\n        seed = worker_seeding(worker_info)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed % (2 ** 32 - 1))\n    else:\n        assert worker_seeding in ('all', 'part')\n        # random / torch seed already called in dataloader iter class w/ worker_info.seed\n        # to reproduce some old results (same seed + hparam combo), partial seeding is required (skip numpy re-seed)\n        if worker_seeding == 'all':\n            np.random.seed(worker_info.seed % (2 ** 32 - 1))\n\n\ndef create_loader(\n        dataset,\n        input_size,\n        batch_size,\n        is_training=False,\n        use_prefetcher=True,\n        no_aug=False,\n        re_prob=0.,\n        re_mode='const',\n        re_count=1,\n        re_split=False,\n        scale=None,\n        ratio=None,\n        hflip=0.5,\n        vflip=0.,\n        color_jitter=0.4,\n        auto_augment=None,\n        num_aug_repeats=0,\n        num_aug_splits=0,\n        interpolation='bilinear',\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD,\n        num_workers=1,\n        distributed=False,\n        crop_pct=None,\n        crop_mode=None,\n        collate_fn=None,\n        pin_memory=False,\n        fp16=False,  # deprecated, use img_dtype\n        img_dtype=torch.float32,\n        device=torch.device('cuda'),\n        tf_preprocessing=False,\n        use_multi_epochs_loader=False,\n        persistent_workers=True,\n        worker_seeding='all',\n):\n    re_num_splits = 0\n    if re_split:\n        # apply RE to second half of batch if no aug split otherwise line up with aug split\n        re_num_splits = num_aug_splits or 2\n    dataset.transform = create_transform(\n        input_size,\n        is_training=is_training,\n        use_prefetcher=use_prefetcher,\n        no_aug=no_aug,\n        scale=scale,\n        ratio=ratio,\n        hflip=hflip,\n        vflip=vflip,\n        color_jitter=color_jitter,\n        auto_augment=auto_augment,\n        interpolation=interpolation,\n        mean=mean,\n        std=std,\n        crop_pct=crop_pct,\n        crop_mode=crop_mode,\n        tf_preprocessing=tf_preprocessing,\n        re_prob=re_prob,\n        re_mode=re_mode,\n        re_count=re_count,\n        re_num_splits=re_num_splits,\n        separate=num_aug_splits > 0,\n    )\n\n    if isinstance(dataset, IterableImageDataset):\n        # give Iterable datasets early knowledge of num_workers so that sample estimates\n        # are correct before worker processes are launched\n        dataset.set_loader_cfg(num_workers=num_workers)\n\n    sampler = None\n    if distributed and not isinstance(dataset, torch.utils.data.IterableDataset):\n        if is_training:\n            if num_aug_repeats:\n                sampler = RepeatAugSampler(dataset, num_repeats=num_aug_repeats)\n            else:\n                sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n        else:\n            # This will add extra duplicate entries to result in equal num\n            # of samples per-process, will slightly alter validation results\n            sampler = OrderedDistributedSampler(dataset)\n    else:\n        assert num_aug_repeats == 0, \"RepeatAugment not currently supported in non-distributed or IterableDataset use\"\n\n    if collate_fn is None:\n        collate_fn = fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate\n\n    loader_class = torch.utils.data.DataLoader\n    if use_multi_epochs_loader:\n        loader_class = MultiEpochsDataLoader\n\n    loader_args = dict(\n        batch_size=batch_size,\n        shuffle=not isinstance(dataset, torch.utils.data.IterableDataset) and sampler is None and is_training,\n        num_workers=num_workers,\n        sampler=sampler,\n        collate_fn=collate_fn,\n        pin_memory=pin_memory,\n        drop_last=is_training,\n        worker_init_fn=partial(_worker_init, worker_seeding=worker_seeding),\n        persistent_workers=persistent_workers\n    )\n    try:\n        loader = loader_class(dataset, **loader_args)\n    except TypeError as e:\n        loader_args.pop('persistent_workers')  # only in Pytorch 1.7+\n        loader = loader_class(dataset, **loader_args)\n    if use_prefetcher:\n        prefetch_re_prob = re_prob if is_training and not no_aug else 0.\n        loader = PrefetchLoader(\n            loader,\n            mean=mean,\n            std=std,\n            channels=input_size[0],\n            device=device,\n            fp16=fp16,  # deprecated, use img_dtype\n            img_dtype=img_dtype,\n            re_prob=prefetch_re_prob,\n            re_mode=re_mode,\n            re_count=re_count,\n            re_num_splits=re_num_splits\n        )\n\n    return loader\n\n\nclass MultiEpochsDataLoader(torch.utils.data.DataLoader):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._DataLoader__initialized = False\n        if self.batch_sampler is None:\n            self.sampler = _RepeatSampler(self.sampler)\n        else:\n            self.batch_sampler = _RepeatSampler(self.batch_sampler)\n        self._DataLoader__initialized = True\n        self.iterator = super().__iter__()\n\n    def __len__(self):\n        return len(self.sampler) if self.batch_sampler is None else len(self.batch_sampler.sampler)\n\n    def __iter__(self):\n        for i in range(len(self)):\n            yield next(self.iterator)\n\n\nclass _RepeatSampler(object):\n    \"\"\" Sampler that repeats forever.\n\n    Args:\n        sampler (Sampler)\n    \"\"\"\n\n    def __init__(self, sampler):\n        self.sampler = sampler\n\n    def __iter__(self):\n        while True:\n            yield from iter(self.sampler)\n",
  "\"\"\" Random Erasing (Cutout)\n\nOriginally inspired by impl at https://github.com/zhunzhong07/Random-Erasing, Apache 2.0\nCopyright Zhun Zhong & Liang Zheng\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\nimport random\nimport math\n\nimport torch\n\n\ndef _get_pixels(per_pixel, rand_color, patch_size, dtype=torch.float32, device='cuda'):\n    # NOTE I've seen CUDA illegal memory access errors being caused by the normal_()\n    # paths, flip the order so normal is run on CPU if this becomes a problem\n    # Issue has been fixed in master https://github.com/pytorch/pytorch/issues/19508\n    if per_pixel:\n        return torch.empty(patch_size, dtype=dtype, device=device).normal_()\n    elif rand_color:\n        return torch.empty((patch_size[0], 1, 1), dtype=dtype, device=device).normal_()\n    else:\n        return torch.zeros((patch_size[0], 1, 1), dtype=dtype, device=device)\n\n\nclass RandomErasing:\n    \"\"\" Randomly selects a rectangle region in an image and erases its pixels.\n        'Random Erasing Data Augmentation' by Zhong et al.\n        See https://arxiv.org/pdf/1708.04896.pdf\n\n        This variant of RandomErasing is intended to be applied to either a batch\n        or single image tensor after it has been normalized by dataset mean and std.\n    Args:\n         probability: Probability that the Random Erasing operation will be performed.\n         min_area: Minimum percentage of erased area wrt input image area.\n         max_area: Maximum percentage of erased area wrt input image area.\n         min_aspect: Minimum aspect ratio of erased area.\n         mode: pixel color mode, one of 'const', 'rand', or 'pixel'\n            'const' - erase block is constant color of 0 for all channels\n            'rand'  - erase block is same per-channel random (normal) color\n            'pixel' - erase block is per-pixel random (normal) color\n        max_count: maximum number of erasing blocks per image, area per box is scaled by count.\n            per-image count is randomly chosen between 1 and this value.\n    \"\"\"\n\n    def __init__(\n            self,\n            probability=0.5,\n            min_area=0.02,\n            max_area=1/3,\n            min_aspect=0.3,\n            max_aspect=None,\n            mode='const',\n            min_count=1,\n            max_count=None,\n            num_splits=0,\n            device='cuda',\n    ):\n        self.probability = probability\n        self.min_area = min_area\n        self.max_area = max_area\n        max_aspect = max_aspect or 1 / min_aspect\n        self.log_aspect_ratio = (math.log(min_aspect), math.log(max_aspect))\n        self.min_count = min_count\n        self.max_count = max_count or min_count\n        self.num_splits = num_splits\n        self.mode = mode.lower()\n        self.rand_color = False\n        self.per_pixel = False\n        if self.mode == 'rand':\n            self.rand_color = True  # per block random normal\n        elif self.mode == 'pixel':\n            self.per_pixel = True  # per pixel random normal\n        else:\n            assert not self.mode or self.mode == 'const'\n        self.device = device\n\n    def _erase(self, img, chan, img_h, img_w, dtype):\n        if random.random() > self.probability:\n            return\n        area = img_h * img_w\n        count = self.min_count if self.min_count == self.max_count else \\\n            random.randint(self.min_count, self.max_count)\n        for _ in range(count):\n            for attempt in range(10):\n                target_area = random.uniform(self.min_area, self.max_area) * area / count\n                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < img_w and h < img_h:\n                    top = random.randint(0, img_h - h)\n                    left = random.randint(0, img_w - w)\n                    img[:, top:top + h, left:left + w] = _get_pixels(\n                        self.per_pixel,\n                        self.rand_color,\n                        (chan, h, w),\n                        dtype=dtype,\n                        device=self.device,\n                    )\n                    break\n\n    def __call__(self, input):\n        if len(input.size()) == 3:\n            self._erase(input, *input.size(), input.dtype)\n        else:\n            batch_size, chan, img_h, img_w = input.size()\n            # skip first slice of batch if num_splits is set (for clean portion of samples)\n            batch_start = batch_size // self.num_splits if self.num_splits > 1 else 0\n            for i in range(batch_start, batch_size):\n                self._erase(input[i], chan, img_h, img_w, input.dtype)\n        return input\n\n    def __repr__(self):\n        # NOTE simplified state for repr\n        fs = self.__class__.__name__ + f'(p={self.probability}, mode={self.mode}'\n        fs += f', count=({self.min_count}, {self.max_count}))'\n        return fs\n",
  "import math\nimport numbers\nimport random\nimport warnings\nfrom typing import List, Sequence\n\nimport torch\nimport torchvision.transforms.functional as F\ntry:\n    from torchvision.transforms.functional import InterpolationMode\n    has_interpolation_mode = True\nexcept ImportError:\n    has_interpolation_mode = False\nfrom PIL import Image\nimport numpy as np\n\n\nclass ToNumpy:\n\n    def __call__(self, pil_img):\n        np_img = np.array(pil_img, dtype=np.uint8)\n        if np_img.ndim < 3:\n            np_img = np.expand_dims(np_img, axis=-1)\n        np_img = np.rollaxis(np_img, 2)  # HWC to CHW\n        return np_img\n\n\nclass ToTensor:\n\n    def __init__(self, dtype=torch.float32):\n        self.dtype = dtype\n\n    def __call__(self, pil_img):\n        np_img = np.array(pil_img, dtype=np.uint8)\n        if np_img.ndim < 3:\n            np_img = np.expand_dims(np_img, axis=-1)\n        np_img = np.rollaxis(np_img, 2)  # HWC to CHW\n        return torch.from_numpy(np_img).to(dtype=self.dtype)\n\n\n# Pillow is deprecating the top-level resampling attributes (e.g., Image.BILINEAR) in\n# favor of the Image.Resampling enum. The top-level resampling attributes will be\n# removed in Pillow 10.\nif hasattr(Image, \"Resampling\"):\n    _pil_interpolation_to_str = {\n        Image.Resampling.NEAREST: 'nearest',\n        Image.Resampling.BILINEAR: 'bilinear',\n        Image.Resampling.BICUBIC: 'bicubic',\n        Image.Resampling.BOX: 'box',\n        Image.Resampling.HAMMING: 'hamming',\n        Image.Resampling.LANCZOS: 'lanczos',\n    }\nelse:\n    _pil_interpolation_to_str = {\n        Image.NEAREST: 'nearest',\n        Image.BILINEAR: 'bilinear',\n        Image.BICUBIC: 'bicubic',\n        Image.BOX: 'box',\n        Image.HAMMING: 'hamming',\n        Image.LANCZOS: 'lanczos',\n    }\n\n_str_to_pil_interpolation = {b: a for a, b in _pil_interpolation_to_str.items()}\n\n\nif has_interpolation_mode:\n    _torch_interpolation_to_str = {\n        InterpolationMode.NEAREST: 'nearest',\n        InterpolationMode.BILINEAR: 'bilinear',\n        InterpolationMode.BICUBIC: 'bicubic',\n        InterpolationMode.BOX: 'box',\n        InterpolationMode.HAMMING: 'hamming',\n        InterpolationMode.LANCZOS: 'lanczos',\n    }\n    _str_to_torch_interpolation = {b: a for a, b in _torch_interpolation_to_str.items()}\nelse:\n    _pil_interpolation_to_torch = {}\n    _torch_interpolation_to_str = {}\n\n\ndef str_to_pil_interp(mode_str):\n    return _str_to_pil_interpolation[mode_str]\n\n\ndef str_to_interp_mode(mode_str):\n    if has_interpolation_mode:\n        return _str_to_torch_interpolation[mode_str]\n    else:\n        return _str_to_pil_interpolation[mode_str]\n\n\ndef interp_mode_to_str(mode):\n    if has_interpolation_mode:\n        return _torch_interpolation_to_str[mode]\n    else:\n        return _pil_interpolation_to_str[mode]\n\n\n_RANDOM_INTERPOLATION = (str_to_interp_mode('bilinear'), str_to_interp_mode('bicubic'))\n\n\ndef _setup_size(size, error_msg):\n    if isinstance(size, numbers.Number):\n        return int(size), int(size)\n\n    if isinstance(size, Sequence) and len(size) == 1:\n        return size[0], size[0]\n\n    if len(size) != 2:\n        raise ValueError(error_msg)\n\n    return size\n\n\nclass RandomResizedCropAndInterpolation:\n    \"\"\"Crop the given PIL Image to random size and aspect ratio with random interpolation.\n\n    A crop of random size (default: of 0.08 to 1.0) of the original size and a random\n    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop\n    is finally resized to given size.\n    This is popularly used to train the Inception networks.\n\n    Args:\n        size: expected output size of each edge\n        scale: range of size of the origin size cropped\n        ratio: range of aspect ratio of the origin aspect ratio cropped\n        interpolation: Default: PIL.Image.BILINEAR\n    \"\"\"\n\n    def __init__(self, size, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.),\n                 interpolation='bilinear'):\n        if isinstance(size, (list, tuple)):\n            self.size = tuple(size)\n        else:\n            self.size = (size, size)\n        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n            warnings.warn(\"range should be of kind (min, max)\")\n\n        if interpolation == 'random':\n            self.interpolation = _RANDOM_INTERPOLATION\n        else:\n            self.interpolation = str_to_interp_mode(interpolation)\n        self.scale = scale\n        self.ratio = ratio\n\n    @staticmethod\n    def get_params(img, scale, ratio):\n        \"\"\"Get parameters for ``crop`` for a random sized crop.\n\n        Args:\n            img (PIL Image): Image to be cropped.\n            scale (tuple): range of size of the origin size cropped\n            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n                sized crop.\n        \"\"\"\n        area = img.size[0] * img.size[1]\n\n        for attempt in range(10):\n            target_area = random.uniform(*scale) * area\n            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n            aspect_ratio = math.exp(random.uniform(*log_ratio))\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w <= img.size[0] and h <= img.size[1]:\n                i = random.randint(0, img.size[1] - h)\n                j = random.randint(0, img.size[0] - w)\n                return i, j, h, w\n\n        # Fallback to central crop\n        in_ratio = img.size[0] / img.size[1]\n        if in_ratio < min(ratio):\n            w = img.size[0]\n            h = int(round(w / min(ratio)))\n        elif in_ratio > max(ratio):\n            h = img.size[1]\n            w = int(round(h * max(ratio)))\n        else:  # whole image\n            w = img.size[0]\n            h = img.size[1]\n        i = (img.size[1] - h) // 2\n        j = (img.size[0] - w) // 2\n        return i, j, h, w\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be cropped and resized.\n\n        Returns:\n            PIL Image: Randomly cropped and resized image.\n        \"\"\"\n        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n        if isinstance(self.interpolation, (tuple, list)):\n            interpolation = random.choice(self.interpolation)\n        else:\n            interpolation = self.interpolation\n        return F.resized_crop(img, i, j, h, w, self.size, interpolation)\n\n    def __repr__(self):\n        if isinstance(self.interpolation, (tuple, list)):\n            interpolate_str = ' '.join([interp_mode_to_str(x) for x in self.interpolation])\n        else:\n            interpolate_str = interp_mode_to_str(self.interpolation)\n        format_string = self.__class__.__name__ + '(size={0}'.format(self.size)\n        format_string += ', scale={0}'.format(tuple(round(s, 4) for s in self.scale))\n        format_string += ', ratio={0}'.format(tuple(round(r, 4) for r in self.ratio))\n        format_string += ', interpolation={0})'.format(interpolate_str)\n        return format_string\n\n\ndef center_crop_or_pad(img: torch.Tensor, output_size: List[int], fill=0) -> torch.Tensor:\n    \"\"\"Center crops and/or pads the given image.\n    If the image is torch Tensor, it is expected\n    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n    If image size is smaller than output size along any edge, image is padded with 0 and then center cropped.\n\n    Args:\n        img (PIL Image or Tensor): Image to be cropped.\n        output_size (sequence or int): (height, width) of the crop box. If int or sequence with single int,\n            it is used for both directions.\n        fill (int, Tuple[int]): Padding color\n\n    Returns:\n        PIL Image or Tensor: Cropped image.\n    \"\"\"\n    if isinstance(output_size, numbers.Number):\n        output_size = (int(output_size), int(output_size))\n    elif isinstance(output_size, (tuple, list)) and len(output_size) == 1:\n        output_size = (output_size[0], output_size[0])\n\n    _, image_height, image_width = F.get_dimensions(img)\n    crop_height, crop_width = output_size\n\n    if crop_width > image_width or crop_height > image_height:\n        padding_ltrb = [\n            (crop_width - image_width) // 2 if crop_width > image_width else 0,\n            (crop_height - image_height) // 2 if crop_height > image_height else 0,\n            (crop_width - image_width + 1) // 2 if crop_width > image_width else 0,\n            (crop_height - image_height + 1) // 2 if crop_height > image_height else 0,\n        ]\n        img = F.pad(img, padding_ltrb, fill=fill)\n        _, image_height, image_width = F.get_dimensions(img)\n        if crop_width == image_width and crop_height == image_height:\n            return img\n\n    crop_top = int(round((image_height - crop_height) / 2.0))\n    crop_left = int(round((image_width - crop_width) / 2.0))\n    return F.crop(img, crop_top, crop_left, crop_height, crop_width)\n\n\nclass CenterCropOrPad(torch.nn.Module):\n    \"\"\"Crops the given image at the center.\n    If the image is torch Tensor, it is expected\n    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n    If image size is smaller than output size along any edge, image is padded with 0 and then center cropped.\n\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n    \"\"\"\n\n    def __init__(self, size, fill=0):\n        super().__init__()\n        self.size = _setup_size(size, error_msg=\"Please provide only two dimensions (h, w) for size.\")\n        self.fill = fill\n\n    def forward(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be cropped.\n\n        Returns:\n            PIL Image or Tensor: Cropped image.\n        \"\"\"\n        return center_crop_or_pad(img, self.size, fill=self.fill)\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}(size={self.size})\"\n\n\nclass ResizeKeepRatio:\n    \"\"\" Resize and Keep Ratio\n    \"\"\"\n\n    def __init__(\n            self,\n            size,\n            longest=0.,\n            interpolation='bilinear',\n            fill=0,\n    ):\n        if isinstance(size, (list, tuple)):\n            self.size = tuple(size)\n        else:\n            self.size = (size, size)\n        self.interpolation = str_to_interp_mode(interpolation)\n        self.longest = float(longest)\n        self.fill = fill\n\n    @staticmethod\n    def get_params(img, target_size, longest):\n        \"\"\"Get parameters\n\n        Args:\n            img (PIL Image): Image to be cropped.\n            target_size (Tuple[int, int]): Size of output\n        Returns:\n            tuple: params (h, w) and (l, r, t, b) to be passed to ``resize`` and ``pad`` respectively\n        \"\"\"\n        source_size = img.size[::-1]  # h, w\n        h, w = source_size\n        target_h, target_w = target_size\n        ratio_h = h / target_h\n        ratio_w = w / target_w\n        ratio = max(ratio_h, ratio_w) * longest + min(ratio_h, ratio_w) * (1. - longest)\n        size = [round(x / ratio) for x in source_size]\n        return size\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be cropped and resized.\n\n        Returns:\n            PIL Image: Resized, padded to at least target size, possibly cropped to exactly target size\n        \"\"\"\n        size = self.get_params(img, self.size, self.longest)\n        img = F.resize(img, size, self.interpolation)\n        return img\n\n    def __repr__(self):\n        interpolate_str = interp_mode_to_str(self.interpolation)\n        format_string = self.__class__.__name__ + '(size={0}'.format(self.size)\n        format_string += f', interpolation={interpolate_str})'\n        format_string += f', longest={self.longest:.3f})'\n        return format_string\n",
  "DEFAULT_CROP_PCT = 0.875\nDEFAULT_CROP_MODE = 'center'\nIMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\nIMAGENET_INCEPTION_MEAN = (0.5, 0.5, 0.5)\nIMAGENET_INCEPTION_STD = (0.5, 0.5, 0.5)\nIMAGENET_DPN_MEAN = (124 / 255, 117 / 255, 104 / 255)\nIMAGENET_DPN_STD = tuple([1 / (.0167 * 255)] * 3)\nOPENAI_CLIP_MEAN = (0.48145466, 0.4578275, 0.40821073)\nOPENAI_CLIP_STD = (0.26862954, 0.26130258, 0.27577711)\n",
  "\"\"\" Dataset Factory\n\nHacked together by / Copyright 2021, Ross Wightman\n\"\"\"\nimport os\n\nfrom torchvision.datasets import CIFAR100, CIFAR10, MNIST, KMNIST, FashionMNIST, ImageFolder\ntry:\n    from torchvision.datasets import Places365\n    has_places365 = True\nexcept ImportError:\n    has_places365 = False\ntry:\n    from torchvision.datasets import INaturalist\n    has_inaturalist = True\nexcept ImportError:\n    has_inaturalist = False\ntry:\n    from torchvision.datasets import QMNIST\n    has_qmnist = True\nexcept ImportError:\n    has_qmnist = False\ntry:\n    from torchvision.datasets import ImageNet\n    has_imagenet = True\nexcept ImportError:\n    has_imagenet = False\n\nfrom .dataset import IterableImageDataset, ImageDataset\n\n_TORCH_BASIC_DS = dict(\n    cifar10=CIFAR10,\n    cifar100=CIFAR100,\n    mnist=MNIST,\n    kmnist=KMNIST,\n    fashion_mnist=FashionMNIST,\n)\n_TRAIN_SYNONYM = dict(train=None, training=None)\n_EVAL_SYNONYM = dict(val=None, valid=None, validation=None, eval=None, evaluation=None)\n\n\ndef _search_split(root, split):\n    # look for sub-folder with name of split in root and use that if it exists\n    split_name = split.split('[')[0]\n    try_root = os.path.join(root, split_name)\n    if os.path.exists(try_root):\n        return try_root\n\n    def _try(syn):\n        for s in syn:\n            try_root = os.path.join(root, s)\n            if os.path.exists(try_root):\n                return try_root\n        return root\n    if split_name in _TRAIN_SYNONYM:\n        root = _try(_TRAIN_SYNONYM)\n    elif split_name in _EVAL_SYNONYM:\n        root = _try(_EVAL_SYNONYM)\n    return root\n\n\ndef create_dataset(\n        name,\n        root,\n        split='validation',\n        search_split=True,\n        class_map=None,\n        load_bytes=False,\n        is_training=False,\n        download=False,\n        batch_size=None,\n        seed=42,\n        repeats=0,\n        **kwargs\n):\n    \"\"\" Dataset factory method\n\n    In parenthesis after each arg are the type of dataset supported for each arg, one of:\n      * folder - default, timm folder (or tar) based ImageDataset\n      * torch - torchvision based datasets\n      * HFDS - Hugging Face Datasets\n      * TFDS - Tensorflow-datasets wrapper in IterabeDataset interface via IterableImageDataset\n      * WDS - Webdataset\n      * all - any of the above\n\n    Args:\n        name: dataset name, empty is okay for folder based datasets\n        root: root folder of dataset (all)\n        split: dataset split (all)\n        search_split: search for split specific child fold from root so one can specify\n            `imagenet/` instead of `/imagenet/val`, etc on cmd line / config. (folder, torch/folder)\n        class_map: specify class -> index mapping via text file or dict (folder)\n        load_bytes: load data, return images as undecoded bytes (folder)\n        download: download dataset if not present and supported (HFDS, TFDS, torch)\n        is_training: create dataset in train mode, this is different from the split.\n            For Iterable / TDFS it enables shuffle, ignored for other datasets. (TFDS, WDS)\n        batch_size: batch size hint for (TFDS, WDS)\n        seed: seed for iterable datasets (TFDS, WDS)\n        repeats: dataset repeats per iteration i.e. epoch (TFDS, WDS)\n        **kwargs: other args to pass to dataset\n\n    Returns:\n        Dataset object\n    \"\"\"\n    name = name.lower()\n    if name.startswith('torch/'):\n        name = name.split('/', 2)[-1]\n        torch_kwargs = dict(root=root, download=download, **kwargs)\n        if name in _TORCH_BASIC_DS:\n            ds_class = _TORCH_BASIC_DS[name]\n            use_train = split in _TRAIN_SYNONYM\n            ds = ds_class(train=use_train, **torch_kwargs)\n        elif name == 'inaturalist' or name == 'inat':\n            assert has_inaturalist, 'Please update to PyTorch 1.10, torchvision 0.11+ for Inaturalist'\n            target_type = 'full'\n            split_split = split.split('/')\n            if len(split_split) > 1:\n                target_type = split_split[0].split('_')\n                if len(target_type) == 1:\n                    target_type = target_type[0]\n                split = split_split[-1]\n            if split in _TRAIN_SYNONYM:\n                split = '2021_train'\n            elif split in _EVAL_SYNONYM:\n                split = '2021_valid'\n            ds = INaturalist(version=split, target_type=target_type, **torch_kwargs)\n        elif name == 'places365':\n            assert has_places365, 'Please update to a newer PyTorch and torchvision for Places365 dataset.'\n            if split in _TRAIN_SYNONYM:\n                split = 'train-standard'\n            elif split in _EVAL_SYNONYM:\n                split = 'val'\n            ds = Places365(split=split, **torch_kwargs)\n        elif name == 'qmnist':\n            assert has_qmnist, 'Please update to a newer PyTorch and torchvision for QMNIST dataset.'\n            use_train = split in _TRAIN_SYNONYM\n            ds = QMNIST(train=use_train, **torch_kwargs)\n        elif name == 'imagenet':\n            assert has_imagenet, 'Please update to a newer PyTorch and torchvision for ImageNet dataset.'\n            if split in _EVAL_SYNONYM:\n                split = 'val'\n            ds = ImageNet(split=split, **torch_kwargs)\n        elif name == 'image_folder' or name == 'folder':\n            # in case torchvision ImageFolder is preferred over timm ImageDataset for some reason\n            if search_split and os.path.isdir(root):\n                # look for split specific sub-folder in root\n                root = _search_split(root, split)\n            ds = ImageFolder(root, **kwargs)\n        else:\n            assert False, f\"Unknown torchvision dataset {name}\"\n    elif name.startswith('hfds/'):\n        # NOTE right now, HF datasets default arrow format is a random-access Dataset,\n        # There will be a IterableDataset variant too, TBD\n        ds = ImageDataset(root, reader=name, split=split, class_map=class_map, **kwargs)\n    elif name.startswith('tfds/'):\n        ds = IterableImageDataset(\n            root,\n            reader=name,\n            split=split,\n            class_map=class_map,\n            is_training=is_training,\n            download=download,\n            batch_size=batch_size,\n            repeats=repeats,\n            seed=seed,\n            **kwargs\n        )\n    elif name.startswith('wds/'):\n        ds = IterableImageDataset(\n            root,\n            reader=name,\n            split=split,\n            class_map=class_map,\n            is_training=is_training,\n            batch_size=batch_size,\n            repeats=repeats,\n            seed=seed,\n            **kwargs\n        )\n    else:\n        # FIXME support more advance split cfg for ImageFolder/Tar datasets in the future\n        if search_split and os.path.isdir(root):\n            # look for split specific sub-folder in root\n            root = _search_split(root, split)\n        ds = ImageDataset(root, reader=name, class_map=class_map, load_bytes=load_bytes, **kwargs)\n    return ds\n",
  "import math\nimport torch\nfrom torch.utils.data import Sampler\nimport torch.distributed as dist\n\n\nclass OrderedDistributedSampler(Sampler):\n    \"\"\"Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    \"\"\"\n\n    def __init__(self, dataset, num_replicas=None, rank=None):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n\n    def __iter__(self):\n        indices = list(range(len(self.dataset)))\n\n        # add extra samples to make it evenly divisible\n        indices += indices[:(self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n\nclass RepeatAugSampler(Sampler):\n    \"\"\"Sampler that restricts data loading to a subset of the dataset for distributed,\n    with repeated augmentation.\n    It ensures that different each augmented version of a sample will be visible to a\n    different process (GPU). Heavily based on torch.utils.data.DistributedSampler\n\n    This sampler was taken from https://github.com/facebookresearch/deit/blob/0c4b8f60/samplers.py\n    Used in\n    Copyright (c) 2015-present, Facebook, Inc.\n    \"\"\"\n\n    def __init__(\n            self,\n            dataset,\n            num_replicas=None,\n            rank=None,\n            shuffle=True,\n            num_repeats=3,\n            selected_round=256,\n            selected_ratio=0,\n    ):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.shuffle = shuffle\n        self.num_repeats = num_repeats\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * num_repeats / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        # Determine the number of samples to select per epoch for each rank.\n        # num_selected logic defaults to be the same as original RASampler impl, but this one can be tweaked\n        # via selected_ratio and selected_round args.\n        selected_ratio = selected_ratio or num_replicas  # ratio to reduce selected samples by, num_replicas if 0\n        if selected_round:\n            self.num_selected_samples = int(math.floor(\n                 len(self.dataset) // selected_round * selected_round / selected_ratio))\n        else:\n            self.num_selected_samples = int(math.ceil(len(self.dataset) / selected_ratio))\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n        if self.shuffle:\n            indices = torch.randperm(len(self.dataset), generator=g)\n        else:\n            indices = torch.arange(start=0, end=len(self.dataset))\n\n        # produce repeats e.g. [0, 0, 0, 1, 1, 1, 2, 2, 2....]\n        if isinstance(self.num_repeats, float) and not self.num_repeats.is_integer():\n            # resample for repeats w/ non-integer ratio\n            repeat_size = math.ceil(self.num_repeats * len(self.dataset))\n            indices = indices[torch.tensor([int(i // self.num_repeats) for i in range(repeat_size)])]\n        else:\n            indices = torch.repeat_interleave(indices, repeats=int(self.num_repeats), dim=0)\n        indices = indices.tolist()  # leaving as tensor thrashes dataloader memory\n        # add extra samples to make it evenly divisible\n        padding_size = self.total_size - len(indices)\n        if padding_size > 0:\n            indices += indices[:padding_size]\n        assert len(indices) == self.total_size\n\n        # subsample per rank\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        # return up to num selected samples\n        return iter(indices[:self.num_selected_samples])\n\n    def __len__(self):\n        return self.num_selected_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n",
  "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Optional, Union\n\n\nclass DatasetInfo(ABC):\n\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def num_classes(self):\n        pass\n\n    @abstractmethod\n    def label_names(self):\n        pass\n\n    @abstractmethod\n    def label_descriptions(self, detailed: bool = False, as_dict: bool = False) -> Union[List[str], Dict[str, str]]:\n        pass\n\n    @abstractmethod\n    def index_to_label_name(self, index) -> str:\n        pass\n\n    @abstractmethod\n    def index_to_description(self, index: int, detailed: bool = False) -> str:\n        pass\n\n    @abstractmethod\n    def label_name_to_description(self, label: str, detailed: bool = False) -> str:\n        pass\n\n\nclass CustomDatasetInfo(DatasetInfo):\n    \"\"\" DatasetInfo that wraps passed values for custom datasets.\"\"\"\n\n    def __init__(\n            self,\n            label_names: Union[List[str], Dict[int, str]],\n            label_descriptions: Optional[Dict[str, str]] = None\n    ):\n        super().__init__()\n        assert len(label_names) > 0\n        self._label_names = label_names  # label index => label name mapping\n        self._label_descriptions = label_descriptions  # label name => label description mapping\n        if self._label_descriptions is not None:\n            # validate descriptions (label names required)\n            assert isinstance(self._label_descriptions, dict)\n            for n in self._label_names:\n                assert n in self._label_descriptions\n\n    def num_classes(self):\n        return len(self._label_names)\n\n    def label_names(self):\n        return self._label_names\n\n    def label_descriptions(self, detailed: bool = False, as_dict: bool = False) -> Union[List[str], Dict[str, str]]:\n        return self._label_descriptions\n\n    def label_name_to_description(self, label: str, detailed: bool = False) -> str:\n        if self._label_descriptions:\n            return self._label_descriptions[label]\n        return label  # return label name itself if a descriptions is not present\n\n    def index_to_label_name(self, index) -> str:\n        assert 0 <= index < len(self._label_names)\n        return self._label_names[index]\n\n    def index_to_description(self, index: int, detailed: bool = False) -> str:\n        label = self.index_to_label_name(index)\n        return self.label_name_to_description(label, detailed=detailed)\n",
  "\"\"\" Quick n Simple Image Folder, Tarfile based DataSet\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\nimport io\nimport logging\nfrom typing import Optional\n\nimport torch\nimport torch.utils.data as data\nfrom PIL import Image\n\nfrom .readers import create_reader\n\n_logger = logging.getLogger(__name__)\n\n\n_ERROR_RETRY = 50\n\n\nclass ImageDataset(data.Dataset):\n\n    def __init__(\n            self,\n            root,\n            reader=None,\n            split='train',\n            class_map=None,\n            load_bytes=False,\n            img_mode='RGB',\n            transform=None,\n            target_transform=None,\n    ):\n        if reader is None or isinstance(reader, str):\n            reader = create_reader(\n                reader or '',\n                root=root,\n                split=split,\n                class_map=class_map\n            )\n        self.reader = reader\n        self.load_bytes = load_bytes\n        self.img_mode = img_mode\n        self.transform = transform\n        self.target_transform = target_transform\n        self._consecutive_errors = 0\n\n    def __getitem__(self, index):\n        img, target = self.reader[index]\n\n        try:\n            img = img.read() if self.load_bytes else Image.open(img)\n        except Exception as e:\n            _logger.warning(f'Skipped sample (index {index}, file {self.reader.filename(index)}). {str(e)}')\n            self._consecutive_errors += 1\n            if self._consecutive_errors < _ERROR_RETRY:\n                return self.__getitem__((index + 1) % len(self.reader))\n            else:\n                raise e\n        self._consecutive_errors = 0\n\n        if self.img_mode and not self.load_bytes:\n            img = img.convert(self.img_mode)\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if target is None:\n            target = -1\n        elif self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.reader)\n\n    def filename(self, index, basename=False, absolute=False):\n        return self.reader.filename(index, basename, absolute)\n\n    def filenames(self, basename=False, absolute=False):\n        return self.reader.filenames(basename, absolute)\n\n\nclass IterableImageDataset(data.IterableDataset):\n\n    def __init__(\n            self,\n            root,\n            reader=None,\n            split='train',\n            class_map=None,\n            is_training=False,\n            batch_size=None,\n            seed=42,\n            repeats=0,\n            download=False,\n            transform=None,\n            target_transform=None,\n    ):\n        assert reader is not None\n        if isinstance(reader, str):\n            self.reader = create_reader(\n                reader,\n                root=root,\n                split=split,\n                class_map=class_map,\n                is_training=is_training,\n                batch_size=batch_size,\n                seed=seed,\n                repeats=repeats,\n                download=download,\n            )\n        else:\n            self.reader = reader\n        self.transform = transform\n        self.target_transform = target_transform\n        self._consecutive_errors = 0\n\n    def __iter__(self):\n        for img, target in self.reader:\n            if self.transform is not None:\n                img = self.transform(img)\n            if self.target_transform is not None:\n                target = self.target_transform(target)\n            yield img, target\n\n    def __len__(self):\n        if hasattr(self.reader, '__len__'):\n            return len(self.reader)\n        else:\n            return 0\n\n    def set_epoch(self, count):\n        # TFDS and WDS need external epoch count for deterministic cross process shuffle\n        if hasattr(self.reader, 'set_epoch'):\n            self.reader.set_epoch(count)\n\n    def set_loader_cfg(\n            self,\n            num_workers: Optional[int] = None,\n    ):\n        # TFDS and WDS readers need # workers for correct # samples estimate before loader processes created\n        if hasattr(self.reader, 'set_loader_cfg'):\n            self.reader.set_loader_cfg(num_workers=num_workers)\n\n    def filename(self, index, basename=False, absolute=False):\n        assert False, 'Filename lookup by index not supported, use filenames().'\n\n    def filenames(self, basename=False, absolute=False):\n        return self.reader.filenames(basename, absolute)\n\n\nclass AugMixDataset(torch.utils.data.Dataset):\n    \"\"\"Dataset wrapper to perform AugMix or other clean/augmentation mixes\"\"\"\n\n    def __init__(self, dataset, num_splits=2):\n        self.augmentation = None\n        self.normalize = None\n        self.dataset = dataset\n        if self.dataset.transform is not None:\n            self._set_transforms(self.dataset.transform)\n        self.num_splits = num_splits\n\n    def _set_transforms(self, x):\n        assert isinstance(x, (list, tuple)) and len(x) == 3, 'Expecting a tuple/list of 3 transforms'\n        self.dataset.transform = x[0]\n        self.augmentation = x[1]\n        self.normalize = x[2]\n\n    @property\n    def transform(self):\n        return self.dataset.transform\n\n    @transform.setter\n    def transform(self, x):\n        self._set_transforms(x)\n\n    def _normalize(self, x):\n        return x if self.normalize is None else self.normalize(x)\n\n    def __getitem__(self, i):\n        x, y = self.dataset[i]  # all splits share the same dataset base transform\n        x_list = [self._normalize(x)]  # first split only normalizes (this is the 'clean' split)\n        # run the full augmentation on the remaining splits\n        for _ in range(self.num_splits - 1):\n            x_list.append(self._normalize(self.augmentation(x)))\n        return tuple(x_list), y\n\n    def __len__(self):\n        return len(self.dataset)\n",
  "\"\"\" Real labels evaluator for ImageNet\nPaper: `Are we done with ImageNet?` - https://arxiv.org/abs/2006.07159\nBased on Numpy example at https://github.com/google-research/reassessed-imagenet\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport os\nimport json\nimport numpy as np\nimport pkgutil\n\n\nclass RealLabelsImagenet:\n\n    def __init__(self, filenames, real_json=None, topk=(1, 5)):\n        if real_json is not None:\n            with open(real_json) as real_labels:\n                real_labels = json.load(real_labels)\n        else:\n            real_labels = json.loads(\n                pkgutil.get_data(__name__, os.path.join('_info', 'imagenet_real_labels.json')).decode('utf-8'))\n        real_labels = {f'ILSVRC2012_val_{i + 1:08d}.JPEG': labels for i, labels in enumerate(real_labels)}\n        self.real_labels = real_labels\n        self.filenames = filenames\n        assert len(self.filenames) == len(self.real_labels)\n        self.topk = topk\n        self.is_correct = {k: [] for k in topk}\n        self.sample_idx = 0\n\n    def add_result(self, output):\n        maxk = max(self.topk)\n        _, pred_batch = output.topk(maxk, 1, True, True)\n        pred_batch = pred_batch.cpu().numpy()\n        for pred in pred_batch:\n            filename = self.filenames[self.sample_idx]\n            filename = os.path.basename(filename)\n            if self.real_labels[filename]:\n                for k in self.topk:\n                    self.is_correct[k].append(\n                        any([p in self.real_labels[filename] for p in pred[:k]]))\n            self.sample_idx += 1\n\n    def get_accuracy(self, k=None):\n        if k is None:\n            return {k: float(np.mean(self.is_correct[k])) * 100 for k in self.topk}\n        else:\n            return float(np.mean(self.is_correct[k])) * 100\n",
  "\"\"\" Tensorflow Preprocessing Adapter\n\nAllows use of Tensorflow preprocessing pipeline in PyTorch Transform\n\nCopyright of original Tensorflow code below.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\n# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"ImageNet preprocessing for MnasNet.\"\"\"\nimport tensorflow.compat.v1 as tf\nimport numpy as np\n\nIMAGE_SIZE = 224\nCROP_PADDING = 32\n\ntf.compat.v1.disable_eager_execution()\n\ndef distorted_bounding_box_crop(image_bytes,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n    \"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n\n    See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n    Args:\n      image_bytes: `Tensor` of binary image data.\n      bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n          where each coordinate is [0, 1) and the coordinates are arranged\n          as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n          image.\n      min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n          area of the image must contain at least this fraction of any bounding\n          box supplied.\n      aspect_ratio_range: An optional list of `float`s. The cropped area of the\n          image must have an aspect ratio = width / height within this range.\n      area_range: An optional list of `float`s. The cropped area of the image\n          must contain a fraction of the supplied image within in this range.\n      max_attempts: An optional `int`. Number of attempts at generating a cropped\n          region of the image of the specified constraints. After `max_attempts`\n          failures, return the entire image.\n      scope: Optional `str` for name scope.\n    Returns:\n      cropped image `Tensor`\n    \"\"\"\n    with tf.name_scope(scope, 'distorted_bounding_box_crop', [image_bytes, bbox]):\n        shape = tf.image.extract_jpeg_shape(image_bytes)\n        sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n            shape,\n            bounding_boxes=bbox,\n            min_object_covered=min_object_covered,\n            aspect_ratio_range=aspect_ratio_range,\n            area_range=area_range,\n            max_attempts=max_attempts,\n            use_image_if_no_bounding_boxes=True)\n        bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n        # Crop the image to the specified bounding box.\n        offset_y, offset_x, _ = tf.unstack(bbox_begin)\n        target_height, target_width, _ = tf.unstack(bbox_size)\n        crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n        image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n\n        return image\n\n\ndef _at_least_x_are_equal(a, b, x):\n    \"\"\"At least `x` of `a` and `b` `Tensors` are equal.\"\"\"\n    match = tf.equal(a, b)\n    match = tf.cast(match, tf.int32)\n    return tf.greater_equal(tf.reduce_sum(match), x)\n\n\ndef _decode_and_random_crop(image_bytes, image_size, resize_method):\n    \"\"\"Make a random crop of image_size.\"\"\"\n    bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n    image = distorted_bounding_box_crop(\n        image_bytes,\n        bbox,\n        min_object_covered=0.1,\n        aspect_ratio_range=(3. / 4, 4. / 3.),\n        area_range=(0.08, 1.0),\n        max_attempts=10,\n        scope=None)\n    original_shape = tf.image.extract_jpeg_shape(image_bytes)\n    bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)\n\n    image = tf.cond(\n        bad,\n        lambda: _decode_and_center_crop(image_bytes, image_size),\n        lambda: tf.image.resize([image], [image_size, image_size], resize_method)[0])\n\n    return image\n\n\ndef _decode_and_center_crop(image_bytes, image_size, resize_method):\n    \"\"\"Crops to center of image with padding then scales image_size.\"\"\"\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n\n    return image\n\n\ndef _flip(image):\n    \"\"\"Random horizontal image flip.\"\"\"\n    image = tf.image.random_flip_left_right(image)\n    return image\n\n\ndef preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic'):\n    \"\"\"Preprocesses the given image for evaluation.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor`.\n    \"\"\"\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_random_crop(image_bytes, image_size, resize_method)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n\ndef preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic'):\n    \"\"\"Preprocesses the given image for evaluation.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor`.\n    \"\"\"\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n\ndef preprocess_image(image_bytes,\n                     is_training=False,\n                     use_bfloat16=False,\n                     image_size=IMAGE_SIZE,\n                     interpolation='bicubic'):\n    \"\"\"Preprocesses the given image.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      is_training: `bool` for whether the preprocessing is for training.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor` with value range of [0, 255].\n    \"\"\"\n    if is_training:\n        return preprocess_for_train(image_bytes, use_bfloat16, image_size, interpolation)\n    else:\n        return preprocess_for_eval(image_bytes, use_bfloat16, image_size, interpolation)\n\n\nclass TfPreprocessTransform:\n\n    def __init__(self, is_training=False, size=224, interpolation='bicubic'):\n        self.is_training = is_training\n        self.size = size[0] if isinstance(size, tuple) else size\n        self.interpolation = interpolation\n        self._image_bytes = None\n        self.process_image = self._build_tf_graph()\n        self.sess = None\n\n    def _build_tf_graph(self):\n        with tf.device('/cpu:0'):\n            self._image_bytes = tf.placeholder(\n                shape=[],\n                dtype=tf.string,\n            )\n            img = preprocess_image(\n                self._image_bytes, self.is_training, False, self.size, self.interpolation)\n        return img\n\n    def __call__(self, image_bytes):\n        if self.sess is None:\n            self.sess = tf.Session()\n        img = self.sess.run(self.process_image, feed_dict={self._image_bytes: image_bytes})\n        img = img.round().clip(0, 255).astype(np.uint8)\n        if img.ndim < 3:\n            img = np.expand_dims(img, axis=-1)\n        img = np.rollaxis(img, 2)  # HWC to CHW\n        return img\n",
  "import csv\nimport os\nimport pkgutil\nimport re\nfrom typing import Dict, List, Optional, Union\n\nfrom .dataset_info import DatasetInfo\n\n\n# NOTE no ambiguity wrt to mapping from # classes to ImageNet subset so far, but likely to change\n_NUM_CLASSES_TO_SUBSET = {\n    1000: 'imagenet-1k',\n    11221: 'imagenet-21k-miil',  # miil subset of fall11\n    11821: 'imagenet-12k',  # timm specific 12k subset of fall11\n    21841: 'imagenet-22k',  # as in fall11.tar\n    21842: 'imagenet-22k-ms',  # a Microsoft (for FocalNet) remapping of 22k w/ moves ImageNet-1k classes to first 1000\n    21843: 'imagenet-21k-goog',  # Google's ImageNet full has two classes not in fall11\n}\n\n_SUBSETS = {\n    'imagenet1k': 'imagenet_synsets.txt',\n    'imagenet12k': 'imagenet12k_synsets.txt',\n    'imagenet22k': 'imagenet22k_synsets.txt',\n    'imagenet21k': 'imagenet21k_goog_synsets.txt',\n    'imagenet21kgoog': 'imagenet21k_goog_synsets.txt',\n    'imagenet21kmiil': 'imagenet21k_miil_synsets.txt',\n    'imagenet22kms': 'imagenet22k_ms_synsets.txt',\n}\n_LEMMA_FILE = 'imagenet_synset_to_lemma.txt'\n_DEFINITION_FILE = 'imagenet_synset_to_definition.txt'\n\n\ndef infer_imagenet_subset(model_or_cfg) -> Optional[str]:\n    if isinstance(model_or_cfg, dict):\n        num_classes = model_or_cfg.get('num_classes', None)\n    else:\n        num_classes = getattr(model_or_cfg, 'num_classes', None)\n        if not num_classes:\n            pretrained_cfg = getattr(model_or_cfg, 'pretrained_cfg', {})\n            # FIXME at some point pretrained_cfg should include dataset-tag,\n            # which will be more robust than a guess based on num_classes\n            num_classes = pretrained_cfg.get('num_classes', None)\n    if not num_classes or num_classes not in _NUM_CLASSES_TO_SUBSET:\n        return None\n    return _NUM_CLASSES_TO_SUBSET[num_classes]\n\n\nclass ImageNetInfo(DatasetInfo):\n\n    def __init__(self, subset: str = 'imagenet-1k'):\n        super().__init__()\n        subset = re.sub(r'[-_\\s]', '', subset.lower())\n        assert subset in _SUBSETS, f'Unknown imagenet subset {subset}.'\n\n        # WordNet synsets (part-of-speach + offset) are the unique class label names for ImageNet classifiers\n        synset_file = _SUBSETS[subset]\n        synset_data = pkgutil.get_data(__name__, os.path.join('_info', synset_file))\n        self._synsets = synset_data.decode('utf-8').splitlines()\n\n        # WordNet lemmas (canonical dictionary form of word) and definitions are used to build\n        # the class descriptions. If detailed=True both are used, otherwise just the lemmas.\n        lemma_data = pkgutil.get_data(__name__, os.path.join('_info', _LEMMA_FILE))\n        reader = csv.reader(lemma_data.decode('utf-8').splitlines(), delimiter='\\t')\n        self._lemmas = dict(reader)\n        definition_data = pkgutil.get_data(__name__, os.path.join('_info', _DEFINITION_FILE))\n        reader = csv.reader(definition_data.decode('utf-8').splitlines(), delimiter='\\t')\n        self._definitions = dict(reader)\n\n    def num_classes(self):\n        return len(self._synsets)\n\n    def label_names(self):\n        return self._synsets\n\n    def label_descriptions(self, detailed: bool = False, as_dict: bool = False) -> Union[List[str], Dict[str, str]]:\n        if as_dict:\n            return {label: self.label_name_to_description(label, detailed=detailed) for label in self._synsets}\n        else:\n            return [self.label_name_to_description(label, detailed=detailed) for label in self._synsets]\n\n    def index_to_label_name(self, index) -> str:\n        assert 0 <= index < len(self._synsets), \\\n            f'Index ({index}) out of range for dataset with {len(self._synsets)} classes.'\n        return self._synsets[index]\n\n    def index_to_description(self, index: int, detailed: bool = False) -> str:\n        label = self.index_to_label_name(index)\n        return self.label_name_to_description(label, detailed=detailed)\n\n    def label_name_to_description(self, label: str, detailed: bool = False) -> str:\n        if detailed:\n            description = f'{self._lemmas[label]}: {self._definitions[label]}'\n        else:\n            description = f'{self._lemmas[label]}'\n        return description\n",
  "\"\"\" AutoAugment, RandAugment, AugMix, and 3-Augment for PyTorch\n\nThis code implements the searched ImageNet policies with various tweaks and improvements and\ndoes not include any of the search code.\n\nAA and RA Implementation adapted from:\n    https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py\n\nAugMix adapted from:\n    https://github.com/google-research/augmix\n\n3-Augment based on: https://github.com/facebookresearch/deit/blob/main/README_revenge.md\n\nPapers:\n    AutoAugment: Learning Augmentation Policies from Data - https://arxiv.org/abs/1805.09501\n    Learning Data Augmentation Strategies for Object Detection - https://arxiv.org/abs/1906.11172\n    RandAugment: Practical automated data augmentation... - https://arxiv.org/abs/1909.13719\n    AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty - https://arxiv.org/abs/1912.02781\n    3-Augment: DeiT III: Revenge of the ViT - https://arxiv.org/abs/2204.07118\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\nimport random\nimport math\nimport re\nfrom functools import partial\nfrom typing import Dict, List, Optional, Union\n\nfrom PIL import Image, ImageOps, ImageEnhance, ImageChops, ImageFilter\nimport PIL\nimport numpy as np\n\n\n_PIL_VER = tuple([int(x) for x in PIL.__version__.split('.')[:2]])\n\n_FILL = (128, 128, 128)\n\n_LEVEL_DENOM = 10.  # denominator for conversion from 'Mx' magnitude scale to fractional aug level for op arguments\n\n_HPARAMS_DEFAULT = dict(\n    translate_const=250,\n    img_mean=_FILL,\n)\n\nif hasattr(Image, \"Resampling\"):\n    _RANDOM_INTERPOLATION = (Image.Resampling.BILINEAR, Image.Resampling.BICUBIC)\n    _DEFAULT_INTERPOLATION = Image.Resampling.BICUBIC\nelse:\n    _RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n    _DEFAULT_INTERPOLATION = Image.BICUBIC\n\n\ndef _interpolation(kwargs):\n    interpolation = kwargs.pop('resample', _DEFAULT_INTERPOLATION)\n    if isinstance(interpolation, (list, tuple)):\n        return random.choice(interpolation)\n    return interpolation\n\n\ndef _check_args_tf(kwargs):\n    if 'fillcolor' in kwargs and _PIL_VER < (5, 0):\n        kwargs.pop('fillcolor')\n    kwargs['resample'] = _interpolation(kwargs)\n\n\ndef shear_x(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, factor, 0, 0, 1, 0), **kwargs)\n\n\ndef shear_y(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0), **kwargs)\n\n\ndef translate_x_rel(img, pct, **kwargs):\n    pixels = pct * img.size[0]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)\n\n\ndef translate_y_rel(img, pct, **kwargs):\n    pixels = pct * img.size[1]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)\n\n\ndef translate_x_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)\n\n\ndef translate_y_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)\n\n\ndef rotate(img, degrees, **kwargs):\n    _check_args_tf(kwargs)\n    if _PIL_VER >= (5, 2):\n        return img.rotate(degrees, **kwargs)\n    if _PIL_VER >= (5, 0):\n        w, h = img.size\n        post_trans = (0, 0)\n        rotn_center = (w / 2.0, h / 2.0)\n        angle = -math.radians(degrees)\n        matrix = [\n            round(math.cos(angle), 15),\n            round(math.sin(angle), 15),\n            0.0,\n            round(-math.sin(angle), 15),\n            round(math.cos(angle), 15),\n            0.0,\n        ]\n\n        def transform(x, y, matrix):\n            (a, b, c, d, e, f) = matrix\n            return a * x + b * y + c, d * x + e * y + f\n\n        matrix[2], matrix[5] = transform(\n            -rotn_center[0] - post_trans[0], -rotn_center[1] - post_trans[1], matrix\n        )\n        matrix[2] += rotn_center[0]\n        matrix[5] += rotn_center[1]\n        return img.transform(img.size, Image.AFFINE, matrix, **kwargs)\n    return img.rotate(degrees, resample=kwargs['resample'])\n\n\ndef auto_contrast(img, **__):\n    return ImageOps.autocontrast(img)\n\n\ndef invert(img, **__):\n    return ImageOps.invert(img)\n\n\ndef equalize(img, **__):\n    return ImageOps.equalize(img)\n\n\ndef solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\n\n\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n\n    return img\n\n\ndef posterize(img, bits_to_keep, **__):\n    if bits_to_keep >= 8:\n        return img\n    return ImageOps.posterize(img, bits_to_keep)\n\n\ndef contrast(img, factor, **__):\n    return ImageEnhance.Contrast(img).enhance(factor)\n\n\ndef color(img, factor, **__):\n    return ImageEnhance.Color(img).enhance(factor)\n\n\ndef brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)\n\n\ndef sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\n\n\ndef gaussian_blur(img, factor, **__):\n    img = img.filter(ImageFilter.GaussianBlur(radius=factor))\n    return img\n\n\ndef gaussian_blur_rand(img, factor, **__):\n    radius_min = 0.1\n    radius_max = 2.0\n    img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(radius_min, radius_max * factor)))\n    return img\n\n\ndef desaturate(img, factor, **_):\n    factor = min(1., max(0., 1. - factor))\n    # enhance factor 0 = grayscale, 1.0 = no-change\n    return ImageEnhance.Color(img).enhance(factor)\n\n\ndef _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"\n    return -v if random.random() > 0.5 else v\n\n\ndef _rotate_level_to_arg(level, _hparams):\n    # range [-30, 30]\n    level = (level / _LEVEL_DENOM) * 30.\n    level = _randomly_negate(level)\n    return level,\n\n\ndef _enhance_level_to_arg(level, _hparams):\n    # range [0.1, 1.9]\n    return (level / _LEVEL_DENOM) * 1.8 + 0.1,\n\n\ndef _enhance_increasing_level_to_arg(level, _hparams):\n    # the 'no change' level is 1.0, moving away from that towards 0. or 2.0 increases the enhancement blend\n    # range [0.1, 1.9] if level <= _LEVEL_DENOM\n    level = (level / _LEVEL_DENOM) * .9\n    level = max(0.1, 1.0 + _randomly_negate(level))  # keep it >= 0.1\n    return level,\n\n\ndef _minmax_level_to_arg(level, _hparams, min_val=0., max_val=1.0, clamp=True):\n    level = (level / _LEVEL_DENOM)\n    level = min_val + (max_val - min_val) * level\n    if clamp:\n        level = max(min_val, min(max_val, level))\n    return level,\n\n\ndef _shear_level_to_arg(level, _hparams):\n    # range [-0.3, 0.3]\n    level = (level / _LEVEL_DENOM) * 0.3\n    level = _randomly_negate(level)\n    return level,\n\n\ndef _translate_abs_level_to_arg(level, hparams):\n    translate_const = hparams['translate_const']\n    level = (level / _LEVEL_DENOM) * float(translate_const)\n    level = _randomly_negate(level)\n    return level,\n\n\ndef _translate_rel_level_to_arg(level, hparams):\n    # default range [-0.45, 0.45]\n    translate_pct = hparams.get('translate_pct', 0.45)\n    level = (level / _LEVEL_DENOM) * translate_pct\n    level = _randomly_negate(level)\n    return level,\n\n\ndef _posterize_level_to_arg(level, _hparams):\n    # As per Tensorflow TPU EfficientNet impl\n    # range [0, 4], 'keep 0 up to 4 MSB of original image'\n    # intensity/severity of augmentation decreases with level\n    return int((level / _LEVEL_DENOM) * 4),\n\n\ndef _posterize_increasing_level_to_arg(level, hparams):\n    # As per Tensorflow models research and UDA impl\n    # range [4, 0], 'keep 4 down to 0 MSB of original image',\n    # intensity/severity of augmentation increases with level\n    return 4 - _posterize_level_to_arg(level, hparams)[0],\n\n\ndef _posterize_original_level_to_arg(level, _hparams):\n    # As per original AutoAugment paper description\n    # range [4, 8], 'keep 4 up to 8 MSB of image'\n    # intensity/severity of augmentation decreases with level\n    return int((level / _LEVEL_DENOM) * 4) + 4,\n\n\ndef _solarize_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation decreases with level\n    return min(256, int((level / _LEVEL_DENOM) * 256)),\n\n\ndef _solarize_increasing_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation increases with level\n    return 256 - _solarize_level_to_arg(level, _hparams)[0],\n\n\ndef _solarize_add_level_to_arg(level, _hparams):\n    # range [0, 110]\n    return min(128, int((level / _LEVEL_DENOM) * 110)),\n\n\nLEVEL_TO_ARG = {\n    'AutoContrast': None,\n    'Equalize': None,\n    'Invert': None,\n    'Rotate': _rotate_level_to_arg,\n    # There are several variations of the posterize level scaling in various Tensorflow/Google repositories/papers\n    'Posterize': _posterize_level_to_arg,\n    'PosterizeIncreasing': _posterize_increasing_level_to_arg,\n    'PosterizeOriginal': _posterize_original_level_to_arg,\n    'Solarize': _solarize_level_to_arg,\n    'SolarizeIncreasing': _solarize_increasing_level_to_arg,\n    'SolarizeAdd': _solarize_add_level_to_arg,\n    'Color': _enhance_level_to_arg,\n    'ColorIncreasing': _enhance_increasing_level_to_arg,\n    'Contrast': _enhance_level_to_arg,\n    'ContrastIncreasing': _enhance_increasing_level_to_arg,\n    'Brightness': _enhance_level_to_arg,\n    'BrightnessIncreasing': _enhance_increasing_level_to_arg,\n    'Sharpness': _enhance_level_to_arg,\n    'SharpnessIncreasing': _enhance_increasing_level_to_arg,\n    'ShearX': _shear_level_to_arg,\n    'ShearY': _shear_level_to_arg,\n    'TranslateX': _translate_abs_level_to_arg,\n    'TranslateY': _translate_abs_level_to_arg,\n    'TranslateXRel': _translate_rel_level_to_arg,\n    'TranslateYRel': _translate_rel_level_to_arg,\n    'Desaturate': partial(_minmax_level_to_arg, min_val=0.5, max_val=1.0),\n    'GaussianBlur': partial(_minmax_level_to_arg, min_val=0.1, max_val=2.0),\n    'GaussianBlurRand': _minmax_level_to_arg,\n}\n\n\nNAME_TO_OP = {\n    'AutoContrast': auto_contrast,\n    'Equalize': equalize,\n    'Invert': invert,\n    'Rotate': rotate,\n    'Posterize': posterize,\n    'PosterizeIncreasing': posterize,\n    'PosterizeOriginal': posterize,\n    'Solarize': solarize,\n    'SolarizeIncreasing': solarize,\n    'SolarizeAdd': solarize_add,\n    'Color': color,\n    'ColorIncreasing': color,\n    'Contrast': contrast,\n    'ContrastIncreasing': contrast,\n    'Brightness': brightness,\n    'BrightnessIncreasing': brightness,\n    'Sharpness': sharpness,\n    'SharpnessIncreasing': sharpness,\n    'ShearX': shear_x,\n    'ShearY': shear_y,\n    'TranslateX': translate_x_abs,\n    'TranslateY': translate_y_abs,\n    'TranslateXRel': translate_x_rel,\n    'TranslateYRel': translate_y_rel,\n    'Desaturate': desaturate,\n    'GaussianBlur': gaussian_blur,\n    'GaussianBlurRand': gaussian_blur_rand,\n}\n\n\nclass AugmentOp:\n\n    def __init__(self, name, prob=0.5, magnitude=10, hparams=None):\n        hparams = hparams or _HPARAMS_DEFAULT\n        self.name = name\n        self.aug_fn = NAME_TO_OP[name]\n        self.level_fn = LEVEL_TO_ARG[name]\n        self.prob = prob\n        self.magnitude = magnitude\n        self.hparams = hparams.copy()\n        self.kwargs = dict(\n            fillcolor=hparams['img_mean'] if 'img_mean' in hparams else _FILL,\n            resample=hparams['interpolation'] if 'interpolation' in hparams else _RANDOM_INTERPOLATION,\n        )\n\n        # If magnitude_std is > 0, we introduce some randomness\n        # in the usually fixed policy and sample magnitude from a normal distribution\n        # with mean `magnitude` and std-dev of `magnitude_std`.\n        # NOTE This is my own hack, being tested, not in papers or reference impls.\n        # If magnitude_std is inf, we sample magnitude from a uniform distribution\n        self.magnitude_std = self.hparams.get('magnitude_std', 0)\n        self.magnitude_max = self.hparams.get('magnitude_max', None)\n\n    def __call__(self, img):\n        if self.prob < 1.0 and random.random() > self.prob:\n            return img\n        magnitude = self.magnitude\n        if self.magnitude_std > 0:\n            # magnitude randomization enabled\n            if self.magnitude_std == float('inf'):\n                # inf == uniform sampling\n                magnitude = random.uniform(0, magnitude)\n            elif self.magnitude_std > 0:\n                magnitude = random.gauss(magnitude, self.magnitude_std)\n        # default upper_bound for the timm RA impl is _LEVEL_DENOM (10)\n        # setting magnitude_max overrides this to allow M > 10 (behaviour closer to Google TF RA impl)\n        upper_bound = self.magnitude_max or _LEVEL_DENOM\n        magnitude = max(0., min(magnitude, upper_bound))\n        level_args = self.level_fn(magnitude, self.hparams) if self.level_fn is not None else tuple()\n        return self.aug_fn(img, *level_args, **self.kwargs)\n\n    def __repr__(self):\n        fs = self.__class__.__name__ + f'(name={self.name}, p={self.prob}'\n        fs += f', m={self.magnitude}, mstd={self.magnitude_std}'\n        if self.magnitude_max is not None:\n            fs += f', mmax={self.magnitude_max}'\n        fs += ')'\n        return fs\n\n\ndef auto_augment_policy_v0(hparams):\n    # ImageNet v0 policy from TPU EfficientNet impl, cannot find a paper reference.\n    policy = [\n        [('Equalize', 0.8, 1), ('ShearY', 0.8, 4)],\n        [('Color', 0.4, 9), ('Equalize', 0.6, 3)],\n        [('Color', 0.4, 1), ('Rotate', 0.6, 8)],\n        [('Solarize', 0.8, 3), ('Equalize', 0.4, 7)],\n        [('Solarize', 0.4, 2), ('Solarize', 0.6, 2)],\n        [('Color', 0.2, 0), ('Equalize', 0.8, 8)],\n        [('Equalize', 0.4, 8), ('SolarizeAdd', 0.8, 3)],\n        [('ShearX', 0.2, 9), ('Rotate', 0.6, 8)],\n        [('Color', 0.6, 1), ('Equalize', 1.0, 2)],\n        [('Invert', 0.4, 9), ('Rotate', 0.6, 0)],\n        [('Equalize', 1.0, 9), ('ShearY', 0.6, 3)],\n        [('Color', 0.4, 7), ('Equalize', 0.6, 0)],\n        [('Posterize', 0.4, 6), ('AutoContrast', 0.4, 7)],\n        [('Solarize', 0.6, 8), ('Color', 0.6, 9)],\n        [('Solarize', 0.2, 4), ('Rotate', 0.8, 9)],\n        [('Rotate', 1.0, 7), ('TranslateYRel', 0.8, 9)],\n        [('ShearX', 0.0, 0), ('Solarize', 0.8, 4)],\n        [('ShearY', 0.8, 0), ('Color', 0.6, 4)],\n        [('Color', 1.0, 0), ('Rotate', 0.6, 2)],\n        [('Equalize', 0.8, 4), ('Equalize', 0.0, 8)],\n        [('Equalize', 1.0, 4), ('AutoContrast', 0.6, 2)],\n        [('ShearY', 0.4, 7), ('SolarizeAdd', 0.6, 7)],\n        [('Posterize', 0.8, 2), ('Solarize', 0.6, 10)],  # This results in black image with Tpu posterize\n        [('Solarize', 0.6, 8), ('Equalize', 0.6, 1)],\n        [('Color', 0.8, 6), ('Rotate', 0.4, 5)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_v0r(hparams):\n    # ImageNet v0 policy from TPU EfficientNet impl, with variation of Posterize used\n    # in Google research implementation (number of bits discarded increases with magnitude)\n    policy = [\n        [('Equalize', 0.8, 1), ('ShearY', 0.8, 4)],\n        [('Color', 0.4, 9), ('Equalize', 0.6, 3)],\n        [('Color', 0.4, 1), ('Rotate', 0.6, 8)],\n        [('Solarize', 0.8, 3), ('Equalize', 0.4, 7)],\n        [('Solarize', 0.4, 2), ('Solarize', 0.6, 2)],\n        [('Color', 0.2, 0), ('Equalize', 0.8, 8)],\n        [('Equalize', 0.4, 8), ('SolarizeAdd', 0.8, 3)],\n        [('ShearX', 0.2, 9), ('Rotate', 0.6, 8)],\n        [('Color', 0.6, 1), ('Equalize', 1.0, 2)],\n        [('Invert', 0.4, 9), ('Rotate', 0.6, 0)],\n        [('Equalize', 1.0, 9), ('ShearY', 0.6, 3)],\n        [('Color', 0.4, 7), ('Equalize', 0.6, 0)],\n        [('PosterizeIncreasing', 0.4, 6), ('AutoContrast', 0.4, 7)],\n        [('Solarize', 0.6, 8), ('Color', 0.6, 9)],\n        [('Solarize', 0.2, 4), ('Rotate', 0.8, 9)],\n        [('Rotate', 1.0, 7), ('TranslateYRel', 0.8, 9)],\n        [('ShearX', 0.0, 0), ('Solarize', 0.8, 4)],\n        [('ShearY', 0.8, 0), ('Color', 0.6, 4)],\n        [('Color', 1.0, 0), ('Rotate', 0.6, 2)],\n        [('Equalize', 0.8, 4), ('Equalize', 0.0, 8)],\n        [('Equalize', 1.0, 4), ('AutoContrast', 0.6, 2)],\n        [('ShearY', 0.4, 7), ('SolarizeAdd', 0.6, 7)],\n        [('PosterizeIncreasing', 0.8, 2), ('Solarize', 0.6, 10)],\n        [('Solarize', 0.6, 8), ('Equalize', 0.6, 1)],\n        [('Color', 0.8, 6), ('Rotate', 0.4, 5)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_original(hparams):\n    # ImageNet policy from https://arxiv.org/abs/1805.09501\n    policy = [\n        [('PosterizeOriginal', 0.4, 8), ('Rotate', 0.6, 9)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n        [('PosterizeOriginal', 0.6, 7), ('PosterizeOriginal', 0.6, 6)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Equalize', 0.4, 4), ('Rotate', 0.8, 8)],\n        [('Solarize', 0.6, 3), ('Equalize', 0.6, 7)],\n        [('PosterizeOriginal', 0.8, 5), ('Equalize', 1.0, 2)],\n        [('Rotate', 0.2, 3), ('Solarize', 0.6, 8)],\n        [('Equalize', 0.6, 8), ('PosterizeOriginal', 0.4, 6)],\n        [('Rotate', 0.8, 8), ('Color', 0.4, 0)],\n        [('Rotate', 0.4, 9), ('Equalize', 0.6, 2)],\n        [('Equalize', 0.0, 7), ('Equalize', 0.8, 8)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Rotate', 0.8, 8), ('Color', 1.0, 2)],\n        [('Color', 0.8, 8), ('Solarize', 0.8, 7)],\n        [('Sharpness', 0.4, 7), ('Invert', 0.6, 8)],\n        [('ShearX', 0.6, 5), ('Equalize', 1.0, 9)],\n        [('Color', 0.4, 0), ('Equalize', 0.6, 3)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_originalr(hparams):\n    # ImageNet policy from https://arxiv.org/abs/1805.09501 with research posterize variation\n    policy = [\n        [('PosterizeIncreasing', 0.4, 8), ('Rotate', 0.6, 9)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n        [('PosterizeIncreasing', 0.6, 7), ('PosterizeIncreasing', 0.6, 6)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Equalize', 0.4, 4), ('Rotate', 0.8, 8)],\n        [('Solarize', 0.6, 3), ('Equalize', 0.6, 7)],\n        [('PosterizeIncreasing', 0.8, 5), ('Equalize', 1.0, 2)],\n        [('Rotate', 0.2, 3), ('Solarize', 0.6, 8)],\n        [('Equalize', 0.6, 8), ('PosterizeIncreasing', 0.4, 6)],\n        [('Rotate', 0.8, 8), ('Color', 0.4, 0)],\n        [('Rotate', 0.4, 9), ('Equalize', 0.6, 2)],\n        [('Equalize', 0.0, 7), ('Equalize', 0.8, 8)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Rotate', 0.8, 8), ('Color', 1.0, 2)],\n        [('Color', 0.8, 8), ('Solarize', 0.8, 7)],\n        [('Sharpness', 0.4, 7), ('Invert', 0.6, 8)],\n        [('ShearX', 0.6, 5), ('Equalize', 1.0, 9)],\n        [('Color', 0.4, 0), ('Equalize', 0.6, 3)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_3a(hparams):\n    policy = [\n        [('Solarize', 1.0, 5)],  # 128 solarize threshold @ 5 magnitude\n        [('Desaturate', 1.0, 10)],  # grayscale at 10 magnitude\n        [('GaussianBlurRand', 1.0, 10)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy(name='v0', hparams=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    if name == 'original':\n        return auto_augment_policy_original(hparams)\n    if name == 'originalr':\n        return auto_augment_policy_originalr(hparams)\n    if name == 'v0':\n        return auto_augment_policy_v0(hparams)\n    if name == 'v0r':\n        return auto_augment_policy_v0r(hparams)\n    if name == '3a':\n        return auto_augment_policy_3a(hparams)\n    assert False, f'Unknown AA policy {name}'\n\n\nclass AutoAugment:\n\n    def __init__(self, policy):\n        self.policy = policy\n\n    def __call__(self, img):\n        sub_policy = random.choice(self.policy)\n        for op in sub_policy:\n            img = op(img)\n        return img\n\n    def __repr__(self):\n        fs = self.__class__.__name__ + '(policy='\n        for p in self.policy:\n            fs += '\\n\\t['\n            fs += ', '.join([str(op) for op in p])\n            fs += ']'\n        fs += ')'\n        return fs\n\n\ndef auto_augment_transform(config_str: str, hparams: Optional[Dict] = None):\n    \"\"\"\n    Create a AutoAugment transform\n\n    Args:\n        config_str: String defining configuration of auto augmentation. Consists of multiple sections separated by\n            dashes ('-').\n            The first section defines the AutoAugment policy (one of 'v0', 'v0r', 'original', 'originalr').\n\n            The remaining sections:\n                'mstd' -  float std deviation of magnitude noise applied\n            Ex 'original-mstd0.5' results in AutoAugment with original policy, magnitude_std 0.5\n\n        hparams: Other hparams (kwargs) for the AutoAugmentation scheme\n\n    Returns:\n         A PyTorch compatible Transform\n    \"\"\"\n    config = config_str.split('-')\n    policy_name = config[0]\n    config = config[1:]\n    for c in config:\n        cs = re.split(r'(\\d.*)', c)\n        if len(cs) < 2:\n            continue\n        key, val = cs[:2]\n        if key == 'mstd':\n            # noise param injected via hparams for now\n            hparams.setdefault('magnitude_std', float(val))\n        else:\n            assert False, 'Unknown AutoAugment config section'\n    aa_policy = auto_augment_policy(policy_name, hparams=hparams)\n    return AutoAugment(aa_policy)\n\n\n_RAND_TRANSFORMS = [\n    'AutoContrast',\n    'Equalize',\n    'Invert',\n    'Rotate',\n    'Posterize',\n    'Solarize',\n    'SolarizeAdd',\n    'Color',\n    'Contrast',\n    'Brightness',\n    'Sharpness',\n    'ShearX',\n    'ShearY',\n    'TranslateXRel',\n    'TranslateYRel',\n    # 'Cutout'  # NOTE I've implement this as random erasing separately\n]\n\n\n_RAND_INCREASING_TRANSFORMS = [\n    'AutoContrast',\n    'Equalize',\n    'Invert',\n    'Rotate',\n    'PosterizeIncreasing',\n    'SolarizeIncreasing',\n    'SolarizeAdd',\n    'ColorIncreasing',\n    'ContrastIncreasing',\n    'BrightnessIncreasing',\n    'SharpnessIncreasing',\n    'ShearX',\n    'ShearY',\n    'TranslateXRel',\n    'TranslateYRel',\n    # 'Cutout'  # NOTE I've implement this as random erasing separately\n]\n\n\n_RAND_3A = [\n    'SolarizeIncreasing',\n    'Desaturate',\n    'GaussianBlur',\n]\n\n\n_RAND_WEIGHTED_3A = {\n    'SolarizeIncreasing': 6,\n    'Desaturate': 6,\n    'GaussianBlur': 6,\n    'Rotate': 3,\n    'ShearX': 2,\n    'ShearY': 2,\n    'PosterizeIncreasing': 1,\n    'AutoContrast': 1,\n    'ColorIncreasing': 1,\n    'SharpnessIncreasing': 1,\n    'ContrastIncreasing': 1,\n    'BrightnessIncreasing': 1,\n    'Equalize': 1,\n    'Invert': 1,\n}\n\n\n# These experimental weights are based loosely on the relative improvements mentioned in paper.\n# They may not result in increased performance, but could likely be tuned to so.\n_RAND_WEIGHTED_0 = {\n    'Rotate': 3,\n    'ShearX': 2,\n    'ShearY': 2,\n    'TranslateXRel': 1,\n    'TranslateYRel': 1,\n    'ColorIncreasing': .25,\n    'SharpnessIncreasing': 0.25,\n    'AutoContrast': 0.25,\n    'SolarizeIncreasing': .05,\n    'SolarizeAdd': .05,\n    'ContrastIncreasing': .05,\n    'BrightnessIncreasing': .05,\n    'Equalize': .05,\n    'PosterizeIncreasing': 0.05,\n    'Invert': 0.05,\n}\n\n\ndef _get_weighted_transforms(transforms: Dict):\n    transforms, probs = list(zip(*transforms.items()))\n    probs = np.array(probs)\n    probs = probs / np.sum(probs)\n    return transforms, probs\n\n\ndef rand_augment_choices(name: str, increasing=True):\n    if name == 'weights':\n        return _RAND_WEIGHTED_0\n    if name == '3aw':\n        return _RAND_WEIGHTED_3A\n    if name == '3a':\n        return _RAND_3A\n    return _RAND_INCREASING_TRANSFORMS if increasing else _RAND_TRANSFORMS\n\n\ndef rand_augment_ops(\n        magnitude: Union[int, float] = 10,\n        prob: float = 0.5,\n        hparams: Optional[Dict] = None,\n        transforms: Optional[Union[Dict, List]] = None,\n):\n    hparams = hparams or _HPARAMS_DEFAULT\n    transforms = transforms or _RAND_TRANSFORMS\n    return [AugmentOp(\n        name, prob=prob, magnitude=magnitude, hparams=hparams) for name in transforms]\n\n\nclass RandAugment:\n    def __init__(self, ops, num_layers=2, choice_weights=None):\n        self.ops = ops\n        self.num_layers = num_layers\n        self.choice_weights = choice_weights\n\n    def __call__(self, img):\n        # no replacement when using weighted choice\n        ops = np.random.choice(\n            self.ops,\n            self.num_layers,\n            replace=self.choice_weights is None,\n            p=self.choice_weights,\n        )\n        for op in ops:\n            img = op(img)\n        return img\n\n    def __repr__(self):\n        fs = self.__class__.__name__ + f'(n={self.num_layers}, ops='\n        for op in self.ops:\n            fs += f'\\n\\t{op}'\n        fs += ')'\n        return fs\n\n\ndef rand_augment_transform(\n        config_str: str,\n        hparams: Optional[Dict] = None,\n        transforms: Optional[Union[str, Dict, List]] = None,\n):\n    \"\"\"\n    Create a RandAugment transform\n\n    Args:\n        config_str (str): String defining configuration of random augmentation. Consists of multiple sections separated\n            by dashes ('-'). The first section defines the specific variant of rand augment (currently only 'rand').\n            The remaining sections, not order sepecific determine\n                'm' - integer magnitude of rand augment\n                'n' - integer num layers (number of transform ops selected per image)\n                'p' - float probability of applying each layer (default 0.5)\n                'mstd' -  float std deviation of magnitude noise applied, or uniform sampling if infinity (or > 100)\n                'mmax' - set upper bound for magnitude to something other than default of  _LEVEL_DENOM (10)\n                'inc' - integer (bool), use augmentations that increase in severity with magnitude (default: 0)\n                't' - str name of transform set to use\n            Ex 'rand-m9-n3-mstd0.5' results in RandAugment with magnitude 9, num_layers 3, magnitude_std 0.5\n            'rand-mstd1-tweights' results in mag std 1.0, weighted transforms, default mag of 10 and num_layers 2\n\n        hparams (dict): Other hparams (kwargs) for the RandAugmentation scheme\n\n    Returns:\n         A PyTorch compatible Transform\n    \"\"\"\n    magnitude = _LEVEL_DENOM  # default to _LEVEL_DENOM for magnitude (currently 10)\n    num_layers = 2  # default to 2 ops per image\n    increasing = False\n    prob = 0.5\n    config = config_str.split('-')\n    assert config[0] == 'rand'\n    config = config[1:]\n    for c in config:\n        if c.startswith('t'):\n            # NOTE old 'w' key was removed, 'w0' is not equivalent to 'tweights'\n            val = str(c[1:])\n            if transforms is None:\n                transforms = val\n        else:\n            # numeric options\n            cs = re.split(r'(\\d.*)', c)\n            if len(cs) < 2:\n                continue\n            key, val = cs[:2]\n            if key == 'mstd':\n                # noise param / randomization of magnitude values\n                mstd = float(val)\n                if mstd > 100:\n                    # use uniform sampling in 0 to magnitude if mstd is > 100\n                    mstd = float('inf')\n                hparams.setdefault('magnitude_std', mstd)\n            elif key == 'mmax':\n                # clip magnitude between [0, mmax] instead of default [0, _LEVEL_DENOM]\n                hparams.setdefault('magnitude_max', int(val))\n            elif key == 'inc':\n                if bool(val):\n                    increasing = True\n            elif key == 'm':\n                magnitude = int(val)\n            elif key == 'n':\n                num_layers = int(val)\n            elif key == 'p':\n                prob = float(val)\n            else:\n                assert False, 'Unknown RandAugment config section'\n\n    if isinstance(transforms, str):\n        transforms = rand_augment_choices(transforms, increasing=increasing)\n    elif transforms is None:\n        transforms = _RAND_INCREASING_TRANSFORMS if increasing else _RAND_TRANSFORMS\n\n    choice_weights = None\n    if isinstance(transforms, Dict):\n        transforms, choice_weights = _get_weighted_transforms(transforms)\n\n    ra_ops = rand_augment_ops(magnitude=magnitude, prob=prob, hparams=hparams, transforms=transforms)\n    return RandAugment(ra_ops, num_layers, choice_weights=choice_weights)\n\n\n_AUGMIX_TRANSFORMS = [\n    'AutoContrast',\n    'ColorIncreasing',  # not in paper\n    'ContrastIncreasing',  # not in paper\n    'BrightnessIncreasing',  # not in paper\n    'SharpnessIncreasing',  # not in paper\n    'Equalize',\n    'Rotate',\n    'PosterizeIncreasing',\n    'SolarizeIncreasing',\n    'ShearX',\n    'ShearY',\n    'TranslateXRel',\n    'TranslateYRel',\n]\n\n\ndef augmix_ops(\n        magnitude: Union[int, float] = 10,\n        hparams: Optional[Dict] = None,\n        transforms: Optional[Union[str, Dict, List]] = None,\n):\n    hparams = hparams or _HPARAMS_DEFAULT\n    transforms = transforms or _AUGMIX_TRANSFORMS\n    return [AugmentOp(\n        name,\n        prob=1.0,\n        magnitude=magnitude,\n        hparams=hparams\n    ) for name in transforms]\n\n\nclass AugMixAugment:\n    \"\"\" AugMix Transform\n    Adapted and improved from impl here: https://github.com/google-research/augmix/blob/master/imagenet.py\n    From paper: 'AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty -\n    https://arxiv.org/abs/1912.02781\n    \"\"\"\n    def __init__(self, ops, alpha=1., width=3, depth=-1, blended=False):\n        self.ops = ops\n        self.alpha = alpha\n        self.width = width\n        self.depth = depth\n        self.blended = blended  # blended mode is faster but not well tested\n\n    def _calc_blended_weights(self, ws, m):\n        ws = ws * m\n        cump = 1.\n        rws = []\n        for w in ws[::-1]:\n            alpha = w / cump\n            cump *= (1 - alpha)\n            rws.append(alpha)\n        return np.array(rws[::-1], dtype=np.float32)\n\n    def _apply_blended(self, img, mixing_weights, m):\n        # This is my first crack and implementing a slightly faster mixed augmentation. Instead\n        # of accumulating the mix for each chain in a Numpy array and then blending with original,\n        # it recomputes the blending coefficients and applies one PIL image blend per chain.\n        # TODO the results appear in the right ballpark but they differ by more than rounding.\n        img_orig = img.copy()\n        ws = self._calc_blended_weights(mixing_weights, m)\n        for w in ws:\n            depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n            ops = np.random.choice(self.ops, depth, replace=True)\n            img_aug = img_orig  # no ops are in-place, deep copy not necessary\n            for op in ops:\n                img_aug = op(img_aug)\n            img = Image.blend(img, img_aug, w)\n        return img\n\n    def _apply_basic(self, img, mixing_weights, m):\n        # This is a literal adaptation of the paper/official implementation without normalizations and\n        # PIL <-> Numpy conversions between every op. It is still quite CPU compute heavy compared to the\n        # typical augmentation transforms, could use a GPU / Kornia implementation.\n        img_shape = img.size[0], img.size[1], len(img.getbands())\n        mixed = np.zeros(img_shape, dtype=np.float32)\n        for mw in mixing_weights:\n            depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n            ops = np.random.choice(self.ops, depth, replace=True)\n            img_aug = img  # no ops are in-place, deep copy not necessary\n            for op in ops:\n                img_aug = op(img_aug)\n            mixed += mw * np.asarray(img_aug, dtype=np.float32)\n        np.clip(mixed, 0, 255., out=mixed)\n        mixed = Image.fromarray(mixed.astype(np.uint8))\n        return Image.blend(img, mixed, m)\n\n    def __call__(self, img):\n        mixing_weights = np.float32(np.random.dirichlet([self.alpha] * self.width))\n        m = np.float32(np.random.beta(self.alpha, self.alpha))\n        if self.blended:\n            mixed = self._apply_blended(img, mixing_weights, m)\n        else:\n            mixed = self._apply_basic(img, mixing_weights, m)\n        return mixed\n\n    def __repr__(self):\n        fs = self.__class__.__name__ + f'(alpha={self.alpha}, width={self.width}, depth={self.depth}, ops='\n        for op in self.ops:\n            fs += f'\\n\\t{op}'\n        fs += ')'\n        return fs\n\n\ndef augment_and_mix_transform(config_str: str, hparams: Optional[Dict] = None):\n    \"\"\" Create AugMix PyTorch transform\n\n    Args:\n        config_str (str): String defining configuration of random augmentation. Consists of multiple sections separated\n            by dashes ('-'). The first section defines the specific variant of rand augment (currently only 'rand').\n            The remaining sections, not order sepecific determine\n                'm' - integer magnitude (severity) of augmentation mix (default: 3)\n                'w' - integer width of augmentation chain (default: 3)\n                'd' - integer depth of augmentation chain (-1 is random [1, 3], default: -1)\n                'b' - integer (bool), blend each branch of chain into end result without a final blend, less CPU (default: 0)\n                'mstd' -  float std deviation of magnitude noise applied (default: 0)\n            Ex 'augmix-m5-w4-d2' results in AugMix with severity 5, chain width 4, chain depth 2\n\n        hparams: Other hparams (kwargs) for the Augmentation transforms\n\n    Returns:\n         A PyTorch compatible Transform\n    \"\"\"\n    magnitude = 3\n    width = 3\n    depth = -1\n    alpha = 1.\n    blended = False\n    config = config_str.split('-')\n    assert config[0] == 'augmix'\n    config = config[1:]\n    for c in config:\n        cs = re.split(r'(\\d.*)', c)\n        if len(cs) < 2:\n            continue\n        key, val = cs[:2]\n        if key == 'mstd':\n            # noise param injected via hparams for now\n            hparams.setdefault('magnitude_std', float(val))\n        elif key == 'm':\n            magnitude = int(val)\n        elif key == 'w':\n            width = int(val)\n        elif key == 'd':\n            depth = int(val)\n        elif key == 'a':\n            alpha = float(val)\n        elif key == 'b':\n            blended = bool(val)\n        else:\n            assert False, 'Unknown AugMix config section'\n    hparams.setdefault('magnitude_std', float('inf'))  # default to uniform sampling (if not set via mstd arg)\n    ops = augmix_ops(magnitude=magnitude, hparams=hparams)\n    return AugMixAugment(ops, alpha=alpha, width=width, depth=depth, blended=blended)\n",
  "\"\"\" Mixup and Cutmix\n\nPapers:\nmixup: Beyond Empirical Risk Minimization (https://arxiv.org/abs/1710.09412)\n\nCutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features (https://arxiv.org/abs/1905.04899)\n\nCode Reference:\nCutMix: https://github.com/clovaai/CutMix-PyTorch\n\nHacked together by / Copyright 2019, Ross Wightman\n\"\"\"\nimport numpy as np\nimport torch\n\n\ndef one_hot(x, num_classes, on_value=1., off_value=0.):\n    x = x.long().view(-1, 1)\n    return torch.full((x.size()[0], num_classes), off_value, device=x.device).scatter_(1, x, on_value)\n\n\ndef mixup_target(target, num_classes, lam=1., smoothing=0.0):\n    off_value = smoothing / num_classes\n    on_value = 1. - smoothing + off_value\n    y1 = one_hot(target, num_classes, on_value=on_value, off_value=off_value)\n    y2 = one_hot(target.flip(0), num_classes, on_value=on_value, off_value=off_value)\n    return y1 * lam + y2 * (1. - lam)\n\n\ndef rand_bbox(img_shape, lam, margin=0., count=None):\n    \"\"\" Standard CutMix bounding-box\n    Generates a random square bbox based on lambda value. This impl includes\n    support for enforcing a border margin as percent of bbox dimensions.\n\n    Args:\n        img_shape (tuple): Image shape as tuple\n        lam (float): Cutmix lambda value\n        margin (float): Percentage of bbox dimension to enforce as margin (reduce amount of box outside image)\n        count (int): Number of bbox to generate\n    \"\"\"\n    ratio = np.sqrt(1 - lam)\n    img_h, img_w = img_shape[-2:]\n    cut_h, cut_w = int(img_h * ratio), int(img_w * ratio)\n    margin_y, margin_x = int(margin * cut_h), int(margin * cut_w)\n    cy = np.random.randint(0 + margin_y, img_h - margin_y, size=count)\n    cx = np.random.randint(0 + margin_x, img_w - margin_x, size=count)\n    yl = np.clip(cy - cut_h // 2, 0, img_h)\n    yh = np.clip(cy + cut_h // 2, 0, img_h)\n    xl = np.clip(cx - cut_w // 2, 0, img_w)\n    xh = np.clip(cx + cut_w // 2, 0, img_w)\n    return yl, yh, xl, xh\n\n\ndef rand_bbox_minmax(img_shape, minmax, count=None):\n    \"\"\" Min-Max CutMix bounding-box\n    Inspired by Darknet cutmix impl, generates a random rectangular bbox\n    based on min/max percent values applied to each dimension of the input image.\n\n    Typical defaults for minmax are usually in the  .2-.3 for min and .8-.9 range for max.\n\n    Args:\n        img_shape (tuple): Image shape as tuple\n        minmax (tuple or list): Min and max bbox ratios (as percent of image size)\n        count (int): Number of bbox to generate\n    \"\"\"\n    assert len(minmax) == 2\n    img_h, img_w = img_shape[-2:]\n    cut_h = np.random.randint(int(img_h * minmax[0]), int(img_h * minmax[1]), size=count)\n    cut_w = np.random.randint(int(img_w * minmax[0]), int(img_w * minmax[1]), size=count)\n    yl = np.random.randint(0, img_h - cut_h, size=count)\n    xl = np.random.randint(0, img_w - cut_w, size=count)\n    yu = yl + cut_h\n    xu = xl + cut_w\n    return yl, yu, xl, xu\n\n\ndef cutmix_bbox_and_lam(img_shape, lam, ratio_minmax=None, correct_lam=True, count=None):\n    \"\"\" Generate bbox and apply lambda correction.\n    \"\"\"\n    if ratio_minmax is not None:\n        yl, yu, xl, xu = rand_bbox_minmax(img_shape, ratio_minmax, count=count)\n    else:\n        yl, yu, xl, xu = rand_bbox(img_shape, lam, count=count)\n    if correct_lam or ratio_minmax is not None:\n        bbox_area = (yu - yl) * (xu - xl)\n        lam = 1. - bbox_area / float(img_shape[-2] * img_shape[-1])\n    return (yl, yu, xl, xu), lam\n\n\nclass Mixup:\n    \"\"\" Mixup/Cutmix that applies different params to each element or whole batch\n\n    Args:\n        mixup_alpha (float): mixup alpha value, mixup is active if > 0.\n        cutmix_alpha (float): cutmix alpha value, cutmix is active if > 0.\n        cutmix_minmax (List[float]): cutmix min/max image ratio, cutmix is active and uses this vs alpha if not None.\n        prob (float): probability of applying mixup or cutmix per batch or element\n        switch_prob (float): probability of switching to cutmix instead of mixup when both are active\n        mode (str): how to apply mixup/cutmix params (per 'batch', 'pair' (pair of elements), 'elem' (element)\n        correct_lam (bool): apply lambda correction when cutmix bbox clipped by image borders\n        label_smoothing (float): apply label smoothing to the mixed target tensor\n        num_classes (int): number of classes for target\n    \"\"\"\n    def __init__(self, mixup_alpha=1., cutmix_alpha=0., cutmix_minmax=None, prob=1.0, switch_prob=0.5,\n                 mode='batch', correct_lam=True, label_smoothing=0.1, num_classes=1000):\n        self.mixup_alpha = mixup_alpha\n        self.cutmix_alpha = cutmix_alpha\n        self.cutmix_minmax = cutmix_minmax\n        if self.cutmix_minmax is not None:\n            assert len(self.cutmix_minmax) == 2\n            # force cutmix alpha == 1.0 when minmax active to keep logic simple & safe\n            self.cutmix_alpha = 1.0\n        self.mix_prob = prob\n        self.switch_prob = switch_prob\n        self.label_smoothing = label_smoothing\n        self.num_classes = num_classes\n        self.mode = mode\n        self.correct_lam = correct_lam  # correct lambda based on clipped area for cutmix\n        self.mixup_enabled = True  # set to false to disable mixing (intended tp be set by train loop)\n\n    def _params_per_elem(self, batch_size):\n        lam = np.ones(batch_size, dtype=np.float32)\n        use_cutmix = np.zeros(batch_size, dtype=bool)\n        if self.mixup_enabled:\n            if self.mixup_alpha > 0. and self.cutmix_alpha > 0.:\n                use_cutmix = np.random.rand(batch_size) < self.switch_prob\n                lam_mix = np.where(\n                    use_cutmix,\n                    np.random.beta(self.cutmix_alpha, self.cutmix_alpha, size=batch_size),\n                    np.random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size))\n            elif self.mixup_alpha > 0.:\n                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size)\n            elif self.cutmix_alpha > 0.:\n                use_cutmix = np.ones(batch_size, dtype=bool)\n                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha, size=batch_size)\n            else:\n                assert False, \"One of mixup_alpha > 0., cutmix_alpha > 0., cutmix_minmax not None should be true.\"\n            lam = np.where(np.random.rand(batch_size) < self.mix_prob, lam_mix.astype(np.float32), lam)\n        return lam, use_cutmix\n\n    def _params_per_batch(self):\n        lam = 1.\n        use_cutmix = False\n        if self.mixup_enabled and np.random.rand() < self.mix_prob:\n            if self.mixup_alpha > 0. and self.cutmix_alpha > 0.:\n                use_cutmix = np.random.rand() < self.switch_prob\n                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha) if use_cutmix else \\\n                    np.random.beta(self.mixup_alpha, self.mixup_alpha)\n            elif self.mixup_alpha > 0.:\n                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n            elif self.cutmix_alpha > 0.:\n                use_cutmix = True\n                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n            else:\n                assert False, \"One of mixup_alpha > 0., cutmix_alpha > 0., cutmix_minmax not None should be true.\"\n            lam = float(lam_mix)\n        return lam, use_cutmix\n\n    def _mix_elem(self, x):\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n\n    def _mix_pair(self, x):\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size // 2)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size // 2):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    x[j][:, yl:yh, xl:xh] = x_orig[i][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n                    x[j] = x[j] * lam + x_orig[i] * (1 - lam)\n        lam_batch = np.concatenate((lam_batch, lam_batch[::-1]))\n        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n\n    def _mix_batch(self, x):\n        lam, use_cutmix = self._params_per_batch()\n        if lam == 1.:\n            return 1.\n        if use_cutmix:\n            (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                x.shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n            x[:, :, yl:yh, xl:xh] = x.flip(0)[:, :, yl:yh, xl:xh]\n        else:\n            x_flipped = x.flip(0).mul_(1. - lam)\n            x.mul_(lam).add_(x_flipped)\n        return lam\n\n    def __call__(self, x, target):\n        assert len(x) % 2 == 0, 'Batch size should be even when using this'\n        if self.mode == 'elem':\n            lam = self._mix_elem(x)\n        elif self.mode == 'pair':\n            lam = self._mix_pair(x)\n        else:\n            lam = self._mix_batch(x)\n        target = mixup_target(target, self.num_classes, lam, self.label_smoothing)\n        return x, target\n\n\nclass FastCollateMixup(Mixup):\n    \"\"\" Fast Collate w/ Mixup/Cutmix that applies different params to each element or whole batch\n\n    A Mixup impl that's performed while collating the batches.\n    \"\"\"\n\n    def _mix_elem_collate(self, output, batch, half=False):\n        batch_size = len(batch)\n        num_elem = batch_size // 2 if half else batch_size\n        assert len(output) == num_elem\n        lam_batch, use_cutmix = self._params_per_elem(num_elem)\n        for i in range(num_elem):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            mixed = batch[i][0]\n            if lam != 1.:\n                if use_cutmix[i]:\n                    if not half:\n                        mixed = mixed.copy()\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        output.shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n                    mixed[:, yl:yh, xl:xh] = batch[j][0][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    mixed = mixed.astype(np.float32) * lam + batch[j][0].astype(np.float32) * (1 - lam)\n                    np.rint(mixed, out=mixed)\n            output[i] += torch.from_numpy(mixed.astype(np.uint8))\n        if half:\n            lam_batch = np.concatenate((lam_batch, np.ones(num_elem)))\n        return torch.tensor(lam_batch).unsqueeze(1)\n\n    def _mix_pair_collate(self, output, batch):\n        batch_size = len(batch)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size // 2)\n        for i in range(batch_size // 2):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            mixed_i = batch[i][0]\n            mixed_j = batch[j][0]\n            assert 0 <= lam <= 1.0\n            if lam < 1.:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        output.shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n                    patch_i = mixed_i[:, yl:yh, xl:xh].copy()\n                    mixed_i[:, yl:yh, xl:xh] = mixed_j[:, yl:yh, xl:xh]\n                    mixed_j[:, yl:yh, xl:xh] = patch_i\n                    lam_batch[i] = lam\n                else:\n                    mixed_temp = mixed_i.astype(np.float32) * lam + mixed_j.astype(np.float32) * (1 - lam)\n                    mixed_j = mixed_j.astype(np.float32) * lam + mixed_i.astype(np.float32) * (1 - lam)\n                    mixed_i = mixed_temp\n                    np.rint(mixed_j, out=mixed_j)\n                    np.rint(mixed_i, out=mixed_i)\n            output[i] += torch.from_numpy(mixed_i.astype(np.uint8))\n            output[j] += torch.from_numpy(mixed_j.astype(np.uint8))\n        lam_batch = np.concatenate((lam_batch, lam_batch[::-1]))\n        return torch.tensor(lam_batch).unsqueeze(1)\n\n    def _mix_batch_collate(self, output, batch):\n        batch_size = len(batch)\n        lam, use_cutmix = self._params_per_batch()\n        if use_cutmix:\n            (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                output.shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n        for i in range(batch_size):\n            j = batch_size - i - 1\n            mixed = batch[i][0]\n            if lam != 1.:\n                if use_cutmix:\n                    mixed = mixed.copy()  # don't want to modify the original while iterating\n                    mixed[:, yl:yh, xl:xh] = batch[j][0][:, yl:yh, xl:xh]\n                else:\n                    mixed = mixed.astype(np.float32) * lam + batch[j][0].astype(np.float32) * (1 - lam)\n                    np.rint(mixed, out=mixed)\n            output[i] += torch.from_numpy(mixed.astype(np.uint8))\n        return lam\n\n    def __call__(self, batch, _=None):\n        batch_size = len(batch)\n        assert batch_size % 2 == 0, 'Batch size should be even when using this'\n        half = 'half' in self.mode\n        if half:\n            batch_size //= 2\n        output = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n        if self.mode == 'elem' or self.mode == 'half':\n            lam = self._mix_elem_collate(output, batch, half=half)\n        elif self.mode == 'pair':\n            lam = self._mix_pair_collate(output, batch)\n        else:\n            lam = self._mix_batch_collate(output, batch)\n        target = torch.tensor([b[1] for b in batch], dtype=torch.int64)\n        target = mixup_target(target, self.num_classes, lam, self.label_smoothing)\n        target = target[:batch_size]\n        return output, target\n\n",
  "\"\"\" A dataset reader that extracts images from folders\n\nFolders are scanned recursively to find image files. Labels are based\non the folder hierarchy, just leaf folders by default.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport os\nfrom typing import Dict, List, Optional, Set, Tuple, Union\n\nfrom timm.utils.misc import natural_key\n\nfrom .class_map import load_class_map\nfrom .img_extensions import get_img_extensions\nfrom .reader import Reader\n\n\ndef find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n):\n    \"\"\" Walk folder recursively to discover images and map them to classes by folder names.\n\n    Args:\n        folder: root of folder to recrusively search\n        types: types (file extensions) to search for in path\n        class_to_idx: specify mapping for class (folder name) to class index if set\n        leaf_name_only: use only leaf-name of folder walk for class names\n        sort: re-sort found images by name (for consistent ordering)\n\n    Returns:\n        A list of image and target tuples, class_to_idx mapping\n    \"\"\"\n    types = get_img_extensions(as_set=True) if not types else set(types)\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder, topdown=False, followlinks=True):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_')\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(label)\n    if class_to_idx is None:\n        # building class index\n        unique_labels = set(labels)\n        sorted_labels = list(sorted(unique_labels, key=natural_key))\n        class_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n    images_and_targets = [(f, class_to_idx[l]) for f, l in zip(filenames, labels) if l in class_to_idx]\n    if sort:\n        images_and_targets = sorted(images_and_targets, key=lambda k: natural_key(k[0]))\n    return images_and_targets, class_to_idx\n\n\nclass ReaderImageFolder(Reader):\n\n    def __init__(\n            self,\n            root,\n            class_map=''):\n        super().__init__()\n\n        self.root = root\n        class_to_idx = None\n        if class_map:\n            class_to_idx = load_class_map(class_map, root)\n        self.samples, self.class_to_idx = find_images_and_targets(root, class_to_idx=class_to_idx)\n        if len(self.samples) == 0:\n            raise RuntimeError(\n                f'Found 0 images in subfolders of {root}. '\n                f'Supported image extensions are {\", \".join(get_img_extensions())}')\n\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        return open(path, 'rb'), target\n\n    def __len__(self):\n        return len(self.samples)\n\n    def _filename(self, index, basename=False, absolute=False):\n        filename = self.samples[index][0]\n        if basename:\n            filename = os.path.basename(filename)\n        elif not absolute:\n            filename = os.path.relpath(filename, self.root)\n        return filename\n",
  "from .reader_factory import create_reader\nfrom .img_extensions import *\n",
  "import os\nimport pickle\n\n\ndef load_class_map(map_or_filename, root=''):\n    if isinstance(map_or_filename, dict):\n        assert dict, 'class_map dict must be non-empty'\n        return map_or_filename\n    class_map_path = map_or_filename\n    if not os.path.exists(class_map_path):\n        class_map_path = os.path.join(root, class_map_path)\n        assert os.path.exists(class_map_path), 'Cannot locate specified class map file (%s)' % map_or_filename\n    class_map_ext = os.path.splitext(map_or_filename)[-1].lower()\n    if class_map_ext == '.txt':\n        with open(class_map_path) as f:\n            class_to_idx = {v.strip(): k for k, v in enumerate(f)}\n    elif class_map_ext == '.pkl':\n        with open(class_map_path, 'rb') as f:\n            class_to_idx = pickle.load(f)\n    else:\n        assert False, f'Unsupported class map file extension ({class_map_ext}).'\n    return class_to_idx\n\n",
  "from multiprocessing import Value\n\n\nclass SharedCount:\n    def __init__(self, epoch: int = 0):\n        self.shared_epoch = Value('i', epoch)\n\n    @property\n    def value(self):\n        return self.shared_epoch.value\n\n    @value.setter\n    def value(self, epoch):\n        self.shared_epoch.value = epoch\n",
  "import os\n\nfrom .reader_image_folder import ReaderImageFolder\nfrom .reader_image_in_tar import ReaderImageInTar\n\n\ndef create_reader(name, root, split='train', **kwargs):\n    name = name.lower()\n    name = name.split('/', 1)\n    prefix = ''\n    if len(name) > 1:\n        prefix = name[0]\n    name = name[-1]\n\n    # FIXME improve the selection right now just tfds prefix or fallback path, will need options to\n    # explicitly select other options shortly\n    if prefix == 'hfds':\n        from .reader_hfds import ReaderHfds  # defer tensorflow import\n        reader = ReaderHfds(root, name, split=split, **kwargs)\n    elif prefix == 'tfds':\n        from .reader_tfds import ReaderTfds  # defer tensorflow import\n        reader = ReaderTfds(root, name, split=split, **kwargs)\n    elif prefix == 'wds':\n        from .reader_wds import ReaderWds\n        kwargs.pop('download', False)\n        reader = ReaderWds(root, name, split=split, **kwargs)\n    else:\n        assert os.path.exists(root)\n        # default fallback path (backwards compat), use image tar if root is a .tar file, otherwise image folder\n        # FIXME support split here or in reader?\n        if os.path.isfile(root) and os.path.splitext(root)[1] == '.tar':\n            reader = ReaderImageInTar(root, **kwargs)\n        else:\n            reader = ReaderImageFolder(root, **kwargs)\n    return reader\n",
  "\"\"\" Dataset reader that wraps Hugging Face datasets\n\nHacked together by / Copyright 2022 Ross Wightman\n\"\"\"\nimport io\nimport math\nimport torch\nimport torch.distributed as dist\nfrom PIL import Image\n\ntry:\n    import datasets\nexcept ImportError as e:\n    print(\"Please install Hugging Face datasets package `pip install datasets`.\")\n    exit(1)\nfrom .class_map import load_class_map\nfrom .reader import Reader\n\n\ndef get_class_labels(info, label_key='label'):\n    if 'label' not in info.features:\n        return {}\n    class_label = info.features[label_key]\n    class_to_idx = {n: class_label.str2int(n) for n in class_label.names}\n    return class_to_idx\n\n\nclass ReaderHfds(Reader):\n\n    def __init__(\n            self,\n            root,\n            name,\n            split='train',\n            class_map=None,\n            label_key='label',\n            download=False,\n    ):\n        \"\"\"\n        \"\"\"\n        super().__init__()\n        self.root = root\n        self.split = split\n        self.dataset = datasets.load_dataset(\n            name,  # 'name' maps to path arg in hf datasets\n            split=split,\n            cache_dir=self.root,  # timm doesn't expect hidden cache dir for datasets, specify a path\n        )\n        # leave decode for caller, plus we want easy access to original path names...\n        self.dataset = self.dataset.cast_column('image', datasets.Image(decode=False))\n\n        self.label_key = label_key\n        self.remap_class = False\n        if class_map:\n            self.class_to_idx = load_class_map(class_map)\n            self.remap_class = True\n        else:\n            self.class_to_idx = get_class_labels(self.dataset.info, self.label_key)\n        self.split_info = self.dataset.info.splits[split]\n        self.num_samples = self.split_info.num_examples\n\n    def __getitem__(self, index):\n        item = self.dataset[index]\n        image = item['image']\n        if 'bytes' in image and image['bytes']:\n            image = io.BytesIO(image['bytes'])\n        else:\n            assert 'path' in image and image['path']\n            image = open(image['path'], 'rb')\n        label = item[self.label_key]\n        if self.remap_class:\n            label = self.class_to_idx[label]\n        return image, label\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def _filename(self, index, basename=False, absolute=False):\n        item = self.dataset[index]\n        return item['image']['path']\n",
  "\"\"\" Dataset reader that wraps TFDS datasets\n\nWraps many (most?) TFDS image-classification datasets\nfrom https://github.com/tensorflow/datasets\nhttps://www.tensorflow.org/datasets/catalog/overview#image_classification\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nimport os\nfrom typing import Optional\n\nimport torch\nimport torch.distributed as dist\nfrom PIL import Image\n\ntry:\n    import tensorflow as tf\n    tf.config.set_visible_devices([], 'GPU')  # Hands off my GPU! (or pip install tensorflow-cpu)\n    import tensorflow_datasets as tfds\n    try:\n        tfds.even_splits('', 1, drop_remainder=False)  # non-buggy even_splits has drop_remainder arg\n        has_buggy_even_splits = False\n    except TypeError:\n        print(\"Warning: This version of tfds doesn't have the latest even_splits impl. \"\n              \"Please update or use tfds-nightly for better fine-grained split behaviour.\")\n        has_buggy_even_splits = True\n    # NOTE uncomment below if having file limit issues on dataset build (or alter your OS defaults)\n    # import resource\n    # low, high = resource.getrlimit(resource.RLIMIT_NOFILE)\n    # resource.setrlimit(resource.RLIMIT_NOFILE, (high, high))\nexcept ImportError as e:\n    print(e)\n    print(\"Please install tensorflow_datasets package `pip install tensorflow-datasets`.\")\n    exit(1)\n\nfrom .class_map import load_class_map\nfrom .reader import Reader\nfrom .shared_count import SharedCount\n\n\nMAX_TP_SIZE = int(os.environ.get('TFDS_TP_SIZE', 8))  # maximum TF threadpool size, for jpeg decodes and queuing activities\nSHUFFLE_SIZE = int(os.environ.get('TFDS_SHUFFLE_SIZE', 8192))  # samples to shuffle in DS queue\nPREFETCH_SIZE = int(os.environ.get('TFDS_PREFETCH_SIZE', 2048))  # samples to prefetch\n\n\n@tfds.decode.make_decoder()\ndef decode_example(serialized_image, feature, dct_method='INTEGER_ACCURATE'):\n    return tf.image.decode_jpeg(\n        serialized_image,\n        channels=3,\n        dct_method=dct_method,\n    )\n\n\ndef even_split_indices(split, n, num_samples):\n    partitions = [round(i * num_samples / n) for i in range(n + 1)]\n    return [f\"{split}[{partitions[i]}:{partitions[i + 1]}]\" for i in range(n)]\n\n\ndef get_class_labels(info):\n    if 'label' not in info.features:\n        return {}\n    class_label = info.features['label']\n    class_to_idx = {n: class_label.str2int(n) for n in class_label.names}\n    return class_to_idx\n\n\nclass ReaderTfds(Reader):\n    \"\"\" Wrap Tensorflow Datasets for use in PyTorch\n\n    There several things to be aware of:\n      * To prevent excessive samples being dropped per epoch w/ distributed training or multiplicity of\n         dataloader workers, the train iterator wraps to avoid returning partial batches that trigger drop_last\n         https://github.com/pytorch/pytorch/issues/33413\n      * With PyTorch IterableDatasets, each worker in each replica operates in isolation, the final batch\n        from each worker could be a different size. For training this is worked around by option above, for\n        validation extra samples are inserted iff distributed mode is enabled so that the batches being reduced\n        across replicas are of same size. This will slightly alter the results, distributed validation will not be\n        100% correct. This is similar to common handling in DistributedSampler for normal Datasets but a bit worse\n        since there are up to N * J extra samples with IterableDatasets.\n      * The sharding (splitting of dataset into TFRecord) files imposes limitations on the number of\n        replicas and dataloader workers you can use. For really small datasets that only contain a few shards\n        you may have to train non-distributed w/ 1-2 dataloader workers. This is likely not a huge concern as the\n        benefit of distributed training or fast dataloading should be much less for small datasets.\n      * This wrapper is currently configured to return individual, decompressed image samples from the TFDS\n        dataset. The augmentation (transforms) and batching is still done in PyTorch. It would be possible\n        to specify TF augmentation fn and return augmented batches w/ some modifications to other downstream\n        components.\n\n    \"\"\"\n\n    def __init__(\n            self,\n            root,\n            name,\n            split='train',\n            class_map=None,\n            is_training=False,\n            batch_size=None,\n            download=False,\n            repeats=0,\n            seed=42,\n            input_name='image',\n            input_img_mode='RGB',\n            target_name='label',\n            target_img_mode='',\n            prefetch_size=None,\n            shuffle_size=None,\n            max_threadpool_size=None\n    ):\n        \"\"\" Tensorflow-datasets Wrapper\n\n        Args:\n            root: root data dir (ie your TFDS_DATA_DIR. not dataset specific sub-dir)\n            name: tfds dataset name (eg `imagenet2012`)\n            split: tfds dataset split (can use all TFDS split strings eg `train[:10%]`)\n            is_training: training mode, shuffle enabled, dataset len rounded by batch_size\n            batch_size: batch_size to use to unsure total samples % batch_size == 0 in training across all dis nodes\n            download: download and build TFDS dataset if set, otherwise must use tfds CLI\n            repeats: iterate through (repeat) the dataset this many times per iteration (once if 0 or 1)\n            seed: common seed for shard shuffle across all distributed/worker instances\n            input_name: name of Feature to return as data (input)\n            input_img_mode: image mode if input is an image (currently PIL mode string)\n            target_name: name of Feature to return as target (label)\n            target_img_mode: image mode if target is an image (currently PIL mode string)\n            prefetch_size: override default tf.data prefetch buffer size\n            shuffle_size: override default tf.data shuffle buffer size\n            max_threadpool_size: override default threadpool size for tf.data\n        \"\"\"\n        super().__init__()\n        self.root = root\n        self.split = split\n        self.is_training = is_training\n        if self.is_training:\n            assert batch_size is not None, \\\n                \"Must specify batch_size in training mode for reasonable behaviour w/ TFDS wrapper\"\n        self.batch_size = batch_size\n        self.repeats = repeats\n        self.common_seed = seed  # a seed that's fixed across all worker / distributed instances\n\n        # performance settings\n        self.prefetch_size = prefetch_size or PREFETCH_SIZE\n        self.shuffle_size = shuffle_size or SHUFFLE_SIZE\n        self.max_threadpool_size = max_threadpool_size or MAX_TP_SIZE\n\n        # TFDS builder and split information\n        self.input_name = input_name  # FIXME support tuples / lists of inputs and targets and full range of Feature\n        self.input_img_mode = input_img_mode\n        self.target_name = target_name\n        self.target_img_mode = target_img_mode\n        self.builder = tfds.builder(name, data_dir=root)\n        # NOTE: the tfds command line app can be used download & prepare datasets if you don't enable download flag\n        if download:\n            self.builder.download_and_prepare()\n        self.remap_class = False\n        if class_map:\n            self.class_to_idx = load_class_map(class_map)\n            self.remap_class = True\n        else:\n            self.class_to_idx = get_class_labels(self.builder.info) if self.target_name == 'label' else {}\n        self.split_info = self.builder.info.splits[split]\n        self.num_samples = self.split_info.num_examples\n\n        # Distributed world state\n        self.dist_rank = 0\n        self.dist_num_replicas = 1\n        if dist.is_available() and dist.is_initialized() and dist.get_world_size() > 1:\n            self.dist_rank = dist.get_rank()\n            self.dist_num_replicas = dist.get_world_size()\n\n        # Attributes that are updated in _lazy_init, including the tf.data pipeline itself\n        self.global_num_workers = 1\n        self.num_workers = 1\n        self.worker_info = None\n        self.worker_seed = 0  # seed unique to each work instance\n        self.subsplit = None  # set when data is distributed across workers using sub-splits\n        self.ds = None  # initialized lazily on each dataloader worker process\n        self.init_count = 0  # number of ds TF data pipeline initializations\n        self.epoch_count = SharedCount()\n        # FIXME need to determine if reinit_each_iter is necessary. I'm don't completely trust behaviour\n        #  of `shuffle_reshuffle_each_iteration` when there are multiple workers / nodes across epochs\n        self.reinit_each_iter = self.is_training\n\n    def set_epoch(self, count):\n        self.epoch_count.value = count\n\n    def set_loader_cfg(\n            self,\n            num_workers: Optional[int] = None,\n    ):\n        if self.ds is not None:\n            return\n        if num_workers is not None:\n            self.num_workers = num_workers\n            self.global_num_workers = self.dist_num_replicas * self.num_workers\n\n    def _lazy_init(self):\n        \"\"\" Lazily initialize the dataset.\n\n        This is necessary to init the Tensorflow dataset pipeline in the (dataloader) process that\n        will be using the dataset instance. The __init__ method is called on the main process,\n        this will be called in a dataloader worker process.\n\n        NOTE: There will be problems if you try to re-use this dataset across different loader/worker\n        instances once it has been initialized. Do not call any dataset methods that can call _lazy_init\n        before it is passed to dataloader.\n        \"\"\"\n        worker_info = torch.utils.data.get_worker_info()\n\n        # setup input context to split dataset across distributed processes\n        num_workers = 1\n        global_worker_id = 0\n        if worker_info is not None:\n            self.worker_info = worker_info\n            self.worker_seed = worker_info.seed\n            self.num_workers = worker_info.num_workers\n            self.global_num_workers = self.dist_num_replicas * self.num_workers\n            global_worker_id = self.dist_rank * self.num_workers + worker_info.id\n\n            \"\"\" Data sharding\n            InputContext will assign subset of underlying TFRecord files to each 'pipeline' if used.\n            My understanding is that using split, the underling TFRecord files will shuffle (shuffle_files=True)\n            between the splits each iteration, but that understanding could be wrong.\n\n            I am currently using a mix of InputContext shard assignment and fine-grained sub-splits for distributing\n            the data across workers. For training InputContext is used to assign shards to nodes unless num_shards\n            in dataset < total number of workers. Otherwise sub-split API is used for datasets without enough shards or\n            for validation where we can't drop samples and need to avoid minimize uneven splits to avoid padding.\n            \"\"\"\n            should_subsplit = self.global_num_workers > 1 and (\n                    self.split_info.num_shards < self.global_num_workers or not self.is_training)\n            if should_subsplit:\n                # split the dataset w/o using sharding for more even samples / worker, can result in less optimal\n                # read patterns for distributed training (overlap across shards) so better to use InputContext there\n                if has_buggy_even_splits:\n                    # my even_split workaround doesn't work on subsplits, upgrade tfds!\n                    if not isinstance(self.split_info, tfds.core.splits.SubSplitInfo):\n                        subsplits = even_split_indices(self.split, self.global_num_workers, self.num_samples)\n                        self.subsplit = subsplits[global_worker_id]\n                else:\n                    subsplits = tfds.even_splits(self.split, self.global_num_workers)\n                    self.subsplit = subsplits[global_worker_id]\n\n        input_context = None\n        if self.global_num_workers > 1 and self.subsplit is None:\n            # set input context to divide shards among distributed replicas\n            input_context = tf.distribute.InputContext(\n                num_input_pipelines=self.global_num_workers,\n                input_pipeline_id=global_worker_id,\n                num_replicas_in_sync=self.dist_num_replicas  # FIXME does this arg have any impact?\n            )\n        read_config = tfds.ReadConfig(\n            shuffle_seed=self.common_seed + self.epoch_count.value,\n            shuffle_reshuffle_each_iteration=True,\n            input_context=input_context,\n        )\n        ds = self.builder.as_dataset(\n            split=self.subsplit or self.split,\n            shuffle_files=self.is_training,\n            decoders=dict(image=decode_example()),\n            read_config=read_config,\n        )\n        # avoid overloading threading w/ combo of TF ds threads + PyTorch workers\n        options = tf.data.Options()\n        thread_member = 'threading' if hasattr(options, 'threading') else 'experimental_threading'\n        getattr(options, thread_member).private_threadpool_size = max(1, self.max_threadpool_size // self.num_workers)\n        getattr(options, thread_member).max_intra_op_parallelism = 1\n        ds = ds.with_options(options)\n        if self.is_training or self.repeats > 1:\n            # to prevent excessive drop_last batch behaviour w/ IterableDatasets\n            # see warnings at https://pytorch.org/docs/stable/data.html#multi-process-data-loading\n            ds = ds.repeat()  # allow wrap around and break iteration manually\n        if self.is_training:\n            ds = ds.shuffle(min(self.num_samples, self.shuffle_size) // self.global_num_workers, seed=self.worker_seed)\n        ds = ds.prefetch(min(self.num_samples // self.global_num_workers, self.prefetch_size))\n        self.ds = tfds.as_numpy(ds)\n        self.init_count += 1\n\n    def _num_samples_per_worker(self):\n        num_worker_samples = \\\n            max(1, self.repeats) * self.num_samples / max(self.global_num_workers, self.dist_num_replicas)\n        if self.is_training or self.dist_num_replicas > 1:\n            num_worker_samples = math.ceil(num_worker_samples)\n        if self.is_training and self.batch_size is not None:\n            num_worker_samples = math.ceil(num_worker_samples / self.batch_size) * self.batch_size\n        return int(num_worker_samples)\n\n    def __iter__(self):\n        if self.ds is None or self.reinit_each_iter:\n            self._lazy_init()\n\n        # Compute a rounded up sample count that is used to:\n        #   1. make batches even cross workers & replicas in distributed validation.\n        #     This adds extra samples and will slightly alter validation results.\n        #   2. determine loop ending condition in training w/ repeat enabled so that only full batch_size\n        #     batches are produced (underlying tfds iter wraps around)\n        target_sample_count = self._num_samples_per_worker()\n\n        # Iterate until exhausted or sample count hits target when training (ds.repeat enabled)\n        sample_count = 0\n        for sample in self.ds:\n            input_data = sample[self.input_name]\n            if self.input_img_mode:\n                input_data = Image.fromarray(input_data, mode=self.input_img_mode)\n            target_data = sample[self.target_name]\n            if self.target_img_mode:\n                target_data = Image.fromarray(target_data, mode=self.target_img_mode)\n            elif self.remap_class:\n                target_data = self.class_to_idx[target_data]\n            yield input_data, target_data\n            sample_count += 1\n            if self.is_training and sample_count >= target_sample_count:\n                # Need to break out of loop when repeat() is enabled for training w/ oversampling\n                # this results in extra samples per epoch but seems more desirable than dropping\n                # up to N*J batches per epoch (where N = num distributed processes, and J = num worker processes)\n                break\n\n        # Pad across distributed nodes (make counts equal by adding samples)\n        if not self.is_training and self.dist_num_replicas > 1 and self.subsplit is not None and \\\n                0 < sample_count < target_sample_count:\n            # Validation batch padding only done for distributed training where results are reduced across nodes.\n            # For single process case, it won't matter if workers return different batch sizes.\n            # If using input_context or % based splits, sample count can vary significantly across workers and this\n            # approach should not be used (hence disabled if self.subsplit isn't set).\n            while sample_count < target_sample_count:\n                yield input_data, target_data  # yield prev sample again\n                sample_count += 1\n\n    def __len__(self):\n        num_samples = self._num_samples_per_worker() * self.num_workers\n        return num_samples\n\n    def _filename(self, index, basename=False, absolute=False):\n        assert False, \"Not supported\"  # no random access to samples\n\n    def filenames(self, basename=False, absolute=False):\n        \"\"\" Return all filenames in dataset, overrides base\"\"\"\n        if self.ds is None:\n            self._lazy_init()\n        names = []\n        for sample in self.ds:\n            if len(names) > self.num_samples:\n                break  # safety for ds.repeat() case\n            if 'file_name' in sample:\n                name = sample['file_name']\n            elif 'filename' in sample:\n                name = sample['filename']\n            elif 'id' in sample:\n                name = sample['id']\n            else:\n                assert False, \"No supported name field present\"\n            names.append(name)\n        return names\n",
  "from abc import abstractmethod\n\n\nclass Reader:\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def _filename(self, index, basename=False, absolute=False):\n        pass\n\n    def filename(self, index, basename=False, absolute=False):\n        return self._filename(index, basename=basename, absolute=absolute)\n\n    def filenames(self, basename=False, absolute=False):\n        return [self._filename(index, basename=basename, absolute=absolute) for index in range(len(self))]\n\n",
  "\"\"\" A dataset reader that reads single tarfile based datasets\n\nThis reader can read datasets consisting if a single tarfile containing images.\nI am planning to deprecated it in favour of ParerImageInTar.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport os\nimport tarfile\n\nfrom timm.utils.misc import natural_key\n\nfrom .class_map import load_class_map\nfrom .img_extensions import get_img_extensions\nfrom .reader import Reader\n\n\ndef extract_tarinfo(tarfile, class_to_idx=None, sort=True):\n    extensions = get_img_extensions(as_set=True)\n    files = []\n    labels = []\n    for ti in tarfile.getmembers():\n        if not ti.isfile():\n            continue\n        dirname, basename = os.path.split(ti.path)\n        label = os.path.basename(dirname)\n        ext = os.path.splitext(basename)[1]\n        if ext.lower() in extensions:\n            files.append(ti)\n            labels.append(label)\n    if class_to_idx is None:\n        unique_labels = set(labels)\n        sorted_labels = list(sorted(unique_labels, key=natural_key))\n        class_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n    tarinfo_and_targets = [(f, class_to_idx[l]) for f, l in zip(files, labels) if l in class_to_idx]\n    if sort:\n        tarinfo_and_targets = sorted(tarinfo_and_targets, key=lambda k: natural_key(k[0].path))\n    return tarinfo_and_targets, class_to_idx\n\n\nclass ReaderImageTar(Reader):\n    \"\"\" Single tarfile dataset where classes are mapped to folders within tar\n    NOTE: This class is being deprecated in favour of the more capable ReaderImageInTar that can\n    operate on folders of tars or tars in tars.\n    \"\"\"\n    def __init__(self, root, class_map=''):\n        super().__init__()\n\n        class_to_idx = None\n        if class_map:\n            class_to_idx = load_class_map(class_map, root)\n        assert os.path.isfile(root)\n        self.root = root\n\n        with tarfile.open(root) as tf:  # cannot keep this open across processes, reopen later\n            self.samples, self.class_to_idx = extract_tarinfo(tf, class_to_idx)\n        self.imgs = self.samples\n        self.tarfile = None  # lazy init in __getitem__\n\n    def __getitem__(self, index):\n        if self.tarfile is None:\n            self.tarfile = tarfile.open(self.root)\n        tarinfo, target = self.samples[index]\n        fileobj = self.tarfile.extractfile(tarinfo)\n        return fileobj, target\n\n    def __len__(self):\n        return len(self.samples)\n\n    def _filename(self, index, basename=False, absolute=False):\n        filename = self.samples[index][0].name\n        if basename:\n            filename = os.path.basename(filename)\n        return filename\n",
  "\"\"\" A dataset reader that reads tarfile based datasets\n\nThis reader can extract image samples from:\n* a single tar of image files\n* a folder of multiple tarfiles containing imagefiles\n* a tar of tars containing image files\n\nLabels are based on the combined folder and/or tar name structure.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport logging\nimport os\nimport pickle\nimport tarfile\nfrom glob import glob\nfrom typing import List, Tuple, Dict, Set, Optional, Union\n\nimport numpy as np\n\nfrom timm.utils.misc import natural_key\n\nfrom .class_map import load_class_map\nfrom .img_extensions import get_img_extensions\nfrom .reader import Reader\n\n_logger = logging.getLogger(__name__)\nCACHE_FILENAME_SUFFIX = '_tarinfos.pickle'\n\n\nclass TarState:\n\n    def __init__(self, tf: tarfile.TarFile = None, ti: tarfile.TarInfo = None):\n        self.tf: tarfile.TarFile = tf\n        self.ti: tarfile.TarInfo = ti\n        self.children: Dict[str, TarState] = {}  # child states (tars within tars)\n\n    def reset(self):\n        self.tf = None\n\n\ndef _extract_tarinfo(tf: tarfile.TarFile, parent_info: Dict, extensions: Set[str]):\n    sample_count = 0\n    for i, ti in enumerate(tf):\n        if not ti.isfile():\n            continue\n        dirname, basename = os.path.split(ti.path)\n        name, ext = os.path.splitext(basename)\n        ext = ext.lower()\n        if ext == '.tar':\n            with tarfile.open(fileobj=tf.extractfile(ti), mode='r|') as ctf:\n                child_info = dict(\n                    name=ti.name, path=os.path.join(parent_info['path'], name), ti=ti, children=[], samples=[])\n                sample_count += _extract_tarinfo(ctf, child_info, extensions=extensions)\n                _logger.debug(f'{i}/?. Extracted child tarinfos from {ti.name}. {len(child_info[\"samples\"])} images.')\n                parent_info['children'].append(child_info)\n        elif ext in extensions:\n            parent_info['samples'].append(ti)\n            sample_count += 1\n    return sample_count\n\n\ndef extract_tarinfos(\n        root,\n        class_name_to_idx: Optional[Dict] = None,\n        cache_tarinfo: Optional[bool] = None,\n        extensions: Optional[Union[List, Tuple, Set]] = None,\n        sort: bool = True\n):\n    extensions = get_img_extensions(as_set=True) if not extensions else set(extensions)\n    root_is_tar = False\n    if os.path.isfile(root):\n        assert os.path.splitext(root)[-1].lower() == '.tar'\n        tar_filenames = [root]\n        root, root_name = os.path.split(root)\n        root_name = os.path.splitext(root_name)[0]\n        root_is_tar = True\n    else:\n        root_name = root.strip(os.path.sep).split(os.path.sep)[-1]\n        tar_filenames = glob(os.path.join(root, '*.tar'), recursive=True)\n    num_tars = len(tar_filenames)\n    tar_bytes = sum([os.path.getsize(f) for f in tar_filenames])\n    assert num_tars, f'No .tar files found at specified path ({root}).'\n\n    _logger.info(f'Scanning {tar_bytes/1024**2:.2f}MB of tar files...')\n    info = dict(tartrees=[])\n    cache_path = ''\n    if cache_tarinfo is None:\n        cache_tarinfo = True if tar_bytes > 10*1024**3 else False  # FIXME magic number, 10GB\n    if cache_tarinfo:\n        cache_filename = '_' + root_name + CACHE_FILENAME_SUFFIX\n        cache_path = os.path.join(root, cache_filename)\n    if os.path.exists(cache_path):\n        _logger.info(f'Reading tar info from cache file {cache_path}.')\n        with open(cache_path, 'rb') as pf:\n            info = pickle.load(pf)\n        assert len(info['tartrees']) == num_tars, \"Cached tartree len doesn't match number of tarfiles\"\n    else:\n        for i, fn in enumerate(tar_filenames):\n            path = '' if root_is_tar else os.path.splitext(os.path.basename(fn))[0]\n            with tarfile.open(fn, mode='r|') as tf:  # tarinfo scans done in streaming mode\n                parent_info = dict(name=os.path.relpath(fn, root), path=path, ti=None, children=[], samples=[])\n                num_samples = _extract_tarinfo(tf, parent_info, extensions=extensions)\n                num_children = len(parent_info[\"children\"])\n                _logger.debug(\n                    f'{i}/{num_tars}. Extracted tarinfos from {fn}. {num_children} children, {num_samples} samples.')\n            info['tartrees'].append(parent_info)\n        if cache_path:\n            _logger.info(f'Writing tar info to cache file {cache_path}.')\n            with open(cache_path, 'wb') as pf:\n                pickle.dump(info, pf)\n\n    samples = []\n    labels = []\n    build_class_map = False\n    if class_name_to_idx is None:\n        build_class_map = True\n\n    # Flatten tartree info into lists of samples and targets w/ targets based on label id via\n    # class map arg or from unique paths.\n    # NOTE: currently only flattening up to two-levels, filesystem .tars and then one level of sub-tar children\n    # this covers my current use cases and keeps things a little easier to test for now.\n    tarfiles = []\n\n    def _label_from_paths(*path, leaf_only=True):\n        path = os.path.join(*path).strip(os.path.sep)\n        return path.split(os.path.sep)[-1] if leaf_only else path.replace(os.path.sep, '_')\n\n    def _add_samples(info, fn):\n        added = 0\n        for s in info['samples']:\n            label = _label_from_paths(info['path'], os.path.dirname(s.path))\n            if not build_class_map and label not in class_name_to_idx:\n                continue\n            samples.append((s, fn, info['ti']))\n            labels.append(label)\n            added += 1\n        return added\n\n    _logger.info(f'Collecting samples and building tar states.')\n    for parent_info in info['tartrees']:\n        # if tartree has children, we assume all samples are at the child level\n        tar_name = None if root_is_tar else parent_info['name']\n        tar_state = TarState()\n        parent_added = 0\n        for child_info in parent_info['children']:\n            child_added = _add_samples(child_info, fn=tar_name)\n            if child_added:\n                tar_state.children[child_info['name']] = TarState(ti=child_info['ti'])\n            parent_added += child_added\n        parent_added += _add_samples(parent_info, fn=tar_name)\n        if parent_added:\n            tarfiles.append((tar_name, tar_state))\n    del info\n\n    if build_class_map:\n        # build class index\n        sorted_labels = list(sorted(set(labels), key=natural_key))\n        class_name_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n\n    _logger.info(f'Mapping targets and sorting samples.')\n    samples_and_targets = [(s, class_name_to_idx[l]) for s, l in zip(samples, labels) if l in class_name_to_idx]\n    if sort:\n        samples_and_targets = sorted(samples_and_targets, key=lambda k: natural_key(k[0][0].path))\n    samples, targets = zip(*samples_and_targets)\n    samples = np.array(samples)\n    targets = np.array(targets)\n    _logger.info(f'Finished processing {len(samples)} samples across {len(tarfiles)} tar files.')\n    return samples, targets, class_name_to_idx, tarfiles\n\n\nclass ReaderImageInTar(Reader):\n    \"\"\" Multi-tarfile dataset reader where there is one .tar file per class\n    \"\"\"\n\n    def __init__(self, root, class_map='', cache_tarfiles=True, cache_tarinfo=None):\n        super().__init__()\n\n        class_name_to_idx = None\n        if class_map:\n            class_name_to_idx = load_class_map(class_map, root)\n        self.root = root\n        self.samples, self.targets, self.class_name_to_idx, tarfiles = extract_tarinfos(\n            self.root,\n            class_name_to_idx=class_name_to_idx,\n            cache_tarinfo=cache_tarinfo\n        )\n        self.class_idx_to_name = {v: k for k, v in self.class_name_to_idx.items()}\n        if len(tarfiles) == 1 and tarfiles[0][0] is None:\n            self.root_is_tar = True\n            self.tar_state = tarfiles[0][1]\n        else:\n            self.root_is_tar = False\n            self.tar_state = dict(tarfiles)\n        self.cache_tarfiles = cache_tarfiles\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, index):\n        sample = self.samples[index]\n        target = self.targets[index]\n        sample_ti, parent_fn, child_ti = sample\n        parent_abs = os.path.join(self.root, parent_fn) if parent_fn else self.root\n\n        tf = None\n        cache_state = None\n        if self.cache_tarfiles:\n            cache_state = self.tar_state if self.root_is_tar else self.tar_state[parent_fn]\n            tf = cache_state.tf\n        if tf is None:\n            tf = tarfile.open(parent_abs)\n            if self.cache_tarfiles:\n                cache_state.tf = tf\n        if child_ti is not None:\n            ctf = cache_state.children[child_ti.name].tf if self.cache_tarfiles else None\n            if ctf is None:\n                ctf = tarfile.open(fileobj=tf.extractfile(child_ti))\n                if self.cache_tarfiles:\n                    cache_state.children[child_ti.name].tf = ctf\n            tf = ctf\n\n        return tf.extractfile(sample_ti), target\n\n    def _filename(self, index, basename=False, absolute=False):\n        filename = self.samples[index][0].name\n        if basename:\n            filename = os.path.basename(filename)\n        return filename\n",
  "\"\"\" Dataset reader for webdataset\n\nHacked together by / Copyright 2022 Ross Wightman\n\"\"\"\nimport io\nimport json\nimport logging\nimport math\nimport os\nimport random\nimport sys\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom itertools import islice\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\n\nimport torch\nimport torch.distributed as dist\nimport yaml\nfrom PIL import Image\nfrom torch.utils.data import Dataset, IterableDataset, get_worker_info\n\ntry:\n    import webdataset as wds\n    from webdataset.filters import _shuffle\n    from webdataset.shardlists import expand_urls\n    from webdataset.tariterators import base_plus_ext, url_opener, tar_file_expander, valid_sample\nexcept ImportError:\n    wds = None\n    expand_urls = None\n\nfrom .class_map import load_class_map\nfrom .reader import Reader\nfrom .shared_count import SharedCount\n\n_logger = logging.getLogger(__name__)\n\nSHUFFLE_SIZE = int(os.environ.get('WDS_SHUFFLE_SIZE', 8192))\n\n\ndef _load_info(root, basename='info'):\n    info_json = os.path.join(root, basename + '.json')\n    info_yaml = os.path.join(root, basename + '.yaml')\n    err_str = ''\n    try:\n        with wds.gopen(info_json) as f:\n            info_dict = json.load(f)\n        return info_dict\n    except Exception as e:\n        err_str = str(e)\n    try:\n        with wds.gopen(info_yaml) as f:\n            info_dict = yaml.safe_load(f)\n        return info_dict\n    except Exception:\n        pass\n    _logger.warning(\n        f'Dataset info file not found at {info_json} or {info_yaml}. Error: {err_str}. '\n        'Falling back to provided split and size arg.')\n    return {}\n\n\n@dataclass\nclass SplitInfo:\n    num_samples: int\n    filenames: Tuple[str]\n    shard_lengths: Tuple[int] = ()\n    alt_label: str = ''\n    name: str = ''\n\n\ndef _parse_split_info(split: str, info: Dict):\n    def _info_convert(dict_info):\n        return SplitInfo(\n            num_samples=dict_info['num_samples'],\n            filenames=tuple(dict_info['filenames']),\n            shard_lengths=tuple(dict_info['shard_lengths']),\n            alt_label=dict_info.get('alt_label', ''),\n            name=dict_info['name'],\n        )\n\n    if 'tar' in split or '..' in split:\n        # split in WDS string braceexpand format, sample count can be included with a | separator\n        # ex: `dataset-split-{0000..9999}.tar|100000` for 9999 shards, covering 100,000 samples\n        split = split.split('|')\n        num_samples = 0\n        split_name = ''\n        if len(split) > 1:\n            num_samples = int(split[1])\n        split = split[0]\n        if '::' not in split:\n            split_parts = split.split('-', 3)\n            split_idx = len(split_parts) - 1\n            if split_idx and 'splits' in info and split_parts[split_idx] in info['splits']:\n                split_name = split_parts[split_idx]\n\n        split_filenames = expand_urls(split)\n        if split_name:\n            split_info = info['splits'][split_name]\n            if not num_samples:\n                _fc = {f: c for f, c in zip(split_info['filenames'], split_info['shard_lengths'])}\n                num_samples = sum(_fc[f] for f in split_filenames)\n                split_info['filenames'] = tuple(_fc.keys())\n                split_info['shard_lengths'] = tuple(_fc.values())\n                split_info['num_samples'] = num_samples\n            split_info = _info_convert(split_info)\n        else:\n            split_info = SplitInfo(\n                name=split_name,\n                num_samples=num_samples,\n                filenames=split_filenames,\n            )\n    else:\n        if 'splits' not in info or split not in info['splits']:\n            raise RuntimeError(f\"split {split} not found in info ({info.get('splits', {}).keys()})\")\n        split = split\n        split_info = info['splits'][split]\n        split_info = _info_convert(split_info)\n\n    return split_info\n\n\ndef log_and_continue(exn):\n    \"\"\"Call in an exception handler to ignore any exception, isssue a warning, and continue.\"\"\"\n    _logger.warning(f'Handling webdataset error ({repr(exn)}). Ignoring.')\n    return True\n\n\ndef _decode(\n        sample,\n        image_key='jpg',\n        image_format='RGB',\n        target_key='cls',\n        alt_label=''\n):\n    \"\"\" Custom sample decode\n    * decode and convert PIL Image\n    * cls byte string label to int\n    * pass through JSON byte string (if it exists) without parse\n    \"\"\"\n    # decode class label, skip if alternate label not valid\n    if alt_label:\n        # alternative labels are encoded in json metadata\n        meta = json.loads(sample['json'])\n        class_label = int(meta[alt_label])\n        if class_label < 0:\n            # skipped labels currently encoded as -1, may change to a null/None value\n            return None\n    else:\n        class_label = int(sample[target_key])\n\n    # decode image\n    with io.BytesIO(sample[image_key]) as b:\n        img = Image.open(b)\n        img.load()\n    if image_format:\n        img = img.convert(image_format)\n\n    # json passed through in undecoded state\n    decoded = dict(jpg=img, cls=class_label, json=sample.get('json', None))\n    return decoded\n\n\ndef _decode_samples(\n        data,\n        image_key='jpg',\n        image_format='RGB',\n        target_key='cls',\n        alt_label='',\n        handler=log_and_continue):\n    \"\"\"Decode samples with skip.\"\"\"\n    for sample in data:\n        try:\n            result = _decode(\n                sample,\n                image_key=image_key,\n                image_format=image_format,\n                target_key=target_key,\n                alt_label=alt_label\n            )\n        except Exception as exn:\n            if handler(exn):\n                continue\n            else:\n                break\n\n        # null results are skipped\n        if result is not None:\n            if isinstance(sample, dict) and isinstance(result, dict):\n                result[\"__key__\"] = sample.get(\"__key__\")\n            yield result\n\n\ndef pytorch_worker_seed():\n    \"\"\"get dataloader worker seed from pytorch\"\"\"\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()\n\n\nif wds is not None:\n    # conditional to avoid mandatory wds import (via inheritance of wds.PipelineStage)\n    class detshuffle2(wds.PipelineStage):\n        def __init__(\n                self,\n                bufsize=1000,\n                initial=100,\n                seed=0,\n                epoch=-1,\n        ):\n            self.bufsize = bufsize\n            self.initial = initial\n            self.seed = seed\n            self.epoch = epoch\n\n        def run(self, src):\n            if isinstance(self.epoch, SharedCount):\n                epoch = self.epoch.value\n            else:\n                # NOTE: this is epoch tracking is problematic in a multiprocess (dataloader workers or train)\n                # situation as different workers may wrap at different times (or not at all).\n                self.epoch += 1\n                epoch = self.epoch\n\n            if self.seed < 0:\n                seed = pytorch_worker_seed() + epoch\n            else:\n                seed = self.seed + epoch\n            # _logger.info(f'shuffle seed: {self.seed}, {seed}, epoch: {epoch}')  # FIXME temporary\n            rng = random.Random(seed)\n            return _shuffle(src, self.bufsize, self.initial, rng)\n\nelse:\n    detshuffle2 = None\n\n\nclass ResampledShards2(IterableDataset):\n    \"\"\"An iterable dataset yielding a list of urls.\"\"\"\n\n    def __init__(\n        self,\n        urls,\n        nshards=sys.maxsize,\n        worker_seed=None,\n        deterministic=True,\n        epoch=-1,\n    ):\n        \"\"\"Sample shards from the shard list with replacement.\n\n        :param urls: a list of URLs as a Python list or brace notation string\n        \"\"\"\n        super().__init__()\n        urls = wds.shardlists.expand_urls(urls)\n        self.urls = urls\n        assert isinstance(self.urls[0], str)\n        self.nshards = nshards\n        self.rng = random.Random()\n        self.worker_seed = pytorch_worker_seed if worker_seed is None else worker_seed\n        self.deterministic = deterministic\n        self.epoch = epoch\n\n    def __iter__(self):\n        \"\"\"Return an iterator over the shards.\"\"\"\n        if isinstance(self.epoch, SharedCount):\n            epoch = self.epoch.value\n        else:\n            # NOTE: this is epoch tracking is problematic in a multiprocess (dataloader workers or train)\n            # situation as different workers may wrap at different times (or not at all).\n            self.epoch += 1\n            epoch = self.epoch\n\n        if self.deterministic:\n            # reset seed w/ epoch if deterministic, worker seed should be deterministic due to arg.seed\n            self.rng = random.Random(self.worker_seed() + epoch)\n\n        for _ in range(self.nshards):\n            index = self.rng.randint(0, len(self.urls) - 1)\n            yield dict(url=self.urls[index])\n\n\nclass ReaderWds(Reader):\n    def __init__(\n            self,\n            root,\n            name,\n            split,\n            is_training=False,\n            batch_size=None,\n            repeats=0,\n            seed=42,\n            class_map=None,\n            input_name='jpg',\n            input_image='RGB',\n            target_name='cls',\n            target_image='',\n            prefetch_size=None,\n            shuffle_size=None,\n    ):\n        super().__init__()\n        if wds is None:\n            raise RuntimeError(\n                'Please install webdataset 0.2.x package `pip install git+https://github.com/webdataset/webdataset`.')\n        self.root = root\n        self.is_training = is_training\n        self.batch_size = batch_size\n        self.repeats = repeats\n        self.common_seed = seed  # a seed that's fixed across all worker / distributed instances\n        self.shard_shuffle_size = 500\n        self.sample_shuffle_size = shuffle_size or SHUFFLE_SIZE\n\n        self.image_key = input_name\n        self.image_format = input_image\n        self.target_key = target_name\n        self.filename_key = 'filename'\n        self.key_ext = '.JPEG'  # extension to add to key for original filenames (DS specific, default ImageNet)\n\n        self.info = _load_info(self.root)\n        self.split_info = _parse_split_info(split, self.info)\n        self.num_samples = self.split_info.num_samples\n        if not self.num_samples:\n            raise RuntimeError(f'Invalid split definition, no samples found.')\n        self.remap_class = False\n        if class_map:\n            self.class_to_idx = load_class_map(class_map)\n            self.remap_class = True\n        else:\n            self.class_to_idx = {}\n\n        # Distributed world state\n        self.dist_rank = 0\n        self.dist_num_replicas = 1\n        if dist.is_available() and dist.is_initialized() and dist.get_world_size() > 1:\n            self.dist_rank = dist.get_rank()\n            self.dist_num_replicas = dist.get_world_size()\n\n        # Attributes that are updated in _lazy_init\n        self.worker_info = None\n        self.worker_id = 0\n        self.worker_seed = seed  # seed unique to each worker instance\n        self.num_workers = 1\n        self.global_worker_id = 0\n        self.global_num_workers = 1\n        self.init_count = 0\n        self.epoch_count = SharedCount()\n\n        # DataPipeline is lazy init, majority of WDS DataPipeline could be init here, BUT, shuffle seed\n        # is not handled in manner where it can be deterministic for each worker AND initialized up front\n        self.ds = None\n\n    def set_epoch(self, count):\n        self.epoch_count.value = count\n\n    def set_loader_cfg(\n            self,\n            num_workers: Optional[int] = None,\n    ):\n        if self.ds is not None:\n            return\n        if num_workers is not None:\n            self.num_workers = num_workers\n            self.global_num_workers = self.dist_num_replicas * self.num_workers\n\n    def _lazy_init(self):\n        \"\"\" Lazily initialize worker (in worker processes)\n        \"\"\"\n        if self.worker_info is None:\n            worker_info = torch.utils.data.get_worker_info()\n            if worker_info is not None:\n                self.worker_info = worker_info\n                self.worker_id = worker_info.id\n                self.worker_seed = worker_info.seed\n                self.num_workers = worker_info.num_workers\n            self.global_num_workers = self.dist_num_replicas * self.num_workers\n            self.global_worker_id = self.dist_rank * self.num_workers + self.worker_id\n\n        # init data pipeline\n        abs_shard_filenames = [os.path.join(self.root, f) for f in self.split_info.filenames]\n        pipeline = [wds.SimpleShardList(abs_shard_filenames)]\n        # at this point we have an iterator over all the shards\n        if self.is_training:\n            pipeline.extend([\n                detshuffle2(self.shard_shuffle_size, seed=self.common_seed, epoch=self.epoch_count),\n                self._split_by_node_and_worker,\n                # at this point, we have an iterator over the shards assigned to each worker\n                wds.tarfile_to_samples(handler=log_and_continue),\n                wds.shuffle(\n                    self.sample_shuffle_size,\n                    rng=random.Random(self.worker_seed)),  # this is why we lazy-init whole DataPipeline\n            ])\n        else:\n            pipeline.extend([\n                self._split_by_node_and_worker,\n                # at this point, we have an iterator over the shards assigned to each worker\n                wds.tarfile_to_samples(handler=log_and_continue),\n            ])\n        pipeline.extend([\n            partial(\n                _decode_samples,\n                image_key=self.image_key,\n                image_format=self.image_format,\n                alt_label=self.split_info.alt_label\n            )\n        ])\n        self.ds = wds.DataPipeline(*pipeline)\n\n    def _split_by_node_and_worker(self, src):\n        if self.global_num_workers > 1:\n            for s in islice(src, self.global_worker_id, None, self.global_num_workers):\n                yield s\n        else:\n            for s in src:\n                yield s\n\n    def _num_samples_per_worker(self):\n        num_worker_samples = self.num_samples / max(self.global_num_workers, self.dist_num_replicas)\n        if self.is_training or self.dist_num_replicas > 1:\n            num_worker_samples = math.ceil(num_worker_samples)\n        if self.is_training and self.batch_size is not None:\n            num_worker_samples = math.ceil(num_worker_samples / self.batch_size) * self.batch_size\n        return int(num_worker_samples)\n\n    def __iter__(self):\n        if self.ds is None:\n            self._lazy_init()\n\n        num_worker_samples = self._num_samples_per_worker()\n        if self.is_training or self.dist_num_replicas > 1:\n            # NOTE: doing distributed validation w/ WDS is messy, hard to meet constraints that\n            # same # of batches needed across all replicas w/ seeing each sample once.\n            # with_epoch() is simple but could miss a shard's worth of samples in some workers,\n            # and duplicate in others. Best to keep num DL workers low and a divisor of #val shards.\n            ds = self.ds.with_epoch(num_worker_samples)\n        else:\n            ds = self.ds\n\n        i = 0\n        # _logger.info(f'start {i}, {self.worker_id}')  # FIXME temporary debug\n        for sample in ds:\n            target = sample[self.target_key]\n            if self.remap_class:\n                target = self.class_to_idx[target]\n            yield sample[self.image_key], target\n            i += 1\n        # _logger.info(f'end {i}, {self.worker_id}')  # FIXME temporary debug\n\n    def __len__(self):\n        num_samples = self._num_samples_per_worker() * self.num_workers\n        return num_samples\n\n    def _filename(self, index, basename=False, absolute=False):\n        assert False, \"Not supported\"  # no random access to examples\n\n    def filenames(self, basename=False, absolute=False):\n        \"\"\" Return all filenames in dataset, overrides base\"\"\"\n        if self.ds is None:\n            self._lazy_init()\n\n        names = []\n        for sample in self.ds:\n            if self.filename_key in sample:\n                name = sample[self.filename_key]\n            elif '__key__' in sample:\n                name = sample['__key__'] + self.key_ext\n            else:\n                assert False, \"No supported name field present\"\n            names.append(name)\n            if len(names) >= self.num_samples:\n                break  # safety for ds.repeat() case\n        return names\n",
  "from copy import deepcopy\n\n__all__ = ['get_img_extensions', 'is_img_extension', 'set_img_extensions', 'add_img_extensions', 'del_img_extensions']\n\n\nIMG_EXTENSIONS = ('.png', '.jpg', '.jpeg')  # singleton, kept public for bwd compat use\n_IMG_EXTENSIONS_SET = set(IMG_EXTENSIONS)  # set version, private, kept in sync\n\n\ndef _set_extensions(extensions):\n    global IMG_EXTENSIONS\n    global _IMG_EXTENSIONS_SET\n    dedupe = set()  # NOTE de-duping tuple while keeping original order\n    IMG_EXTENSIONS = tuple(x for x in extensions if x not in dedupe and not dedupe.add(x))\n    _IMG_EXTENSIONS_SET = set(extensions)\n\n\ndef _valid_extension(x: str):\n    return x and isinstance(x, str) and len(x) >= 2 and x.startswith('.')\n\n\ndef is_img_extension(ext):\n    return ext in _IMG_EXTENSIONS_SET\n\n\ndef get_img_extensions(as_set=False):\n    return deepcopy(_IMG_EXTENSIONS_SET if as_set else IMG_EXTENSIONS)\n\n\ndef set_img_extensions(extensions):\n    assert len(extensions)\n    for x in extensions:\n        assert _valid_extension(x)\n    _set_extensions(extensions)\n\n\ndef add_img_extensions(ext):\n    if not isinstance(ext, (list, tuple, set)):\n        ext = (ext,)\n    for x in ext:\n        assert _valid_extension(x)\n    extensions = IMG_EXTENSIONS + tuple(ext)\n    _set_extensions(extensions)\n\n\ndef del_img_extensions(ext):\n    if not isinstance(ext, (list, tuple, set)):\n        ext = (ext,)\n    extensions = tuple(x for x in IMG_EXTENSIONS if x not in ext)\n    _set_extensions(extensions)\n",
  "\"\"\"RAdam Optimizer.\nImplementation lifted from: https://github.com/LiyuanLucasLiu/RAdam\nPaper: `On the Variance of the Adaptive Learning Rate and Beyond` - https://arxiv.org/abs/1908.03265\n\"\"\"\nimport math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(\n            lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n            buffer=[[None, None, None] for _ in range(10)])\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_fp32 = p.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n\n                state['step'] += 1\n                buffered = group['buffer'][int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    num_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    num_sma_max = 2 / (1 - beta2) - 1\n                    num_sma = num_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = num_sma\n\n                    # more conservative since it's an approximated value\n                    if num_sma >= 5:\n                        step_size = group['lr'] * math.sqrt(\n                            (1 - beta2_t) *\n                            (num_sma - 4) / (num_sma_max - 4) *\n                            (num_sma - 2) / num_sma *\n                            num_sma_max / (num_sma_max - 2)) / (1 - beta1 ** state['step'])\n                    else:\n                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n\n                if group['weight_decay'] != 0:\n                    p_fp32.add_(p_fp32, alpha=-group['weight_decay'] * group['lr'])\n\n                # more conservative since it's an approximated value\n                if num_sma >= 5:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_fp32.addcdiv_(exp_avg, denom, value=-step_size)\n                else:\n                    p_fp32.add_(exp_avg, alpha=-step_size)\n\n                p.copy_(p_fp32)\n\n        return loss\n",
  "from .adabelief import AdaBelief\nfrom .adafactor import Adafactor\nfrom .adahessian import Adahessian\nfrom .adamp import AdamP\nfrom .adamw import AdamW\nfrom .adan import Adan\nfrom .lamb import Lamb\nfrom .lars import Lars\nfrom .lookahead import Lookahead\nfrom .madgrad import MADGRAD\nfrom .nadam import Nadam\nfrom .nvnovograd import NvNovoGrad\nfrom .radam import RAdam\nfrom .rmsprop_tf import RMSpropTF\nfrom .sgdp import SGDP\nfrom .lion import Lion\nfrom .optim_factory import create_optimizer, create_optimizer_v2, optimizer_kwargs\n",
  "\"\"\" Adafactor Optimizer\n\nLifted from https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py\n\nOriginal header/copyright below.\n\n\"\"\"\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport torch\nimport math\n\n\nclass Adafactor(torch.optim.Optimizer):\n    \"\"\"Implements Adafactor algorithm.\n    This implementation is based on: `Adafactor: Adaptive Learning Rates with Sublinear Memory Cost`\n    (see https://arxiv.org/abs/1804.04235)\n\n    Note that this optimizer internally adjusts the learning rate depending on the\n    *scale_parameter*, *relative_step* and *warmup_init* options.\n\n    To use a manual (external) learning rate schedule you should set `scale_parameter=False` and\n    `relative_step=False`.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining parameter groups\n        lr (float, optional): external learning rate (default: None)\n        eps (tuple[float, float]): regularization constants for square gradient\n            and parameter scale respectively (default: (1e-30, 1e-3))\n        clip_threshold (float): threshold of root mean square of final gradient update (default: 1.0)\n        decay_rate (float): coefficient used to compute running averages of square gradient (default: -0.8)\n        beta1 (float): coefficient used for computing running averages of gradient (default: None)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        scale_parameter (bool): if True, learning rate is scaled by root mean square of parameter (default: True)\n        warmup_init (bool): time-dependent learning rate computation depends on\n            whether warm-up initialization is being used (default: False)\n    \"\"\"\n\n    def __init__(self, params, lr=None, eps=1e-30, eps_scale=1e-3, clip_threshold=1.0,\n                 decay_rate=-0.8, betas=None, weight_decay=0.0, scale_parameter=True, warmup_init=False):\n        relative_step = not lr\n        if warmup_init and not relative_step:\n            raise ValueError('warmup_init requires relative_step=True')\n\n        beta1 = None if betas is None else betas[0]   # make it compat with standard betas arg\n        defaults = dict(lr=lr, eps=eps, eps_scale=eps_scale, clip_threshold=clip_threshold, decay_rate=decay_rate,\n                        beta1=beta1, weight_decay=weight_decay, scale_parameter=scale_parameter,\n                        relative_step=relative_step, warmup_init=warmup_init)\n        super(Adafactor, self).__init__(params, defaults)\n\n    @staticmethod\n    def _get_lr(param_group, param_state):\n        if param_group['relative_step']:\n            min_step = 1e-6 * param_state['step'] if param_group['warmup_init'] else 1e-2\n            lr_t = min(min_step, 1.0 / math.sqrt(param_state['step']))\n            param_scale = 1.0\n            if param_group['scale_parameter']:\n                param_scale = max(param_group['eps_scale'], param_state['RMS'])\n            param_group['lr'] = lr_t * param_scale\n        return param_group['lr']\n\n    @staticmethod\n    def _get_options(param_group, param_shape):\n        factored = len(param_shape) >= 2\n        use_first_moment = param_group['beta1'] is not None\n        return factored, use_first_moment\n\n    @staticmethod\n    def _rms(tensor):\n        return tensor.norm(2) / (tensor.numel() ** 0.5)\n\n    def _approx_sq_grad(self, exp_avg_sq_row, exp_avg_sq_col):\n        r_factor = (exp_avg_sq_row / exp_avg_sq_row.mean(dim=-1, keepdim=True)).rsqrt_().unsqueeze(-1)\n        c_factor = exp_avg_sq_col.unsqueeze(-2).rsqrt()\n        return torch.mul(r_factor, c_factor)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n                if grad.dtype in {torch.float16, torch.bfloat16}:\n                    grad = grad.float()\n                if grad.is_sparse:\n                    raise RuntimeError('Adafactor does not support sparse gradients.')\n\n                state = self.state[p]\n\n                factored, use_first_moment = self._get_options(group, grad.shape)\n                # State Initialization\n                if len(state) == 0:\n                    state['step'] = 0\n\n                    if use_first_moment:\n                        # Exponential moving average of gradient values\n                        state['exp_avg'] = torch.zeros_like(grad)\n                    if factored:\n                        state['exp_avg_sq_row'] = torch.zeros(grad.shape[:-1]).to(grad)\n                        state['exp_avg_sq_col'] = torch.zeros(grad.shape[:-2] + grad.shape[-1:]).to(grad)\n                    else:\n                        state['exp_avg_sq'] = torch.zeros_like(grad)\n\n                    state['RMS'] = 0\n                else:\n                    if use_first_moment:\n                        state['exp_avg'] = state['exp_avg'].to(grad)\n                    if factored:\n                        state['exp_avg_sq_row'] = state['exp_avg_sq_row'].to(grad)\n                        state['exp_avg_sq_col'] = state['exp_avg_sq_col'].to(grad)\n                    else:\n                        state['exp_avg_sq'] = state['exp_avg_sq'].to(grad)\n\n                p_fp32 = p\n                if p.dtype in {torch.float16, torch.bfloat16}:\n                    p_fp32 = p_fp32.float()\n\n                state['step'] += 1\n                state['RMS'] = self._rms(p_fp32)\n                lr_t = self._get_lr(group, state)\n\n                beta2t = 1.0 - math.pow(state['step'], group['decay_rate'])\n                update = grad ** 2 + group['eps']\n                if factored:\n                    exp_avg_sq_row = state['exp_avg_sq_row']\n                    exp_avg_sq_col = state['exp_avg_sq_col']\n\n                    exp_avg_sq_row.mul_(beta2t).add_(update.mean(dim=-1), alpha=1.0 - beta2t)\n                    exp_avg_sq_col.mul_(beta2t).add_(update.mean(dim=-2), alpha=1.0 - beta2t)\n\n                    # Approximation of exponential moving average of square of gradient\n                    update = self._approx_sq_grad(exp_avg_sq_row, exp_avg_sq_col)\n                    update.mul_(grad)\n                else:\n                    exp_avg_sq = state['exp_avg_sq']\n\n                    exp_avg_sq.mul_(beta2t).add_(update, alpha=1.0 - beta2t)\n                    update = exp_avg_sq.rsqrt().mul_(grad)\n\n                update.div_((self._rms(update) / group['clip_threshold']).clamp_(min=1.0))\n                update.mul_(lr_t)\n\n                if use_first_moment:\n                    exp_avg = state['exp_avg']\n                    exp_avg.mul_(group['beta1']).add_(update, alpha=1 - group['beta1'])\n                    update = exp_avg\n\n                if group['weight_decay'] != 0:\n                    p_fp32.add_(p_fp32, alpha=-group['weight_decay'] * lr_t)\n\n                p_fp32.add_(-update)\n                if p.dtype in {torch.float16, torch.bfloat16}:\n                    p.copy_(p_fp32)\n\n        return loss\n",
  "\"\"\" PyTorch LARS / LARC Optimizer\n\nAn implementation of LARS (SGD) + LARC in PyTorch\n\nBased on:\n  * PyTorch SGD: https://github.com/pytorch/pytorch/blob/1.7/torch/optim/sgd.py#L100\n  * NVIDIA APEX LARC: https://github.com/NVIDIA/apex/blob/master/apex/parallel/LARC.py\n\nAdditional cleanup and modifications to properly support PyTorch XLA.\n\nCopyright 2021 Ross Wightman\n\"\"\"\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass Lars(Optimizer):\n    \"\"\" LARS for PyTorch\n    \n    Paper: `Large batch training of Convolutional Networks` - https://arxiv.org/pdf/1708.03888.pdf\n\n    Args:\n        params (iterable): iterable of parameters to optimize or dicts defining parameter groups.\n        lr (float, optional): learning rate (default: 1.0).\n        momentum (float, optional): momentum factor (default: 0)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        dampening (float, optional): dampening for momentum (default: 0)\n        nesterov (bool, optional): enables Nesterov momentum (default: False)\n        trust_coeff (float): trust coefficient for computing adaptive lr / trust_ratio (default: 0.001)\n        eps (float): eps for division denominator (default: 1e-8)\n        trust_clip (bool): enable LARC trust ratio clipping (default: False)\n        always_adapt (bool): always apply LARS LR adapt, otherwise only when group weight_decay != 0 (default: False)\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr=1.0,\n        momentum=0,\n        dampening=0,\n        weight_decay=0,\n        nesterov=False,\n        trust_coeff=0.001,\n        eps=1e-8,\n        trust_clip=False,\n        always_adapt=False,\n    ):\n        if lr < 0.0:\n            raise ValueError(f\"Invalid learning rate: {lr}\")\n        if momentum < 0.0:\n            raise ValueError(f\"Invalid momentum value: {momentum}\")\n        if weight_decay < 0.0:\n            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n        if nesterov and (momentum <= 0 or dampening != 0):\n            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n\n        defaults = dict(\n            lr=lr,\n            momentum=momentum,\n            dampening=dampening,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n            trust_coeff=trust_coeff,\n            eps=eps,\n            trust_clip=trust_clip,\n            always_adapt=always_adapt,\n        )\n        super().__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\"nesterov\", False)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Args:\n            closure (callable, optional): A closure that reevaluates the model and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        device = self.param_groups[0]['params'][0].device\n        one_tensor = torch.tensor(1.0, device=device)  # because torch.where doesn't handle scalars correctly\n\n        for group in self.param_groups:\n            weight_decay = group['weight_decay']\n            momentum = group['momentum']\n            dampening = group['dampening']\n            nesterov = group['nesterov']\n            trust_coeff = group['trust_coeff']\n            eps = group['eps']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n\n                # apply LARS LR adaptation, LARC clipping, weight decay\n                # ref: https://github.com/NVIDIA/apex/blob/master/apex/parallel/LARC.py\n                if weight_decay != 0 or group['always_adapt']:\n                    w_norm = p.norm(2.0)\n                    g_norm = grad.norm(2.0)\n                    trust_ratio = trust_coeff * w_norm / (g_norm + w_norm * weight_decay + eps)\n                    # FIXME nested where required since logical and/or not working in PT XLA\n                    trust_ratio = torch.where(\n                        w_norm > 0,\n                        torch.where(g_norm > 0, trust_ratio, one_tensor),\n                        one_tensor,\n                    )\n                    if group['trust_clip']:\n                        trust_ratio = torch.minimum(trust_ratio / group['lr'], one_tensor)\n                    grad.add_(p, alpha=weight_decay)\n                    grad.mul_(trust_ratio)\n\n                # apply SGD update https://github.com/pytorch/pytorch/blob/1.7/torch/optim/sgd.py#L100\n                if momentum != 0:\n                    param_state = self.state[p]\n                    if 'momentum_buffer' not in param_state:\n                        buf = param_state['momentum_buffer'] = torch.clone(grad).detach()\n                    else:\n                        buf = param_state['momentum_buffer']\n                        buf.mul_(momentum).add_(grad, alpha=1. - dampening)\n                    if nesterov:\n                        grad = grad.add(buf, alpha=momentum)\n                    else:\n                        grad = buf\n\n                p.add_(grad, alpha=-group['lr'])\n\n        return loss",
  "\"\"\" Adan Optimizer\n\nAdan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models[J]. arXiv preprint arXiv:2208.06677, 2022.\n    https://arxiv.org/abs/2208.06677\n\nImplementation adapted from https://github.com/sail-sg/Adan\n\"\"\"\n\nimport math\n\nimport torch\n\nfrom torch.optim import Optimizer\n\n\nclass Adan(Optimizer):\n    \"\"\"\n    Implements a pytorch variant of Adan\n    Adan was proposed in\n    Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models[J]. arXiv preprint arXiv:2208.06677, 2022.\n    https://arxiv.org/abs/2208.06677\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        betas (Tuple[float, float, flot], optional): coefficients used for computing\n            running averages of gradient and its norm. (default: (0.98, 0.92, 0.99))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability. (default: 1e-8)\n        weight_decay (float, optional): decoupled weight decay (L2 penalty) (default: 0)\n        no_prox (bool): how to perform the decoupled weight decay (default: False)\n    \"\"\"\n\n    def __init__(\n            self,\n            params,\n            lr=1e-3,\n            betas=(0.98, 0.92, 0.99),\n            eps=1e-8,\n            weight_decay=0.0,\n            no_prox=False,\n    ):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        if not 0.0 <= betas[2] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 2: {}\".format(betas[2]))\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, no_prox=no_prox)\n        super(Adan, self).__init__(params, defaults)\n\n    @torch.no_grad()\n    def restart_opt(self):\n        for group in self.param_groups:\n            group['step'] = 0\n            for p in group['params']:\n                if p.requires_grad:\n                    state = self.state[p]\n                    # State initialization\n\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p)\n                    # Exponential moving average of gradient difference\n                    state['exp_avg_diff'] = torch.zeros_like(p)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\" Performs a single optimization step.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            beta1, beta2, beta3 = group['betas']\n            # assume same step across group now to simplify things\n            # per parameter step can be easily support by making it tensor, or pass list into kernel\n            if 'step' in group:\n                group['step'] += 1\n            else:\n                group['step'] = 1\n\n            bias_correction1 = 1.0 - beta1 ** group['step']\n            bias_correction2 = 1.0 - beta2 ** group['step']\n            bias_correction3 = 1.0 - beta3 ** group['step']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n\n                state = self.state[p]\n                if len(state) == 0:\n                    state['exp_avg'] = torch.zeros_like(p)\n                    state['exp_avg_diff'] = torch.zeros_like(p)\n                    state['exp_avg_sq'] = torch.zeros_like(p)\n                    state['pre_grad'] = grad.clone()\n\n                exp_avg, exp_avg_sq, exp_avg_diff = state['exp_avg'], state['exp_avg_diff'], state['exp_avg_sq']\n                grad_diff = grad - state['pre_grad']\n\n                exp_avg.lerp_(grad, 1. - beta1)  # m_t\n                exp_avg_diff.lerp_(grad_diff, 1. - beta2)  # diff_t (v)\n                update = grad + beta2 * grad_diff\n                exp_avg_sq.mul_(beta3).addcmul_(update, update, value=1. - beta3)  # n_t\n\n                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction3)).add_(group['eps'])\n                update = (exp_avg / bias_correction1 + beta2 * exp_avg_diff / bias_correction2).div_(denom)\n                if group['no_prox']:\n                    p.data.mul_(1 - group['lr'] * group['weight_decay'])\n                    p.add_(update, alpha=-group['lr'])\n                else:\n                    p.add_(update, alpha=-group['lr'])\n                    p.data.div_(1 + group['lr'] * group['weight_decay'])\n\n                state['pre_grad'].copy_(grad)\n\n        return loss\n",
  "\"\"\" NAdamW Optimizer\n\nBased on simplified algorithm in https://github.com/mlcommons/algorithmic-efficiency/tree/main/baselines/nadamw\n\nAdded multi-tensor (foreach) path.\n\"\"\"\nimport math\nfrom typing import List, Optional\n\nimport torch\nfrom torch import Tensor\n\n\n# Modified from github.com/pytorch/pytorch/blob/v1.12.1/torch/optim/adamw.py.\nclass NAdamW(torch.optim.Optimizer):\n    r\"\"\"Implements NAdamW algorithm.\n\n      See Table 1 in https://arxiv.org/abs/1910.05446 for the implementation of\n      the NAdam algorithm (there is also a comment in the code which highlights\n      the only difference of NAdamW and AdamW).\n      For further details regarding the algorithm we refer to\n      `Decoupled Weight Decay Regularization`_.\n\n      Args:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n      .. _Decoupled Weight Decay Regularization:\n          https://arxiv.org/abs/1711.05101\n      .. _On the Convergence of Adam and Beyond:\n          https://openreview.net/forum?id=ryQu7f-RZ\n    \"\"\"\n\n    def __init__(\n            self,\n            params,\n            lr=1e-3,\n            betas=(0.9, 0.999),\n            eps=1e-8,\n            weight_decay=1e-2,\n            maximize: bool = False,\n            foreach: Optional[bool] = None,\n            capturable: bool = False,\n    ):\n        if not 0.0 <= lr:\n            raise ValueError(f'Invalid learning rate: {lr}')\n        if not 0.0 <= eps:\n            raise ValueError(f'Invalid epsilon value: {eps}')\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(f'Invalid beta parameter at index 0: {betas[0]}')\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(f'Invalid beta parameter at index 1: {betas[1]}')\n        if not 0.0 <= weight_decay:\n            raise ValueError(f'Invalid weight_decay value: {weight_decay}')\n        defaults = dict(\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            foreach=foreach,\n            maximize=maximize,\n            capturable=capturable,\n        )\n        super().__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        state_values = list(self.state.values())\n        step_is_tensor = (len(state_values) != 0) and torch.is_tensor(\n            state_values[0]['step'])\n        if not step_is_tensor:\n            for s in state_values:\n                s['step'] = torch.tensor(float(s['step']))\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n            Args:\n              closure (callable, optional): A closure that reevaluates the model\n                  and returns the loss.\n        \"\"\"\n        self._cuda_graph_capture_health_check()\n\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            params_with_grad = []\n            grads = []\n            exp_avgs = []\n            exp_avg_sqs = []\n            state_steps = []\n            beta1, beta2 = group['betas']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                params_with_grad.append(p)\n                if p.grad.is_sparse:\n                    raise RuntimeError('NAdamW does not support sparse gradients')\n                grads.append(p.grad)\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = torch.tensor(0.)\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n\n                exp_avgs.append(state['exp_avg'])\n                exp_avg_sqs.append(state['exp_avg_sq'])\n                state_steps.append(state['step'])\n\n            nadamw(\n                params_with_grad,\n                grads,\n                exp_avgs,\n                exp_avg_sqs,\n                state_steps,\n                beta1=beta1,\n                beta2=beta2,\n                lr=group['lr'],\n                weight_decay=group['weight_decay'],\n                eps=group['eps'],\n                maximize=group['maximize'],\n                capturable=group['capturable'],\n            )\n\n        return loss\n\n\ndef nadamw(\n        params: List[Tensor],\n        grads: List[Tensor],\n        exp_avgs: List[Tensor],\n        exp_avg_sqs: List[Tensor],\n        state_steps: List[Tensor],\n        foreach: Optional[bool] = None,\n        capturable: bool = False,\n        *,\n        beta1: float,\n        beta2: float,\n        lr: float,\n        weight_decay: float,\n        eps: float,\n        maximize: bool,\n) -> None:\n    r\"\"\"Functional API that performs NAdamW algorithm computation.\n      See NAdamW class for details.\n    \"\"\"\n\n    if not all(isinstance(t, torch.Tensor) for t in state_steps):\n        raise RuntimeError(\n            'API has changed, `state_steps` argument must contain a list of' +\n            ' singleton tensors')\n\n    if foreach is None:\n        foreach = True\n    if foreach and not torch.jit.is_scripting():\n        func = _multi_tensor_nadamw\n    else:\n        func = _single_tensor_nadamw\n\n    func(\n        params,\n        grads,\n        exp_avgs,\n        exp_avg_sqs,\n        state_steps,\n        beta1=beta1,\n        beta2=beta2,\n        lr=lr,\n        weight_decay=weight_decay,\n        eps=eps,\n        maximize=maximize,\n        capturable=capturable,\n    )\n\n\ndef _single_tensor_nadamw(\n        params: List[Tensor],\n        grads: List[Tensor],\n        exp_avgs: List[Tensor],\n        exp_avg_sqs: List[Tensor],\n        state_steps: List[Tensor],\n        *,\n        beta1: float,\n        beta2: float,\n        lr: float,\n        weight_decay: float,\n        eps: float,\n        maximize: bool,\n        capturable: bool\n):\n\n    for i, param in enumerate(params):\n        grad = grads[i] if not maximize else -grads[i]\n        exp_avg = exp_avgs[i]\n        exp_avg_sq = exp_avg_sqs[i]\n        step_t = state_steps[i]\n\n        # Update step.\n        step_t += 1\n\n        # Perform stepweight decay.\n        param.mul_(1. - lr * weight_decay)\n\n        # Decay the first and second moment running average coefficient.\n        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n        exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n        if capturable:\n            step = step_t\n\n            # 1 - beta1 ** step can't be captured in a CUDA graph, even if step is a CUDA tensor\n            # (incurs \"RuntimeError: CUDA error: operation not permitted when stream is capturing\")\n            bias_correction1 = 1 - torch.pow(beta1, step)\n            bias_correction2 = 1 - torch.pow(beta2, step)\n\n            step_size = lr / bias_correction1\n            step_size_neg = step_size.neg()\n\n            bias_correction2_sqrt = bias_correction2.sqrt()\n\n            # Only difference between NAdamW and AdamW in this implementation.\n            # The official PyTorch implementation of NAdam uses a different algorithm.\n            exp_avg = exp_avg.mul(beta1).add_(grad, alpha=1 - beta1)\n\n            denom = (exp_avg_sq.sqrt() / (bias_correction2_sqrt * step_size_neg)).add_(eps / step_size_neg)\n            param.addcdiv_(exp_avg, denom)\n        else:\n            step = step_t.item()\n            bias_correction1 = 1 - beta1 ** step\n            bias_correction2 = 1 - beta2 ** step\n            step_size = lr / bias_correction1\n            bias_correction2_sqrt = math.sqrt(bias_correction2)\n\n            # Only difference between NAdamW and AdamW in this implementation.\n            # The official PyTorch implementation of NAdam uses a different algorithm.\n            exp_avg = exp_avg.mul(beta1).add_(grad, alpha=1 - beta1)\n\n            denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n            param.addcdiv_(exp_avg, denom, value=-step_size)\n\n\ndef _multi_tensor_nadamw(\n        params: List[Tensor],\n        grads: List[Tensor],\n        exp_avgs: List[Tensor],\n        exp_avg_sqs: List[Tensor],\n        state_steps: List[Tensor],\n        *,\n        beta1: float,\n        beta2: float,\n        lr: float,\n        weight_decay: float,\n        eps: float,\n        maximize: bool,\n        capturable: bool,\n):\n    if len(params) == 0:\n        return\n\n    if capturable:\n        assert all(\n            p.is_cuda and step.is_cuda for p, step in zip(params, state_steps)\n        ), \"If capturable=True, params and state_steps must be CUDA tensors.\"\n\n    if maximize:\n        grads = torch._foreach_neg(tuple(grads))  # type: ignore[assignment]\n\n    grads = [torch.view_as_real(x) if torch.is_complex(x) else x for x in grads]\n    exp_avgs = [torch.view_as_real(x) if torch.is_complex(x) else x for x in exp_avgs]\n    exp_avg_sqs = [torch.view_as_real(x) if torch.is_complex(x) else x for x in exp_avg_sqs]\n    params = [torch.view_as_real(x) if torch.is_complex(x) else x for x in params]\n\n    # update steps\n    torch._foreach_add_(state_steps, 1)\n\n    # Perform stepweight decay\n    torch._foreach_mul_(params, 1 - lr * weight_decay)\n\n    # Decay the first and second moment running average coefficient\n    torch._foreach_mul_(exp_avgs, beta1)\n    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)\n\n    torch._foreach_mul_(exp_avg_sqs, beta2)\n    torch._foreach_addcmul_(exp_avg_sqs, grads, grads, 1 - beta2)\n\n    if capturable:\n        # TODO: use foreach_pow if/when foreach_pow is added\n        bias_correction1 = [torch.pow(beta1, step) for step in state_steps]\n        bias_correction2 = [torch.pow(beta2, step) for step in state_steps]\n        # foreach_sub doesn't allow a scalar as the first arg\n        torch._foreach_sub_(bias_correction1, 1)\n        torch._foreach_sub_(bias_correction2, 1)\n        torch._foreach_neg_(bias_correction1)\n        torch._foreach_neg_(bias_correction2)\n\n        # foreach_div doesn't allow a scalar as the first arg\n        step_size = torch._foreach_div(bias_correction1, lr)\n        torch._foreach_reciprocal_(step_size)\n        torch._foreach_neg_(step_size)\n\n        bias_correction2_sqrt = torch._foreach_sqrt(bias_correction2)\n\n        # Only difference between NAdamW and AdamW in this implementation.\n        # The official PyTorch implementation of NAdam uses a different algorithm.\n        exp_avgs = torch._foreach_mul(exp_avgs, beta1)\n        torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)\n\n        exp_avg_sq_sqrt = torch._foreach_sqrt(exp_avg_sqs)\n        torch._foreach_div_(\n            exp_avg_sq_sqrt, torch._foreach_mul(bias_correction2_sqrt, step_size)\n        )\n        eps_over_step_size = torch._foreach_div(step_size, eps)\n        torch._foreach_reciprocal_(eps_over_step_size)\n        denom = torch._foreach_add(exp_avg_sq_sqrt, eps_over_step_size)\n\n        torch._foreach_addcdiv_(params, exp_avgs, denom)\n    else:\n        bias_correction1 = [1 - beta1 ** step.item() for step in state_steps]\n        bias_correction2 = [1 - beta2 ** step.item() for step in state_steps]\n\n        step_size = [(lr / bc) * -1 for bc in bias_correction1]\n\n        bias_correction2_sqrt = [math.sqrt(bc) for bc in bias_correction2]\n\n        # Only difference between NAdamW and AdamW in this implementation.\n        # The official PyTorch implementation of NAdam uses a different algorithm.\n        exp_avgs = torch._foreach_mul(exp_avgs, beta1)\n        torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)\n\n        exp_avg_sq_sqrt = torch._foreach_sqrt(exp_avg_sqs)\n        torch._foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n        denom = torch._foreach_add(exp_avg_sq_sqrt, eps)\n\n        torch._foreach_addcdiv_(params, exp_avgs, denom, step_size)\n",
  "\"\"\" RMSProp modified to behave like Tensorflow impl\n\nOriginally cut & paste from PyTorch RMSProp\nhttps://github.com/pytorch/pytorch/blob/063946d2b3f3f1e953a2a3b54e0b34f1393de295/torch/optim/rmsprop.py\nLicensed under BSD-Clause 3 (ish), https://github.com/pytorch/pytorch/blob/master/LICENSE\n\nModifications Copyright 2021 Ross Wightman\n\"\"\"\n\nimport torch\nfrom torch.optim import Optimizer\n\n\nclass RMSpropTF(Optimizer):\n    \"\"\"Implements RMSprop algorithm (TensorFlow style epsilon)\n\n    NOTE: This is a direct cut-and-paste of PyTorch RMSprop with eps applied before sqrt\n    and a few other modifications to closer match Tensorflow for matching hyper-params.\n\n    Noteworthy changes include:\n    1. Epsilon applied inside square-root\n    2. square_avg initialized to ones\n    3. LR scaling of update accumulated in momentum buffer\n\n    Proposed by G. Hinton in his\n    `course <http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>`_.\n\n    The centered version first appears in `Generating Sequences\n    With Recurrent Neural Networks <https://arxiv.org/pdf/1308.0850v5.pdf>`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-2)\n        momentum (float, optional): momentum factor (default: 0)\n        alpha (float, optional): smoothing (decay) constant (default: 0.9)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-10)\n        centered (bool, optional) : if ``True``, compute the centered RMSProp,\n            the gradient is normalized by an estimation of its variance\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        decoupled_decay (bool, optional): decoupled weight decay as per https://arxiv.org/abs/1711.05101\n        lr_in_momentum (bool, optional): learning rate scaling is included in the momentum buffer\n            update as per defaults in Tensorflow\n\n    \"\"\"\n\n    def __init__(self, params, lr=1e-2, alpha=0.9, eps=1e-10, weight_decay=0, momentum=0., centered=False,\n                 decoupled_decay=False, lr_in_momentum=True):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= momentum:\n            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n        if not 0.0 <= weight_decay:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n        if not 0.0 <= alpha:\n            raise ValueError(\"Invalid alpha value: {}\".format(alpha))\n\n        defaults = dict(\n            lr=lr, momentum=momentum, alpha=alpha, eps=eps, centered=centered, weight_decay=weight_decay,\n            decoupled_decay=decoupled_decay, lr_in_momentum=lr_in_momentum)\n        super(RMSpropTF, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RMSpropTF, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('momentum', 0)\n            group.setdefault('centered', False)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n                if grad.is_sparse:\n                    raise RuntimeError('RMSprop does not support sparse gradients')\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['square_avg'] = torch.ones_like(p)  # PyTorch inits to zero\n                    if group['momentum'] > 0:\n                        state['momentum_buffer'] = torch.zeros_like(p)\n                    if group['centered']:\n                        state['grad_avg'] = torch.zeros_like(p)\n\n                square_avg = state['square_avg']\n                one_minus_alpha = 1. - group['alpha']\n\n                state['step'] += 1\n\n                if group['weight_decay'] != 0:\n                    if group['decoupled_decay']:\n                        p.mul_(1. - group['lr'] * group['weight_decay'])\n                    else:\n                        grad = grad.add(p, alpha=group['weight_decay'])\n\n                # Tensorflow order of ops for updating squared avg\n                square_avg.add_(grad.pow(2) - square_avg, alpha=one_minus_alpha)\n                # square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)  # PyTorch original\n\n                if group['centered']:\n                    grad_avg = state['grad_avg']\n                    grad_avg.add_(grad - grad_avg, alpha=one_minus_alpha)\n                    avg = square_avg.addcmul(grad_avg, grad_avg, value=-1).add(group['eps']).sqrt_()  # eps in sqrt\n                    # grad_avg.mul_(alpha).add_(grad, alpha=1 - alpha)  # PyTorch original\n                else:\n                    avg = square_avg.add(group['eps']).sqrt_()  # eps moved in sqrt\n\n                if group['momentum'] > 0:\n                    buf = state['momentum_buffer']\n                    # Tensorflow accumulates the LR scaling in the momentum buffer\n                    if group['lr_in_momentum']:\n                        buf.mul_(group['momentum']).addcdiv_(grad, avg, value=group['lr'])\n                        p.add_(-buf)\n                    else:\n                        # PyTorch scales the param update by LR\n                        buf.mul_(group['momentum']).addcdiv_(grad, avg)\n                        p.add_(buf, alpha=-group['lr'])\n                else:\n                    p.addcdiv_(grad, avg, value=-group['lr'])\n\n        return loss\n",
  "\"\"\" AdaHessian Optimizer\n\nLifted from https://github.com/davda54/ada-hessian/blob/master/ada_hessian.py\nOriginally licensed MIT, Copyright 2020, David Samuel\n\"\"\"\nimport torch\n\n\nclass Adahessian(torch.optim.Optimizer):\n    \"\"\"\n    Implements the AdaHessian algorithm from \"ADAHESSIAN: An Adaptive Second OrderOptimizer for Machine Learning\"\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining parameter groups\n        lr (float, optional): learning rate (default: 0.1)\n        betas ((float, float), optional): coefficients used for computing running averages of gradient and the\n            squared hessian trace (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0.0)\n        hessian_power (float, optional): exponent of the hessian trace (default: 1.0)\n        update_each (int, optional): compute the hessian trace approximation only after *this* number of steps\n            (to save time) (default: 1)\n        n_samples (int, optional): how many times to sample `z` for the approximation of the hessian trace (default: 1)\n    \"\"\"\n\n    def __init__(self, params, lr=0.1, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.0,\n                 hessian_power=1.0, update_each=1, n_samples=1, avg_conv_kernel=False):\n        if not 0.0 <= lr:\n            raise ValueError(f\"Invalid learning rate: {lr}\")\n        if not 0.0 <= eps:\n            raise ValueError(f\"Invalid epsilon value: {eps}\")\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n        if not 0.0 <= hessian_power <= 1.0:\n            raise ValueError(f\"Invalid Hessian power value: {hessian_power}\")\n\n        self.n_samples = n_samples\n        self.update_each = update_each\n        self.avg_conv_kernel = avg_conv_kernel\n\n        # use a separate generator that deterministically generates the same `z`s across all GPUs in case of distributed training\n        self.seed = 2147483647\n        self.generator = torch.Generator().manual_seed(self.seed)\n\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, hessian_power=hessian_power)\n        super(Adahessian, self).__init__(params, defaults)\n\n        for p in self.get_params():\n            p.hess = 0.0\n            self.state[p][\"hessian step\"] = 0\n\n    @property\n    def is_second_order(self):\n        return True\n\n    def get_params(self):\n        \"\"\"\n        Gets all parameters in all param_groups with gradients\n        \"\"\"\n\n        return (p for group in self.param_groups for p in group['params'] if p.requires_grad)\n\n    def zero_hessian(self):\n        \"\"\"\n        Zeros out the accumalated hessian traces.\n        \"\"\"\n\n        for p in self.get_params():\n            if not isinstance(p.hess, float) and self.state[p][\"hessian step\"] % self.update_each == 0:\n                p.hess.zero_()\n\n    @torch.no_grad()\n    def set_hessian(self):\n        \"\"\"\n        Computes the Hutchinson approximation of the hessian trace and accumulates it for each trainable parameter.\n        \"\"\"\n\n        params = []\n        for p in filter(lambda p: p.grad is not None, self.get_params()):\n            if self.state[p][\"hessian step\"] % self.update_each == 0:  # compute the trace only each `update_each` step\n                params.append(p)\n            self.state[p][\"hessian step\"] += 1\n\n        if len(params) == 0:\n            return\n\n        if self.generator.device != params[0].device:  # hackish way of casting the generator to the right device\n            self.generator = torch.Generator(params[0].device).manual_seed(self.seed)\n\n        grads = [p.grad for p in params]\n\n        for i in range(self.n_samples):\n            # Rademacher distribution {-1.0, 1.0}\n            zs = [torch.randint(0, 2, p.size(), generator=self.generator, device=p.device) * 2.0 - 1.0 for p in params]\n            h_zs = torch.autograd.grad(\n                grads, params, grad_outputs=zs, only_inputs=True, retain_graph=i < self.n_samples - 1)\n            for h_z, z, p in zip(h_zs, zs, params):\n                p.hess += h_z * z / self.n_samples  # approximate the expected values of z*(H@z)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"\n        Performs a single optimization step.\n        Arguments:\n            closure (callable, optional) -- a closure that reevaluates the model and returns the loss (default: None)\n        \"\"\"\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        self.zero_hessian()\n        self.set_hessian()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None or p.hess is None:\n                    continue\n\n                if self.avg_conv_kernel and p.dim() == 4:\n                    p.hess = torch.abs(p.hess).mean(dim=[2, 3], keepdim=True).expand_as(p.hess).clone()\n\n                # Perform correct stepweight decay as in AdamW\n                p.mul_(1 - group['lr'] * group['weight_decay'])\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 1:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p)\n                    # Exponential moving average of Hessian diagonal square values\n                    state['exp_hessian_diag_sq'] = torch.zeros_like(p)\n\n                exp_avg, exp_hessian_diag_sq = state['exp_avg'], state['exp_hessian_diag_sq']\n                beta1, beta2 = group['betas']\n                state['step'] += 1\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(p.grad, alpha=1 - beta1)\n                exp_hessian_diag_sq.mul_(beta2).addcmul_(p.hess, p.hess, value=1 - beta2)\n\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n\n                k = group['hessian_power']\n                denom = (exp_hessian_diag_sq / bias_correction2).pow_(k / 2).add_(group['eps'])\n\n                # make update\n                step_size = group['lr'] / bias_correction1\n                p.addcdiv_(exp_avg, denom, value=-step_size)\n\n        return loss\n",
  "\"\"\" PyTorch MADGRAD optimizer\n\nMADGRAD: https://arxiv.org/abs/2101.11075\n\nCode from: https://github.com/facebookresearch/madgrad\n\"\"\"\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom typing import TYPE_CHECKING, Any, Callable, Optional\n\nimport torch\nimport torch.optim\n\nif TYPE_CHECKING:\n    from torch.optim.optimizer import _params_t\nelse:\n    _params_t = Any\n\n\nclass MADGRAD(torch.optim.Optimizer):\n    \"\"\"\n    MADGRAD_: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic\n    Optimization.\n\n    .. _MADGRAD: https://arxiv.org/abs/2101.11075\n\n    MADGRAD is a general purpose optimizer that can be used in place of SGD or\n    Adam may converge faster and generalize better. Currently GPU-only.\n    Typically, the same learning rate schedule that is used for SGD or Adam may\n    be used. The overall learning rate is not comparable to either method and\n    should be determined by a hyper-parameter sweep.\n\n    MADGRAD requires less weight decay than other methods, often as little as\n    zero. Momentum values used for SGD or Adam's beta1 should work here also.\n\n    On sparse problems both weight_decay and momentum should be set to 0.\n\n    Arguments:\n        params (iterable):\n            Iterable of parameters to optimize or dicts defining parameter groups.\n        lr (float):\n            Learning rate (default: 1e-2).\n        momentum (float):\n            Momentum value in  the range [0,1) (default: 0.9).\n        weight_decay (float):\n            Weight decay, i.e. a L2 penalty (default: 0).\n        eps (float):\n            Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-6).\n    \"\"\"\n\n    def __init__(\n            self,\n            params: _params_t,\n            lr: float = 1e-2,\n            momentum: float = 0.9,\n            weight_decay: float = 0,\n            eps: float = 1e-6,\n            decoupled_decay: bool = False,\n    ):\n        if momentum < 0 or momentum >= 1:\n            raise ValueError(f\"Momentum {momentum} must be in the range [0,1]\")\n        if lr <= 0:\n            raise ValueError(f\"Learning rate {lr} must be positive\")\n        if weight_decay < 0:\n            raise ValueError(f\"Weight decay {weight_decay} must be non-negative\")\n        if eps < 0:\n            raise ValueError(f\"Eps must be non-negative\")\n\n        defaults = dict(\n            lr=lr, eps=eps, momentum=momentum, weight_decay=weight_decay, decoupled_decay=decoupled_decay)\n        super().__init__(params, defaults)\n\n    @property\n    def supports_memory_efficient_fp16(self) -> bool:\n        return False\n\n    @property\n    def supports_flat_params(self) -> bool:\n        return True\n\n    @torch.no_grad()\n    def step(self, closure: Optional[Callable[[], float]] = None) -> Optional[float]:\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            eps = group['eps']\n            lr = group['lr'] + eps\n            weight_decay = group['weight_decay']\n            momentum = group['momentum']\n            ck = 1 - momentum\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n                if momentum != 0.0 and grad.is_sparse:\n                    raise RuntimeError(\"momentum != 0 is not compatible with sparse gradients\")\n\n                state = self.state[p]\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['grad_sum_sq'] = torch.zeros_like(p)\n                    state['s'] = torch.zeros_like(p)\n                    if momentum != 0:\n                        state['x0'] = torch.clone(p).detach()\n\n                state['step'] += 1\n                grad_sum_sq = state['grad_sum_sq']\n                s = state['s']\n                lamb = lr * math.sqrt(state['step'])\n\n                # Apply weight decay\n                if weight_decay != 0:\n                    if group['decoupled_decay']:\n                        p.mul_(1.0 - group['lr'] * weight_decay)\n                    else:\n                        if grad.is_sparse:\n                            raise RuntimeError(\"weight_decay option is not compatible with sparse gradients\")\n                        grad.add_(p, alpha=weight_decay)\n\n                if grad.is_sparse:\n                    grad = grad.coalesce()\n                    grad_val = grad._values()\n\n                    p_masked = p.sparse_mask(grad)\n                    grad_sum_sq_masked = grad_sum_sq.sparse_mask(grad)\n                    s_masked = s.sparse_mask(grad)\n\n                    # Compute x_0 from other known quantities\n                    rms_masked_vals = grad_sum_sq_masked._values().pow(1 / 3).add_(eps)\n                    x0_masked_vals = p_masked._values().addcdiv(s_masked._values(), rms_masked_vals, value=1)\n\n                    # Dense + sparse op\n                    grad_sq = grad * grad\n                    grad_sum_sq.add_(grad_sq, alpha=lamb)\n                    grad_sum_sq_masked.add_(grad_sq, alpha=lamb)\n\n                    rms_masked_vals = grad_sum_sq_masked._values().pow_(1 / 3).add_(eps)\n\n                    s.add_(grad, alpha=lamb)\n                    s_masked._values().add_(grad_val, alpha=lamb)\n\n                    # update masked copy of p\n                    p_kp1_masked_vals = x0_masked_vals.addcdiv(s_masked._values(), rms_masked_vals, value=-1)\n                    # Copy updated masked p to dense p using an add operation\n                    p_masked._values().add_(p_kp1_masked_vals, alpha=-1)\n                    p.add_(p_masked, alpha=-1)\n                else:\n                    if momentum == 0:\n                        # Compute x_0 from other known quantities\n                        rms = grad_sum_sq.pow(1 / 3).add_(eps)\n                        x0 = p.addcdiv(s, rms, value=1)\n                    else:\n                        x0 = state['x0']\n\n                    # Accumulate second moments\n                    grad_sum_sq.addcmul_(grad, grad, value=lamb)\n                    rms = grad_sum_sq.pow(1 / 3).add_(eps)\n\n                    # Update s\n                    s.add_(grad, alpha=lamb)\n\n                    # Step\n                    if momentum == 0:\n                        p.copy_(x0.addcdiv(s, rms, value=-1))\n                    else:\n                        z = x0.addcdiv(s, rms, value=-1)\n\n                        # p is a moving average of z\n                        p.mul_(1 - ck).add_(z, alpha=ck)\n\n        return loss\n",
  "\"\"\"\nSGDP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py\n\nPaper: `Slowing Down the Weight Norm Increase in Momentum-based Optimizers` - https://arxiv.org/abs/2006.08217\nCode: https://github.com/clovaai/AdamP\n\nCopyright (c) 2020-present NAVER Corp.\nMIT license\n\"\"\"\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.optim.optimizer import Optimizer, required\nimport math\n\nfrom .adamp import projection\n\n\nclass SGDP(Optimizer):\n    def __init__(self, params, lr=required, momentum=0, dampening=0,\n                 weight_decay=0, nesterov=False, eps=1e-8, delta=0.1, wd_ratio=0.1):\n        defaults = dict(\n            lr=lr, momentum=momentum, dampening=dampening, weight_decay=weight_decay,\n            nesterov=nesterov, eps=eps, delta=delta, wd_ratio=wd_ratio)\n        super(SGDP, self).__init__(params, defaults)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group['weight_decay']\n            momentum = group['momentum']\n            dampening = group['dampening']\n            nesterov = group['nesterov']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['momentum'] = torch.zeros_like(p)\n\n                # SGD\n                buf = state['momentum']\n                buf.mul_(momentum).add_(grad, alpha=1. - dampening)\n                if nesterov:\n                    d_p = grad + momentum * buf\n                else:\n                    d_p = buf\n\n                # Projection\n                wd_ratio = 1.\n                if len(p.shape) > 1:\n                    d_p, wd_ratio = projection(p, grad, d_p, group['delta'], group['wd_ratio'], group['eps'])\n\n                # Weight decay\n                if weight_decay != 0:\n                    p.mul_(1. - group['lr'] * group['weight_decay'] * wd_ratio / (1-momentum))\n\n                # Step\n                p.add_(d_p, alpha=-group['lr'])\n\n        return loss\n",
  "\"\"\" Optimizer Factory w/ Custom Weight Decay\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nimport logging\nfrom itertools import islice\nfrom typing import Optional, Callable, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom timm.models import group_parameters\n\nfrom .adabelief import AdaBelief\nfrom .adafactor import Adafactor\nfrom .adahessian import Adahessian\nfrom .adamp import AdamP\nfrom .adan import Adan\nfrom .lamb import Lamb\nfrom .lars import Lars\nfrom .lion import Lion\nfrom .lookahead import Lookahead\nfrom .madgrad import MADGRAD\nfrom .nadam import Nadam\nfrom .nadamw import NAdamW\nfrom .nvnovograd import NvNovoGrad\nfrom .radam import RAdam\nfrom .rmsprop_tf import RMSpropTF\nfrom .sgdp import SGDP\n\n\n_logger = logging.getLogger(__name__)\n\n\n# optimizers to default to multi-tensor\n_DEFAULT_FOREACH = {\n    'lion',\n}\n\n\ndef param_groups_weight_decay(\n        model: nn.Module,\n        weight_decay=1e-5,\n        no_weight_decay_list=()\n):\n    no_weight_decay_list = set(no_weight_decay_list)\n    decay = []\n    no_decay = []\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n\n        if param.ndim <= 1 or name.endswith(\".bias\") or name in no_weight_decay_list:\n            no_decay.append(param)\n        else:\n            decay.append(param)\n\n    return [\n        {'params': no_decay, 'weight_decay': 0.},\n        {'params': decay, 'weight_decay': weight_decay}]\n\n\ndef _group(it, size):\n    it = iter(it)\n    return iter(lambda: tuple(islice(it, size)), ())\n\n\ndef _layer_map(model, layers_per_group=12, num_groups=None):\n    def _in_head(n, hp):\n        if not hp:\n            return True\n        elif isinstance(hp, (tuple, list)):\n            return any([n.startswith(hpi) for hpi in hp])\n        else:\n            return n.startswith(hp)\n\n    head_prefix = getattr(model, 'pretrained_cfg', {}).get('classifier', None)\n    names_trunk = []\n    names_head = []\n    for n, _ in model.named_parameters():\n        names_head.append(n) if _in_head(n, head_prefix) else names_trunk.append(n)\n\n    # group non-head layers\n    num_trunk_layers = len(names_trunk)\n    if num_groups is not None:\n        layers_per_group = -(num_trunk_layers // -num_groups)\n    names_trunk = list(_group(names_trunk, layers_per_group))\n\n    num_trunk_groups = len(names_trunk)\n    layer_map = {n: i for i, l in enumerate(names_trunk) for n in l}\n    layer_map.update({n: num_trunk_groups for n in names_head})\n    return layer_map\n\n\ndef param_groups_layer_decay(\n        model: nn.Module,\n        weight_decay: float = 0.05,\n        no_weight_decay_list: Tuple[str] = (),\n        layer_decay: float = .75,\n        end_layer_decay: Optional[float] = None,\n        verbose: bool = False,\n):\n    \"\"\"\n    Parameter groups for layer-wise lr decay & weight decay\n    Based on BEiT: https://github.com/microsoft/unilm/blob/master/beit/optim_factory.py#L58\n    \"\"\"\n    no_weight_decay_list = set(no_weight_decay_list)\n    param_group_names = {}  # NOTE for debugging\n    param_groups = {}\n\n    if hasattr(model, 'group_matcher'):\n        # FIXME interface needs more work\n        layer_map = group_parameters(model, model.group_matcher(coarse=False), reverse=True)\n    else:\n        # fallback\n        layer_map = _layer_map(model)\n    num_layers = max(layer_map.values()) + 1\n    layer_max = num_layers - 1\n    layer_scales = list(layer_decay ** (layer_max - i) for i in range(num_layers))\n\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n\n        # no decay: all 1D parameters and model specific ones\n        if param.ndim == 1 or name in no_weight_decay_list:\n            g_decay = \"no_decay\"\n            this_decay = 0.\n        else:\n            g_decay = \"decay\"\n            this_decay = weight_decay\n\n        layer_id = layer_map.get(name, layer_max)\n        group_name = \"layer_%d_%s\" % (layer_id, g_decay)\n\n        if group_name not in param_groups:\n            this_scale = layer_scales[layer_id]\n            param_group_names[group_name] = {\n                \"lr_scale\": this_scale,\n                \"weight_decay\": this_decay,\n                \"param_names\": [],\n            }\n            param_groups[group_name] = {\n                \"lr_scale\": this_scale,\n                \"weight_decay\": this_decay,\n                \"params\": [],\n            }\n\n        param_group_names[group_name][\"param_names\"].append(name)\n        param_groups[group_name][\"params\"].append(param)\n\n    if verbose:\n        import json\n        _logger.info(\"parameter groups: \\n%s\" % json.dumps(param_group_names, indent=2))\n\n    return list(param_groups.values())\n\n\ndef optimizer_kwargs(cfg):\n    \"\"\" cfg/argparse to kwargs helper\n    Convert optimizer args in argparse args or cfg like object to keyword args for updated create fn.\n    \"\"\"\n    kwargs = dict(\n        opt=cfg.opt,\n        lr=cfg.lr,\n        weight_decay=cfg.weight_decay,\n        momentum=cfg.momentum,\n    )\n    if getattr(cfg, 'opt_eps', None) is not None:\n        kwargs['eps'] = cfg.opt_eps\n    if getattr(cfg, 'opt_betas', None) is not None:\n        kwargs['betas'] = cfg.opt_betas\n    if getattr(cfg, 'layer_decay', None) is not None:\n        kwargs['layer_decay'] = cfg.layer_decay\n    if getattr(cfg, 'opt_args', None) is not None:\n        kwargs.update(cfg.opt_args)\n    if getattr(cfg, 'opt_foreach', None) is not None:\n        kwargs['foreach'] = cfg.opt_foreach\n    return kwargs\n\n\ndef create_optimizer(args, model, filter_bias_and_bn=True):\n    \"\"\" Legacy optimizer factory for backwards compatibility.\n    NOTE: Use create_optimizer_v2 for new code.\n    \"\"\"\n    return create_optimizer_v2(\n        model,\n        **optimizer_kwargs(cfg=args),\n        filter_bias_and_bn=filter_bias_and_bn,\n    )\n\n\ndef create_optimizer_v2(\n        model_or_params,\n        opt: str = 'sgd',\n        lr: Optional[float] = None,\n        weight_decay: float = 0.,\n        momentum: float = 0.9,\n        foreach: Optional[bool] = None,\n        filter_bias_and_bn: bool = True,\n        layer_decay: Optional[float] = None,\n        param_group_fn: Optional[Callable] = None,\n        **kwargs,\n):\n    \"\"\" Create an optimizer.\n\n    TODO currently the model is passed in and all parameters are selected for optimization.\n    For more general use an interface that allows selection of parameters to optimize and lr groups, one of:\n      * a filter fn interface that further breaks params into groups in a weight_decay compatible fashion\n      * expose the parameters interface and leave it up to caller\n\n    Args:\n        model_or_params (nn.Module): model containing parameters to optimize\n        opt: name of optimizer to create\n        lr: initial learning rate\n        weight_decay: weight decay to apply in optimizer\n        momentum:  momentum for momentum based optimizers (others may use betas via kwargs)\n        foreach: Enable / disable foreach (multi-tensor) operation if True / False. Choose safe default if None\n        filter_bias_and_bn:  filter out bias, bn and other 1d params from weight decay\n        **kwargs: extra optimizer specific kwargs to pass through\n\n    Returns:\n        Optimizer\n    \"\"\"\n    if isinstance(model_or_params, nn.Module):\n        # a model was passed in, extract parameters and add weight decays to appropriate layers\n        no_weight_decay = {}\n        if hasattr(model_or_params, 'no_weight_decay'):\n            no_weight_decay = model_or_params.no_weight_decay()\n\n        if param_group_fn:\n            parameters = param_group_fn(model_or_params)\n        elif layer_decay is not None:\n            parameters = param_groups_layer_decay(\n                model_or_params,\n                weight_decay=weight_decay,\n                layer_decay=layer_decay,\n                no_weight_decay_list=no_weight_decay,\n            )\n            weight_decay = 0.\n        elif weight_decay and filter_bias_and_bn:\n            parameters = param_groups_weight_decay(model_or_params, weight_decay, no_weight_decay)\n            weight_decay = 0.\n        else:\n            parameters = model_or_params.parameters()\n    else:\n        # iterable of parameters or param groups passed in\n        parameters = model_or_params\n\n    opt_lower = opt.lower()\n    opt_split = opt_lower.split('_')\n    opt_lower = opt_split[-1]\n\n    if opt_lower.startswith('fused'):\n        try:\n            from apex.optimizers import FusedNovoGrad, FusedAdam, FusedLAMB, FusedSGD\n            has_apex = True\n        except ImportError:\n            has_apex = False\n        assert has_apex and torch.cuda.is_available(), 'APEX and CUDA required for fused optimizers'\n\n    if opt_lower.startswith('bnb'):\n        try:\n            import bitsandbytes as bnb\n            has_bnb = True\n        except ImportError:\n            has_bnb = False\n        assert has_bnb and torch.cuda.is_available(), 'bitsandbytes and CUDA required for bnb optimizers'\n\n    opt_args = dict(weight_decay=weight_decay, **kwargs)\n\n    if lr is not None:\n        opt_args.setdefault('lr', lr)\n\n    if foreach is None:\n        if opt in _DEFAULT_FOREACH:\n            opt_args.setdefault('foreach', True)\n    else:\n        opt_args['foreach'] = foreach\n\n    # basic SGD & related\n    if opt_lower == 'sgd' or opt_lower == 'nesterov':\n        # NOTE 'sgd' refers to SGD + nesterov momentum for legacy / backwards compat reasons\n        opt_args.pop('eps', None)\n        optimizer = optim.SGD(parameters, momentum=momentum, nesterov=True, **opt_args)\n    elif opt_lower == 'momentum':\n        opt_args.pop('eps', None)\n        optimizer = optim.SGD(parameters, momentum=momentum, nesterov=False, **opt_args)\n    elif opt_lower == 'sgdp':\n        optimizer = SGDP(parameters, momentum=momentum, nesterov=True, **opt_args)\n\n    # adaptive\n    elif opt_lower == 'adam':\n        optimizer = optim.Adam(parameters, **opt_args) \n    elif opt_lower == 'adamw':\n        optimizer = optim.AdamW(parameters, **opt_args)\n    elif opt_lower == 'adamp':\n        optimizer = AdamP(parameters, wd_ratio=0.01, nesterov=True, **opt_args)\n    elif opt_lower == 'nadam':\n        try:\n            # NOTE PyTorch >= 1.10 should have native NAdam\n            optimizer = optim.Nadam(parameters, **opt_args)\n        except AttributeError:\n            optimizer = Nadam(parameters, **opt_args)\n    elif opt_lower == 'nadamw':\n        optimizer = NAdamW(parameters, **opt_args)\n    elif opt_lower == 'radam':\n        optimizer = RAdam(parameters, **opt_args)\n    elif opt_lower == 'adamax':\n        optimizer = optim.Adamax(parameters, **opt_args)\n    elif opt_lower == 'adabelief':\n        optimizer = AdaBelief(parameters, rectify=False, **opt_args)\n    elif opt_lower == 'radabelief':\n        optimizer = AdaBelief(parameters, rectify=True, **opt_args)\n    elif opt_lower == 'adadelta':\n        optimizer = optim.Adadelta(parameters, **opt_args)\n    elif opt_lower == 'adagrad':\n        opt_args.setdefault('eps', 1e-8)\n        optimizer = optim.Adagrad(parameters, **opt_args)\n    elif opt_lower == 'adafactor':\n        optimizer = Adafactor(parameters, **opt_args)\n    elif opt_lower == 'adanp':\n        optimizer = Adan(parameters, no_prox=False, **opt_args)\n    elif opt_lower == 'adanw':\n        optimizer = Adan(parameters, no_prox=True, **opt_args)\n    elif opt_lower == 'lamb':\n        optimizer = Lamb(parameters, **opt_args)\n    elif opt_lower == 'lambc':\n        optimizer = Lamb(parameters, trust_clip=True, **opt_args)\n    elif opt_lower == 'larc':\n        optimizer = Lars(parameters, momentum=momentum, trust_clip=True, **opt_args)\n    elif opt_lower == 'lars':\n        optimizer = Lars(parameters, momentum=momentum, **opt_args)\n    elif opt_lower == 'nlarc':\n        optimizer = Lars(parameters, momentum=momentum, trust_clip=True, nesterov=True, **opt_args)\n    elif opt_lower == 'nlars':\n        optimizer = Lars(parameters, momentum=momentum, nesterov=True, **opt_args)\n    elif opt_lower == 'madgrad':\n        optimizer = MADGRAD(parameters, momentum=momentum, **opt_args)\n    elif opt_lower == 'madgradw':\n        optimizer = MADGRAD(parameters, momentum=momentum, decoupled_decay=True, **opt_args)\n    elif opt_lower == 'novograd' or opt_lower == 'nvnovograd':\n        optimizer = NvNovoGrad(parameters, **opt_args)\n    elif opt_lower == 'rmsprop':\n        optimizer = optim.RMSprop(parameters, alpha=0.9, momentum=momentum, **opt_args)\n    elif opt_lower == 'rmsproptf':\n        optimizer = RMSpropTF(parameters, alpha=0.9, momentum=momentum, **opt_args)\n    elif opt_lower == 'lion':\n        opt_args.pop('eps', None)\n        optimizer = Lion(parameters, **opt_args)\n\n    # second order\n    elif opt_lower == 'adahessian':\n        optimizer = Adahessian(parameters, **opt_args)\n\n    # NVIDIA fused optimizers, require APEX to be installed\n    elif opt_lower == 'fusedsgd':\n        opt_args.pop('eps', None)\n        optimizer = FusedSGD(parameters, momentum=momentum, nesterov=True, **opt_args)\n    elif opt_lower == 'fusedmomentum':\n        opt_args.pop('eps', None)\n        optimizer = FusedSGD(parameters, momentum=momentum, nesterov=False, **opt_args)\n    elif opt_lower == 'fusedadam':\n        optimizer = FusedAdam(parameters, adam_w_mode=False, **opt_args)\n    elif opt_lower == 'fusedadamw':\n        optimizer = FusedAdam(parameters, adam_w_mode=True, **opt_args)\n    elif opt_lower == 'fusedlamb':\n        optimizer = FusedLAMB(parameters, **opt_args)\n    elif opt_lower == 'fusednovograd':\n        opt_args.setdefault('betas', (0.95, 0.98))\n        optimizer = FusedNovoGrad(parameters, **opt_args)\n\n    # bitsandbytes optimizers, require bitsandbytes to be installed\n    elif opt_lower == 'bnbsgd':\n        opt_args.pop('eps', None)\n        optimizer = bnb.optim.SGD(parameters, momentum=momentum, nesterov=True, **opt_args)\n    elif opt_lower == 'bnbsgd8bit':\n        opt_args.pop('eps', None)\n        optimizer = bnb.optim.SGD8bit(parameters, momentum=momentum, nesterov=True, **opt_args)\n    elif opt_lower == 'bnbmomentum':\n        opt_args.pop('eps', None)\n        optimizer = bnb.optim.SGD(parameters, momentum=momentum, **opt_args)\n    elif opt_lower == 'bnbmomentum8bit':\n        opt_args.pop('eps', None)\n        optimizer = bnb.optim.SGD8bit(parameters, momentum=momentum, **opt_args)\n    elif opt_lower == 'bnbadam':\n        optimizer = bnb.optim.Adam(parameters, **opt_args)\n    elif opt_lower == 'bnbadam8bit':\n        optimizer = bnb.optim.Adam8bit(parameters, **opt_args)\n    elif opt_lower == 'bnbadamw':\n        optimizer = bnb.optim.AdamW(parameters, **opt_args)\n    elif opt_lower == 'bnbadamw8bit':\n        optimizer = bnb.optim.AdamW8bit(parameters, **opt_args)\n    elif opt_lower == 'bnblamb':\n        optimizer = bnb.optim.LAMB(parameters, **opt_args)\n    elif opt_lower == 'bnblamb8bit':\n        optimizer = bnb.optim.LAMB8bit(parameters, **opt_args)\n    elif opt_lower == 'bnblars':\n        optimizer = bnb.optim.LARS(parameters, **opt_args)\n    elif opt_lower == 'bnblarsb8bit':\n        optimizer = bnb.optim.LAMB8bit(parameters, **opt_args)\n    elif opt_lower == 'bnblion':\n        optimizer = bnb.optim.Lion(parameters, **opt_args)\n    elif opt_lower == 'bnblion8bit':\n        optimizer = bnb.optim.Lion8bit(parameters, **opt_args)\n\n    else:\n        assert False and \"Invalid optimizer\"\n        raise ValueError\n\n    if len(opt_split) > 1:\n        if opt_split[0] == 'lookahead':\n            optimizer = Lookahead(optimizer)\n\n    return optimizer\n",
  "\"\"\" Lion Optimizer\nPaper: `Symbolic Discovery of Optimization Algorithms` - https://arxiv.org/abs/2302.06675\nOriginal Impl: https://github.com/google/automl/tree/master/lion\n\"\"\"\n# Copyright 2023 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom typing import List\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass Lion(Optimizer):\n    r\"\"\"Implements Lion algorithm.\"\"\"\n\n    def __init__(\n            self,\n            params,\n            lr=1e-4,\n            betas=(0.9, 0.99),\n            weight_decay=0.0,\n            maximize=False,\n            foreach=None,\n    ):\n        \"\"\"Initialize the hyperparameters.\n\n        Args:\n          params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n          lr (float, optional): learning rate (default: 1e-4)\n          betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.99))\n          weight_decay (float, optional): weight decay coefficient (default: 0)\n        \"\"\"\n\n        if not 0.0 <= lr:\n            raise ValueError('Invalid learning rate: {}'.format(lr))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError('Invalid beta parameter at index 0: {}'.format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError('Invalid beta parameter at index 1: {}'.format(betas[1]))\n        defaults = dict(\n            lr=lr,\n            betas=betas,\n            weight_decay=weight_decay,\n            foreach=foreach,\n            maximize=maximize,\n        )\n        super().__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('maximize', False)\n            group.setdefault('foreach', None)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Args:\n          closure (callable, optional): A closure that reevaluates the model\n            and returns the loss.\n\n        Returns:\n          the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            params_with_grad = []\n            grads = []\n            exp_avgs = []\n            beta1, beta2 = group['betas']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                params_with_grad.append(p)\n                if p.grad.is_sparse:\n                    raise RuntimeError('Lion does not support sparse gradients')\n                grads.append(p.grad)\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n\n                exp_avgs.append(state['exp_avg'])\n\n            lion(\n                params_with_grad,\n                grads,\n                exp_avgs,\n                beta1=beta1,\n                beta2=beta2,\n                lr=group['lr'],\n                weight_decay=group['weight_decay'],\n                maximize=group['maximize'],\n                foreach=group['foreach'],\n            )\n\n        return loss\n\n\ndef lion(\n        params: List[torch.Tensor],\n        grads: List[torch.Tensor],\n        exp_avgs: List[torch.Tensor],\n        # kwonly args with defaults are not supported by functions compiled with torchscript issue #70627\n        # setting this as kwarg for now as functional API is compiled by torch/distributed/optim\n        maximize: bool = False,\n        foreach: bool = None,\n        *,\n        beta1: float,\n        beta2: float,\n        lr: float,\n        weight_decay: float,\n):\n    r\"\"\"Functional API that performs Lion algorithm computation.\n    \"\"\"\n    if foreach is None:\n        # Placeholder for more complex foreach logic to be added when value is not set\n        foreach = False\n\n    if foreach and torch.jit.is_scripting():\n        raise RuntimeError('torch.jit.script not supported with foreach optimizers')\n\n    if foreach and not torch.jit.is_scripting():\n        func = _multi_tensor_lion\n    else:\n        func = _single_tensor_lion\n\n    func(\n        params,\n        grads,\n        exp_avgs,\n        beta1=beta1,\n        beta2=beta2,\n        lr=lr,\n        weight_decay=weight_decay,\n        maximize=maximize,\n    )\n\n\ndef _single_tensor_lion(\n        params: List[torch.Tensor],\n        grads: List[torch.Tensor],\n        exp_avgs: List[torch.Tensor],\n        *,\n        beta1: float,\n        beta2: float,\n        lr: float,\n        weight_decay: float,\n        maximize: bool,\n):\n    for i, param in enumerate(params):\n        grad = grads[i] if not maximize else -grads[i]\n        exp_avg = exp_avgs[i]\n\n        if torch.is_complex(param):\n            grad = torch.view_as_real(grad)\n            exp_avg = torch.view_as_real(exp_avg)\n            param = torch.view_as_real(param)\n\n        # Perform stepweight decay\n        param.mul_(1 - lr * weight_decay)\n\n        # Weight update\n        update = exp_avg.mul(beta1).add_(grad, alpha=1 - beta1)\n        param.add_(torch.sign(update), alpha=-lr)\n\n        # Decay the momentum running average coefficient\n        exp_avg.lerp_(grad, 1 - beta2)\n\n\ndef _multi_tensor_lion(\n        params: List[torch.Tensor],\n        grads: List[torch.Tensor],\n        exp_avgs: List[torch.Tensor],\n        *,\n        beta1: float,\n        beta2: float,\n        lr: float,\n        weight_decay: float,\n        maximize: bool,\n):\n    if len(params) == 0:\n        return\n\n    if maximize:\n        grads = torch._foreach_neg(tuple(grads))  # type: ignore[assignment]\n\n    grads = [torch.view_as_real(x) if torch.is_complex(x) else x for x in grads]\n    exp_avgs = [torch.view_as_real(x) if torch.is_complex(x) else x for x in exp_avgs]\n    params = [torch.view_as_real(x) if torch.is_complex(x) else x for x in params]\n\n    # Perform stepweight decay\n    torch._foreach_mul_(params, 1 - lr * weight_decay)\n\n    # Weight update\n    updates = torch._foreach_mul(exp_avgs, beta1)\n    torch._foreach_add_(updates, grads, alpha=1 - beta1)\n\n    updates = [u.sign() for u in updates]\n    torch._foreach_add_(params, updates, alpha=-lr)\n\n    # Decay the momentum running average coefficient\n    torch._foreach_mul_(exp_avgs, beta2)\n    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta2)\n",
  "\"\"\" Nvidia NovoGrad Optimizer.\nOriginal impl by Nvidia from Jasper example:\n    - https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechRecognition/Jasper\nPaper: `Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks`\n    - https://arxiv.org/abs/1905.11286\n\"\"\"\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\nimport math\n\n\nclass NvNovoGrad(Optimizer):\n    \"\"\"\n    Implements Novograd algorithm.\n\n    Args:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.95, 0.98))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        grad_averaging: gradient averaging\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False)\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.95, 0.98), eps=1e-8,\n                 weight_decay=0, grad_averaging=False, amsgrad=False):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay,\n                        grad_averaging=grad_averaging,\n                        amsgrad=amsgrad)\n\n        super(NvNovoGrad, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(NvNovoGrad, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsgrad', False)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n            and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n                if grad.is_sparse:\n                    raise RuntimeError('Sparse gradients are not supported.')\n                amsgrad = group['amsgrad']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros([]).to(state['exp_avg'].device)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_sq'] = torch.zeros([]).to(state['exp_avg'].device)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                if amsgrad:\n                    max_exp_avg_sq = state['max_exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                norm = torch.sum(torch.pow(grad, 2))\n\n                if exp_avg_sq == 0:\n                    exp_avg_sq.copy_(norm)\n                else:\n                    exp_avg_sq.mul_(beta2).add_(norm, alpha=1 - beta2)\n\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n                grad.div_(denom)\n                if group['weight_decay'] != 0:\n                    grad.add_(p, alpha=group['weight_decay'])\n                if group['grad_averaging']:\n                    grad.mul_(1 - beta1)\n                exp_avg.mul_(beta1).add_(grad)\n\n                p.add_(exp_avg, alpha=-group['lr'])\n\n        return loss\n",
  "\"\"\" Lookahead Optimizer Wrapper.\nImplementation modified from: https://github.com/alphadl/lookahead.pytorch\nPaper: `Lookahead Optimizer: k steps forward, 1 step back` - https://arxiv.org/abs/1907.08610\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom collections import OrderedDict\nfrom typing import Callable, Dict\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\nfrom collections import defaultdict\n\n\nclass Lookahead(Optimizer):\n    def __init__(self, base_optimizer, alpha=0.5, k=6):\n        # NOTE super().__init__() not called on purpose\n        self._optimizer_step_pre_hooks: Dict[int, Callable] = OrderedDict()\n        self._optimizer_step_post_hooks: Dict[int, Callable] = OrderedDict()\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f'Invalid slow update rate: {alpha}')\n        if not 1 <= k:\n            raise ValueError(f'Invalid lookahead steps: {k}')\n        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n        self._base_optimizer = base_optimizer\n        self.param_groups = base_optimizer.param_groups\n        self.defaults = base_optimizer.defaults\n        self.defaults.update(defaults)\n        self.state = defaultdict(dict)\n        # manually add our defaults to the param groups\n        for name, default in defaults.items():\n            for group in self._base_optimizer.param_groups:\n                group.setdefault(name, default)\n\n    @torch.no_grad()\n    def update_slow(self, group):\n        for fast_p in group[\"params\"]:\n            if fast_p.grad is None:\n                continue\n            param_state = self._base_optimizer.state[fast_p]\n            if 'lookahead_slow_buff' not in param_state:\n                param_state['lookahead_slow_buff'] = torch.empty_like(fast_p)\n                param_state['lookahead_slow_buff'].copy_(fast_p)\n            slow = param_state['lookahead_slow_buff']\n            slow.add_(fast_p - slow, alpha=group['lookahead_alpha'])\n            fast_p.copy_(slow)\n\n    def sync_lookahead(self):\n        for group in self._base_optimizer.param_groups:\n            self.update_slow(group)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = self._base_optimizer.step(closure)\n        for group in self._base_optimizer.param_groups:\n            group['lookahead_step'] += 1\n            if group['lookahead_step'] % group['lookahead_k'] == 0:\n                self.update_slow(group)\n        return loss\n\n    def state_dict(self):\n        return self._base_optimizer.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self._base_optimizer.load_state_dict(state_dict)\n        self.param_groups = self._base_optimizer.param_groups\n",
  "import math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass AdaBelief(Optimizer):\n    r\"\"\"Implements AdaBelief algorithm. Modified from Adam in PyTorch\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-16)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False)\n        decoupled_decay (boolean, optional): (default: True) If set as True, then\n            the optimizer uses decoupled weight decay as in AdamW\n        fixed_decay (boolean, optional): (default: False) This is used when weight_decouple\n            is set as True.\n            When fixed_decay == True, the weight decay is performed as\n            $W_{new} = W_{old} - W_{old} \\times decay$.\n            When fixed_decay == False, the weight decay is performed as\n            $W_{new} = W_{old} - W_{old} \\times decay \\times lr$. Note that in this case, the\n            weight decay ratio decreases with learning rate (lr).\n        rectify (boolean, optional): (default: True) If set as True, then perform the rectified\n            update similar to RAdam\n        degenerated_to_sgd (boolean, optional) (default:True) If set as True, then perform SGD update\n            when variance of gradient is high\n    reference: AdaBelief Optimizer, adapting stepsizes by the belief in observed gradients, NeurIPS 2020\n\n    For a complete table of recommended hyperparameters, see https://github.com/juntang-zhuang/Adabelief-Optimizer'\n    For example train/args for EfficientNet see these gists\n      - link to train_scipt: https://gist.github.com/juntang-zhuang/0a501dd51c02278d952cf159bc233037\n      - link to args.yaml: https://gist.github.com/juntang-zhuang/517ce3c27022b908bb93f78e4f786dc3\n    \"\"\"\n\n    def __init__(\n            self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-16, weight_decay=0, amsgrad=False,\n            decoupled_decay=True, fixed_decay=False, rectify=True, degenerated_to_sgd=True):\n\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n\n        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n            for param in params:\n                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n                    param['buffer'] = [[None, None, None] for _ in range(10)]\n\n        defaults = dict(\n            lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad,\n            degenerated_to_sgd=degenerated_to_sgd, decoupled_decay=decoupled_decay, rectify=rectify,\n            fixed_decay=fixed_decay, buffer=[[None, None, None] for _ in range(10)])\n        super(AdaBelief, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(AdaBelief, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsgrad', False)\n\n    @torch.no_grad()\n    def reset(self):\n        for group in self.param_groups:\n            for p in group['params']:\n                state = self.state[p]\n                amsgrad = group['amsgrad']\n\n                # State initialization\n                state['step'] = 0\n                # Exponential moving average of gradient values\n                state['exp_avg'] = torch.zeros_like(p)\n\n                # Exponential moving average of squared gradient values\n                state['exp_avg_var'] = torch.zeros_like(p)\n                if amsgrad:\n                    # Maintains max of all exp. moving avg. of sq. grad. values\n                    state['max_exp_avg_var'] = torch.zeros_like(p)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n                if grad.dtype in {torch.float16, torch.bfloat16}:\n                    grad = grad.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        'AdaBelief does not support sparse gradients, please consider SparseAdam instead')\n\n                p_fp32 = p\n                if p.dtype in {torch.float16, torch.bfloat16}:\n                    p_fp32 = p_fp32.float()\n\n                amsgrad = group['amsgrad']\n                beta1, beta2 = group['betas']\n                state = self.state[p]\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p_fp32)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_var'] = torch.zeros_like(p_fp32)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_var'] = torch.zeros_like(p_fp32)\n                \n                # perform weight decay, check if decoupled weight decay\n                if group['decoupled_decay']:\n                    if not group['fixed_decay']:\n                        p_fp32.mul_(1.0 - group['lr'] * group['weight_decay'])\n                    else:\n                        p_fp32.mul_(1.0 - group['weight_decay'])\n                else:\n                    if group['weight_decay'] != 0:\n                        grad.add_(p_fp32, alpha=group['weight_decay'])\n\n                # get current state variable\n                exp_avg, exp_avg_var = state['exp_avg'], state['exp_avg_var']\n\n                state['step'] += 1\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n\n                # Update first and second moment running average\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                grad_residual = grad - exp_avg\n                exp_avg_var.mul_(beta2).addcmul_(grad_residual, grad_residual, value=1 - beta2)\n\n                if amsgrad:\n                    max_exp_avg_var = state['max_exp_avg_var']\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_var, exp_avg_var.add_(group['eps']), out=max_exp_avg_var)\n\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = (max_exp_avg_var.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n                else:\n                    denom = (exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n                \n                # update\n                if not group['rectify']:\n                    # Default update\n                    step_size = group['lr'] / bias_correction1\n                    p_fp32.addcdiv_(exp_avg, denom, value=-step_size)\n                else:\n                    # Rectified update, forked from RAdam\n                    buffered = group['buffer'][int(state['step'] % 10)]\n                    if state['step'] == buffered[0]:\n                        num_sma, step_size = buffered[1], buffered[2]\n                    else:\n                        buffered[0] = state['step']\n                        beta2_t = beta2 ** state['step']\n                        num_sma_max = 2 / (1 - beta2) - 1\n                        num_sma = num_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n                        buffered[1] = num_sma\n\n                        # more conservative since it's an approximated value\n                        if num_sma >= 5:\n                            step_size = math.sqrt(\n                                (1 - beta2_t) *\n                                (num_sma - 4) / (num_sma_max - 4) *\n                                (num_sma - 2) / num_sma *\n                                num_sma_max / (num_sma_max - 2)) / (1 - beta1 ** state['step'])\n                        elif group['degenerated_to_sgd']:\n                            step_size = 1.0 / (1 - beta1 ** state['step'])\n                        else:\n                            step_size = -1\n                        buffered[2] = step_size\n\n                    if num_sma >= 5:\n                        denom = exp_avg_var.sqrt().add_(group['eps'])\n                        p_fp32.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n                    elif step_size > 0:\n                        p_fp32.add_(exp_avg, alpha=-step_size * group['lr'])\n                \n                if p.dtype in {torch.float16, torch.bfloat16}:\n                    p.copy_(p_fp32)\n\n        return loss\n",
  "\"\"\" PyTorch Lamb optimizer w/ behaviour similar to NVIDIA FusedLamb\n\nThis optimizer code was adapted from the following (starting with latest)\n* https://github.com/HabanaAI/Model-References/blob/2b435114fe8e31f159b1d3063b8280ae37af7423/PyTorch/nlp/bert/pretraining/lamb.py\n* https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/Transformer-XL/pytorch/lamb.py\n* https://github.com/cybertronai/pytorch-lamb\n\nUse FusedLamb if you can (GPU). The reason for including this variant of Lamb is to have a version that is\nsimilar in behaviour to APEX FusedLamb if you aren't using NVIDIA GPUs or cannot install/use APEX.\n\nIn addition to some cleanup, this Lamb impl has been modified to support PyTorch XLA and has been tested on TPU.\n\nOriginal copyrights for above sources are below.\n\nModifications Copyright 2021 Ross Wightman\n\"\"\"\n# Copyright (c) 2021, Habana Labs Ltd.  All rights reserved.\n\n# Copyright (c) 2019-2020, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# MIT License\n#\n# Copyright (c) 2019 cybertronai\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\nimport math\n\nimport torch\nfrom torch.optim import Optimizer\n\n\nclass Lamb(Optimizer):\n    \"\"\"Implements a pure pytorch variant of FuseLAMB (NvLamb variant) optimizer from apex.optimizers.FusedLAMB\n    reference: https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/Transformer-XL/pytorch/lamb.py\n\n    LAMB was proposed in `Large Batch Optimization for Deep Learning: Training BERT in 76 minutes`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its norm. (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability. (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        grad_averaging (bool, optional): whether apply (1-beta2) to grad when\n            calculating running averages of gradient. (default: True)\n        max_grad_norm (float, optional): value used to clip global grad norm (default: 1.0)\n        trust_clip (bool): enable LAMBC trust ratio clipping (default: False)\n        always_adapt (boolean, optional): Apply adaptive learning rate to 0.0\n            weight decay parameter (default: False)\n\n    .. _Large Batch Optimization for Deep Learning - Training BERT in 76 minutes:\n        https://arxiv.org/abs/1904.00962\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    \"\"\"\n\n    def __init__(\n            self, params, lr=1e-3, bias_correction=True, betas=(0.9, 0.999), eps=1e-6,\n            weight_decay=0.01, grad_averaging=True, max_grad_norm=1.0, trust_clip=False, always_adapt=False):\n        defaults = dict(\n            lr=lr, bias_correction=bias_correction, betas=betas, eps=eps, weight_decay=weight_decay,\n            grad_averaging=grad_averaging, max_grad_norm=max_grad_norm,\n            trust_clip=trust_clip, always_adapt=always_adapt)\n        super().__init__(params, defaults)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        device = self.param_groups[0]['params'][0].device\n        one_tensor = torch.tensor(1.0, device=device)  # because torch.where doesn't handle scalars correctly\n        global_grad_norm = torch.zeros(1, device=device)\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n                if grad.is_sparse:\n                    raise RuntimeError('Lamb does not support sparse gradients, consider SparseAdam instad.')\n                global_grad_norm.add_(grad.pow(2).sum())\n\n        global_grad_norm = torch.sqrt(global_grad_norm)\n        # FIXME it'd be nice to remove explicit tensor conversion of scalars when torch.where promotes\n        # scalar types properly https://github.com/pytorch/pytorch/issues/9190\n        max_grad_norm = torch.tensor(self.defaults['max_grad_norm'], device=device)\n        clip_global_grad_norm = torch.where(\n            global_grad_norm > max_grad_norm,\n            global_grad_norm / max_grad_norm,\n            one_tensor)\n\n        for group in self.param_groups:\n            bias_correction = 1 if group['bias_correction'] else 0\n            beta1, beta2 = group['betas']\n            grad_averaging = 1 if group['grad_averaging'] else 0\n            beta3 = 1 - beta1 if grad_averaging else 1.0\n\n            # assume same step across group now to simplify things\n            # per parameter step can be easily support by making it tensor, or pass list into kernel\n            if 'step' in group:\n                group['step'] += 1\n            else:\n                group['step'] = 1\n\n            if bias_correction:\n                bias_correction1 = 1 - beta1 ** group['step']\n                bias_correction2 = 1 - beta2 ** group['step']\n            else:\n                bias_correction1, bias_correction2 = 1.0, 1.0\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.div_(clip_global_grad_norm)\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    # Exponential moving average of gradient valuesa\n                    state['exp_avg'] = torch.zeros_like(p)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(grad, alpha=beta3)  # m_t\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)  # v_t\n\n                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n                update = (exp_avg / bias_correction1).div_(denom)\n\n                weight_decay = group['weight_decay']\n                if weight_decay != 0:\n                    update.add_(p, alpha=weight_decay)\n\n                if weight_decay != 0 or group['always_adapt']:\n                    # Layer-wise LR adaptation. By default, skip adaptation on parameters that are\n                    # excluded from weight decay, unless always_adapt == True, then always enabled.\n                    w_norm = p.norm(2.0)\n                    g_norm = update.norm(2.0)\n                    # FIXME nested where required since logical and/or not working in PT XLA\n                    trust_ratio = torch.where(\n                        w_norm > 0,\n                        torch.where(g_norm > 0, w_norm / g_norm, one_tensor),\n                        one_tensor,\n                    )\n                    if group['trust_clip']:\n                        # LAMBC trust clipping, upper bound fixed at one\n                        trust_ratio = torch.minimum(trust_ratio, one_tensor)\n                    update.mul_(trust_ratio)\n\n                p.add_(update, alpha=-group['lr'])\n\n        return loss\n",
  "\"\"\"\nAdamP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/adamp.py\n\nPaper: `Slowing Down the Weight Norm Increase in Momentum-based Optimizers` - https://arxiv.org/abs/2006.08217\nCode: https://github.com/clovaai/AdamP\n\nCopyright (c) 2020-present NAVER Corp.\nMIT license\n\"\"\"\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.optim.optimizer import Optimizer\nimport math\n\n\ndef _channel_view(x) -> torch.Tensor:\n    return x.reshape(x.size(0), -1)\n\n\ndef _layer_view(x) -> torch.Tensor:\n    return x.reshape(1, -1)\n\n\ndef projection(p, grad, perturb, delta: float, wd_ratio: float, eps: float):\n    wd = 1.\n    expand_size = (-1,) + (1,) * (len(p.shape) - 1)\n    for view_func in [_channel_view, _layer_view]:\n        param_view = view_func(p)\n        grad_view = view_func(grad)\n        cosine_sim = F.cosine_similarity(grad_view, param_view, dim=1, eps=eps).abs_()\n\n        # FIXME this is a problem for PyTorch XLA\n        if cosine_sim.max() < delta / math.sqrt(param_view.size(1)):\n            p_n = p / param_view.norm(p=2, dim=1).add_(eps).reshape(expand_size)\n            perturb -= p_n * view_func(p_n * perturb).sum(dim=1).reshape(expand_size)\n            wd = wd_ratio\n            return perturb, wd\n\n    return perturb, wd\n\n\nclass AdamP(Optimizer):\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0, delta=0.1, wd_ratio=0.1, nesterov=False):\n        defaults = dict(\n            lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n            delta=delta, wd_ratio=wd_ratio, nesterov=nesterov)\n        super(AdamP, self).__init__(params, defaults)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n\n                grad = p.grad\n                beta1, beta2 = group['betas']\n                nesterov = group['nesterov']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p)\n                    state['exp_avg_sq'] = torch.zeros_like(p)\n\n                # Adam\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n\n                state['step'] += 1\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n                step_size = group['lr'] / bias_correction1\n\n                if nesterov:\n                    perturb = (beta1 * exp_avg + (1 - beta1) * grad) / denom\n                else:\n                    perturb = exp_avg / denom\n\n                # Projection\n                wd_ratio = 1.\n                if len(p.shape) > 1:\n                    perturb, wd_ratio = projection(p, grad, perturb, group['delta'], group['wd_ratio'], group['eps'])\n\n                # Weight decay\n                if group['weight_decay'] > 0:\n                    p.mul_(1. - group['lr'] * group['weight_decay'] * wd_ratio)\n\n                # Step\n                p.add_(perturb, alpha=-step_size)\n\n        return loss\n",
  "\"\"\" AdamW Optimizer\nImpl copied from PyTorch master\n\nNOTE: Builtin optim.AdamW is used by the factory, this impl only serves as a Python based reference, will be removed\nsomeday\n\"\"\"\nimport math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass AdamW(Optimizer):\n    r\"\"\"Implements AdamW algorithm.\n\n    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.\n    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False)\n\n    .. _Adam\\: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _Decoupled Weight Decay Regularization:\n        https://arxiv.org/abs/1711.05101\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=1e-2, amsgrad=False):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, amsgrad=amsgrad)\n        super(AdamW, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(AdamW, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsgrad', False)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n\n                # Perform stepweight decay\n                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n\n                # Perform optimization step\n                grad = p.grad\n                if grad.is_sparse:\n                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n                amsgrad = group['amsgrad']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_sq'] = torch.zeros_like(p)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                if amsgrad:\n                    max_exp_avg_sq = state['max_exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n                else:\n                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n\n                step_size = group['lr'] / bias_correction1\n\n                p.addcdiv_(exp_avg, denom, value=-step_size)\n\n        return loss\n",
  "import math\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass Nadam(Optimizer):\n    \"\"\"Implements Nadam algorithm (a variant of Adam based on Nesterov momentum).\n\n    It has been proposed in `Incorporating Nesterov Momentum into Adam`__.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 2e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        schedule_decay (float, optional): momentum schedule decay (default: 4e-3)\n\n    __ http://cs229.stanford.edu/proj2015/054_report.pdf\n    __ http://www.cs.toronto.edu/~fritz/absps/momentum.pdf\n\n        Originally taken from: https://github.com/pytorch/pytorch/pull/1408\n        NOTE: Has potential issues but does work well on some problems.\n    \"\"\"\n\n    def __init__(self, params, lr=2e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0, schedule_decay=4e-3):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        defaults = dict(\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            schedule_decay=schedule_decay,\n        )\n        super(Nadam, self).__init__(params, defaults)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['m_schedule'] = 1.\n                    state['exp_avg'] = torch.zeros_like(p)\n                    state['exp_avg_sq'] = torch.zeros_like(p)\n\n                # Warming momentum schedule\n                m_schedule = state['m_schedule']\n                schedule_decay = group['schedule_decay']\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n                eps = group['eps']\n                state['step'] += 1\n                t = state['step']\n                bias_correction2 = 1 - beta2 ** t\n\n                if group['weight_decay'] != 0:\n                    grad = grad.add(p, alpha=group['weight_decay'])\n\n                momentum_cache_t = beta1 * (1. - 0.5 * (0.96 ** (t * schedule_decay)))\n                momentum_cache_t_1 = beta1 * (1. - 0.5 * (0.96 ** ((t + 1) * schedule_decay)))\n                m_schedule_new = m_schedule * momentum_cache_t\n                m_schedule_next = m_schedule * momentum_cache_t * momentum_cache_t_1\n                state['m_schedule'] = m_schedule_new\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(grad, alpha=1. - beta1)\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1. - beta2)\n\n                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n                p.addcdiv_(grad, denom, value=-group['lr'] * (1. - momentum_cache_t) / (1. - m_schedule_new))\n                p.addcdiv_(exp_avg, denom, value=-group['lr'] * momentum_cache_t_1 / (1. - m_schedule_next))\n\n        return loss\n",
  "import numpy as np\nimport pandas as pd\n\n\nresults = {\n    'results-imagenet.csv': [\n        'results-imagenet-real.csv',\n        'results-imagenetv2-matched-frequency.csv',\n        'results-sketch.csv'\n    ],\n    'results-imagenet-a-clean.csv': [\n        'results-imagenet-a.csv',\n    ],\n    'results-imagenet-r-clean.csv': [\n        'results-imagenet-r.csv',\n    ],\n}\n\n\ndef diff(base_df, test_csv):\n    base_models = base_df['model'].values\n    test_df = pd.read_csv(test_csv)\n    test_models  = test_df['model'].values\n\n    rank_diff = np.zeros_like(test_models, dtype='object')\n    top1_diff = np.zeros_like(test_models, dtype='object')\n    top5_diff = np.zeros_like(test_models, dtype='object')\n    \n    for rank, model in enumerate(test_models):\n        if model in base_models:            \n            base_rank = int(np.where(base_models == model)[0])\n            top1_d = test_df['top1'][rank] - base_df['top1'][base_rank]\n            top5_d = test_df['top5'][rank] - base_df['top5'][base_rank]\n            \n            # rank_diff\n            if rank == base_rank:\n                rank_diff[rank] = f'0'\n            elif rank > base_rank:\n                rank_diff[rank] = f'-{rank - base_rank}'\n            else:\n                rank_diff[rank] = f'+{base_rank - rank}'\n                \n            # top1_diff\n            if top1_d >= .0:\n                top1_diff[rank] = f'+{top1_d:.3f}'\n            else:\n                top1_diff[rank] = f'-{abs(top1_d):.3f}'\n            \n            # top5_diff\n            if top5_d >= .0:\n                top5_diff[rank] = f'+{top5_d:.3f}'\n            else:\n                top5_diff[rank] = f'-{abs(top5_d):.3f}'\n                \n        else: \n            rank_diff[rank] = ''\n            top1_diff[rank] = ''\n            top5_diff[rank] = ''\n\n    test_df['top1_diff'] = top1_diff\n    test_df['top5_diff'] = top5_diff\n    test_df['rank_diff'] = rank_diff\n\n    test_df['param_count'] = test_df['param_count'].map('{:,.2f}'.format)\n    test_df.sort_values(['top1', 'top5', 'model'], ascending=[False, False, True], inplace=True)\n    test_df.to_csv(test_csv, index=False, float_format='%.3f')\n\n\nfor base_results, test_results in results.items():\n    base_df = pd.read_csv(base_results)\n    base_df.sort_values(['top1', 'top5', 'model'], ascending=[False, False, True], inplace=True)\n    for test_csv in test_results:\n        diff(base_df, test_csv)\n    base_df['param_count'] = base_df['param_count'].map('{:,.2f}'.format)\n    base_df.to_csv(base_results, index=False, float_format='%.3f')\n"
]