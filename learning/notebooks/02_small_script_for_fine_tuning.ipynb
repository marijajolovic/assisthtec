{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Ucitavanje dataset-a sa HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "dataset_python = load_dataset(\"bigcode/the-stack-smol-xs\", \"python\")\n",
    "dataset_c = load_dataset(\"bigcode/the-stack-smol-xs\", \"c\")\n",
    "dataset_cpp = load_dataset(\"bigcode/the-stack-smol-xs\", \"c++\")\n",
    "\n",
    "# Split the datasets into training and evaluation sets\n",
    "train_test_split = lambda dataset: dataset['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "split_python = train_test_split(dataset_python)\n",
    "split_c = train_test_split(dataset_c)\n",
    "split_cpp = train_test_split(dataset_cpp)\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_python_train = pd.DataFrame(split_python['train'])\n",
    "df_python_eval = pd.DataFrame(split_python['test'])\n",
    "\n",
    "df_c_train = pd.DataFrame(split_c['train'])\n",
    "df_c_eval = pd.DataFrame(split_c['test'])\n",
    "\n",
    "df_cpp_train = pd.DataFrame(split_cpp['train'])\n",
    "df_cpp_eval = pd.DataFrame(split_cpp['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>lang</th>\n",
       "      <th>size</th>\n",
       "      <th>ext</th>\n",
       "      <th>max_stars_count</th>\n",
       "      <th>avg_line_length</th>\n",
       "      <th>max_line_length</th>\n",
       "      <th>alphanum_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># -*- coding: utf-8 -*-\\n'''\\nReturn data to a...</td>\n",
       "      <td>Python</td>\n",
       "      <td>4218</td>\n",
       "      <td>py</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.257485</td>\n",
       "      <td>101</td>\n",
       "      <td>0.621622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td># -*- coding: utf-8 -*-\\n# Copyright 2019 Cohe...</td>\n",
       "      <td>Python</td>\n",
       "      <td>584</td>\n",
       "      <td>py</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.461538</td>\n",
       "      <td>64</td>\n",
       "      <td>0.648973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#\\n# This file is part of pySMT.\\n#\\n#   Copyr...</td>\n",
       "      <td>Python</td>\n",
       "      <td>7256</td>\n",
       "      <td>py</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.221622</td>\n",
       "      <td>80</td>\n",
       "      <td>0.580072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>import os\\nimport signal\\nimport subprocess\\ni...</td>\n",
       "      <td>Python</td>\n",
       "      <td>1292</td>\n",
       "      <td>py</td>\n",
       "      <td>35.0</td>\n",
       "      <td>28.086957</td>\n",
       "      <td>84</td>\n",
       "      <td>0.692724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def raises(err, lamda):\\n    try:\\n        lam...</td>\n",
       "      <td>Python</td>\n",
       "      <td>2982</td>\n",
       "      <td>py</td>\n",
       "      <td>3.0</td>\n",
       "      <td>27.357798</td>\n",
       "      <td>78</td>\n",
       "      <td>0.551643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content    lang  size ext  \\\n",
       "0  # -*- coding: utf-8 -*-\\n'''\\nReturn data to a...  Python  4218  py   \n",
       "1  # -*- coding: utf-8 -*-\\n# Copyright 2019 Cohe...  Python   584  py   \n",
       "2  #\\n# This file is part of pySMT.\\n#\\n#   Copyr...  Python  7256  py   \n",
       "3  import os\\nimport signal\\nimport subprocess\\ni...  Python  1292  py   \n",
       "4  def raises(err, lamda):\\n    try:\\n        lam...  Python  2982  py   \n",
       "\n",
       "   max_stars_count  avg_line_length  max_line_length  alphanum_fraction  \n",
       "0              NaN        25.257485              101           0.621622  \n",
       "1              NaN        22.461538               64           0.648973  \n",
       "2              NaN        39.221622               80           0.580072  \n",
       "3             35.0        28.086957               84           0.692724  \n",
       "4              3.0        27.357798               78           0.551643  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_python_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>lang</th>\n",
       "      <th>size</th>\n",
       "      <th>ext</th>\n",
       "      <th>max_stars_count</th>\n",
       "      <th>avg_line_length</th>\n",
       "      <th>max_line_length</th>\n",
       "      <th>alphanum_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>import sqlite3\\n\\n\\ndef init():\\n    global da...</td>\n",
       "      <td>Python</td>\n",
       "      <td>576</td>\n",
       "      <td>py</td>\n",
       "      <td>11.0</td>\n",
       "      <td>23.040000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.604167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td># This file is a part of Arjuna\\n# Copyright 2...</td>\n",
       "      <td>Python</td>\n",
       "      <td>3844</td>\n",
       "      <td>py</td>\n",
       "      <td>13.0</td>\n",
       "      <td>33.719298</td>\n",
       "      <td>136</td>\n",
       "      <td>0.706296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>import matplotlib.pyplot as plt\\nimport seabor...</td>\n",
       "      <td>Python</td>\n",
       "      <td>116081</td>\n",
       "      <td>py</td>\n",
       "      <td>1.0</td>\n",
       "      <td>530.050228</td>\n",
       "      <td>10170</td>\n",
       "      <td>0.711874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>import numpy as np\\n\\nimport mss\\n\\nfrom redis...</td>\n",
       "      <td>Python</td>\n",
       "      <td>6780</td>\n",
       "      <td>py</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.684211</td>\n",
       "      <td>176</td>\n",
       "      <td>0.631416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>import six\\n\\nfrom odin import bases\\nfrom odi...</td>\n",
       "      <td>Python</td>\n",
       "      <td>2523</td>\n",
       "      <td>py</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.397590</td>\n",
       "      <td>119</td>\n",
       "      <td>0.662703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content    lang    size ext  \\\n",
       "0  import sqlite3\\n\\n\\ndef init():\\n    global da...  Python     576  py   \n",
       "1  # This file is a part of Arjuna\\n# Copyright 2...  Python    3844  py   \n",
       "2  import matplotlib.pyplot as plt\\nimport seabor...  Python  116081  py   \n",
       "3  import numpy as np\\n\\nimport mss\\n\\nfrom redis...  Python    6780  py   \n",
       "4  import six\\n\\nfrom odin import bases\\nfrom odi...  Python    2523  py   \n",
       "\n",
       "   max_stars_count  avg_line_length  max_line_length  alphanum_fraction  \n",
       "0             11.0        23.040000               47           0.604167  \n",
       "1             13.0        33.719298              136           0.706296  \n",
       "2              1.0       530.050228            10170           0.711874  \n",
       "3              2.0        35.684211              176           0.631416  \n",
       "4             22.0        30.397590              119           0.662703  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_python_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ucitavanje pretreniranog modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizovanje podataka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset_python = Dataset.from_pandas(df_python_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"# -*- coding: utf-8 -*-\\n'''\\nReturn data to a mongodb server\\n\\nRequired python modules: pymongo\\n\\n\\nThis returner will send data from the minions to a MongoDB server. To\\nconfigure the settings for your MongoDB server, add the following lines\\nto the minion config files::\\n\\n    mongo.db: <database name>\\n    mongo.host: <server ip address>\\n    mongo.user: <MongoDB username>\\n    mongo.password: <MongoDB user password>\\n    mongo.port: 27017\\n\\nAlternative configuration values can be used by prefacing the configuration.\\nAny values not found in the alternative configuration will be pulled from\\nthe default location::\\n\\n    alternative.mongo.db: <database name>\\n    alternative.mongo.host: <server ip address>\\n    alternative.mongo.user: <MongoDB username>\\n    alternative.mongo.password: <MongoDB user password>\\n    alternative.mongo.port: 27017\\n\\n  To use the mongo returner, append '--return mongo' to the salt command. ex:\\n\\n    salt '*' test.ping --return mongo_return\\n\\n  To use the alternative configuration, append '--return_config alternative' to the salt command. ex:\\n\\n    salt '*' test.ping --return mongo_return --return_config alternative\\n'''\\nfrom __future__ import absolute_import\\n\\n# Import python libs\\nimport logging\\n\\n# import Salt libs\\nimport salt.utils\\nimport salt.returners\\nimport six\\n\\n# Import third party libs\\ntry:\\n    import pymongo\\n    HAS_PYMONGO = True\\nexcept ImportError:\\n    HAS_PYMONGO = False\\n\\n\\nlog = logging.getLogger(__name__)\\n\\n# Define the module's virtual name\\n# currently only used iby _get_options\\n__virtualname__ = 'mongo'\\n\\n\\ndef __virtual__():\\n    if not HAS_PYMONGO:\\n        return False\\n    return 'mongo_return'\\n\\n\\ndef _remove_dots(src):\\n    '''\\n    Remove dots from the given data structure\\n    '''\\n    output = {}\\n    for key, val in six.iteritems(src):\\n        if isinstance(val, dict):\\n            val = _remove_dots(val)\\n        output[key.replace('.', '-')] = val\\n    return output\\n\\n\\ndef _get_options(ret):\\n    '''\\n    Get the monogo_return options from salt.\\n    '''\\n    attrs = {'host': 'host',\\n             'port': 'port',\\n             'db': 'db',\\n             'username': 'username',\\n             'password': 'password'}\\n\\n    _options = salt.returners.get_returner_options(__virtualname__,\\n                                                   ret,\\n                                                   attrs,\\n                                                   __salt__=__salt__,\\n                                                   __opts__=__opts__)\\n    return _options\\n\\n\\ndef _get_conn(ret):\\n    '''\\n    Return a mongodb connection object\\n    '''\\n    _options = _get_options(ret)\\n\\n    host = _options.get('host')\\n    port = _options.get('port')\\n    db_ = _options.get('db')\\n    user = _options.get('user')\\n    password = _options.get('password')\\n\\n    conn = pymongo.Connection(host, port)\\n    mdb = conn[db_]\\n\\n    if user and password:\\n        mdb.authenticate(user, password)\\n    return conn, mdb\\n\\n\\ndef returner(ret):\\n    '''\\n    Return data to a mongodb server\\n    '''\\n    conn, mdb = _get_conn(ret)\\n    col = mdb[ret['id']]\\n\\n    if isinstance(ret['return'], dict):\\n        back = _remove_dots(ret['return'])\\n    else:\\n        back = ret['return']\\n\\n    log.debug(back)\\n    sdata = {ret['jid']: back, 'fun': ret['fun']}\\n    if 'out' in ret:\\n        sdata['out'] = ret['out']\\n    col.insert(sdata)\\n\\n\\ndef get_jid(jid):\\n    '''\\n    Return the return information associated with a jid\\n    '''\\n    conn, mdb = _get_conn(ret=None)\\n    ret = {}\\n    for collection in mdb.collection_names():\\n        rdata = mdb[collection].find_one({jid: {'$exists': 'true'}})\\n        if rdata:\\n            ret[collection] = rdata\\n    return ret\\n\\n\\ndef get_fun(fun):\\n    '''\\n    Return the most recent jobs that have executed the named function\\n    '''\\n    conn, mdb = _get_conn(ret=None)\\n    ret = {}\\n    for collection in mdb.collection_names():\\n        rdata = mdb[collection].find_one({'fun': fun})\\n        if rdata:\\n            ret[collection] = rdata\\n    return ret\\n\\n\\ndef prep_jid(nocache, passed_jid=None):  # pylint: disable=unused-argument\\n    '''\\n    Do any work necessary to prepare a JID, including sending a custom id\\n    '''\\n    return passed_jid if passed_jid is not None else salt.utils.gen_jid()\\n\",\n",
       " '# -*- coding: utf-8 -*-\\n# Copyright 2019 Cohesity Inc.\\n\\nclass Type21Enum(object):\\n\\n    \"\"\"Implementation of the \\'Type21\\' enum.\\n\\n    Specifies the type of the CloudDeploy target.\\n    \\'kAzure\\' indicates that Azure as a cloud deploy target type.\\n    \\'kAws\\' indicates that AWS as a cloud deploy target type.\\n    \\'kGcp\\' indicates that GCP as a cloud deploy target type.\\n\\n    Attributes:\\n        KAZURE: TODO: type description here.\\n        KAWS: TODO: type description here.\\n        KGCP: TODO: type description here.\\n\\n    \"\"\"\\n\\n    KAZURE = \\'kAzure\\'\\n\\n    KAWS = \\'kAws\\'\\n\\n    KGCP = \\'kGcp\\'\\n\\n',\n",
       " '#\\n# This file is part of pySMT.\\n#\\n#   Copyright 2014 Andrea Micheli and Marco Gario\\n#\\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\\n#   you may not use this file except in compliance with the License.\\n#   You may obtain a copy of the License at\\n#\\n#       http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#   Unless required by applicable law or agreed to in writing, software\\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n#   See the License for the specific language governing permissions and\\n#   limitations under the License.\\n#\\nfrom pysmt.shortcuts import (And, Iff, Or, Symbol, Implies, Not,\\n                             Exists, ForAll,\\n                             Times, Plus, Minus, Equals, Real,\\n                             is_valid)\\nfrom pysmt.test import TestCase, skipIfNoSolverForLogic, main\\nfrom pysmt.rewritings import prenex_normal_form, nnf, conjunctive_partition, aig\\nfrom pysmt.rewritings import disjunctive_partition\\nfrom pysmt.rewritings import TimesDistributor\\nfrom pysmt.test.examples import get_example_formulae\\nfrom pysmt.exceptions import SolverReturnedUnknownResultError\\nfrom pysmt.logics import BOOL, QF_NRA, QF_LRA, QF_LIA\\nfrom pysmt.typing import REAL\\n\\n\\nclass TestRewritings(TestCase):\\n\\n    def test_prenex_basic(self):\\n        a,b,c = (Symbol(x) for x in \"abc\")\\n        f = Not(And(a, Exists([b], And(a, b)), ForAll([c], Or(a, c))))\\n        prenex = prenex_normal_form(f)\\n        # Two prenex normal forms are possible\\n        my_prenex_1 = Exists([c], ForAll([b], Not(And(a, And(a, b), Or(a, c)))))\\n        my_prenex_2 = ForAll([b], Exists([c], Not(And(a, And(a, b), Or(a, c)))))\\n        self.assertTrue(prenex == my_prenex_1 or prenex == my_prenex_2)\\n\\n    @skipIfNoSolverForLogic(BOOL)\\n    def test_prenex_simple_exists(self):\\n        a,b = (Symbol(x) for x in \"ab\")\\n        f = And(b, Exists([b], Implies(a, b)))\\n        prenex = prenex_normal_form(f)\\n        self.assertTrue(prenex.is_exists())\\n        self.assertValid(Iff(f, prenex), logic=BOOL)\\n\\n    @skipIfNoSolverForLogic(BOOL)\\n    def test_prenex_simple_forall(self):\\n        a,b = (Symbol(x) for x in \"ab\")\\n        f = Or(b, ForAll([b], Implies(a, b)))\\n        prenex = prenex_normal_form(f)\\n        self.assertTrue(prenex.is_forall())\\n        self.assertValid(Iff(f, prenex), logic=BOOL)\\n\\n    @skipIfNoSolverForLogic(BOOL)\\n    def test_prenex_negated_exists(self):\\n        a,b = (Symbol(x) for x in \"ab\")\\n        f = Implies(Exists([b], Implies(a, b)), b)\\n        prenex = prenex_normal_form(f)\\n        self.assertTrue(prenex.is_forall())\\n        self.assertValid(Iff(f, prenex), logic=BOOL)\\n\\n    @skipIfNoSolverForLogic(BOOL)\\n    def test_prenex_negated_forall(self):\\n        a,b = (Symbol(x) for x in \"ab\")\\n        f = Implies(ForAll([b], Implies(a, b)), b)\\n        prenex = prenex_normal_form(f)\\n        self.assertTrue(prenex.is_exists())\\n        self.assertValid(Iff(f, prenex), logic=BOOL)\\n\\n    def test_prenex_examples(self):\\n        for (f, _, _, logic) in get_example_formulae():\\n            if self.env.factory.has_solvers(logic=logic):\\n                prenex = prenex_normal_form(f)\\n                if ( prenex is not None):\\n                    try:\\n                        ok = is_valid(Iff(f, prenex), logic=logic)\\n                    except SolverReturnedUnknownResultError:\\n                        ok = not logic.quantifier_free\\n                    self.assertTrue(ok)\\n\\n    def test_nnf_examples(self):\\n        for (f, _, _, logic) in get_example_formulae():\\n            if self.env.factory.has_solvers(logic=logic):\\n                rf = nnf(f)\\n                try:\\n                    ok = is_valid(Iff(f, rf), logic=logic)\\n                except SolverReturnedUnknownResultError:\\n                    ok = not logic.quantifier_free\\n                self.assertTrue(ok)\\n\\n    def test_conj_partitioning(self):\\n        for (f, _, _, logic) in get_example_formulae():\\n            if self.env.factory.has_solvers(logic=logic):\\n                conjuncts = list(conjunctive_partition(f))\\n                try:\\n                    ok = is_valid(Iff(f, And(conjuncts)), logic=logic)\\n                except SolverReturnedUnknownResultError:\\n                    ok = not logic.quantifier_free\\n                self.assertTrue(ok)\\n\\n    def test_disj_partitioning(self):\\n        for (f, _, _, logic) in get_example_formulae():\\n            if self.env.factory.has_solvers(logic=logic):\\n                disjuncts = list(disjunctive_partition(f))\\n                try:\\n                    ok = is_valid(Iff(f, Or(disjuncts)), logic=logic)\\n                except SolverReturnedUnknownResultError:\\n                    ok = not logic.quantifier_free\\n                self.assertTrue(ok)\\n\\n    def test_aig_examples(self):\\n        for (f, _, _, logic) in get_example_formulae():\\n            if self.env.factory.has_solvers(logic=logic):\\n                f_aig = aig(f)\\n                try:\\n                    ok = is_valid(Iff(f, f_aig), logic=logic)\\n                except SolverReturnedUnknownResultError:\\n                    ok = not logic.quantifier_free\\n                self.assertTrue(ok, \"Was: %s\\\\n Got:%s\" % (f, f_aig))\\n\\n    @skipIfNoSolverForLogic(QF_NRA)\\n    def test_times_distributivity(self):\\n        r = Symbol(\"r\", REAL)\\n        s = Symbol(\"s\", REAL)\\n        td = TimesDistributor()\\n\\n        f = Times(Plus(r, Real(1)), Real(3))\\n        fp = td.walk(f)\\n        self.assertValid(Equals(f, fp), (f, fp))\\n\\n        f = Times(Plus(r, Real(1)), s)\\n        fp = td.walk(f)\\n        self.assertValid(Equals(f, fp), (f, fp))\\n\\n        f = Times(Plus(r, Real(1), s), Real(3))\\n        fp = td.walk(f)\\n        self.assertValid(Equals(f, fp), (f, fp))\\n\\n        f = Times(Minus(r, Real(1)), Real(3))\\n        fp = td.walk(f)\\n        self.assertValid(Equals(f, fp), (f, fp))\\n\\n        f = Times(Minus(r, Real(1)), s)\\n        fp = td.walk(f)\\n        self.assertValid(Equals(f, fp), (f, fp))\\n\\n        f = Times(Minus(Real(1), s), Real(3))\\n        fp = td.walk(f)\\n        self.assertValid(Equals(f, fp), (f, fp))\\n\\n        f = Times(Minus(r, Real(1)), Plus(r, s))\\n        fp = td.walk(f)\\n        self.assertValid(Equals(f, fp), (f, fp))\\n\\n        # (r + 1) * (s-1) = r*s + (-r) + s - 1\\n        f = Times(Plus(r, Real(1)), Minus(s, Real(1)))\\n        fp = td.walk(f).simplify()\\n        target = Plus(Times(r, s),\\n                      Times(r, Real(-1)),\\n                      s,\\n                      Real(-1))\\n        self.assertValid(Equals(fp, target), fp)\\n        self.assertTrue(fp.is_plus(), fp)\\n\\n    @skipIfNoSolverForLogic(QF_NRA)\\n    def test_times_distributivity_smtlib_nra(self):\\n        from pysmt.test.smtlib.parser_utils import formulas_from_smtlib_test_set\\n        test_set = formulas_from_smtlib_test_set(logics=[QF_LRA, QF_NRA])\\n        for (_, fname, f, _) in test_set:\\n            td = TimesDistributor()\\n            _ = td.walk(f)\\n            for (old, new) in td.memoization.items():\\n                if not old.is_times(): continue\\n                if old is new: continue # Nothing changed\\n                self.assertValid(Equals(old, new),\\n                                 (old, new), solver_name=\"z3\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n',\n",
       " 'import os\\nimport signal\\nimport subprocess\\nimport json\\nimport time\\nfrom datetime import datetime\\nimport threading\\nimport logging\\n\\nimport django\\n\\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"gpu_tasker.settings\")\\ndjango.setup()\\n\\nfrom base.utils import get_admin_config\\nfrom task.models import GPUTask\\nfrom task.utils import run_task\\nfrom gpu_info.models import GPUServer\\nfrom gpu_info.utils import GPUInfoUpdater\\n\\ntask_logger = logging.getLogger(\\'django.task\\')\\n\\n\\nif __name__ == \\'__main__\\':\\n    server_username, server_private_key_path = get_admin_config()\\n\\n    gpu_updater = GPUInfoUpdater(server_username, server_private_key_path)\\n    while True:\\n        task_logger.info(\\'Running processes: {:d}\\'.format(\\n            threading.active_count() - 1\\n        ))\\n        start_time = time.time()\\n        gpu_updater.update_gpu_info()\\n        for task in GPUTask.objects.filter(status=0):\\n            available_server = task.find_available_server()\\n            if available_server is not None:\\n                t = threading.Thread(target=run_task, args=(task, available_server))\\n                t.start()\\n                time.sleep(5)\\n        end_time = time.time()\\n\\n        # 确保至少间隔十秒更新一次\\n        duration = end_time - start_time\\n        if duration < 10:\\n            time.sleep(10 - duration)\\n',\n",
       " 'def raises(err, lamda):\\n    try:\\n        lamda()\\n        return False\\n    except err:\\n        return True\\n\\n\\ndef expand_tuples(L):\\n    \"\"\"\\n\\n    >>> expand_tuples([1, (2, 3)])\\n    [(1, 2), (1, 3)]\\n\\n    >>> expand_tuples([1, 2])\\n    [(1, 2)]\\n    \"\"\"\\n    if not L:\\n        return [()]\\n    elif not isinstance(L[0], tuple):\\n        rest = expand_tuples(L[1:])\\n        return [(L[0],) + t for t in rest]\\n    else:\\n        rest = expand_tuples(L[1:])\\n        return [(item,) + t for t in rest for item in L[0]]\\n\\n\\n# Taken from theano/theano/gof/sched.py\\n# Avoids licensing issues because this was written by Matthew Rocklin\\ndef _toposort(edges):\\n    \"\"\" Topological sort algorithm by Kahn [1] - O(nodes + vertices)\\n\\n    inputs:\\n        edges - a dict of the form {a: {b, c}} where b and c depend on a\\n    outputs:\\n        L - an ordered list of nodes that satisfy the dependencies of edges\\n\\n    >>> _toposort({1: (2, 3), 2: (3, )})\\n    [1, 2, 3]\\n\\n    Closely follows the wikipedia page [2]\\n\\n    [1] Kahn, Arthur B. (1962), \"Topological sorting of large networks\",\\n    Communications of the ACM\\n    [2] http://en.wikipedia.org/wiki/Toposort#Algorithms\\n    \"\"\"\\n    incoming_edges = reverse_dict(edges)\\n    incoming_edges = dict((k, set(val)) for k, val in incoming_edges.items())\\n    S = set((v for v in edges if v not in incoming_edges))\\n    L = []\\n\\n    while S:\\n        n = S.pop()\\n        L.append(n)\\n        for m in edges.get(n, ()):\\n            assert n in incoming_edges[m]\\n            incoming_edges[m].remove(n)\\n            if not incoming_edges[m]:\\n                S.add(m)\\n    if any(incoming_edges.get(v, None) for v in edges):\\n        raise ValueError(\"Input has cycles\")\\n    return L\\n\\n\\ndef reverse_dict(d):\\n    \"\"\"Reverses direction of dependence dict\\n\\n    >>> d = {\\'a\\': (1, 2), \\'b\\': (2, 3), \\'c\\':()}\\n    >>> reverse_dict(d)  # doctest: +SKIP\\n    {1: (\\'a\\',), 2: (\\'a\\', \\'b\\'), 3: (\\'b\\',)}\\n\\n    :note: dict order are not deterministic. As we iterate on the\\n        input dict, it make the output of this function depend on the\\n        dict order. So this function output order should be considered\\n        as undeterministic.\\n\\n    \"\"\"\\n    result = {}\\n    for key in d:\\n        for val in d[key]:\\n            result[val] = result.get(val, tuple()) + (key, )\\n    return result\\n\\n\\n# Taken from toolz\\n# Avoids licensing issues because this version was authored by Matthew Rocklin\\ndef groupby(func, seq):\\n    \"\"\" Group a collection by a key function\\n\\n    >>> names = [\\'Alice\\', \\'Bob\\', \\'Charlie\\', \\'Dan\\', \\'Edith\\', \\'Frank\\']\\n    >>> groupby(len, names)  # doctest: +SKIP\\n    {3: [\\'Bob\\', \\'Dan\\'], 5: [\\'Alice\\', \\'Edith\\', \\'Frank\\'], 7: [\\'Charlie\\']}\\n\\n    >>> iseven = lambda x: x % 2 == 0\\n    >>> groupby(iseven, [1, 2, 3, 4, 5, 6, 7, 8])  # doctest: +SKIP\\n    {False: [1, 3, 5, 7], True: [2, 4, 6, 8]}\\n\\n    See Also:\\n        ``countby``\\n    \"\"\"\\n\\n    d = dict()\\n    for item in seq:\\n        key = func(item)\\n        if key not in d:\\n            d[key] = list()\\n        d[key].append(item)\\n    return d\\n',\n",
       " 'import os #to run SLiM from python\\nimport numpy as np #for vectors etc\\nimport pyslim, tskit, msprime #to recapitate, mutate, and sample tree sequences, and compute statistics\\nimport csv #to save statistics\\n\\n# parameters for SLiM\\nK = 1e4 #carrying capacity\\nN0 = K #intial population size\\nd = 0.0 #decline rate of ancestral homozygote\\ns = 0.13 #selection coefficient\\nh = 0.5 #dominance coefficient\\nB = 2 #number of offspring per parent\\nL = 2e7 #number of sites on chromosome\\nL0 = round(L/2) #location of selected site (one of the center sites)\\nu = 1e-5 #mutation rate at selected site\\nm = 0 #migration rate\\nrbp = 2e-8 #per basepair recombination rate\\nk = 0 #initial number of beneficial alleles\\ndatadir = \"data/\" #location to put output files\\nnreps = 100 #number of replicates (number of runs where allele fixes and population recovers)\\nmaxt = 1000 #maximum number of generations (safestop that should never be reached)\\n\\n#parameters for msprime\\nNe = round(K * 4/7) #long-term effective population size\\nU = 6e-9 #per basepair mutation rate at neutral sites\\nnsamples = 100 #number of chromosomes to sample for stats\\nnwindows = 100 #number of windows across genome to compute stats for\\nR = 0.001 #recombination distance between neutral loci to calculate LD for\\n\\n#genome window calculation\\nwindows = np.linspace(0, L, nwindows+1) #delimit windows\\nmidpoints = windows[:-1]+(windows[1:]-windows[:-1])/2 #midpoint of each window\\ndistances = midpoints-L0 #distance of midpoint from selected site\\nrecombination = [(1-(1-2*rbp)**abs(i))/2*np.sign(i) for i in distances] #recombination rate at midpoint (signed for plotting)\\n\\n#empty vector for number of unique copies of beneficial allele remaining at fixation in each replicate\\nX = np.zeros(nreps)\\n\\n#for each replicate\\nfor i in range(nreps): \\n\\t\\n\\t# define SLiM script ...\\n\\tscript = \"\"\"\\n\\tinitialize() {\\n\\t\\t\\n\\t\\tdefineConstant(\"K\", %d); //carrying capacity (integer)\\n\\t\\tdefineConstant(\"N0\", %d); //initial pop size (integer)\\n\\t\\tdefineConstant(\"d\", %f); // wildtype decline rate [0,1]\\n\\t\\tdefineConstant(\"s\", %f); //beneficial selection coefficient ([0,1]; s>d for rescue to be possible) \\n\\t\\tdefineConstant(\"h\", %f); //beneficial dominance [0,1]\\n\\t\\tdefineConstant(\"B\", %d); //offspring per parent (positive integer; must be great than 1 for possible persistence) \\n\\t\\tdefineConstant(\"L\", %d - 1); //number of sites (positive integer)\\n\\t\\tdefineConstant(\"L0\", %d - 1); //site number of beneficial locus (positive integer, L0<L)\\n\\t\\tdefineConstant(\"u\", %.8f); //mutation rate at beneficial locus [0,1]\\n\\t\\tdefineConstant(\"m\", %.8f); //migration rate [0,1]\\n\\t\\tdefineConstant(\"rbp\", %.8f); //recombination rate per base pair [0,1]\\n\\t\\tdefineConstant(\"k\", %d); //initial number of mutants\\n\\t\\tdefineConstant(\"outfile_dynamics\", \"%sdynamics_%d.txt\"); //where to save dynamics\\n\\t\\tdefineConstant(\"outfile_tree\", \"%stree_%d.trees\"); //where to save tree\\n\\t\\tdefineConstant(\"simID\", getSeed()); //get the random seed to label temporary file\\n\\n\\t\\tinitializeSLiMModelType(\"nonWF\"); //non Wright Fisher model\\n\\t\\tinitializeTreeSeq(); //record the tree\\n\\t\\tinitializeMutationType(\"m1\", h, \"f\", s); //beneficial mutation characteristics\\n\\t\\tm1.mutationStackPolicy = \"f\"; //keep first mutation\\n\\t\\tinitializeMutationType(\"m2\", 0.5, \"f\", 0.0); //neutral mutations (heritability has no affect)\\n\\t\\tinitializeGenomicElementType(\"g1\", m1, 1.0); //define element g1 to have beneficial mutations\\n\\t\\tinitializeGenomicElementType(\"g2\", m2, 1.0); //define element g2 to have neutral mutations\\n\\t\\tinitializeGenomicElement(g1, L0, L0); //element g1 is just one site\\n\\t\\tinitializeGenomicElement(g2, 0, L0 - 1); // element g2 is everything to the left...\\n\\t\\tinitializeGenomicElement(g2, L0 + 1, L); // ...and everything to the right of LO\\n\\t\\tinitializeMutationRate(c(0,u,0), c(L0-1, L0, L)); //mutation rate per site \\n\\t\\tinitializeRecombinationRate(rbp); //recombination rate between sites\\n\\t\\twriteFile(outfile_dynamics, \"t n p\"); //start writing to the dynamics file     \\n\\t}\\n\\n\\treproduction() { //occurs immediately before early events\\n\\t\\tfor (i in 1:B) //B matings per parent\\n\\t\\t\\tsubpop.addCrossed(individual, subpop.sampleIndividuals(1)); //random mating, 1 offspring per pair\\n\\t}\\n\\n\\t//discrete generations, hard carrying capacity, census and update fitness\\n\\t1:%d early() {\\n\\n\\t\\t//initialize population\\n\\t\\tif (sim.generation == 1) {\\n\\t\\t\\tsim.addSubpop(\"p1\", N0); //initialize population of wildtypes\\n\\t\\t\\ttarget = sample(p1.genomes, k); //choose k chromosomes without replacement...\\n\\t\\t\\tfor (i in target)\\n\\t\\t\\t\\ti.addNewDrawnMutation(m1, L0); //... and give beneficial mutation\\n\\t\\t\\tsim.outputFull(\"/tmp/slim_\" + simID + \".txt\"); //output this initial state to use for future runs if needed\\n\\t\\t}\\n\\n\\t\\t//enforce discrete generations\\n\\t\\tinds = sim.subpopulations.individuals; //get info on all individuals\\n\\t\\tinds[inds.age > 0].fitnessScaling = 0.0; //parents all die at next instance of viability selection\\n\\n\\t\\t//hard carrying capacity by random culling\\n\\t\\toff = inds[inds.age == 0]; //offspring\\n\\t\\tN = length(off); //total number of offspring\\n\\t\\tindices = which(inds.age == 0); //indices of offspring\\n\\t\\tif (N > K) { //if more than K...\\n\\t\\t\\tinds[sample(indices, N-K)].fitnessScaling = 0.0; //...kill a random subset to reduce N to K\\n\\t\\t\\toff = inds[inds.fitnessScaling > 0]; //get surviving offspring\\n\\t\\t}\\t\\n\\n\\t\\t// migration\\n\\t\\tif (m>0 & N>0) { //if adapting from migration and some offspring made\\n\\t\\t\\tif (runif(1)<m) { //with probability m\\n\\t\\t\\t\\ttarget = sample(off.genomes, 1); //choose a chromosome to add a migrant allele to\\n\\t \\t\\t\\ttarget.addNewDrawnMutation(m1, L0); //add the migrant allele\\t\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\t\\n\\t\\t// census offspring\\n\\t\\tN = length(off); //population size\\n\\t\\tfreq = sum(asInteger(off.genomes.countOfMutationsOfType(m1)>0))/(2*N); //frequency of beneficial mutation\\n\\t\\tif ((u==0 & m==0 & freq == 0) | (N==0)) { //if fail to adapt\\n\\t        writeFile(\"data/prescue.csv\", \"0\", append=T); //record extinction\\n\\t        writeFile(outfile_dynamics, \"t n p\"); //erase and restart the output file\\n\\t\\t\\tcatn(\"all hope was lost in generation \" + sim.generation + \" - RESTARTING\"); //alert the user\\n\\t\\t\\tsim.readFromPopulationFile(\"/tmp/slim_\" + simID + \".txt\"); //reinitialize simulation\\n\\t\\t}\\n\\t\\telse {           \\n\\t\\t\\tcatn(sim.generation + \": \" + N + \", \" + freq); //print generation and population size and frequency\\n\\t\\t\\twriteFile(outfile_dynamics, sim.generation + \" \" + N + \" \" + freq, append=T);\\n\\t        if (freq == 1.0 & N == K) { //if mutation fixed and population recovered\\n\\t        \\twriteFile(\"data/prescue.csv\", \"1\", append=T); //record rescue\\n\\t\\t\\t\\tcatn(\"rescue complete in generation \" + sim.generation);\\n\\t            sim.simulationFinished(); //end simulation\\n\\t            sim.treeSeqOutput(outfile_tree); //save tree sequence\\n\\t        }\\n\\t\\t}\\n\\n\\t\\t//fitness scaling (viability selection occurs after early events)\\n\\t\\tp1.fitnessScaling = (1.0 - d)/B; //survival probability V = X(1-d)/B, where X is the fitness effect of the selected site (X=1 for wildtype, X=1+s*h for heterozygotes, X=1+s for mutant homozygotes)\\n\\t}\\n\\n\\t//backup: end simulation if runs too long and print warning to increase maxt\\n\\t%d late () {\\n\\t\\tcatn(\"times up, make maxt longer!\");\\n\\t\\tsim.simulationFinished();\\n\\t}\\n\\t\"\"\" %(K,N0,d,s,h,B,L,L0,u,m,rbp,k,datadir,i,datadir,i,maxt,maxt+1)\\n\\n\\t# and run it\\n\\tos.system(\"echo \\'\" + script + \"\\' | slim\") \\n\\n\\t# then load tree-sequences\\n\\tts = pyslim.load(\"%stree_%d.trees\" %(datadir, i)) \\n\\t\\n\\t# calculate the number of unique copies of beneficial allele\\n\\tX[i] = len(np.unique([i.genotypes for i in ts.variants()][0])) \\n\\t\\n\\t# recapitate the tree sequence and overlay neutral mutations\\n\\tts = msprime.mutate(ts.recapitate(rbp, Ne=Ne).simplify(), rate=U, keep=True)\\n\\t\\n\\t# take a random sample\\n\\toffspring_nodes = (np.where(ts.tables.nodes.time==0.)[0]).astype(np.int32) #chromosomes in offspring (to exclude the parental generation from the samples)\\n\\tsamples = np.random.choice(offspring_nodes, nsamples, replace=False) #random sample of chromosomes in offspring\\n\\tts = ts.simplify(samples) #simplify to sample only\\n\\tts.dump(\"%stree_%d_sample.trees\" %(datadir, i)) #save sample tree (in case need to calculate any more statistics)\\n\\t# ts = tskit.load(\"%stree_%d_sample.trees\" %(datadir,i)) #load dumped version if running new stats below\\n\\n\\t# calculate pairwise diversity\\n\\tsite_div = ts.diversity([ts.samples()], windows=windows) #compute average pairwise diversity in windows\\n\\tsite_div = [i[0] for i in site_div] #flatten site_div list\\n\\t\\n\\t# calculate Tajima\\'s D\\n\\ttajimasD = ts.Tajimas_D([ts.samples()], windows=windows) #compute average Tajima\\'s D in windows\\n\\ttajimasD = [i[0] for i in tajimasD] #flatten tajimasD list\\n\\t\\n\\t# save pairwise diversity and Tajima\\'s D to file, with associated recombination rates\\n\\tcsvData = zip(recombination, site_div, tajimasD)\\n\\twith open(\\'%sstats_%d.csv\\' %(datadir,i), \\'w\\') as csvFile:\\n\\t\\twriter = csv.writer(csvFile)\\n\\t\\twriter.writerows(csvData)\\n\\tcsvFile.close()\\n\\n\\t# calculate site frequency spectrum\\n\\tsfs = ts.allele_frequency_spectrum([ts.samples()], windows=windows, polarised=True) #unfolded SFS, averaged within windows\\n\\tnp.savetxt(\\'%ssfs_%d.txt\\' %(datadir,i), sfs)\\n\\n\\t# calculate linkage disequilibrium\\n\\tall_positions = [i.position for i in ts.mutations()] #positions of all mutations\\n\\tfreqs = [sum(i.genotypes)/nsamples for i in ts.variants()] #frequency of derived alleles at these positions\\n\\tseg = [i for i,j in enumerate(freqs) if 0<j and j<1] #indices of segregating mutations\\n\\tpositions = [all_positions[i] for i in seg] #positions of segregating mutations\\n\\tidxs = [np.argmin(np.abs(positions-i)) for i in midpoints] #find mutation indices nearest the window midpoints\\n\\tidx_positions = [positions[i] for i in idxs] #positions of mutations nearest midpoints\\n\\tdistance = np.log(1-R)/np.log(1-2*rbp) #convert the specified recombination rate between mutations to number of sites\\n\\tother_positions = idx_positions + distance #this is where we want the other mutation\\n\\tother_idxs = [np.argmin(np.abs(positions-i)) for i in other_positions] #mutation indices nearest the desired poition\\n\\tlds = [tskit.LdCalculator(ts).r2(seg[idxs[i]], seg[other_idxs[i]]) for i in range(nwindows)] #linkage disequilibrium between the two mutations\\n\\n\\t# save ld to file with associated recombination rates\\n\\tcsvData = zip(recombination, lds)\\n\\twith open(\\'%sld_%d.csv\\' %(datadir,i), \\'w\\') as csvFile:\\n\\t\\twriter = csv.writer(csvFile)\\n\\t\\twriter.writerows(csvData)\\n\\tcsvFile.close()\\n\\n# save number of unique copies of beneficial allele remaining in each replicate\\nnp.savetxt(\\'%snalleles.txt\\' %datadir, X, fmt=\\'%d\\')\\n',\n",
       " \"from collections import namedtuple\\n\\nfrom prettyparse import create_parser\\n\\nfrom precise.network_runner import Listener\\nfrom precise.params import inject_params\\nfrom precise.train_data import TrainData\\nfrom select import select\\nfrom sys import stdin\\nimport pygame\\nimport time\\nimport os\\nimport shutil\\nfrom collections import Counter\\n\\nusage = '''\\n    Retag false negatives as wakeword or not wakeword\\n    \\n    :model str\\n        Either Keras (.net) or TensorFlow (.pb) model to test\\n    \\n    :-t --use-train\\n        Evaluate training data instead of test data\\n    \\n    :-nf --no-filenames\\n        Don't print out the names of files that failed\\n    \\n    :-ap --append\\n        Append new tags file to old one\\n    ...\\n'''\\n\\nStats = namedtuple('Stats', 'false_pos false_neg true_pos true_neg')\\n\\ndef calc_stats(filenames, targets, predictions) -> Stats:\\n    stats = Stats([], [], [], [])\\n    for name, target, prediction in zip(filenames, targets, predictions):\\n        {\\n            (True, False): stats.false_pos,\\n            (True, True): stats.true_pos,\\n            (False, True): stats.false_neg,\\n            (False, False): stats.true_neg\\n        }[prediction[0] > 0.5, target[0] > 0.5].append(name)\\n    return stats\\n\\ndef main():\\n    args = TrainData.parse_args(create_parser(usage))\\n\\n    inject_params(args.model)\\n\\n    data = TrainData.from_both(args.tags_file, args.tags_folder, args.folder)\\n    train, test = data.load(args.use_train, not args.use_train)\\n    inputs, targets = train if args.use_train else test\\n\\n    filenames = sum(data.train_files if args.use_train else data.test_files, [])\\n    predictions = Listener.find_runner(args.model)(args.model).predict(inputs)\\n    stats = calc_stats(filenames, targets, predictions)\\n    false_negatives_array = stats.false_neg\\n    new_tags = open('tags.txt', 'w')\\n    \\n\\n    changed_tags_array = []\\n    for i in range(0, len(false_negatives_array)):\\n        pygame.mixer.init(frequency=8000, size=-16, channels=2, buffer=4096)\\n        pygame.mixer.music.load(false_negatives_array[i])\\n        pygame.mixer.music.play()\\n        print('False negative ', (i + 1), ' of ', (len(false_negatives_array)) + 1)\\n        user_input = input('Enter y if wakeword, enter n for not wakeword \\\\n')\\n        time.sleep(5)\\n        false_negatives_array[i] = false_negatives_array[i].lstrip('/Users/madmitrienko/wakewords/files/')\\n        false_negatives_array[i] = false_negatives_array[i].rstrip('.wav')\\n        if(user_input == 'y'):\\n            write_to_tags = '\\\\n' + false_negatives_array[i] + '\\twake-word'\\n            new_tags.write(write_to_tags)\\n\\n        elif(user_input == 'n'):\\n            write_to_tags = '\\\\n' + false_negatives_array[i] + '\\tnot-wake-word'          \\n            new_tags.write(write_to_tags)\\n    new_tags.close()\\n    tags.close()\\n    \\nif __name__ == '__main__':\\n    main()\\n\",\n",
       " '\"\"\"\\nPython Challenge Level 15.\\n\\nThe page title is \\'whom?\\' and the image is a calendar of January 1??6 with Monday\\nJan 26th circled. There are two comments in the page source, \\'he ain\\'t the\\nyoungest, he is the second\\' and \\'todo: buy flowers for tomorrow\\'.\\n\"\"\"\\nimport calendar\\nimport datetime\\n\\nGREGORIAN_CALENDAR_START = 1582\\n\\n\\ndef ends_in_6(year):\\n    \"\"\"\\n    Does the year end in \\'6\\'?\\n    \"\"\"\\n    return year % 10 == 6\\n\\n\\ndef jan_26th_is_monday(year):\\n    \"\"\"\\n    In this year, is January 26th a monday?\\n    \"\"\"\\n    return calendar.weekday(year, 1, 26) == 0\\n\\nmatching_years = []\\nfor year in range(GREGORIAN_CALENDAR_START, 2000):\\n    # Determine the years which could match the calendar conditions:\\n    if jan_26th_is_monday(year) and calendar.isleap(year) and ends_in_6(year):\\n        matching_years.append(year)\\n\\n# \\'he ain\\'t the youngest, he is the second\\' - take the second youngest year\\nyear = matching_years[-2]\\n\\n# \\'todo: buy flowers for tomorrow\\nprint(datetime.date(year, 1, 26 + 1))\\n# \\'1756-01-27\\', which is Mozart\\'s birthday\\n# http://www.pythonchallenge.com/pc/return/mozart.html is the next URL.\\n',\n",
       " '\"Rules for running Rollup under Bazel\"\\n\\nload(\"@build_bazel_rules_nodejs//:providers.bzl\", \"JSEcmaScriptModuleInfo\", \"NodeContextInfo\", \"NpmPackageInfo\", \"node_modules_aspect\", \"run_node\")\\nload(\"@build_bazel_rules_nodejs//internal/linker:link_node_modules.bzl\", \"module_mappings_aspect\")\\n\\n_DOC = \"\"\"Runs the Rollup.js CLI under Bazel.\\n\\nSee https://rollupjs.org/guide/en/#command-line-reference\\n\\nTypical example:\\n```python\\nload(\"@npm_bazel_rollup//:index.bzl\", \"rollup_bundle\")\\n\\nrollup_bundle(\\n    name = \"bundle\",\\n    srcs = [\"dependency.js\"],\\n    entry_point = \"input.js\",\\n    config_file = \"rollup.config.js\",\\n)\\n```\\n\\nNote that the command-line options set by Bazel override what appears in the rollup config file.\\nThis means that typically a single `rollup.config.js` can contain settings for your whole repo,\\nand multiple `rollup_bundle` rules can share the configuration.\\n\\nThus, setting options that Bazel controls will have no effect, e.g.\\n\\n```javascript\\nmodule.exports = {\\n    output: { file: \\'this_is_ignored.js\\' },\\n}\\n```\\n\\nYou must determine ahead of time whether Rollup needs to produce a directory output.\\nThis is the case if you have dynamic imports which cause code-splitting, or if you\\nprovide multiple entry points. Use the `output_dir` attribute to specify that you want a\\ndirectory output.\\nRollup\\'s CLI has the same behavior, forcing you to pick `--output.file` or `--output.dir`.\\n\\nTo get multiple output formats, wrap the rule with a macro or list comprehension, e.g.\\n\\n```python\\n[\\n    rollup_bundle(\\n        name = \"bundle.%s\" % format,\\n        entry_point = \"foo.js\",\\n        format = format,\\n    )\\n    for format in [\\n        \"cjs\",\\n        \"umd\",\\n    ]\\n]\\n```\\n\\nThis will produce one output per requested format.\\n\"\"\"\\n\\n_ROLLUP_ATTRS = {\\n    \"srcs\": attr.label_list(\\n        doc = \"\"\"Non-entry point JavaScript source files from the workspace.\\n\\nYou must not repeat file(s) passed to entry_point/entry_points.\\n\"\"\",\\n        # Don\\'t try to constrain the filenames, could be json, svg, whatever\\n        allow_files = True,\\n    ),\\n    \"args\": attr.string_list(\\n        doc = \"\"\"Command line arguments to pass to rollup. Can be used to override config file settings.\\n\\nThese argument passed on the command line before all arguments that are always added by the\\nrule such as `--output.dir` or `--output.file`, `--format`, `--config` and `--preserveSymlinks` and\\nalso those that are optionally added by the rule such as `--sourcemap`.\\n\\nSee rollup CLI docs https://rollupjs.org/guide/en/#command-line-flags for complete list of supported arguments.\"\"\",\\n        default = [],\\n    ),\\n    \"config_file\": attr.label(\\n        doc = \"\"\"A rollup.config.js file\\n\\nPassed to the --config \\nSee https://rollupjs.org/guide/en/#configuration-files\\n\\nIf not set, a default basic Rollup config is used.\\n\"\"\",\\n        allow_single_file = True,\\n        default = \"@npm_bazel_rollup//:rollup.config.js\",\\n    ),\\n    \"entry_point\": attr.label(\\n        doc = \"\"\"The bundle\\'s entry point (e.g. your main.js or app.js or index.js).\\n\\nThis is just a shortcut for the `entry_points` attribute with a single output chunk named the same as the rule.\\n\\nFor example, these are equivalent:\\n\\n```python\\nrollup_bundle(\\n    name = \"bundle\",\\n    entry_point = \"index.js\",\\n)\\n```\\n\\n```python\\nrollup_bundle(\\n    name = \"bundle\",\\n    entry_points = {\\n        \"index.js\": \"bundle\"\\n    }\\n)\\n```\\n\\nIf `rollup_bundle` is used on a `ts_library`, the `rollup_bundle` rule handles selecting the correct outputs from `ts_library`.\\nIn this case, `entry_point` can be specified as the `.ts` file and `rollup_bundle` will handle the mapping to the `.mjs` output file.\\n\\nFor example:\\n\\n```python\\nts_library(\\n    name = \"foo\",\\n    srcs = [\\n        \"foo.ts\",\\n        \"index.ts\",\\n    ],\\n)\\n\\nrollup_bundle(\\n    name = \"bundle\",\\n    deps = [ \"foo\" ],\\n    entry_point = \"index.ts\",\\n)\\n```\\n\"\"\",\\n        allow_single_file = True,\\n    ),\\n    \"entry_points\": attr.label_keyed_string_dict(\\n        doc = \"\"\"The bundle\\'s entry points (e.g. your main.js or app.js or index.js).\\n\\nPassed to the [`--input` option](https://github.com/rollup/rollup/blob/master/docs/999-big-list-of-options.md#input) in Rollup.\\n\\nKeys in this dictionary are labels pointing to .js entry point files.\\nValues are the name to be given to the corresponding output chunk.\\n\\nEither this attribute or `entry_point` must be specified, but not both.\\n\"\"\",\\n        allow_files = True,\\n    ),\\n    \"format\": attr.string(\\n        doc = \"\"\"\"Specifies the format of the generated bundle. One of the following:\\n\\n- `amd`: Asynchronous Module Definition, used with module loaders like RequireJS\\n- `cjs`: CommonJS, suitable for Node and other bundlers\\n- `esm`: Keep the bundle as an ES module file, suitable for other bundlers and inclusion as a `<script type=module>` tag in modern browsers\\n- `iife`: A self-executing function, suitable for inclusion as a `<script>` tag. (If you want to create a bundle for your application, you probably want to use this.)\\n- `umd`: Universal Module Definition, works as amd, cjs and iife all in one\\n- `system`: Native format of the SystemJS loader\\n\"\"\",\\n        values = [\"amd\", \"cjs\", \"esm\", \"iife\", \"umd\", \"system\"],\\n        default = \"esm\",\\n    ),\\n    \"node_context_data\": attr.label(\\n        default = \"@build_bazel_rules_nodejs//internal:node_context_data\",\\n        providers = [NodeContextInfo],\\n        doc = \"Internal use only\",\\n    ),\\n    \"output_dir\": attr.bool(\\n        doc = \"\"\"Whether to produce a directory output.\\n\\nWe will use the [`--output.dir` option](https://github.com/rollup/rollup/blob/master/docs/999-big-list-of-options.md#outputdir) in rollup\\nrather than `--output.file`.\\n\\nIf the program produces multiple chunks, you must specify this attribute.\\nOtherwise, the outputs are assumed to be a single file.\\n\"\"\",\\n    ),\\n    \"rollup_bin\": attr.label(\\n        doc = \"Target that executes the rollup binary\",\\n        executable = True,\\n        cfg = \"host\",\\n        default = \"@npm//rollup/bin:rollup\",\\n    ),\\n    \"rollup_worker_bin\": attr.label(\\n        doc = \"Internal use only\",\\n        executable = True,\\n        cfg = \"host\",\\n        # NB: will be substituted with \"@npm//@bazel/rollup/bin:rollup-worker\" when the pkg_npm target is built\\n        default = \"@npm//@bazel/rollup/bin:rollup-worker\",\\n    ),\\n    \"silent\": attr.bool(\\n        doc = \"\"\"Whether to execute the rollup binary with the --silent flag, defaults to False.\\n\\nUsing --silent can cause rollup to [ignore errors/warnings](https://github.com/rollup/rollup/blob/master/docs/999-big-list-of-options.md#onwarn) \\nwhich are only surfaced via logging.  Since bazel expects printing nothing on success, setting silent to True\\nis a more Bazel-idiomatic experience, however could cause rollup to drop important warnings.\\n\"\"\",\\n    ),\\n    \"sourcemap\": attr.string(\\n        doc = \"\"\"Whether to produce sourcemaps.\\n\\nPassed to the [`--sourcemap` option](https://github.com/rollup/rollup/blob/master/docs/999-big-list-of-options.md#outputsourcemap\") in Rollup\\n\"\"\",\\n        default = \"inline\",\\n        values = [\"inline\", \"hidden\", \"true\", \"false\"],\\n    ),\\n    \"supports_workers\": attr.bool(\\n        doc = \"\"\"Experimental! Use only with caution.\\n\\nAllows you to enable the Bazel Worker strategy for this library.\\nWhen enabled, this rule invokes the \"rollup_worker_bin\"\\nworker aware binary rather than \"rollup_bin\".\"\"\",\\n        default = False,\\n    ),\\n    \"deps\": attr.label_list(\\n        aspects = [module_mappings_aspect, node_modules_aspect],\\n        doc = \"\"\"Other libraries that are required by the code, or by the rollup.config.js\"\"\",\\n    ),\\n}\\n\\ndef _desugar_entry_point_names(name, entry_point, entry_points):\\n    \"\"\"Users can specify entry_point (sugar) or entry_points (long form).\\n\\n    This function allows our code to treat it like they always used the long form.\\n\\n    It also performs validation:\\n    - exactly one of these attributes should be specified\\n    \"\"\"\\n    if entry_point and entry_points:\\n        fail(\"Cannot specify both entry_point and entry_points\")\\n    if not entry_point and not entry_points:\\n        fail(\"One of entry_point or entry_points must be specified\")\\n    if entry_point:\\n        return [name]\\n    return entry_points.values()\\n\\ndef _desugar_entry_points(name, entry_point, entry_points, inputs):\\n    \"\"\"Like above, but used by the implementation function, where the types differ.\\n\\n    It also performs validation:\\n    - attr.label_keyed_string_dict doesn\\'t accept allow_single_file\\n      so we have to do validation now to be sure each key is a label resulting in one file\\n\\n    It converts from dict[target: string] to dict[file: string]\\n    \"\"\"\\n    names = _desugar_entry_point_names(name, entry_point.label if entry_point else None, entry_points)\\n\\n    if entry_point:\\n        return {_resolve_js_input(entry_point.files.to_list()[0], inputs): names[0]}\\n\\n    result = {}\\n    for ep in entry_points.items():\\n        entry_point = ep[0]\\n        name = ep[1]\\n        f = entry_point.files.to_list()\\n        if len(f) != 1:\\n            fail(\"keys in rollup_bundle#entry_points must provide one file, but %s has %s\" % (entry_point.label, len(f)))\\n        result[_resolve_js_input(f[0], inputs)] = name\\n    return result\\n\\ndef _resolve_js_input(f, inputs):\\n    if f.extension == \"js\" or f.extension == \"mjs\":\\n        return f\\n\\n    # look for corresponding js file in inputs\\n    no_ext = _no_ext(f)\\n    for i in inputs:\\n        if i.extension == \"js\" or i.extension == \"mjs\":\\n            if _no_ext(i) == no_ext:\\n                return i\\n    fail(\"Could not find corresponding javascript entry point for %s. Add the %s.js to your deps.\" % (f.path, no_ext))\\n\\ndef _rollup_outs(sourcemap, name, entry_point, entry_points, output_dir):\\n    \"\"\"Supply some labelled outputs in the common case of a single entry point\"\"\"\\n    result = {}\\n    entry_point_outs = _desugar_entry_point_names(name, entry_point, entry_points)\\n    if output_dir:\\n        # We can\\'t declare a directory output here, because RBE will be confused, like\\n        # com.google.devtools.build.lib.remote.ExecutionStatusException:\\n        # INTERNAL: failed to upload outputs: failed to construct CAS files:\\n        # failed to calculate file hash:\\n        # read /b/f/w/bazel-out/k8-fastbuild/bin/packages/rollup/test/multiple_entry_points/chunks: is a directory\\n        #result[\"chunks\"] = output_dir\\n        return {}\\n    else:\\n        if len(entry_point_outs) > 1:\\n            fail(\"Multiple entry points require that output_dir be set\")\\n        out = entry_point_outs[0]\\n        result[out] = out + \".js\"\\n        if sourcemap == \"true\":\\n            result[out + \"_map\"] = \"%s.map\" % result[out]\\n    return result\\n\\ndef _no_ext(f):\\n    return f.short_path[:-len(f.extension) - 1]\\n\\ndef _filter_js(files):\\n    return [f for f in files if f.extension == \"js\" or f.extension == \"mjs\"]\\n\\ndef _rollup_bundle(ctx):\\n    \"Generate a rollup config file and run rollup\"\\n\\n    # rollup_bundle supports deps with JS providers. For each dep,\\n    # JSEcmaScriptModuleInfo is used if found, then JSModuleInfo and finally\\n    # the DefaultInfo files are used if the former providers are not found.\\n    deps_depsets = []\\n    for dep in ctx.attr.deps:\\n        if JSEcmaScriptModuleInfo in dep:\\n            deps_depsets.append(dep[JSEcmaScriptModuleInfo].sources)\\n        elif hasattr(dep, \"files\"):\\n            deps_depsets.append(dep.files)\\n\\n        # Also include files from npm deps as inputs.\\n        # These deps are identified by the NpmPackageInfo provider.\\n        if NpmPackageInfo in dep:\\n            deps_depsets.append(dep[NpmPackageInfo].sources)\\n    deps_inputs = depset(transitive = deps_depsets).to_list()\\n\\n    inputs = _filter_js(ctx.files.entry_point) + _filter_js(ctx.files.entry_points) + ctx.files.srcs + deps_inputs\\n    outputs = [getattr(ctx.outputs, o) for o in dir(ctx.outputs)]\\n\\n    # See CLI documentation at https://rollupjs.org/guide/en/#command-line-reference\\n    args = ctx.actions.args()\\n\\n    if ctx.attr.supports_workers:\\n        # Set to use a multiline param-file for worker mode\\n        args.use_param_file(\"@%s\", use_always = True)\\n        args.set_param_file_format(\"multiline\")\\n\\n    # Add user specified arguments *before* rule supplied arguments\\n    args.add_all(ctx.attr.args)\\n\\n    # List entry point argument first to save some argv space\\n    # Rollup doc says\\n    # When provided as the first options, it is equivalent to not prefix them with --input\\n    entry_points = _desugar_entry_points(ctx.label.name, ctx.attr.entry_point, ctx.attr.entry_points, inputs).items()\\n\\n    # If user requests an output_dir, then use output.dir rather than output.file\\n    if ctx.attr.output_dir:\\n        outputs.append(ctx.actions.declare_directory(ctx.label.name))\\n        for entry_point in entry_points:\\n            args.add_joined([entry_point[1], entry_point[0]], join_with = \"=\")\\n        args.add_all([\"--output.dir\", outputs[0].path])\\n    else:\\n        args.add(entry_points[0][0])\\n        args.add_all([\"--output.file\", outputs[0].path])\\n\\n    args.add_all([\"--format\", ctx.attr.format])\\n\\n    if ctx.attr.silent:\\n        # Run the rollup binary with the --silent flag\\n        args.add(\"--silent\")\\n\\n    stamp = ctx.attr.node_context_data[NodeContextInfo].stamp\\n\\n    config = ctx.actions.declare_file(\"_%s.rollup_config.js\" % ctx.label.name)\\n    ctx.actions.expand_template(\\n        template = ctx.file.config_file,\\n        output = config,\\n        substitutions = {\\n            \"bazel_stamp_file\": \"\\\\\"%s\\\\\"\" % ctx.version_file.path if stamp else \"undefined\",\\n        },\\n    )\\n\\n    args.add_all([\"--config\", config.path])\\n    inputs.append(config)\\n\\n    if stamp:\\n        inputs.append(ctx.version_file)\\n\\n    # Prevent rollup\\'s module resolver from hopping outside Bazel\\'s sandbox\\n    # When set to false, symbolic links are followed when resolving a file.\\n    # When set to true, instead of being followed, symbolic links are treated as if the file is\\n    # where the link is.\\n    args.add(\"--preserveSymlinks\")\\n\\n    if (ctx.attr.sourcemap and ctx.attr.sourcemap != \"false\"):\\n        args.add_all([\"--sourcemap\", ctx.attr.sourcemap])\\n\\n    executable = \"rollup_bin\"\\n    execution_requirements = {}\\n\\n    if ctx.attr.supports_workers:\\n        executable = \"rollup_worker_bin\"\\n        execution_requirements[\"supports-workers\"] = str(int(ctx.attr.supports_workers))\\n\\n    run_node(\\n        ctx,\\n        progress_message = \"Bundling JavaScript %s [rollup]\" % outputs[0].short_path,\\n        executable = executable,\\n        inputs = inputs,\\n        outputs = outputs,\\n        arguments = [args],\\n        mnemonic = \"Rollup\",\\n        execution_requirements = execution_requirements,\\n        env = {\"COMPILATION_MODE\": ctx.var[\"COMPILATION_MODE\"]},\\n    )\\n\\n    return [\\n        DefaultInfo(files = depset(outputs)),\\n    ]\\n\\nrollup_bundle = rule(\\n    doc = _DOC,\\n    implementation = _rollup_bundle,\\n    attrs = _ROLLUP_ATTRS,\\n    outputs = _rollup_outs,\\n)\\n',\n",
       " 'import os\\nimport sys\\nimport pprint\\nfrom math import sin, cos, sqrt, atan2, radians\\nfrom deprecated import deprecated\\n\\n\\n# classe di supporto per controllare la quantità\\n# di output stampato a video\\nclass Verbosity:\\n    def __init__(self, quiet, verbose, more_verbose):\\n        self.quiet = quiet\\n        self.verbose = verbose or more_verbose  # se ho output more_verbose voglio che si stampi anche il verbose\\n        self.more_verbose = more_verbose\\n\\n\\n# Inutilizzato.\\n# Creato in origine per calcolare la distanza in 2D, ora si usa\\n# un metodo più preciso che tiene conto della curvatura terrestre.\\n@deprecated(reason=\"Utilizzare il metodo distance(), che tiene conto della curvatura terrestre\")\\ndef distance_in_2d(sens_one, sens_two):\\n    x_0 = sens_one.longitudine\\n    y_0 = sens_one.latitudine\\n    x_1 = sens_two.longitudine\\n    y_1 = sens_two.latitudine\\n    return sqrt((y_0 - y_1) ** 2 + (x_0 - x_1) ** 2)\\n\\n\\ndef find_sensor_by_id(sensor):\\n    for sen in get_global_sensors():\\n        if sen.id == sensor:\\n            return sen\\n    return None\\n\\n\\n# Prende in input due tuple di coordinate e restituisce la loro distanza sulla superficie terrestre\\ndef distance_by_coord(node_one, node_two):\\n    # Approssimazione del raggio della Terra in Km\\n    raggio_terra = 6373.0\\n\\n    lat1 = radians(node_one[0])\\n    lon1 = radians(node_one[1])\\n    lat2 = radians(node_two[0])\\n    lon2 = radians(node_two[1])\\n\\n    diff_lon = lon2 - lon1\\n    diff_lat = lat2 - lat1\\n\\n    a = sin(diff_lat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(diff_lon / 2) ** 2\\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\\n\\n    distanza = raggio_terra * c * 1000\\n    return distanza\\n\\n\\n# Prende in input due sensori e restituisce\\n# la loro distanza sulla superficie terrestre\\ndef distance(sens_one, sens_two):\\n    return distance_by_coord((sens_one.latitudine, sens_one.longitudine),\\n                             (sens_two.latitudine, sens_two.longitudine))\\n\\n\\ndef print_scenario(a_dict, order_by):\\n    print(\"\\\\n\\\\n\\\\n\\\\n\\\\n---------------------------------------------------\\\\n\\\\n\\\\n\\\\n\\\\n\")\\n    print(\"SCENARIO - ORDINATO PER: \" + order_by)\\n    for temp_sens in a_dict.keys():\\n        print(\"\\\\nSensore \" + str(temp_sens.id) + \":\")\\n        temp_val = a_dict[temp_sens]\\n        temp_sens_list = temp_val[\"senders\"]\\n        temp_tot_cap = temp_val[\"tot_capacita\"]\\n        temp_rapp_cap_costo = temp_val[\"rapp_cap_costo\"]\\n        temp_rapp_numsensori_costo = temp_val[\"rapp_numsensori_costo\"]\\n        print(\"Senders: \", end=\\'\\')\\n        for temp_sender in temp_sens_list:\\n            print(str(temp_sender.id) + \" \", end=\\'\\')\\n        print(\"\\\\nTot_capacità: \" + str(temp_tot_cap))\\n        print(\"Rapporto capacità/costo: \" + str(temp_rapp_cap_costo))\\n        print(\"Rapporto numsensori/costo: \" + str(temp_rapp_numsensori_costo))\\n    print(\"\\\\n\\\\n\")\\n\\n\\ndef print_greedy_result(result):\\n    if get_verbosity().verbose:\\n        print(\"\\\\n\\\\n\\\\n\")\\n        print(\"Dispositivi installati dalla greedy:\\\\n\")\\n        pp = pprint.PrettyPrinter(indent=3)\\n        pp.pprint(result)\\n    elif not get_verbosity().quiet:  # Se ho verbosity \"normale\" stampo solo i primi 3\\n        print(\"\\\\n\\\\n\\\\n\")\\n        print(\"Dispositivi installati dalla greedy (parziale):\\\\n\")\\n        pp = pprint.PrettyPrinter(indent=3)\\n        pp.pprint(dict(list(result.items())[:3]))\\n        print(\"\\\\t\\\\t.\\\\n\\\\t\\\\t.\\\\n\\\\t\\\\t.\\\\n\")\\n\\n\\ndef print_mst_result(mst):\\n    if get_verbosity().verbose:\\n        print(\"\\\\n\\\\n\\\\nArchi selezionati per il MST:\\\\n\")\\n        for edge in mst:\\n            print(f\"{edge[\\'node_one\\']} - {edge[\\'node_two\\']} - Costo {edge[\\'costo\\']}\")\\n    elif not get_verbosity().quiet:  # Se ho verbosity \"normale\" stampo solo i primi 3\\n        print(\"\\\\n\\\\n\\\\nArchi selezionati per il MST (parziale):\\\\n\")\\n        for edge in mst[:3]:\\n            print(f\"{edge[\\'node_one\\']} - {edge[\\'node_two\\']} - Costo {edge[\\'costo\\']}\")\\n        print(\"\\\\t.\\\\n\\\\t.\\\\n\\\\t.\\\\n\")\\n\\n\\ndef prepara_cartelle_e_file(num_sensori, order_by, pack_by, num_iter, no_display):\\n    if not os.path.isdir(\"./solutions\"):\\n        os.mkdir(\"./solutions\")\\n\\n    intestazione_csv = \"seed,numsensori,order_by,pack_by,num_iter_ls,\" + \\\\\\n                       \"greedy_cost,mst_cost,first_tot,first_ls_tot,second_ls_tot,\" + \\\\\\n                       \"num_gw_class_1,fattore_riduzione\"\\n\\n    # Se viene passata l\\'opzione --no-display si aggiunge solamente il risultato\\n    # dell\\'esecuzione al file .csv (per analisi e creazione di grafici)\\n    if no_display:\\n        text_output_path_grafici = f\"./solutions/graph_data.csv\"\\n\\n        if not os.path.isfile(text_output_path_grafici):\\n            with open(text_output_path_grafici, \\'w\\') as f:\\n                original_stdout = sys.stdout\\n                sys.stdout = f\\n                print(intestazione_csv)\\n                sys.stdout = original_stdout\\n\\n        return None, None, None, text_output_path_grafici\\n\\n    saving_path = f\"./solutions/{num_sensori}/{get_seed()}/{order_by}+{pack_by}+{num_iter}/\"\\n    saving_path_ls = saving_path + \"localsearch/\"\\n    text_output_path = saving_path + \"output.txt\"\\n    text_output_path_grafici = f\"./solutions/graph_data.csv\"\\n\\n    if not os.path.isdir(f\"./solutions/{num_sensori}\"):\\n        os.mkdir(f\"./solutions/{num_sensori}\")\\n\\n    if not os.path.isdir(f\"./solutions/{num_sensori}/{get_seed()}\"):\\n        os.mkdir(f\"./solutions/{num_sensori}/{get_seed()}\")\\n\\n    if not os.path.isdir(saving_path):\\n        os.mkdir(saving_path)\\n\\n    if not os.path.isdir(saving_path_ls):\\n        os.mkdir(saving_path_ls)\\n\\n    if os.path.isfile(text_output_path):\\n        os.remove(text_output_path)\\n\\n    if not os.path.isfile(text_output_path_grafici):\\n        with open(text_output_path_grafici, \\'w\\') as f:\\n            original_stdout = sys.stdout\\n            sys.stdout = f\\n            print(intestazione_csv)\\n            sys.stdout = original_stdout\\n\\n    return saving_path, saving_path_ls, text_output_path, text_output_path_grafici\\n\\n\\nverbosity = Verbosity(False, False, False)\\n\\n\\ndef get_verbosity():\\n    return verbosity\\n\\n\\ndef set_verbosity(quiet=False, verbose=False, more_verbose=False):\\n    global verbosity\\n    verbosity = Verbosity(quiet, verbose, more_verbose)\\n\\n\\nrandom_seed = 12345  # Per la riproducibilità degli esempi\\n# Il seed originale è 1625\\n\\n\\ndef get_seed():\\n    return random_seed\\n\\n\\ndef set_seed(new_seed):\\n    global random_seed\\n    random_seed = new_seed\\n\\n\\ngateway_classes = []\\n\\n\\ndef get_gateways_classes():\\n    return gateway_classes\\n\\n\\ndef set_gateways_classes(new_gateway_classes):\\n    global gateway_classes\\n    gateway_classes = new_gateway_classes\\n\\n\\nsensors = []\\n\\n\\ndef get_global_sensors():\\n    return sensors\\n\\n\\ndef set_global_sensors(new_sensors):\\n    global sensors\\n    sensors = new_sensors\\n',\n",
       " '\"\"\"\\nA simple IO staging mechanism\\n\\n\\n\\n\"\"\"\\n\\n#-----------------------------------------------------------------------------\\n# Copyright (c) 2013, yt Development Team.\\n#\\n# Distributed under the terms of the Modified BSD License.\\n#\\n# The full license is in the file COPYING.txt, distributed with this software.\\n#-----------------------------------------------------------------------------\\n\\nimport np\\nfrom yt.utilities.logger import ytLogger as mylog\\nfrom .parallel_analysis_interface import \\\\\\n    ProcessorPool, parallel_objects\\nfrom yt.utilities.io_handler import BaseIOHandler\\nfrom contextlib import contextmanager\\nimport time\\n\\ntry:\\n    from .parallel_analysis_interface import MPI\\nexcept ImportError:\\n    pass\\n\\nYT_TAG_MESSAGE = 317 # Cell 317 knows where to go\\n\\nclass IOCommunicator(BaseIOHandler):\\n    def __init__(self, ds, wg, pool):\\n        mylog.info(\"Initializing IOCommunicator\")\\n        self.ds = ds\\n        self.wg = wg # We don\\'t need to use this!\\n        self.pool = pool\\n        self.comm = pool.comm\\n        # We read our grids here\\n        self.grids = []\\n        storage = {}\\n        grids = ds.index.grids.tolist()\\n        grids.sort(key=lambda a:a.filename)\\n        for sto, g in parallel_objects(grids, storage = storage):\\n            sto.result = self.comm.rank\\n            sto.result_id = g.id\\n            self.grids.append(g)\\n        self._id_offset = ds.index.grids[0]._id_offset\\n        mylog.info(\"Reading from disk ...\")\\n        self.initialize_data()\\n        mylog.info(\"Broadcasting ...\")\\n        self.comm.comm.bcast(storage, root = wg.ranks[0])\\n        mylog.info(\"Done.\")\\n        self.hooks = []\\n\\n    def initialize_data(self):\\n        ds = self.ds\\n        fields = [f for f in ds.field_list\\n                  if not ds.field_info[f].particle_type]\\n        pfields = [f for f in ds.field_list\\n                   if ds.field_info[f].particle_type]\\n        # Preload is only defined for Enzo ...\\n        if ds.index.io._dataset_type == \"enzo_packed_3d\":\\n            self.queue = ds.index.io.queue\\n            ds.index.io.preload(self.grids, fields)\\n            for g in self.grids:\\n                for f in fields:\\n                    if f not in self.queue[g.id]:\\n                        d = np.zeros(g.ActiveDimensions, dtype=\\'float64\\')\\n                        self.queue[g.id][f] = d\\n                for f in pfields:\\n                    self.queue[g.id][f] = self._read(g, f)\\n        else:\\n            self.queue = {}\\n            for g in self.grids:\\n                for f in fields + pfields:\\n                    self.queue[g.id][f] = ds.index.io._read(g, f)\\n\\n    def _read(self, g, f):\\n        fi = self.ds.field_info[f]\\n        if fi.particle_type and g.NumberOfParticles == 0:\\n            # because this gets upcast to float\\n            return np.array([],dtype=\\'float64\\')\\n        try:\\n            temp = self.ds.index.io._read_data_set(g, f)\\n        except:# self.ds.index.io._read_exception as exc:\\n            if fi.not_in_all:\\n                temp = np.zeros(g.ActiveDimensions, dtype=\\'float64\\')\\n            else:\\n                raise\\n        return temp\\n\\n    def wait(self):\\n        status = MPI.Status()\\n        while 1:\\n            if self.comm.comm.Iprobe(MPI.ANY_SOURCE,\\n                                YT_TAG_MESSAGE,\\n                                status = status):\\n                msg = self.comm.comm.recv(\\n                        source = status.source, tag = YT_TAG_MESSAGE)\\n                if msg[\\'op\\'] == \"end\":\\n                    mylog.debug(\"Shutting down IO.\")\\n                    break\\n                self._send_data(msg, status.source)\\n                status = MPI.Status()\\n            else:\\n                time.sleep(1e-2)\\n\\n    def _send_data(self, msg, dest):\\n        grid_id = msg[\\'grid_id\\']\\n        field = msg[\\'field\\']\\n        ts = self.queue[grid_id][field].astype(\"float64\")\\n        mylog.debug(\"Opening send to %s (%s)\", dest, ts.shape)\\n        self.hooks.append(self.comm.comm.Isend([ts, MPI.DOUBLE], dest = dest))\\n\\nclass IOHandlerRemote(BaseIOHandler):\\n    _dataset_type = \"remote\"\\n\\n    def __init__(self, ds, wg, pool):\\n        self.ds = ds\\n        self.wg = wg # probably won\\'t need\\n        self.pool = pool\\n        self.comm = pool.comm\\n        self.proc_map = self.comm.comm.bcast(None,\\n                root = pool[\\'io\\'].ranks[0])\\n        super(IOHandlerRemote, self).__init__()\\n\\n    def _read_data_set(self, grid, field):\\n        dest = self.proc_map[grid.id]\\n        msg = dict(grid_id = grid.id, field = field, op=\"read\")\\n        mylog.debug(\"Requesting %s for %s from %s\", field, grid, dest)\\n        if self.ds.field_info[field].particle_type:\\n            data = np.empty(grid.NumberOfParticles, \\'float64\\')\\n        else:\\n            data = np.empty(grid.ActiveDimensions, \\'float64\\')\\n        hook = self.comm.comm.Irecv([data, MPI.DOUBLE], source = dest)\\n        self.comm.comm.send(msg, dest = dest, tag = YT_TAG_MESSAGE)\\n        mylog.debug(\"Waiting for data.\")\\n        MPI.Request.Wait(hook)\\n        return data\\n\\n    def _read_data_slice(self, grid, field, axis, coord):\\n        sl = [slice(None), slice(None), slice(None)]\\n        sl[axis] = slice(coord, coord + 1)\\n        #sl = tuple(reversed(sl))\\n        return self._read_data_set(grid,field)[sl]\\n\\n    def terminate(self):\\n        msg = dict(op=\\'end\\')\\n        if self.wg.comm.rank == 0:\\n            for rank in self.pool[\\'io\\'].ranks:\\n                mylog.debug(\"Sending termination message to %s\", rank)\\n                self.comm.comm.send(msg, dest=rank, tag=YT_TAG_MESSAGE)\\n\\n@contextmanager\\ndef remote_io(ds, wg, pool):\\n    original_io = ds.index.io\\n    ds.index.io = IOHandlerRemote(ds, wg, pool)\\n    yield\\n    ds.index.io.terminate()\\n    ds.index.io = original_io\\n\\ndef io_nodes(fn, n_io, n_work, func, *args, **kwargs):\\n    from yt.mods import load\\n    pool, wg = ProcessorPool.from_sizes([(n_io, \"io\"), (n_work, \"work\")])\\n    rv = None\\n    if wg.name == \"work\":\\n        ds = load(fn)\\n        with remote_io(ds, wg, pool):\\n            rv = func(ds, *args, **kwargs)\\n    elif wg.name == \"io\":\\n        ds = load(fn)\\n        io = IOCommunicator(ds, wg, pool)\\n        io.wait()\\n    # We should broadcast the result\\n    rv = pool.comm.mpi_bcast(rv, root=pool[\\'work\\'].ranks[0])\\n    pool.free_all()\\n    mylog.debug(\"Return value: %s\", rv)\\n    return rv\\n\\n# Here is an example of how to use this functionality.\\nif __name__ == \"__main__\":\\n    def gq(ds):\\n        dd = ds.all_data()\\n        return dd.quantities[\"TotalQuantity\"](\"CellMassMsun\")\\n    q = io_nodes(\"DD0087/DD0087\", 8, 24, gq)\\n    mylog.info(q)\\n\\n\\n',\n",
       " '# -*- coding: utf-8 -*-\\n\\nfrom setuptools import setup, find_packages\\n\\n\\nwith open(\\'README.md\\') as f:\\n    readme = f.read()\\n\\nwith open(\\'LICENSE\\') as f:\\n    license = f.read()\\n\\nsetup(\\n    name=\\'xcodeproject\\',\\n    version=\\'0.9\\',\\n    description=\\'Xcode project file inspection utilities\\',\\n    long_description=readme,\\n    author=\\'Marc Liyanage\\',\\n    author_email=\\'reg.python-xcodeproject@entropy.ch\\',\\n    url=\\'https://github.com/liyanage/python-xcodeproject\\',\\n    license=license,\\n    packages=find_packages(exclude=(\\'tests\\', \\'docs\\')),\\n    entry_points = {\\n        \"console_scripts\": [\\n            \"xcodeproject-util=xcodeproject.tool:XcodeprojectTool.main\",\\n        ],\\n    }\\n)\\n',\n",
       " '# Licensed to the Apache Software Foundation (ASF) under one\\n# or more contributor license agreements.  See the NOTICE file\\n# distributed with this work for additional information\\n# regarding copyright ownership.  The ASF licenses this file\\n# to you under the Apache License, Version 2.0 (the\\n# \"License\"); you may not use this file except in compliance\\n# with the License.  You may obtain a copy of the License at\\n#\\n#   http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing,\\n# software distributed under the License is distributed on an\\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\n# KIND, either express or implied.  See the License for the\\n# specific language governing permissions and limitations\\n# under the License.\\nimport datetime\\nimport json\\nimport random\\n\\nimport pandas as pd\\nfrom sqlalchemy import Date, Float, String\\n\\nfrom superset import db\\nfrom superset.models.dashboard import Dashboard\\nfrom superset.models.slice import Slice\\nfrom superset.utils import core as utils\\n\\nfrom .helpers import (\\n    config,\\n    get_example_data,\\n    get_slice_json,\\n    merge_slice,\\n    TBL,\\n    update_slice_ids,\\n)\\n\\n\\ndef load_unicode_test_data(only_metadata: bool = False, force: bool = False) -> None:\\n    \"\"\"Loading unicode test dataset from a csv file in the repo\"\"\"\\n    tbl_name = \"unicode_test\"\\n    database = utils.get_example_database()\\n    table_exists = database.has_table_by_name(tbl_name)\\n\\n    if not only_metadata and (not table_exists or force):\\n        data = get_example_data(\\n            \"unicode_utf8_unixnl_test.csv\", make_bytes=False\\n        )\\n        df = pd.read_csv(data, encoding=\"utf-8\")\\n        # generate date/numeric data\\n        df[\"dttm\"] = datetime.datetime.now().date()\\n        df[\"value\"] = [random.randint(1, 100) for _ in range(len(df))]\\n        df.to_sql(  # pylint: disable=no-member\\n            tbl_name,\\n            database.get_sqla_engine(),\\n            if_exists=\"replace\",\\n            chunksize=500,\\n            dtype={\\n                \"phrase\": String(500),\\n                \"short_phrase\": String(10),\\n                \"with_missing\": String(100),\\n                \"dttm\": Date(),\\n                \"value\": Float(),\\n            },\\n            index=False,\\n        )\\n        print(\"Done loading table!\")\\n        print(\"-\" * 80)\\n\\n    print(\"Creating table [unicode_test] reference\")\\n    obj = db.session.query(TBL).filter_by(table_name=tbl_name).first()\\n    if not obj:\\n        obj = TBL(table_name=tbl_name)\\n    obj.main_dttm_col = \"dttm\"\\n    obj.database = database\\n    db.session.merge(obj)\\n    db.session.commit()\\n    obj.fetch_metadata()\\n    tbl = obj\\n\\n    slice_data = {\\n        \"granularity_sqla\": \"dttm\",\\n        \"groupby\": [],\\n        \"metric\": {\\n            \"aggregate\": \"SUM\",\\n            \"column\": {\"column_name\": \"value\"},\\n            \"expressionType\": \"SIMPLE\",\\n            \"label\": \"Value\",\\n        },\\n        \"row_limit\": config[\"ROW_LIMIT\"],\\n        \"since\": \"100 years ago\",\\n        \"until\": \"now\",\\n        \"viz_type\": \"word_cloud\",\\n        \"size_from\": \"10\",\\n        \"series\": \"short_phrase\",\\n        \"size_to\": \"70\",\\n        \"rotation\": \"square\",\\n        \"limit\": \"100\",\\n    }\\n\\n    print(\"Creating a slice\")\\n    slc = Slice(\\n        slice_name=\"Unicode Cloud\",\\n        viz_type=\"word_cloud\",\\n        datasource_type=\"table\",\\n        datasource_id=tbl.id,\\n        params=get_slice_json(slice_data),\\n    )\\n    merge_slice(slc)\\n\\n    print(\"Creating a dashboard\")\\n    dash = db.session.query(Dashboard).filter_by(slug=\"unicode-test\").first()\\n\\n    if not dash:\\n        dash = Dashboard()\\n    js = \"\"\"\\\\\\n{\\n    \"CHART-Hkx6154FEm\": {\\n        \"children\": [],\\n        \"id\": \"CHART-Hkx6154FEm\",\\n        \"meta\": {\\n            \"chartId\": 2225,\\n            \"height\": 30,\\n            \"sliceName\": \"slice 1\",\\n            \"width\": 4\\n        },\\n        \"type\": \"CHART\"\\n    },\\n    \"GRID_ID\": {\\n        \"children\": [\\n            \"ROW-SyT19EFEQ\"\\n        ],\\n        \"id\": \"GRID_ID\",\\n        \"type\": \"GRID\"\\n    },\\n    \"ROOT_ID\": {\\n        \"children\": [\\n            \"GRID_ID\"\\n        ],\\n        \"id\": \"ROOT_ID\",\\n        \"type\": \"ROOT\"\\n    },\\n    \"ROW-SyT19EFEQ\": {\\n        \"children\": [\\n            \"CHART-Hkx6154FEm\"\\n        ],\\n        \"id\": \"ROW-SyT19EFEQ\",\\n        \"meta\": {\\n            \"background\": \"BACKGROUND_TRANSPARENT\"\\n        },\\n        \"type\": \"ROW\"\\n    },\\n    \"DASHBOARD_VERSION_KEY\": \"v2\"\\n}\\n    \"\"\"\\n    dash.dashboard_title = \"Unicode Test\"\\n    pos = json.loads(js)\\n    update_slice_ids(pos, [slc])\\n    dash.position_json = json.dumps(pos, indent=4)\\n    dash.slug = \"unicode-test\"\\n    dash.slices = [slc]\\n    db.session.merge(dash)\\n    db.session.commit()\\n',\n",
       " 'import os\\nimport time\\nimport datetime\\nimport collections\\nimport socket\\nfrom calendar import timegm\\nfrom future.utils import iteritems\\n\\nimport json\\ntry:\\n    import cPickle as pickle\\nexcept ImportError:\\n    import pickle\\n\\ntry:\\n    from threading import get_ident\\nexcept ImportError:\\n    from thread import get_ident\\n\\nfrom pandaharvester.harvesterconfig import harvester_config\\nfrom pandaharvester.harvestercore import core_utils\\nfrom pandaharvester.harvestercore.plugin_factory import PluginFactory\\nfrom pandaharvester.harvestercore.db_proxy_pool import DBProxyPool as DBProxy\\nfrom pandaharvester.harvestercore.db_interface import DBInterface\\n\\n# attribute list\\n_attribute_list = [\\'id\\', \\'item\\', \\'score\\']\\n\\n# fifo object spec\\nFifoObject = collections.namedtuple(\\'FifoObject\\', _attribute_list, verbose=False, rename=False)\\n\\n# logger\\n_logger = core_utils.setup_logger(\\'fifos\\')\\n\\n# base class of fifo message queue\\nclass FIFOBase(object):\\n    # constructor\\n    def __init__(self, **kwarg):\\n        for tmpKey, tmpVal in iteritems(kwarg):\\n            setattr(self, tmpKey, tmpVal)\\n        self.hostname = socket.gethostname()\\n        self.os_pid = os.getpid()\\n        self.dbProxy = DBProxy()\\n        self.dbInterface = DBInterface()\\n\\n    # get process identifier\\n    def get_pid(self):\\n        thread_id = get_ident()\\n        if thread_id is None:\\n            thread_id = 0\\n        return \\'{0}_{1}-{2}\\'.format(self.hostname, self.os_pid, format(get_ident(), \\'x\\'))\\n\\n    # make logger\\n    def make_logger(self, base_log, token=None, method_name=None, send_dialog=True):\\n        if send_dialog and hasattr(self, \\'dbInterface\\'):\\n            hook = self.dbInterface\\n        else:\\n            hook = None\\n        return core_utils.make_logger(base_log, token=token, method_name=method_name, hook=hook)\\n\\n    # intialize fifo from harvester configuration\\n    def _initialize_fifo(self, force_enable=False):\\n        self.fifoName = \\'{0}_fifo\\'.format(self.titleName)\\n        self.config = getattr(harvester_config, self.titleName)\\n        if force_enable:\\n            self.enabled = True\\n        elif hasattr(self.config, \\'fifoEnable\\') and self.config.fifoEnable:\\n            self.enabled = True\\n        else:\\n            self.enabled = False\\n            return\\n        pluginConf = vars(self.config).copy()\\n        pluginConf.update( {\\'titleName\\': self.titleName} )\\n        if hasattr(self.config, \\'fifoModule\\') and hasattr(self.config, \\'fifoClass\\'):\\n            pluginConf.update( {\\'module\\': self.config.fifoModule,\\n                                \\'name\\': self.config.fifoClass,} )\\n        else:\\n            if not hasattr(harvester_config, \\'fifo\\'):\\n                return\\n            pluginConf.update( {\\'module\\': harvester_config.fifo.fifoModule,\\n                                \\'name\\': harvester_config.fifo.fifoClass,} )\\n        pluginFactory = PluginFactory()\\n        self.fifo = pluginFactory.get_plugin(pluginConf)\\n\\n    # encode\\n    def encode(self, item):\\n        item_serialized = pickle.dumps(item, -1)\\n        return item_serialized\\n\\n    # decode\\n    def decode(self, item_serialized):\\n        item = pickle.loads(item_serialized)\\n        return item\\n\\n    # size of queue\\n    def size(self):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'size\\')\\n        retVal = self.fifo.size()\\n        mainLog.debug(\\'size={0}\\'.format(retVal))\\n        return retVal\\n\\n    # enqueue\\n    def put(self, item, score=None, encode_item=True):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'put\\')\\n        if encode_item:\\n            item_serialized = self.encode(item)\\n        else:\\n            item_serialized = item\\n        if score is None:\\n            score = time.time()\\n        retVal = self.fifo.put(item_serialized, score)\\n        mainLog.debug(\\'score={0}\\'.format(score))\\n        return retVal\\n\\n    # enqueue by id, which is unique\\n    def putbyid(self, id, item, score=None, encode_item=True):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'putbyid\\')\\n        if encode_item:\\n            item_serialized = self.encode(item)\\n        else:\\n            item_serialized = item\\n        if score is None:\\n            score = time.time()\\n        retVal = self.fifo.putbyid(id, item_serialized, score)\\n        mainLog.debug(\\'id={0} score={1}\\'.format(id, score))\\n        return retVal\\n\\n    # dequeue to get the first fifo object\\n    def get(self, timeout=None, protective=False, decode_item=True):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'get\\')\\n        object_tuple = self.fifo.get(timeout, protective)\\n        if object_tuple is None:\\n            retVal = None\\n        else:\\n            id, item_serialized, score = object_tuple\\n            if item_serialized is not None and decode_item:\\n                item = self.decode(item_serialized)\\n            else:\\n                item = item_serialized\\n            retVal = FifoObject(id, item, score)\\n        mainLog.debug(\\'called. protective={0} decode_item={1}\\'.format(protective, decode_item))\\n        return retVal\\n\\n    # dequeue to get the last fifo object\\n    def getlast(self, timeout=None, protective=False, decode_item=True):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'getlast\\')\\n        object_tuple = self.fifo.getlast(timeout, protective)\\n        if object_tuple is None:\\n            retVal = None\\n        else:\\n            id, item_serialized, score = object_tuple\\n            if item_serialized is not None and decode_item:\\n                item = self.decode(item_serialized)\\n            else:\\n                item = item_serialized\\n            retVal = FifoObject(id, item, score)\\n        mainLog.debug(\\'called. protective={0} decode_item={1}\\'.format(protective, decode_item))\\n        return retVal\\n\\n    # dequeue list of objects with some conditions\\n    def getmany(self, mode=\\'first\\', minscore=None, maxscore=None, count=None,\\n                    protective=False, temporary=False, decode_item=True):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'getmany\\')\\n        object_tuple_list = self.fifo.getmany(mode, minscore, maxscore, count, protective, temporary)\\n        if not object_tuple_list:\\n            mainLog.debug(\\'empty list\\')\\n        ret_list = []\\n        for object_tuple in object_tuple_list:\\n            id, item_serialized, score = object_tuple\\n            if item_serialized is not None and decode_item:\\n                item = self.decode(item_serialized)\\n            else:\\n                item = item_serialized\\n            val_tuple = FifoObject(id, item, score)\\n            ret_list.append(val_tuple)\\n        mainLog.debug(\\'mode={0} minscore={1} maxscore={2} count={3} protective={4} temporary={5} decode_item={6}\\'.format(\\n                        mode, minscore, maxscore, count, protective, temporary, decode_item))\\n        return ret_list\\n\\n    # get tuple of the first object and its score without dequeuing\\n    # If item is large un unnecessary to show int peek, set skip_item=True\\n    def peek(self, skip_item=False):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'peek\\')\\n        object_tuple = self.fifo.peek(skip_item=skip_item)\\n        if object_tuple is None:\\n            retVal = None\\n            mainLog.debug(\\'fifo empty\\')\\n        else:\\n            id, item_serialized, score = object_tuple\\n            if item_serialized is None and score is None:\\n                retVal = FifoObject(None, None, None)\\n            else:\\n                if score is None:\\n                    score = time.time()\\n                retVal = FifoObject(id, item_serialized, score)\\n            mainLog.debug(\\'score={0}\\'.format(score))\\n        return retVal\\n\\n    # get tuple of the last object and its score without dequeuing\\n    def peeklast(self, skip_item=False):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'peeklast\\')\\n        object_tuple = self.fifo.peeklast(skip_item=skip_item)\\n        if object_tuple is None:\\n            retVal = None\\n            mainLog.debug(\\'fifo empty\\')\\n        else:\\n            id, item_serialized, score = object_tuple\\n            if item_serialized is None and score is None:\\n                retVal = FifoObject(None, None, None)\\n            else:\\n                if score is None:\\n                    score = time.time()\\n                retVal = FifoObject(id, item_serialized, score)\\n            mainLog.debug(\\'score={0}\\'.format(score))\\n        return retVal\\n\\n    # get tuple of the object by id without dequeuing\\n    def peekbyid(self, id, temporary=False, skip_item=False):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'peekbyid\\')\\n        object_tuple = self.fifo.peekbyid(id, temporary, skip_item=skip_item)\\n        if object_tuple is None:\\n            retVal = None\\n            mainLog.debug(\\'fifo empty\\')\\n        else:\\n            id_gotten, item_serialized, score = object_tuple\\n            if item_serialized is None and score is None:\\n                retVal = FifoObject(None, None, None)\\n            else:\\n                if score is None:\\n                    score = time.time()\\n                retVal = FifoObject(id, item_serialized, score)\\n            mainLog.debug(\\'id={0} score={1} temporary={2}\\'.format(id, score, temporary))\\n        return retVal\\n\\n    # get list of object tuples without dequeuing\\n    def peekmany(self, mode=\\'first\\', minscore=None, maxscore=None, count=None, skip_item=False):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'peekmany\\')\\n        object_tuple_list = self.fifo.peekmany(mode, minscore, maxscore, count, skip_item)\\n        if not object_tuple_list:\\n            mainLog.debug(\\'empty list\\')\\n        ret_list = []\\n        for object_tuple in object_tuple_list:\\n            id_gotten, item_serialized, score = object_tuple\\n            if item_serialized is None and score is None:\\n                val_tuple = FifoObject(None, None, None)\\n            else:\\n                if score is None:\\n                    score = time.time()\\n                val_tuple = FifoObject(id, item_serialized, score)\\n            ret_list.append(val_tuple)\\n        mainLog.debug(\\'mode={0} minscore={1} maxscore={2} count={3}\\'.format(mode, minscore, maxscore, count))\\n        return ret_list\\n\\n    # delete objects by list of ids from temporary space, return the number of objects successfully deleted\\n    def delete(self, ids):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'release\\')\\n        retVal = self.fifo.delete(ids)\\n        mainLog.debug(\\'released {0} objects in {1}\\'.format(retVal, ids))\\n        return retVal\\n\\n    # restore objects by list of ids from temporary space to fifo; ids=None to restore all objects\\n    def restore(self, ids=None):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'restore\\')\\n        retVal = self.fifo.restore(ids)\\n        if ids is None:\\n            mainLog.debug(\\'restored all objects\\')\\n        else:\\n            mainLog.debug(\\'restored objects in {0}\\'.format(ids))\\n        return retVal\\n\\n    # update a object by its id with some conditions\\n    def update(self, id, item=None, score=None, temporary=None, cond_score=\\'gt\\'):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'update\\')\\n        retVal = self.fifo.update(id, item, score, temporary, cond_score)\\n        update_report_list = []\\n        if item is not None:\\n            update_report_list.append(\\'item={0}\\'.format(item))\\n        if score is not None:\\n            update_report_list.append(\\'score={0}\\'.format(score))\\n        if temporary is not None:\\n            update_report_list.append(\\'temporary={0}\\'.format(temporary))\\n        update_report = \\' \\'.join(update_report_list)\\n        mainLog.debug(\\'update id={0} cond_score={1}: return={2}, {3}\\'.format(id, cond_score, retVal, update_report))\\n        return retVal\\n\\n\\n# Special fifo base for non havester-agent\\nclass SpecialFIFOBase(FIFOBase):\\n    # constructor\\n    def __init__(self, **kwarg):\\n        FIFOBase.__init__(self, **kwarg)\\n        self.fifoName = \\'{0}_fifo\\'.format(self.titleName)\\n        pluginConf = {}\\n        pluginConf.update( {\\'titleName\\': self.titleName} )\\n        pluginConf.update( {\\'module\\': harvester_config.fifo.fifoModule,\\n                            \\'name\\': harvester_config.fifo.fifoClass,} )\\n        pluginFactory = PluginFactory()\\n        self.fifo = pluginFactory.get_plugin(pluginConf)\\n\\n\\n# Benchmark fifo\\nclass BenchmarkFIFO(SpecialFIFOBase):\\n    titleName = \\'benchmark\\'\\n\\n\\n# monitor fifo\\nclass MonitorFIFO(FIFOBase):\\n    titleName = \\'monitor\\'\\n\\n    # constructor\\n    def __init__(self, **kwarg):\\n        FIFOBase.__init__(self, **kwarg)\\n        self._initialize_fifo()\\n\\n    def populate(self, seconds_ago=0, clear_fifo=False):\\n        \"\"\"\\n        Populate monitor fifo with all active worker chunks and timeNow as score from DB\\n        with modificationTime earlier than seconds_ago seconds ago\\n        object in fifo = [(queueName_1, [[worker_1_1], [worker_1_2], ...]), (queueName_2, ...)]\\n        \"\"\"\\n        if clear_fifo:\\n            self.fifo.clear()\\n        try:\\n            fifoMaxWorkersToPopulate = self.config.fifoMaxWorkersToPopulate\\n        except AttributeError:\\n            fifoMaxWorkersToPopulate = 2**32\\n        try:\\n            fifoMaxWorkersPerChunk = self.config.fifoMaxWorkersPerChunk\\n        except AttributeError:\\n            fifoMaxWorkersPerChunk = 500\\n        workspec_iterator = self.dbProxy.get_active_workers(fifoMaxWorkersToPopulate, seconds_ago)\\n        last_queueName = None\\n        workspec_chunk = []\\n        timeNow_timestamp = time.time()\\n        score = timeNow_timestamp\\n        for workspec in workspec_iterator:\\n            workspec.set_work_params({\\'lastCheckAt\\': timeNow_timestamp})\\n            if last_queueName is None:\\n                try:\\n                    score = timegm(workspec.modificationTime.utctimetuple())\\n                except Exception:\\n                    pass\\n                workspec_chunk = [[workspec]]\\n                last_queueName = workspec.computingSite\\n            elif workspec.computingSite == last_queueName \\\\\\n                and len(workspec_chunk) < fifoMaxWorkersPerChunk:\\n                workspec_chunk.append([workspec])\\n            else:\\n                self.put((last_queueName, workspec_chunk), score)\\n                try:\\n                    score = timegm(workspec.modificationTime.utctimetuple())\\n                except Exception:\\n                    pass\\n                workspec_chunk = [[workspec]]\\n                last_queueName = workspec.computingSite\\n        if len(workspec_chunk) > 0:\\n            self.put((last_queueName, workspec_chunk), score)\\n\\n    def to_check_workers(self, check_interval=harvester_config.monitor.checkInterval):\\n        \"\"\"\\n        Justify whether to check any worker by the modificationTime of the first worker in fifo\\n        retVal True if OK to dequeue to check;\\n        retVal False otherwise.\\n        Return retVal, overhead_time\\n        \"\"\"\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'to_check_worker\\')\\n        retVal = False\\n        overhead_time = None\\n        timeNow_timestamp = time.time()\\n        peeked_tuple = self.peek(skip_item=True)\\n        if peeked_tuple is not None:\\n            score = peeked_tuple.score\\n            overhead_time = timeNow_timestamp - score\\n            if overhead_time > 0:\\n                retVal = True\\n                if score < 0:\\n                    mainLog.debug(\\'True. Preempting\\')\\n                    overhead_time = None\\n                else:\\n                    mainLog.debug(\\'True\\')\\n                    mainLog.info(\\'Overhead time is {0} sec\\'.format(overhead_time))\\n            else:\\n                mainLog.debug(\\'False. Workers too young to check\\')\\n                mainLog.debug(\\'Overhead time is {0} sec\\'.format(overhead_time))\\n        else:\\n            mainLog.debug(\\'False. Got nothing in FIFO\\')\\n        return retVal, overhead_time\\n\\n\\nclass MonitorEventFIFO(SpecialFIFOBase):\\n    titleName = \\'monitorEvent\\'\\n\\n    # constructor\\n    def __init__(self, **kwarg):\\n        self.config = getattr(harvester_config, \\'monitor\\')\\n        self.enabled = False\\n        if hasattr(self.config, \\'fifoEnable\\') and self.config.fifoEnable \\\\\\n            and getattr(self.config, \\'eventBasedEnable\\', False):\\n            self.enabled = True\\n        SpecialFIFOBase.__init__(self, **kwarg)\\n',\n",
       " '#! /usr/bin/env python\\n# for fgas ppruns only\\nppruns=[ \"2009-03-28_W-S-I+\" ,\"2009-09-19_W-J-V\" ,\"2009-04-29_W-S-Z+\" ,\"2009-04-29_W-J-B\" ,\"2010-03-12_W-J-B\" ,\"2010-03-12_W-S-Z+\" ,\"2010-03-12_W-C-RC\" ,\"2010-11-04_W-J-B\" ,\"2010-11-04_W-S-Z+\" ,\"2012-07-23_W-C-RC\" ,\"2013-06-10_W-S-Z+\" ,\"2007-02-13_W-S-I+\" ,\"2007-02-13_W-J-V\" ,\"2010-03-12_W-J-V\" ,\"2010-03-12_W-S-I+\" ,\"2010-12-05_W-J-V\" ,\"2015-12-15_W-J-B\" ,\"2015-12-15_W-C-RC\" ,\"2015-12-15_W-S-Z+\"]\\nppruns_MACS1115=[ \"2009-04-29_W-S-Z+\" ,\"2009-04-29_W-J-B\" ,\"2010-03-12_W-J-B\" ,\"2010-03-12_W-S-Z+\" ,\"2010-03-12_W-C-RC\"]\\nppruns_preH=[ \"2010-03-12_W-S-I+\" ,\"2010-12-05_W-J-V\" ,\"2007-02-13_W-J-V\" ,\"2007-02-13_W-S-I+\" ,\"2009-03-28_W-S-I+\" ,\"2010-03-12_W-J-V\"]\\nppruns_10_3= [\\'2009-03-28_W-S-I+\\', \\'2009-09-19_W-J-V\\', \\'2009-04-29_W-S-Z+\\', \\'2009-04-29_W-J-B\\', \\'2010-03-12_W-J-B\\', \\'2010-03-12_W-S-Z+\\', \\'2010-03-12_W-C-RC\\', \\'2010-11-04_W-J-B\\', \\'2010-11-04_W-S-Z+\\', \\'2012-07-23_W-C-RC\\', \\'2013-06-10_W-S-Z+\\', \\'2010-03-12_W-J-V\\', \\'2010-03-12_W-S-I+\\', \\'2010-12-05_W-J-V\\', \\'2015-12-15_W-J-B\\', \\'2015-12-15_W-C-RC\\', \\'2015-12-15_W-S-Z+\\']\\nppruns_10_2= [\\'2007-02-13_W-S-I+\\', \\'2007-02-13_W-J-V\\']\\nppruns_postH= [\\'2009-09-19_W-J-V\\', \\'2009-04-29_W-S-Z+\\', \\'2009-04-29_W-J-B\\', \\'2010-03-12_W-J-B\\', \\'2010-03-12_W-S-Z+\\', \\'2010-03-12_W-C-RC\\', \\'2010-11-04_W-J-B\\', \\'2010-11-04_W-S-Z+\\', \\'2012-07-23_W-C-RC\\', \\'2013-06-10_W-S-Z+\\', \\'2015-12-15_W-J-B\\', \\'2015-12-15_W-C-RC\\', \\'2015-12-15_W-S-Z+\\']\\nppruns_nonMACS1115= [\\'2009-03-28_W-S-I+\\', \\'2009-09-19_W-J-V\\', \\'2010-11-04_W-J-B\\', \\'2010-11-04_W-S-Z+\\', \\'2012-07-23_W-C-RC\\', \\'2013-06-10_W-S-Z+\\', \\'2007-02-13_W-S-I+\\', \\'2007-02-13_W-J-V\\', \\'2010-03-12_W-J-V\\', \\'2010-03-12_W-S-I+\\', \\'2010-12-05_W-J-V\\', \\'2015-12-15_W-J-B\\', \\'2015-12-15_W-C-RC\\', \\'2015-12-15_W-S-Z+\\']\\nppruns_dec= [\\'2009-03-28_W-S-I+\\', \\'2009-09-19_W-J-V\\', \\'2010-11-04_W-J-B\\', \\'2010-11-04_W-S-Z+\\', \\'2012-07-23_W-C-RC\\', \\'2013-06-10_W-S-Z+\\', \\'2010-03-12_W-J-V\\', \\'2010-03-12_W-S-I+\\', \\'2010-12-05_W-J-V\\', \\'2015-12-15_W-J-B\\', \\'2015-12-15_W-C-RC\\', \\'2015-12-15_W-S-Z+\\']\\n\\nic_cldata={\\'MACS0429-02\\':{},\\'MACS1226+21\\':{},\\'RXJ2129\\':{},\\'MACS1115+01\\':{},\"MACS0416-24\":{},\\'MACS0159-08\\':{}, \\'Zw2089\\':{}, \\'Zw2701\\':{}, \\'A2204\\':{}}\\nic_cldata[\\'MACS1226+21\\'][\\'FILTERs\\']=[\"W-J-B\",\"W-J-V\",\"W-C-RC\",\"W-C-IC\",\"W-S-Z+\"]\\nic_cldata[\\'MACS1226+21\\'][\\'PPRUNs\\']=[\"W-C-IC_2010-02-12\", \"W-C-IC_2011-01-06\",\"W-C-RC_2010-02-12\", \"W-J-B_2010-02-12\", \"W-J-V_2010-02-12\", \"W-S-Z+_2011-01-06\"]\\nic_cldata[\\'MACS1226+21\\'][\\'FILTERs_matching_PPRUNs\\']=[\"W-C-IC\", \"W-C-IC\",\"W-C-RC\", \"W-J-B\", \"W-J-V\", \"W-S-Z+\"] \\nic_cldata[\\'MACS0429-02\\'][\\'FILTERs\\']=[\"W-J-B\",\"W-S-Z+\"]\\nic_cldata[\\'MACS0429-02\\'][\\'FILTERs_matching_PPRUNs\\']=[\"W-J-B\",\"W-S-Z+\"]\\nic_cldata[\\'MACS0429-02\\'][\\'PPRUNs\\']=[\"W-J-B_2015-12-15\",\"W-S-Z+_2015-12-15\"]\\nic_cldata[\\'RXJ2129\\'][\\'FILTERs\\']=[\"W-J-B\",\"W-C-RC\",\"W-S-Z+\"]\\nic_cldata[\\'RXJ2129\\'][\\'FILTERs_matching_PPRUNs\\']=[\"W-J-B\",\"W-C-RC\",\"W-S-Z+\"]\\nic_cldata[\\'RXJ2129\\'][\\'PPRUNs\\']=[\"W-J-B_2010-11-04\",\"W-C-RC_2012-07-23\",\"W-S-Z+_2010-11-04\"]\\nic_cldata[\\'MACS1115+01\\'][\\'FILTERs\\']=[\"W-J-B\",\"W-C-RC\",\"W-S-Z+\"] \\nic_cldata[\\'MACS1115+01\\'][\\'FILTERs_matching_PPRUNs\\']=[\"W-S-Z+\",\"W-J-B\",\"W-J-B\",\"W-S-Z+\",\"W-C-RC\"]\\nic_cldata[\\'MACS1115+01\\'][\\'PPRUNs\\']=[\"W-S-Z+_2009-04-29\",\"W-J-B_2009-04-29\",\"W-J-B_2010-03-12\",\"W-S-Z+_2010-03-12\",\"W-C-RC_2010-03-12\"]\\nic_cldata[\\'MACS0416-24\\'][\\'FILTERs\\']=[\"W-J-B\",\"W-C-RC\",\"W-S-Z+\"] \\nic_cldata[\\'MACS0416-24\\'][\\'FILTERs_matching_PPRUNs\\']=[\"W-J-B\",\"W-C-RC\",\"W-S-Z+\"] \\nic_cldata[\\'MACS0416-24\\'][\\'PPRUNs\\']=[\"W-J-B_2010-11-04\",\"W-C-RC_2010-11-04\",\"W-S-Z+_2010-11-04\"] \\nic_cldata[\\'MACS0159-08\\'][\\'FILTERs\\']=[\"W-J-B\"]\\nic_cldata[\\'MACS0159-08\\'][\\'FILTERs_matching_PPRUNs\\']=[\"W-J-B\"]\\nic_cldata[\\'MACS0159-08\\'][\\'PPRUNs\\']=[\"W-J-B_2015-12-15\"]\\nic_cldata[\\'Zw2089\\'][\\'FILTERs\\']=[\\'W-J-B\\',\\'W-J-V\\',\\'W-C-RC\\',\\'W-S-I+\\',\\'W-S-Z+\\']\\nic_cldata[\\'Zw2089\\'][\\'FILTERs_matching_PPRUNs\\']=[\\'W-C-RC\\',\\'W-J-B\\',\\'W-J-V\\',\\'W-S-I+\\',\\'W-S-I+\\',\\'W-S-Z+\\']\\nic_cldata[\\'Zw2089\\'][\\'PPRUNs\\']=[\\'W-C-RC_2015-12-15\\',\\'W-J-B_2015-12-15\\',\\'W-J-V_2010-12-05\\',\\'W-S-I+_2007-02-13\\',\\'W-S-I+_2009-03-28\\',\\'W-S-Z+_2015-12-15\\']\\nic_cldata[\\'Zw2701\\'][\\'FILTERs\\']=[\\'W-J-B\\',\\'W-J-V\\',\\'W-C-RC\\',\\'W-S-I+\\']\\nic_cldata[\\'Zw2701\\'][\\'FILTERs_matching_PPRUNs\\']=[\\'W-C-RC\\',\\'W-J-B\\',\\'W-J-V\\',\\'W-J-V\\',\\'W-S-I+\\']\\nic_cldata[\\'Zw2701\\'][\\'PPRUNs\\']=[\\'W-C-RC_2015-12-15\\',\\'W-J-B_2015-12-15\\',\\'W-J-V_2010-03-12\\',\\'W-J-V_2010-12-05\\',\\'W-S-I+_2010-03-12\\']\\nic_cldata[\\'A2204\\'][\\'FILTERs\\']=[\\'W-J-V\\',\\'W-S-I+\\']\\nic_cldata[\\'A2204\\'][\\'FILTERs_matching_PPRUNs\\']=[\\'W-J-V\\',\\'W-S-I+\\']\\nic_cldata[\\'A2204\\'][\\'PPRUNs\\']=[\\'W-J-V_2009-09-19\\',\\'W-S-I+_2009-03-28\\']\\nra_cluster={}\\ndec_cluster={}\\nra_cluster[\\'MACS0429-02\\']=67.40041667 ; dec_cluster[\\'MACS0429-02\\']=-2.88555556\\nra_cluster[\\'RXJ2129\\']=322.41625000 ; dec_cluster[\\'RXJ2129\\']=0.08888889\\nra_cluster[\\'MACS1226+21\\']=186.71268; dec_cluster[\\'MACS1226+21\\']=21.831938\\nra_cluster[\\'A2204\\']=248.19666667; dec_cluster[\\'A2204\\']=5.57555556\\nra_cluster[\\'MACS0159-08\\']=29.9579;dec_cluster[\\'MACS0159-08\\']=-8.83028\\n#MACSJ0429.6-0253\\t67.4\\t-2.88375\\n#RXJ2129.6+0005\\t322.408\\t0.094\\nra_cluster[\\'Zw2089\\']=135.158;dec_cluster[\\'Z2089\\']=20.916\\nra_cluster[\\'Zw2701\\']=148.198;dec_cluster[\\'Z2701\\']=51.891\\nra_cluster[\\'MACS1115+01\\']=168.972;dec_cluster[\\'MACS1115+01\\']=1.49639\\nra_cluster[\\'MACS0416-24\\']=64.0413;dec_cluster[\\'MACS0416-24\\']=6-24.0662\\n\\n\\nclusters =[\\'MACS0429-02\\',\\'MACS1226+21\\',\\'RXJ2129\\',\\'MACS1115+01\\',\"MACS0416-24\",\\'MACS0159-08\\', \\'Zw2089\\', \\'Zw2701\\', \\'A2204\\']\\nfgas_clusters =[\\'MACS0429-02\\',\\'RXJ2129\\',\\'MACS1115+01\\',\\'MACS0159-08\\', \\'Zw2089\\', \\'Zw2701\\', \\'A2204\\']\\nactive_fgas_clusters =[\\'Zw2089\\',\\'MACS0429-02\\',\\'RXJ2129\\', \\'MACS1115+01\\', \\'A2204\\']\\nclusters_refcats={}\\nclusters_refcats[\\'MACS0429-02\\']=\\'PANSTARRS\\'\\nclusters_refcats[\\'RXJ2129\\']=\\'SDSS\\'\\nclusters_refcats[\\'MACS1226+21\\']=\\'SDSS\\'\\nclusters_refcats[\\'A2204\\']=\\'PANSTARRS\\'\\nclusters_refcats[\\'MACS0159-08\\']=\\'SDSS\\'\\nclusters_refcats[\\'Zw2089\\']=\\'SDSS\\'\\nclusters_refcats[\\'Zw2701\\']=\\'SDSS\\'\\nclusters_refcats[\\'MACS1115+01\\']=\\'SDSS\\'\\nclusters_refcats[\\'MACS0416-24\\']=\\'PANSTARRS\\'\\n\\nstuff_todo=[\\'Coadding (1 day)\\',\\n\\'By-hand masking (1 day)\\',\\n\\'Coadd Masking (1/3 day)\\',\\n\\'Photometric Measurement and Calibration\\',\\n\\'Photo-Zs\\', \\'cc masses\\', \\'p(z) masses\\']\\n\\nfor cl in active_fgas_clusters:\\n\\tfor item in stuff_todo:\\n\\t\\tprint \":\".join([item,cl])\\n\\nmask_todo=[\\'coadd init\\',\\'standard by-hand masking\\',\\'backmasking\\',\\'autosuppression\\',\\'asteroid\\',\\'star (ALL BANDS)\\',\\'edgemask (ALL BANDS)\\',\\'check background-sub errors (ALL BANDS)\\',\\'coadd final\\']\\nfor cl in active_fgas_clusters:\\n\\tfor item in  mask_todo:\\n\\t\\tprint \":\".join([item,cl])\\n',\n",
       " 'import argparse\\nimport tools.find_mxnet\\nimport mxnet as mx\\nimport os\\nimport sys\\nfrom train.train_net import train_net\\n\\ndef parse_args():\\n    parser = argparse.ArgumentParser(description=\\'Train a Single-shot detection network\\')\\n    parser.add_argument(\\'--dataset\\', dest=\\'dataset\\', help=\\'which dataset to use\\',\\n                        default=\\'pascal\\', type=str)\\n    parser.add_argument(\\'--image-set\\', dest=\\'image_set\\', help=\\'train set, can be trainval or train\\',\\n                        default=\\'trainval\\', type=str)\\n    parser.add_argument(\\'--year\\', dest=\\'year\\', help=\\'can be 2007, 2012\\',\\n                        default=\\'2007,2012\\', type=str)\\n    parser.add_argument(\\'--val-image-set\\', dest=\\'val_image_set\\', help=\\'validation set, can be val or test\\',\\n                        default=\\'test\\', type=str)\\n    parser.add_argument(\\'--val-year\\', dest=\\'val_year\\', help=\\'can be 2007, 2010, 2012\\',\\n                        default=\\'2007\\', type=str)\\n    parser.add_argument(\\'--devkit-path\\', dest=\\'devkit_path\\', help=\\'VOCdevkit path\\',\\n                        default=os.path.join(os.getcwd(), \\'data\\', \\'VOCdevkit\\'), type=str)\\n    parser.add_argument(\\'--network\\', dest=\\'network\\', type=str, default=\\'vgg16_reduced\\',\\n                        choices=[\\'vgg16_reduced\\'], help=\\'which network to use\\')\\n    parser.add_argument(\\'--batch-size\\', dest=\\'batch_size\\', type=int, default=32,\\n                        help=\\'training batch size\\')\\n    parser.add_argument(\\'--resume\\', dest=\\'resume\\', type=int, default=-1,\\n                        help=\\'resume training from epoch n\\')\\n    parser.add_argument(\\'--finetune\\', dest=\\'finetune\\', type=int, default=-1,\\n                        help=\\'finetune from epoch n, rename the model before doing this\\')\\n    parser.add_argument(\\'--pretrained\\', dest=\\'pretrained\\', help=\\'pretrained model prefix\\',\\n                        default=os.path.join(os.getcwd(), \\'model\\', \\'vgg16_reduced\\'), type=str)\\n    parser.add_argument(\\'--epoch\\', dest=\\'epoch\\', help=\\'epoch of pretrained model\\',\\n                        default=1, type=int)\\n    parser.add_argument(\\'--prefix\\', dest=\\'prefix\\', help=\\'new model prefix\\',\\n                        default=os.path.join(os.getcwd(), \\'model\\', \\'ssd\\'), type=str)\\n    parser.add_argument(\\'--gpus\\', dest=\\'gpus\\', help=\\'GPU devices to train with\\',\\n                        default=\\'0\\', type=str)\\n    parser.add_argument(\\'--begin-epoch\\', dest=\\'begin_epoch\\', help=\\'begin epoch of training\\',\\n                        default=0, type=int)\\n    parser.add_argument(\\'--end-epoch\\', dest=\\'end_epoch\\', help=\\'end epoch of training\\',\\n                        default=100, type=int)\\n    parser.add_argument(\\'--frequent\\', dest=\\'frequent\\', help=\\'frequency of logging\\',\\n                        default=20, type=int)\\n    parser.add_argument(\\'--data-shape\\', dest=\\'data_shape\\', type=int, default=300,\\n                        help=\\'set image shape\\')\\n    parser.add_argument(\\'--lr\\', dest=\\'learning_rate\\', type=float, default=0.001,\\n                        help=\\'learning rate\\')\\n    parser.add_argument(\\'--momentum\\', dest=\\'momentum\\', type=float, default=0.9,\\n                        help=\\'momentum\\')\\n    parser.add_argument(\\'--wd\\', dest=\\'weight_decay\\', type=float, default=0.0001,\\n                        help=\\'weight decay\\')\\n    parser.add_argument(\\'--mean-r\\', dest=\\'mean_r\\', type=float, default=123,\\n                        help=\\'red mean value\\')\\n    parser.add_argument(\\'--mean-g\\', dest=\\'mean_g\\', type=float, default=117,\\n                        help=\\'green mean value\\')\\n    parser.add_argument(\\'--mean-b\\', dest=\\'mean_b\\', type=float, default=104,\\n                        help=\\'blue mean value\\')\\n    parser.add_argument(\\'--lr-epoch\\', dest=\\'lr_refactor_epoch\\', type=int, default=50,\\n                        help=\\'refactor learning rate every N epoch\\')\\n    parser.add_argument(\\'--lr-ratio\\', dest=\\'lr_refactor_ratio\\', type=float, default=0.9,\\n                        help=\\'ratio to refactor learning rate\\')\\n    parser.add_argument(\\'--log\\', dest=\\'log_file\\', type=str, default=\"train.log\",\\n                        help=\\'save training log to file\\')\\n    parser.add_argument(\\'--monitor\\', dest=\\'monitor\\', type=int, default=0,\\n                        help=\\'log network parameters every N iters if larger than 0\\')\\n    args = parser.parse_args()\\n    return args\\n\\nif __name__ == \\'__main__\\':\\n    args = parse_args()\\n    ctx = [mx.gpu(int(i)) for i in args.gpus.split(\\',\\')]\\n    ctx = mx.cpu() if not ctx else ctx\\n    train_net(args.network, args.dataset, args.image_set, args.year,\\n              args.devkit_path, args.batch_size,\\n              args.data_shape, [args.mean_r, args.mean_g, args.mean_b],\\n              args.resume, args.finetune, args.pretrained,\\n              args.epoch, args.prefix, ctx, args.begin_epoch, args.end_epoch,\\n              args.frequent, args.learning_rate, args.momentum, args.weight_decay,\\n              args.val_image_set, args.val_year, args.lr_refactor_epoch,\\n              args.lr_refactor_ratio, args.monitor, args.log_file)\\n',\n",
       " '\"\"\"Generic SSH class providing method to connect switches, routers and other devices using SSHv2.\\r\\n\\r\\ncNetSSH uses paramiko, see API docs http://docs.paramiko.org/en/latest/ \\r\\n\"\"\"\\r\\n\\r\\n__author__    = \"dumplab\"\\r\\n__copyright__ = \"2015 dumplab\"\\r\\n__license__   = \"MIT\"\\r\\n__version__   = \"0.5\"\\r\\n__status__    = \"Developement\"\\r\\n\\r\\nimport paramiko,re,sys,time\\r\\n\\r\\nclass cNetSSH(object):\\r\\n\\t\"\"\"SSH connection object\"\"\"\\r\\n\\t\\r\\n        def __init__(self):\\r\\n\\t\\t\"\"\"Set default attribute values only\\r\\n\\t\\t\\r\\n\\t\\tNo arguments\\r\\n\\t\\t\"\"\"\\r\\n\\t\\tself.__host      = \"\"\\r\\n\\t\\tself.__user      = \"\"\\r\\n\\t\\tself.__pass      = \"\"\\r\\n\\t\\tself.__conn      = None # paramiko connection\\r\\n\\t\\tself.__shell     = None # when using a channel\\r\\n\\t\\tself.__connected = False\\r\\n\\t\\tself.__input     = \"\"\\r\\n\\t\\tself.__output    = \"\"\\r\\n\\t\\tself.__timeout   = 1.0  # right now we use a timeout of 2 seconds to connect\\r\\n\\t\\tself.__outEnd    = \"\"   # will be considered as the prompt and end of an output see recv is discovered during the login\\r\\n\\t\\tself.__debug     = False\\r\\n\\r\\n\\tdef connect(self,hostName,userName=\"\",userPass=\"\",newShell=True,paging=False):\\r\\n\\t\\t\"\"\"connect a device using provided credentials and start an interactive shell\\r\\n\\t\\t\\r\\n\\t\\tKeyword arguments:\\r\\n\\t\\thostName = hostname (default \"\")\\r\\n\\t\\tuserName = username to authenticate (default \"\")\\r\\n\\t\\tuserPass = password to use for authenticating and for unlocking a private key (default \"\")\\r\\n\\t\\tnewShell = shall we start a shell, opens a new Channel (default True)\\r\\n\\t\\tpaging   = enable or disable paging/more (Default False)\\r\\n\\t\\treturns the configuration as a string\\r\\n\\t\\t\"\"\"\\r\\n\\t\\tself.__host = hostName\\r\\n\\t\\tself.__user = userName\\r\\n\\t\\tself.__pass = userPass\\r\\n\\r\\n\\t\\ttry:\\r\\n\\t\\t\\tself.__conn = paramiko.SSHClient()\\r\\n\\t\\t\\t# add untrusted hosts\\r\\n\\t\\t\\tself.__conn.set_missing_host_key_policy(paramiko.AutoAddPolicy())\\r\\n\\t\\t\\t# connect to host\\r\\n\\t\\t\\tself.__conn.connect(self.__host,username=self.__user,password=self.__pass,look_for_keys=False,timeout=1.0)\\r\\n\\t\\t\\t# set connected flag\\r\\n\\t\\t\\tself.__connected = True\\r\\n\\t\\t\\t# debug\\r\\n\\t\\t\\tif self.__debug:\\r\\n\\t\\t\\t\\tprint(\"cNetSSH::connect - connected to device \" + self.__outEnd)\\r\\n\\t\\t\\t# Start an interactive shell session on the SSH server. A new Channel is opened and connected to a pseudo-terminal using the requested terminal type and size.\\r\\n\\t\\t\\tif newShell==True:\\r\\n\\t\\t\\t\\tself.__shell = self.__conn.invoke_shell()\\r\\n\\t\\t\\t\\ttime.sleep(0.3)\\r\\n\\t\\t\\t\\t# Save the initial router prompt\\r\\n\\t\\t\\t\\tself.__output = self.__shell.recv(32000)\\r\\n\\t\\t\\t\\tself.__output = self.__output.splitlines(True)\\r\\n\\t\\t\\t\\tself.__outEnd = self.__output[len(self.__output)-1]\\r\\n\\t\\t\\t\\tif self.__debug:\\r\\n\\t\\t\\t\\t\\tprint(\"cNetSSH::connect - I\\'ll consider this string as the prompt in further requests: \" + self.__outEnd)\\r\\n\\t\\t\\t\\tif paging==False:\\r\\n\\t\\t\\t\\t\\tself.__disablePaging()\\r\\n\\t\\texcept paramiko.AuthenticationException:\\r\\n\\t\\t\\tprint(\"Authentication failed when connecting to \" + self.__host)\\r\\n\\t\\t\\tsys.exit(1)\\r\\n\\t\\texcept:\\r\\n\\t\\t\\tprint(\"Could not connect to host \" + self.__host)\\r\\n\\r\\n\\r\\n\\tdef enable(self,password=\"\"):\\r\\n\\t\\t\"\"\"enable - enter enable mode. please use this method as it stores the new prompt to spped up futher command processing\\r\\n\\t\\t\\r\\n\\t\\tKeyword arguments:\\r\\n\\t\\tpassword = enable password (default \"\")\\r\\n\\t\\t\"\"\"\\r\\n\\t\\tif self.__connected:\\r\\n\\t\\t\\tself.__input = \"enable\"\\r\\n\\t\\t\\tnumBytes     = self.__shell.send(self.__input + \"\\\\n\" + password + \"\\\\n\")\\r\\n\\t\\t\\tif numBytes > 0:\\r\\n\\t\\t\\t\\ttime.sleep(0.3)\\r\\n\\t\\t\\t\\t# Save the router prompt\\r\\n\\t\\t\\t\\tself.__output = self.__shell.recv(32000)\\r\\n\\t\\t\\t\\tself.__output = self.__output.splitlines(True)\\r\\n\\t\\t\\t\\tself.__outEnd = self.__output[len(self.__output)-1]\\r\\n\\t\\t\\t\\tif self.__debug:\\r\\n\\t\\t\\t\\t\\tprint(\"cNetSSH::enable - change expected prompt to \" + self.__outEnd)\\r\\n\\t\\t\\t\\treturn True\\r\\n\\t\\telse:\\r\\n\\t\\t\\tprint(\"Not connected to \" + self.__host + \". Connect first.\")\\r\\n\\r\\n\\tdef send(self,command):\\r\\n\\t\\tif self.__connected:\\r\\n\\t\\t\\tself.__input = command\\r\\n\\t\\t\\tnumBytes     = self.__shell.send(command + \"\\\\n\")\\r\\n\\t\\t\\tif numBytes > 0:\\r\\n\\t\\t\\t\\tself.__output = \"\"\\r\\n\\t\\t\\t\\tmyTempBuffer  = \"\"\\r\\n\\t\\t\\t\\tmax_try       = 500\\r\\n\\t\\t\\t\\tx             = 0\\r\\n\\t\\t\\t\\tsTime         = time.time()\\r\\n\\t\\t\\t\\tbailedOut     = False\\r\\n\\t\\t\\t\\twhile x < max_try:\\r\\n\\t\\t\\t\\t\\tif self.__shell.recv_ready():\\r\\n\\t\\t\\t\\t\\t\\tif self.__debug:\\r\\n\\t\\t\\t\\t\\t\\t\\tprint(\"cNetSSH::send - recv_ready() on cycle=\" + str(x))\\r\\n\\t\\t\\t\\t\\t\\twhile True:\\r\\n\\t\\t\\t\\t\\t\\t\\t# note recv returns if there is > 0 < 1024\\r\\n\\t\\t\\t\\t\\t\\t\\tmyTempBuffer = self.__shell.recv(1024)\\r\\n\\t\\t\\t\\t\\t\\t\\tself.__output += myTempBuffer\\r\\n\\t\\t\\t\\t\\t\\t\\t#print(\"cNetSSH: recv() returned ... len=\" + str(len(myTempBuffer)))\\r\\n\\t\\t\\t\\t\\t\\t\\tif len(myTempBuffer)==0 or self.__shell.recv_ready()==False:\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tbreak\\r\\n\\t\\t\\t\\t\\telse:\\r\\n\\t\\t\\t\\t\\t\\ttime.sleep(0.00005)\\r\\n\\t\\t\\t\\t\\tx += 1\\r\\n\\t\\t\\t\\t\\t# bail out if we\\'ve found the prompt again\\r\\n\\t\\t\\t\\t\\tif re.search(self.__outEnd,self.__output):\\r\\n\\t\\t\\t\\t\\t\\tbailedOut = True\\r\\n\\t\\t\\t\\t\\t\\tbreak\\r\\n\\t\\t\\t\\tif self.__debug:\\r\\n\\t\\t\\t\\t\\teTime = time.time()-sTime\\r\\n\\t\\t\\t\\t\\tprint(\"cNetSSH::send - received \" + str(len(self.__output)) + \" bytes in \" + str(x) + \" cycles and \" + str(eTime) + \"s. BailedOut: \" + str(bailedOut))\\r\\n\\t\\t\\t\\treturn self.__sanitizeOutput(self.__output)\\r\\n\\t\\telse:\\r\\n\\t\\t\\tprint(\"Not connected to \" + self.__host + \". Connect first.\")\\r\\n\\r\\n\\tdef disconnect(self):\\r\\n\\t\\t\"\"\"disconnect\\r\\n\\t\\t\\r\\n\\t\\treturns the configuration as a string\\r\\n\\t\\t\"\"\"\\r\\n\\t\\tself.__conn = None\\r\\n\\r\\n\\tdef __sanitizeOutput(self,output):\\r\\n\\t\\t\"\"\"sanitizeOutput - remove the sent command from output and the prompt\\r\\n\\t\\t\\r\\n\\t\\treturns the configuration as a string\\r\\n\\t\\t\"\"\"\\r\\n\\t\\ttempOut = output.splitlines(True)\\r\\n\\t\\tnewOut  = \"\"\\r\\n\\t\\tfor line in tempOut:\\r\\n\\t\\t\\tif not re.search(\"^\" + self.__outEnd,line) and not re.search(self.__input,line):\\r\\n\\t\\t\\t\\tnewOut += line\\r\\n\\t\\treturn newOut\\r\\n\\r\\n\\tdef __disablePaging(self):\\r\\n\\t\\tif self.__connected:\\r\\n\\t\\t\\tself.__shell.send(\"terminal length 0\\\\n\")\\r\\n\\t\\t\\ttime.sleep(0.25)\\r\\n\\t\\t\\tif self.__shell.recv_ready():\\r\\n\\t\\t\\t\\tself.__output = self.__shell.recv(200)\\r\\n\\t\\telse:\\r\\n\\t\\t\\tprint(\"Not connected to \" + self.__host + \". Connect first.\")\\r\\n\\r\\n\\tdef configure(self):\\r\\n\\t\\t\"\"\"enter configuration mode using configure terminal\\r\\n\\t\\t\\r\\n\\t\\treturns True on success, False on problems\\r\\n\\t\\t\"\"\"\\r\\n\\t\\tif self.__connected:\\r\\n\\t\\t\\tself.__shell.send(\"\\\\nconfigure terminal\\\\n\")\\r\\n\\t\\t\\ttime.sleep(0.25)\\r\\n\\t\\t\\tif self.__shell.recv_ready():\\r\\n\\t\\t\\t\\tself.__output = self.__shell.recv(200)\\r\\n\\t\\t\\t\\t# we expect (config)#\\r\\n\\t\\t\\t\\tif re.search(\"config\\\\)#\",self.__output):\\r\\n\\t\\t\\t\\t\\treturn True\\r\\n\\t\\t\\t\\treturn False\\r\\n\\t\\telse:\\r\\n\\t\\t\\tprint(\"Not connected to \" + self.__host + \". Connect first.\")\\r\\n\\t\\t\\treturn False\\r\\n\\r\\n\\tdef noconfigure(self):\\r\\n\\t\\t\"\"\"leave configuration mode using configure terminal\\r\\n\\t\\t\\r\\n\\t\\treturns True on success, False on problems\\r\\n\\t\\t\"\"\"\\r\\n\\t\\tif self.__connected:\\r\\n\\t\\t\\tself.__shell.send(\"\\\\nend\\\\n\")\\r\\n\\t\\t\\ttime.sleep(0.25)\\r\\n\\t\\t\\tif self.__shell.recv_ready():\\r\\n\\t\\t\\t\\tself.__output = self.__shell.recv(200)\\r\\n\\t\\t\\t\\t# we expect (config)#\\r\\n\\t\\t\\t\\tif re.search(\"#\",self.__output):\\r\\n\\t\\t\\t\\t\\treturn True\\r\\n\\t\\t\\t\\treturn False\\r\\n\\t\\telse:\\r\\n\\t\\t\\tprint(\"Not connected to \" + self.__host + \". Connect first.\")\\r\\n\\t\\t\\treturn False\\r\\n\\r\\nif __name__ == \\'__main__\\':\\r\\n\\tprint(\"This class should only be imported and not run directly!\")\\r\\n',\n",
       " 'from skimage.draw import polygon\\nfrom scipy.spatial import distance\\nimport numpy as np\\nimport cv2\\n\\n\\nclass PicLabeler:\\n    def __init__(self, model, config):\\n        self.model = model\\n        self.slots = config\\n\\n    def run(self, image):\\n\\n        self.image = image\\n        self.image = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)\\n\\n        self.pts2 = np.float32([[0, 60], [0, 0], [40, 0], [40, 60]])\\n        slots = []  # list of preprocessed slot images\\n        ids = []  # list of slot ids\\n\\n        for index, space in enumerate(self.slots):\\n            slot, _ = self.process_slot(space)\\n            ids.append(index + 1)\\n            slots.append(slot)\\n\\n        return self.predict(slots, ids)\\n\\n    def preprocess_coords(self, xs, ys):\\n        distances = []\\n        # calculate all side lengths of the quadrilateral\\n        for i in range(4):\\n            distances.append(\\n                distance.euclidean(\\n                    np.float32([xs[i], ys[i]]),\\n                    np.float32([xs[(i + 1) % 4], ys[(i + 1) % 4]])))\\n        # which one is the longest?\\n        starting_point = np.argmax(np.array(distances))\\n        # rearrange coordinates cyclically, so that longest side goes first\\n        new_xs = xs[starting_point:] + xs[:starting_point]\\n        new_ys = ys[starting_point:] + ys[:starting_point]\\n        return new_xs, new_ys\\n\\n    def predict(self, slots, ids):\\n        answer = {}\\n        if not slots:\\n            print(\"answer empty\")\\n            return answer\\n        # batch_size = 16\\n        # Verbosity mode: 1 = progress bar\\n        pred = self.model.predict(np.array(slots), 16, 1)\\n\\n        # construct a JSON entity with results\\n        pred = pred.ravel().tolist()\\n        for i, one_id in enumerate(ids):\\n            answer[one_id] = \\'Occupied\\' if pred[i] else \\'Empty\\'\\n        return answer\\n\\n    def process_slot(self, space):\\n        xs = []\\n        ys = []\\n        for point in space:\\n            xs.append(point[0])\\n            ys.append(point[1])\\n        # ensure contour is a quadrilateral. This assertion failed once.\\n        assert len(xs) == 4\\n        assert len(ys) == 4\\n        # preprocess and save coordinates\\n        xs, ys = self.preprocess_coords(xs, ys)\\n        xs = np.float32(xs)\\n        ys = np.float32(ys)\\n        coords = np.vstack((xs, ys)).T\\n        # get a matrix for perspective transformation\\n        M = cv2.getPerspectiveTransform(coords, self.pts2)\\n\\n        # transform a quadrilateral into a solid rectangle\\n        dst = cv2.warpPerspective(self.image, M, (40, 60))\\n        # apply the perspective transformation matrix to a slot\\n        # and return as 40x60x1 NumPy array\\n        return np.reshape(dst, (40, 60, 1)), coords\\n',\n",
       " \"# -*- coding: utf-8 -*-\\n# Generated by Django 1.9.1 on 2016-11-14 19:51\\nfrom __future__ import unicode_literals\\n\\nfrom django.db import migrations, models\\n\\n\\nclass Migration(migrations.Migration):\\n\\n    dependencies = [\\n        ('annotation', '0031_auto_20161111_1943'),\\n    ]\\n\\n    operations = [\\n        migrations.AddField(\\n            model_name='masterobservation',\\n            name='observation_time',\\n            field=models.PositiveIntegerField(blank=True, null=True),\\n        ),\\n        migrations.AddField(\\n            model_name='observation',\\n            name='observation_time',\\n            field=models.PositiveIntegerField(blank=True, null=True),\\n        ),\\n    ]\\n\",\n",
       " '# -*- coding: UTF-8 -*-\\n#addonHandler.py\\n#A part of NonVisual Desktop Access (NVDA)\\n#Copyright (C) 2012-2014 Rui Batista, NV Access Limited, Noelia Ruiz Martínez\\n#This file is covered by the GNU General Public License.\\n#See the file COPYING for more details.\\n\\nimport sys\\nimport os.path\\nimport gettext\\nimport glob\\nimport tempfile\\nimport cPickle\\nimport inspect\\nimport itertools\\nimport collections\\nimport pkgutil\\nimport shutil\\nfrom cStringIO import StringIO\\nimport zipfile\\n\\nfrom configobj import ConfigObj, ConfigObjError\\nfrom validate import Validator\\n\\nimport config\\nimport globalVars\\nimport languageHandler\\nfrom logHandler import log\\nimport winKernel\\n\\nMANIFEST_FILENAME = \"manifest.ini\"\\nstateFilename=\"addonsState.pickle\"\\nBUNDLE_EXTENSION = \"nvda-addon\"\\nBUNDLE_MIMETYPE = \"application/x-nvda-addon\"\\nNVDA_ADDON_PROG_ID = \"NVDA.Addon.1\"\\nADDON_PENDINGINSTALL_SUFFIX=\".pendingInstall\"\\nDELETEDIR_SUFFIX=\".delete\"\\n\\nstate={}\\n\\ndef loadState():\\n\\tglobal state\\n\\tstatePath=os.path.join(globalVars.appArgs.configPath,stateFilename)\\n\\ttry:\\n\\t\\tstate = cPickle.load(file(statePath, \"r\"))\\n\\texcept:\\n\\t\\t# Defaults.\\n\\t\\tstate = {\\n\\t\\t\\t\"pendingRemovesSet\":set(),\\n\\t\\t\\t\"pendingInstallsSet\":set(),\\n\\t\\t}\\n\\ndef saveState():\\n\\tstatePath=os.path.join(globalVars.appArgs.configPath,stateFilename)\\n\\ttry:\\n\\t\\tcPickle.dump(state, file(statePath, \"wb\"))\\n\\texcept:\\n\\t\\tlog.debugWarning(\"Error saving state\", exc_info=True)\\n\\ndef getRunningAddons():\\n\\t\"\"\" Returns currently loaded addons.\\n\\t\"\"\"\\n\\treturn (addon for addon in getAvailableAddons() if addon.isRunning)\\n\\ndef completePendingAddonRemoves():\\n\\t\"\"\"Removes any addons that could not be removed on the last run of NVDA\"\"\"\\n\\tuser_addons = os.path.abspath(os.path.join(globalVars.appArgs.configPath, \"addons\"))\\n\\tpendingRemovesSet=state[\\'pendingRemovesSet\\']\\n\\tfor addonName in list(pendingRemovesSet):\\n\\t\\taddonPath=os.path.join(user_addons,addonName)\\n\\t\\tif os.path.isdir(addonPath):\\n\\t\\t\\taddon=Addon(addonPath)\\n\\t\\t\\ttry:\\n\\t\\t\\t\\taddon.completeRemove()\\n\\t\\t\\texcept RuntimeError:\\n\\t\\t\\t\\tlog.exception(\"Failed to remove %s add-on\"%addonName)\\n\\t\\t\\t\\tcontinue\\n\\t\\tpendingRemovesSet.discard(addonName)\\n\\ndef completePendingAddonInstalls():\\n\\tuser_addons = os.path.abspath(os.path.join(globalVars.appArgs.configPath, \"addons\"))\\n\\tpendingInstallsSet=state[\\'pendingInstallsSet\\']\\n\\tfor addonName in pendingInstallsSet:\\n\\t\\tnewPath=os.path.join(user_addons,addonName)\\n\\t\\toldPath=newPath+ADDON_PENDINGINSTALL_SUFFIX\\n\\t\\ttry:\\n\\t\\t\\tos.rename(oldPath,newPath)\\n\\t\\texcept:\\n\\t\\t\\tlog.error(\"Failed to complete addon installation for %s\"%addonName,exc_info=True)\\n\\tpendingInstallsSet.clear()\\n\\ndef removeFailedDeletions():\\n\\tuser_addons = os.path.abspath(os.path.join(globalVars.appArgs.configPath, \"addons\"))\\n\\tfor p in os.listdir(user_addons):\\n\\t\\tif p.endswith(DELETEDIR_SUFFIX):\\n\\t\\t\\tpath=os.path.join(user_addons,p)\\n\\t\\t\\tshutil.rmtree(path,ignore_errors=True)\\n\\t\\t\\tif os.path.exists(path):\\n\\t\\t\\t\\tlog.error(\"Failed to delete path %s, try removing manually\"%path)\\n\\ndef initialize():\\n\\t\"\"\" Initializes the add-ons subsystem. \"\"\"\\n\\tloadState()\\n\\tremoveFailedDeletions()\\n\\tcompletePendingAddonRemoves()\\n\\tcompletePendingAddonInstalls()\\n\\tsaveState()\\n\\tgetAvailableAddons(refresh=True)\\n\\ndef terminate():\\n\\t\"\"\" Terminates the add-ons subsystem. \"\"\"\\n\\tpass\\n\\ndef _getDefaultAddonPaths():\\n\\t\"\"\" Returns paths where addons can be found.\\n\\tFor now, only <userConfig\\\\addons is supported.\\n\\t@rtype: list(string)\\n\\t\"\"\"\\n\\taddon_paths = []\\n\\tuser_addons = os.path.abspath(os.path.join(globalVars.appArgs.configPath, \"addons\"))\\n\\tif os.path.isdir(user_addons):\\n\\t\\taddon_paths.append(user_addons)\\n\\treturn addon_paths\\n\\ndef _getAvailableAddonsFromPath(path):\\n\\t\"\"\" Gets available add-ons from path.\\n\\tAn addon is only considered available if the manifest file is loaded with no errors.\\n\\t@param path: path from where to find addon directories.\\n\\t@type path: string\\n\\t@rtype generator of Addon instances\\n\\t\"\"\"\\n\\tlog.debug(\"Listing add-ons from %s\", path)\\n\\tfor p in os.listdir(path):\\n\\t\\tif p.endswith(DELETEDIR_SUFFIX): continue\\n\\t\\taddon_path = os.path.join(path, p)\\n\\t\\tif os.path.isdir(addon_path) and addon_path not in (\\'.\\', \\'..\\'):\\n\\t\\t\\tlog.debug(\"Loading add-on from %s\", addon_path)\\n\\t\\t\\ttry:\\n\\t\\t\\t\\ta = Addon(addon_path)\\n\\t\\t\\t\\tlog.debug(\"Found add-on %s\", a.manifest[\\'name\\'])\\n\\t\\t\\t\\tyield a\\n\\t\\t\\texcept:\\n\\t\\t\\t\\tlog.error(\"Error loading Addon from path: %s\", addon_path, exc_info=True)\\n\\n_availableAddons = collections.OrderedDict()\\ndef getAvailableAddons(refresh=False):\\n\\t\"\"\" Gets all available addons on the system.\\n\\t@rtype generator of Addon instances.\\n\\t\"\"\"\\n\\tif refresh:\\n\\t\\t_availableAddons.clear()\\n\\t\\tgenerators = [_getAvailableAddonsFromPath(path) for path in _getDefaultAddonPaths()]\\n\\t\\tfor addon in itertools.chain(*generators):\\n\\t\\t\\t_availableAddons[addon.path] = addon\\n\\treturn _availableAddons.itervalues()\\n\\ndef installAddonBundle(bundle):\\n\\t\"\"\"Extracts an Addon bundle in to a unique subdirectory of the user addons directory, marking the addon as needing install completion on NVDA restart.\"\"\"\\n\\taddonPath = os.path.join(globalVars.appArgs.configPath, \"addons\",bundle.manifest[\\'name\\']+ADDON_PENDINGINSTALL_SUFFIX)\\n\\tbundle.extract(addonPath)\\n\\taddon=Addon(addonPath)\\n\\t# #2715: The add-on must be added to _availableAddons here so that\\n\\t# translations can be used in installTasks module.\\n\\t_availableAddons[addon.path]=addon\\n\\ttry:\\n\\t\\taddon.runInstallTask(\"onInstall\")\\n\\texcept:\\n\\t\\tlog.error(\"task \\'onInstall\\' on addon \\'%s\\' failed\"%addon.name,exc_info=True)\\n\\t\\tdel _availableAddons[addon.path]\\n\\t\\taddon.completeRemove(runUninstallTask=False)\\n\\t\\traise AddonError(\"Installation failed\")\\n\\tstate[\\'pendingInstallsSet\\'].add(bundle.manifest[\\'name\\'])\\n\\tsaveState()\\n\\treturn addon\\n\\nclass AddonError(Exception):\\n\\t\"\"\" Represents an exception coming from the addon subsystem. \"\"\"\\n\\n\\nclass Addon(object):\\n\\t\"\"\" Represents an Add-on available on the file system.\"\"\"\\n\\tdef __init__(self, path):\\n\\t\\t\"\"\" Constructs an L[Addon} from.\\n\\t\\t@param path: the base directory for the addon data.\\n\\t\\t@type path: string\\n\\t\\t\"\"\"\\n\\t\\tself.path = os.path.abspath(path)\\n\\t\\tself._extendedPackages = set()\\n\\t\\tself._isLoaded = False\\n\\t\\tmanifest_path = os.path.join(path, MANIFEST_FILENAME)\\n\\t\\twith open(manifest_path) as f:\\n\\t\\t\\ttranslatedInput = None\\n\\t\\t\\tfor translatedPath in _translatedManifestPaths():\\n\\t\\t\\t\\tp = os.path.join(self.path, translatedPath)\\n\\t\\t\\t\\tif os.path.exists(p):\\n\\t\\t\\t\\t\\tlog.debug(\"Using manifest translation from %s\", p)\\n\\t\\t\\t\\t\\ttranslatedInput = open(p, \\'r\\')\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\tself.manifest = AddonManifest(f, translatedInput)\\n\\n\\t@property\\n\\tdef isPendingInstall(self):\\n\\t\\t\"\"\"True if this addon has not yet been fully installed.\"\"\"\\n\\t\\treturn self.path.endswith(ADDON_PENDINGINSTALL_SUFFIX)\\n\\n\\t@property\\n\\tdef isPendingRemove(self):\\n\\t\\t\"\"\"True if this addon is marked for removal.\"\"\"\\n\\t\\treturn not self.isPendingInstall and self.name in state[\\'pendingRemovesSet\\']\\n\\n\\tdef requestRemove(self):\\n\\t\\t\"\"\"Markes this addon for removal on NVDA restart.\"\"\"\\n\\t\\tif self.isPendingInstall:\\n\\t\\t\\tself.completeRemove()\\n\\t\\t\\tstate[\\'pendingInstallsSet\\'].discard(self.name)\\n\\t\\t\\t#Force availableAddons to be updated\\n\\t\\t\\tgetAvailableAddons(refresh=True)\\n\\t\\telse:\\n\\t\\t\\tstate[\\'pendingRemovesSet\\'].add(self.name)\\n\\t\\tsaveState()\\n\\n\\tdef completeRemove(self,runUninstallTask=True):\\n\\t\\tif runUninstallTask:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\t# #2715: The add-on must be added to _availableAddons here so that\\n\\t\\t\\t\\t# translations can be used in installTasks module.\\n\\t\\t\\t\\t_availableAddons[self.path] = self\\n\\t\\t\\t\\tself.runInstallTask(\"onUninstall\")\\n\\t\\t\\texcept:\\n\\t\\t\\t\\tlog.error(\"task \\'onUninstall\\' on addon \\'%s\\' failed\"%self.name,exc_info=True)\\n\\t\\t\\tfinally:\\n\\t\\t\\t\\tdel _availableAddons[self.path]\\n\\t\\ttempPath=tempfile.mktemp(suffix=DELETEDIR_SUFFIX,dir=os.path.dirname(self.path))\\n\\t\\ttry:\\n\\t\\t\\tos.rename(self.path,tempPath)\\n\\t\\texcept (WindowsError,IOError):\\n\\t\\t\\traise RuntimeError(\"Cannot rename add-on path for deletion\")\\n\\t\\tshutil.rmtree(tempPath,ignore_errors=True)\\n\\t\\tif os.path.exists(tempPath):\\n\\t\\t\\tlog.error(\"Error removing addon directory %s, deferring until next NVDA restart\"%self.path)\\n\\n\\t@property\\n\\tdef name(self):\\n\\t\\treturn self.manifest[\\'name\\']\\n\\n\\tdef addToPackagePath(self, package):\\n\\t\\t\"\"\" Adds this L{Addon} extensions to the specific package path if those exist.\\n\\t\\t@param package: the python module representing the package.\\n\\t\\t@type package: python module.\\n\\t\\t\"\"\"\\n\\t\\textension_path = os.path.join(self.path, package.__name__)\\n\\t\\tif not os.path.isdir(extension_path):\\n\\t\\t\\t# This addon does not have extension points for this package\\n\\t\\t\\treturn\\n\\t\\t# Python 2.x doesn\\'t properly handle unicode import paths, so convert them before adding.\\n\\t\\tconverted_path = self._getPathForInclusionInPackage(package)\\n\\t\\tpackage.__path__.insert(0, converted_path)\\n\\t\\tself._extendedPackages.add(package)\\n\\t\\tlog.debug(\"Addon %s added to %s package path\", self.manifest[\\'name\\'], package.__name__)\\n\\n\\t@property\\n\\tdef isRunning(self):\\n\\t\\treturn not self.isPendingInstall\\n\\n\\tdef _getPathForInclusionInPackage(self, package):\\n\\t\\textension_path = os.path.join(self.path, package.__name__)\\n\\t\\treturn extension_path.encode(\"mbcs\")\\n\\n\\tdef loadModule(self, name):\\n\\t\\t\"\"\" loads a python module from the addon directory\\n\\t\\t@param name: the module name\\n\\t\\t@type name: string\\n\\t\\t@returns the python module with C[name}\\n\\t\\t@rtype python module\\n\\t\\t\"\"\"\\n\\t\\tlog.debug(\"Importing module %s from plugin %s\", name, self.name)\\n\\t\\timporter = pkgutil.ImpImporter(self.path)\\n\\t\\tloader = importer.find_module(name)\\n\\t\\tif not loader:\\n\\t\\t\\treturn None\\n\\t\\t# Create a qualified full name to avoid modules with the same name on sys.modules.\\n\\t\\tfullname = \"addons.%s.%s\" % (self.name, name)\\n\\t\\ttry:\\n\\t\\t\\treturn loader.load_module(fullname)\\n\\t\\texcept ImportError:\\n\\t\\t\\t# in this case return None, any other error throw to be handled elsewhere\\n\\t\\t\\treturn None\\n\\n\\tdef getTranslationsInstance(self, domain=\\'nvda\\'):\\n\\t\\t\"\"\" Gets the gettext translation instance for this addon.\\n\\t\\t<addon-path<\\\\locale will be used to find .mo files, if exists.\\n\\t\\tIf a translation file is not found the default fallback null translation is returned.\\n\\t\\t@param domain: the tranlation domain to retrieve. The \\'nvda\\' default should be used in most cases.\\n\\t\\t@returns: the gettext translation class.\\n\\t\\t\"\"\"\\n\\t\\tlocaledir = os.path.join(self.path, \"locale\")\\n\\t\\treturn gettext.translation(domain, localedir=localedir, languages=[languageHandler.getLanguage()], fallback=True)\\n\\n\\tdef runInstallTask(self,taskName,*args,**kwargs):\\n\\t\\t\"\"\"\\n\\t\\tExecutes the function having the given taskName with the given args and kwargs in the addon\\'s installTasks module if it exists.\\n\\t\\t\"\"\"\\n\\t\\tif not hasattr(self,\\'_installTasksModule\\'):\\n\\t\\t\\tself._installTasksModule=self.loadModule(\\'installTasks\\')\\n\\t\\tif self._installTasksModule:\\n\\t\\t\\tfunc=getattr(self._installTasksModule,taskName,None)\\n\\t\\t\\tif func:\\n\\t\\t\\t\\tfunc(*args,**kwargs)\\n\\n\\tdef getDocFilePath(self, fileName=None):\\n\\t\\t\"\"\"Get the path to a documentation file for this add-on.\\n\\t\\tThe file should be located in C{doc\\\\lang\\\\file} inside the add-on,\\n\\t\\twhere C{lang} is the language code and C{file} is the requested file name.\\n\\t\\tFailing that, the language without country is tried.\\n\\t\\tEnglish is tried as a last resort.\\n\\t\\tAn add-on can specify a default documentation file name\\n\\t\\tvia the docFileName parameter in its manifest.\\n\\t\\t@param fileName: The requested file name or C{None} for the add-on\\'s default.\\n\\t\\t@type fileName: basestring\\n\\t\\t@return: The path to the requested file or C{None} if it wasn\\'t found.\\n\\t\\t@rtype: basestring\\n\\t\\t\"\"\"\\n\\t\\tif not fileName:\\n\\t\\t\\tfileName = self.manifest[\"docFileName\"]\\n\\t\\t\\tif not fileName:\\n\\t\\t\\t\\treturn None\\n\\t\\tdocRoot = os.path.join(self.path, \"doc\")\\n\\t\\tlang = languageHandler.getLanguage()\\n\\t\\tlangs = [lang]\\n\\t\\tif \"_\" in lang:\\n\\t\\t\\tlang = lang.split(\"_\", 1)[0]\\n\\t\\t\\tlangs.append(lang)\\n\\t\\tif lang != \"en\":\\n\\t\\t\\tlangs.append(\"en\")\\n\\t\\tfor lang in langs:\\n\\t\\t\\tdocFile = os.path.join(docRoot, lang, fileName)\\n\\t\\t\\tif os.path.isfile(docFile):\\n\\t\\t\\t\\treturn docFile\\n\\t\\treturn None\\n\\ndef getCodeAddon(obj=None, frameDist=1):\\n\\t\"\"\" Returns the L{Addon} where C{obj} is defined. If obj is None the caller code frame is assumed to allow simple retrieval of \"current calling addon\".\\n\\t@param obj: python object or None for default behaviour.\\n\\t@param frameDist: howmany frames is the caller code. Only change this for functions in this module.\\n\\t@return: L{Addon} instance or None if no code does not belong to a add-on package.\\n\\t@rtype: C{Addon}\\n\\t\"\"\"\\n\\tglobal _availableAddons\\n\\tif obj is None:\\n\\t\\tobj = sys._getframe(frameDist)\\n\\tfileName  = inspect.getfile(obj)\\n\\tdir= unicode(os.path.abspath(os.path.dirname(fileName)), \"mbcs\")\\n\\t# if fileName is not a subdir of one of the addon paths\\n\\t# It does not belong to an addon.\\n\\tfor p in _getDefaultAddonPaths():\\n\\t\\tif dir.startswith(p):\\n\\t\\t\\tbreak\\n\\telse:\\n\\t\\traise AddonError(\"Code does not belong to an addon package.\")\\n\\tcurdir = dir\\n\\twhile curdir not in _getDefaultAddonPaths():\\n\\t\\tif curdir in _availableAddons.keys():\\n\\t\\t\\treturn _availableAddons[curdir]\\n\\t\\tcurdir = os.path.abspath(os.path.join(curdir, \"..\"))\\n\\t# Not found!\\n\\traise AddonError(\"Code does not belong to an addon\")\\n\\ndef initTranslation():\\n\\taddon = getCodeAddon(frameDist=2)\\n\\ttranslations = addon.getTranslationsInstance()\\n\\t# Point _ to the translation object in the globals namespace of the caller frame\\n\\t# FIXME: shall we retrieve the caller module object explicitly?\\n\\ttry:\\n\\t\\tcallerFrame = inspect.currentframe().f_back\\n\\t\\tcallerFrame.f_globals[\\'_\\'] = translations.ugettext\\n\\t\\t# Install our pgettext function.\\n\\t\\tcallerFrame.f_globals[\\'pgettext\\'] = languageHandler.makePgettext(translations)\\n\\tfinally:\\n\\t\\tdel callerFrame # Avoid reference problems with frames (per python docs)\\n\\ndef _translatedManifestPaths(lang=None, forBundle=False):\\n\\tif lang is None:\\n\\t\\tlang = languageHandler.getLanguage() # can\\'t rely on default keyword arguments here.\\n\\tlangs=[lang]\\n\\tif \\'_\\' in lang:\\n\\t\\tlangs.append(lang.split(\\'_\\')[0])\\n\\t\\tif lang!=\\'en\\' and not lang.startswith(\\'en_\\'):\\n\\t\\t\\tlangs.append(\\'en\\')\\n\\tsep = \"/\" if forBundle else os.path.sep\\n\\treturn [sep.join((\"locale\", lang, MANIFEST_FILENAME)) for lang in langs]\\n\\n\\nclass AddonBundle(object):\\n\\t\"\"\" Represents the contents of an NVDA addon suitable for distribution.\\n\\tThe bundle is compressed using the zip file format. Manifest information\\n\\tis available without the need for extraction.\"\"\"\\n\\tdef __init__(self, bundlePath):\\n\\t\\t\"\"\" Constructs an L{AddonBundle} from a filename.\\n\\t\\t@param bundlePath: The path for the bundle file.\\n\\t\\t\"\"\"\\n\\t\\tself._path = bundlePath if isinstance(bundlePath, unicode) else unicode(bundlePath, \"mbcs\")\\n\\t\\t# Read manifest:\\n\\t\\ttranslatedInput=None\\n\\t\\twith zipfile.ZipFile(self._path, \\'r\\') as z:\\n\\t\\t\\tfor translationPath in _translatedManifestPaths(forBundle=True):\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\ttranslatedInput = z.open(translationPath, \\'r\\')\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\texcept KeyError:\\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\tself._manifest = AddonManifest(z.open(MANIFEST_FILENAME), translatedInput=translatedInput)\\n\\n\\tdef extract(self, addonPath):\\n\\t\\t\"\"\" Extracts the bundle content to the specified path.\\n\\t\\tThe addon will be extracted to L{addonPath}\\n\\t\\t@param addonPath: Path where to extract contents.\\n\\t\\t@type addonPath: string\\n\\t\\t\"\"\"\\n\\t\\twith zipfile.ZipFile(self._path, \\'r\\') as z:\\n\\t\\t\\tfor info in z.infolist():\\n\\t\\t\\t\\tif isinstance(info.filename, str):\\n\\t\\t\\t\\t\\t# #2505: Handle non-Unicode file names.\\n\\t\\t\\t\\t\\t# Most archivers seem to use the local OEM code page, even though the spec says only cp437.\\n\\t\\t\\t\\t\\t# HACK: Overriding info.filename is a bit ugly, but it avoids a lot of code duplication.\\n\\t\\t\\t\\t\\tinfo.filename = info.filename.decode(\"cp%d\" % winKernel.kernel32.GetOEMCP())\\n\\t\\t\\t\\tz.extract(info, addonPath)\\n\\n\\t@property\\n\\tdef manifest(self):\\n\\t\\t\"\"\" Gets the manifest for the represented Addon.\\n\\t\\t@rtype: AddonManifest\\n\\t\\t\"\"\"\\n\\t\\treturn self._manifest\\n\\n\\tdef __repr__(self):\\n\\t\\treturn \"<AddonBundle at %s>\" % self._path\\n\\ndef createAddonBundleFromPath(path, destDir=None):\\n\\t\"\"\" Creates a bundle from a directory that contains a a addon manifest file.\"\"\"\\n\\tbasedir = os.path.abspath(path)\\n\\t# If  caller did not provide a destination directory name\\n\\t# Put the bundle at the same level of the addon\\'s top directory,\\n\\t# That is, basedir/..\\n\\tif destDir is None:\\n\\t\\tdestDir = os.path.dirname(basedir)\\n\\tmanifest_path = os.path.join(basedir, MANIFEST_FILENAME)\\n\\tif not os.path.isfile(manifest_path):\\n\\t\\traise AddonError(\"Can\\'t find %s manifest file.\" % manifest_path)\\n\\twith open(manifest_path) as f:\\n\\t\\tmanifest = AddonManifest(f)\\n\\tif manifest.errors is not None:\\n\\t\\t_report_manifest_errors(manifest)\\n\\t\\traise AddonError(\"Manifest file as errors.\")\\n\\tbundleFilename = \"%s-%s.%s\" % (manifest[\\'name\\'], manifest[\\'version\\'], BUNDLE_EXTENSION)\\n\\tbundleDestination = os.path.join(destDir, bundleFilename)\\n\\twith zipfile.ZipFile(bundleDestination, \\'w\\') as z:\\n\\t\\t# FIXME: the include/exclude feature may or may not be useful. Also python files can be pre-compiled.\\n\\t\\tfor dir, dirnames, filenames in os.walk(basedir):\\n\\t\\t\\trelativePath = os.path.relpath(dir, basedir)\\n\\t\\t\\tfor filename in filenames:\\n\\t\\t\\t\\tpathInBundle = os.path.join(relativePath, filename)\\n\\t\\t\\t\\tabsPath = os.path.join(dir, filename)\\n\\t\\t\\t\\tz.write(absPath, pathInBundle)\\n\\treturn AddonBundle(bundleDestination)\\n\\n\\ndef _report_manifest_errors(manifest):\\n\\tlog.warning(\"Error loading manifest:\\\\n%s\", manifest.errors)\\n\\nclass AddonManifest(ConfigObj):\\n\\t\"\"\" Add-on manifest file. It contains metadata about an NVDA add-on package. \"\"\"\\n\\tconfigspec = ConfigObj(StringIO(\\n\\t\"\"\"\\n# NVDA Add-on Manifest configuration specification\\n# Add-on unique name\\nname = string()\\n# short  summary (label) of the add-on to show to users.\\nsummary = string()\\n# Long description with further information and instructions\\ndescription = string(default=None)\\n# Name of the author or entity that created the add-on\\nauthor = string()\\n# Version of the add-on. Should preferably in some standard format such as x.y.z\\nversion = string()\\n# URL for more information about the add-on. New versions and such.\\nurl= string(default=None)\\n# Name of default documentation file for the add-on.\\ndocFileName = string(default=None)\\n\\n\"\"\"))\\n\\n\\tdef __init__(self, input, translatedInput=None):\\n\\t\\t\"\"\" Constructs an L{AddonManifest} instance from manifest string data\\n\\t\\t@param input: data to read the manifest informatinon\\n\\t\\t@type input: a fie-like object.\\n\\t\\t@param translatedInput: translated manifest input\\n\\t\\t@type translatedInput: file-like object\\n\\t\\t\"\"\"\\n\\t\\tsuper(AddonManifest, self).__init__(input, configspec=self.configspec, encoding=\\'utf-8\\', default_encoding=\\'utf-8\\')\\n\\t\\tself._errors = []\\n\\t\\tval = Validator()\\n\\t\\tresult = self.validate(val, copy=True, preserve_errors=True)\\n\\t\\tif result != True:\\n\\t\\t\\tself._errors = result\\n\\t\\tself._translatedConfig = None\\n\\t\\tif translatedInput is not None:\\n\\t\\t\\tself._translatedConfig = ConfigObj(translatedInput, encoding=\\'utf-8\\', default_encoding=\\'utf-8\\')\\n\\t\\t\\tfor k in (\\'summary\\',\\'description\\'):\\n\\t\\t\\t\\tval=self._translatedConfig.get(k)\\n\\t\\t\\t\\tif val:\\n\\t\\t\\t\\t\\tself[k]=val\\n\\n\\t@property\\n\\tdef errors(self):\\n\\t\\treturn self._errors\\n',\n",
       " 'from .entry_storage import EntryOperations\\nfrom .functions import batch_list\\n',\n",
       " 'import gc\\nimport hashlib\\nimport itertools\\nimport logging\\nimport math\\nimport sys\\nimport traceback\\nfrom logging import warning, debug\\n\\nimport numpy as np\\nfrom pulp import LpProblem, LpMinimize, LpVariable, LpInteger, CPLEX, LpStatus\\n\\nfrom src.dbms.utils import sql_get_all_attributes, sql_table_column_data_type\\nfrom src.paql.constraints import *\\nfrom src.paql.expression_trees.expression_trees import ArithmeticExpression\\nfrom src.paql.expression_trees.syntax_tree import Expression\\nfrom src.paql.objectives import *\\nfrom src.utils.utils import op_to_opstr\\n\\n\\n\\nclass NotPackageQueryException(Exception):\\n    pass\\n\\n\\nclass PaQLParserError(Exception):\\n    pass\\n\\n\\n\\n\\n\\n\\nclass PackageQuery(object):\\n    allowed_dbms_data_types = {\\n        \"integer\",\\n        \"bigint\",\\n        \"double precision\",\\n        # \"numeric\",\\n        # \"numeric(15,2)\"\\n    }\\n\\n    @property\\n    def table_name(self):\\n        assert len(self.rel_namespace.values()) == 1\\n        return self.rel_namespace.itervalues().next()\\n\\n\\n    @table_name.setter\\n    def table_name(self, table_name):\\n        assert len(self.rel_namespace.values()) == 1\\n        if self.table_name is not None and self.rel_namespace is not None:\\n            for rel, relname in self.rel_namespace.iteritems():\\n                if relname.lower() == self.table_name.lower():\\n                    self.rel_namespace[rel] = table_name\\n\\n        self._paql_query_str_stale = True\\n\\n\\n    @property\\n    def bc_query(self):\\n        bc_query = \"SELECT * FROM {}\".format(\\n            \\',\\'.join([\\n                rel_name + \" \" + rel_alias for rel_alias, rel_name in self.rel_namespace.iteritems()\\n            ]))\\n        where_clause_str = self.where_expr.get_str()\\n        if where_clause_str:\\n            bc_query += \" WHERE {}\".format(where_clause_str)\\n        if self.limit is not None and self.limit[\"TYPE\"] ==\"INPUT\":\\n            bc_query += \" LIMIT {}\".format(self.limit[\"LIMIT\"])\\n        return bc_query\\n\\n\\n    def __init__(self, d):\\n        assert isinstance(d, dict)\\n\\n        self._paql_query_str = None\\n        self._paql_query_str_stale = True\\n\\n        # self.package_rel_names = d[\"package rel names\"]\\n        self.rel_namespace = d[\"namespace\"]\\n        self.rel_repeats = d[\"repeats\"]\\n        self.where_expr = d[\"where expr\"]\\n        self.such_that_expr = d[\"such that expr\"]\\n\\n        if d[\"objective expr\"] is not None:\\n            self.objective = PackageQueryObjective(\\n                sqlquery_expr=d[\"objective expr\"].get_sql_arithmetic_expression(),\\n                sense=d[\"objective sense\"])\\n        else:\\n            self.objective = None\\n\\n        self.limit = d[\"limit\"]\\n\\n        # NOTE: For now, assuming that the query is single-table.\\n        # TODO: We need to take into account REPEAT! It\\'s not implemented yet!\\n        # rel_names = self.rel_namespace.values()\\n        assert len(self.rel_namespace.values()) == 1, \"Not a single-table package query!\"\\n        # self.table_name = self.bc_query.lower().split(\"from\")[1].split(\"where\")[0].split()[0].strip()\\n        # self.table_name = rel_names[0]\\n\\n\\n    def __str__(self):\\n        raise NotImplementedError\\n\\n\\n    def md5(self):\\n        return hashlib.md5(str(self)).hexdigest()\\n\\n\\n    @classmethod\\n    def get_json_from_paql(cls, paql_str):\\n        from subprocess import Popen, PIPE\\n\\n        p = Popen([\"PaQL_Parser\"], stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)\\n        json_str, err = p.communicate(input=paql_str)\\n        p.wait()\\n\\n        if err != \"\":\\n            raise PaQLParserError(err)\\n\\n        return json_str\\n\\n\\n    @classmethod\\n    def from_paql(cls, paql_str):\\n        \"\"\"\\n        Returns a new PackageQuery object from a PaQL query string. This is the method that you would call more often.\\n        :param paql_str: A string containing a PaQL query\\n        :rtype : PackageQuery\\n        \"\"\"\\n        json_str = PackageQuery.get_json_from_paql(paql_str)\\n\\n        try:\\n            package_query = cls.from_json(json_str)\\n        except ValueError as e:\\n            traceback.print_exc(file=sys.stdout)\\n            raise PaQLParserError(e)\\n        else:\\n            package_query._paql_query_str = paql_str\\n            package_query._paql_query_str_stale = False\\n            return package_query\\n        finally:\\n            gc.collect()\\n\\n\\n    @classmethod\\n    def from_json(cls, json_str):\\n        \"\"\"\\n        Returns a new PackageQuery object from a JSON string. This method is usually called by from_PaQL() to\\n        transform the paql parser output (which is a JSON) into a PackageQuery object.\\n        This is the main entry point from the direct output of the paql parser.\\n        :param json_str: A string containing a JSON structure for a parsed PaQL query\\n        \"\"\"\\n        import json\\n\\n        q = json.loads(json_str)\\n\\n        # The namespace of relations defined by the query. A dictionary alias -> relation-name.\\n        # This way, all references to relations can be just made based on the alias names, and we can avoid confusion\\n        # when nested queries contain the same relation names, etc.\\n        rel_namespace = { }\\n\\n        # The mapping from relation aliases into their corresponding REPEAT values.\\n        rel_repeats = { }\\n\\n        # The list of relation aliases which form the PACKAGE.\\n        package_rel_names = []\\n\\n        # TODO: Ideally, if the query is not a package query we may want to just execute it as it is...\\n        # TODO: If it doesn\\'t contain the PACKAGE clause, we should make sure it does not contain SUCH THAT either.\\n\\n        # Check if it\\'s package query and store reference to relation names\\n        for select_item in q[\"SELECT\"]:\\n            assert type(select_item) == dict\\n\\n            if select_item[\"NODE_TYPE\"] == \"*\":\\n                raise NotPackageQueryException()\\n\\n            elif select_item[\"NODE_TYPE\"] == \"COL_REF\":\\n                raise NotPackageQueryException()\\n\\n            elif select_item[\"NODE_TYPE\"] == \"PACKAGE\":\\n                package_rel_names.extend(r[\"REL_NAME\"] for r in select_item[\"PACKAGE_RELS\"])\\n\\n            else:\\n                raise Exception(\"Problem in SELECT clause, NODE_TYPE non recognized: \" + select_item[\"NODE_TYPE\"])\\n\\n        # Store relation names and aliases, and repeat constraint for each of them\\n        # These are stored in a dictionary rel_namespace(key=rel_alias, val=rel_names)\\n        for from_ in q[\"FROM\"]:\\n            assert type(from_) == dict\\n            rel_name = from_[\"REL_NAME\"]\\n            rel_alias = from_.get(\"REL_ALIAS\", rel_name)\\n            repeat = from_.get(\"REPEAT\", -1)\\n            rel_namespace[rel_alias] = rel_name\\n            rel_repeats[rel_alias] = repeat\\n\\n        # Make sure that all relation aliases referred in PACKAGE(...) are in the FROM clause as well\\n        assert all(p_rel_name in rel_namespace for p_rel_name in package_rel_names)\\n\\n        # Stricter (for now): Make sure that they are exactly the same relation references\\n        assert set(package_rel_names) == set(rel_namespace.iterkeys())\\n\\n        # Create WHERE clause expression tree\\n        where_clause = Expression(q[\"WHERE\"])\\n\\n        # Create SUCH THAT clause expression tree\\n        such_that_clause = Expression(q[\"SUCH-THAT\"])\\n\\n        # Create objective clause expression tree\\n        if q[\"OBJECTIVE\"] is not None:\\n            objective_expr = Expression(q[\"OBJECTIVE\"][\"EXPR\"])\\n\\n            if q[\"OBJECTIVE\"][\"TYPE\"] == \"MAXIMIZE\":\\n                # objective = { \"type\": \"maximize\", \"expr\": objective_expr }\\n                objective_sense = ObjectiveSenseMAX()\\n\\n            elif q[\"OBJECTIVE\"][\"TYPE\"] == \"MINIMIZE\":\\n                # objective = { \"type\": \"minimize\", \"expr\": objective_expr }\\n                objective_sense = ObjectiveSenseMIN()\\n\\n            else:\\n                raise Exception(\"Unsupported objective type: `{}\\'\".format(q[\"OBJECTIVE\"][\"TYPE\"]))\\n\\n        else:\\n            objective_expr = objective_sense = None\\n\\n        query_dict = {\\n            # \"package rel names\": package_rel_names,\\n            \"where expr\": where_clause,\\n            \"such that expr\": such_that_clause,\\n            \"objective expr\": objective_expr,\\n            \"objective sense\": objective_sense,\\n            \"namespace\": rel_namespace,\\n            \"repeats\": rel_repeats,\\n            \"limit\": q[\"LIMIT\"],\\n        }\\n\\n        if such_that_clause.is_conjunctive() and where_clause.is_conjunctive():\\n            return ConjunctivePackageQuery(query_dict)\\n        else:\\n            return cls(query_dict)\\n\\n\\n    @staticmethod\\n    def from_uncoalesced_constraints(table_name, unc_bcs, unc_gcs, objective):\\n        \"\"\"\\n        This method creates a new PackageQuery from sets of uncoalesced constraints and an objective.\\n        \"\"\"\\n        bc_query = \"SELECT * FROM {} {}\".format(table_name, \"WHERE true\" if len(unc_bcs) > 0 else \"\")\\n        for attr, op, n in unc_bcs:\\n            bc_query += \" AND {a} {o} {b}\".format(a=attr, o=op_to_opstr(op), b=n)\\n\\n        gc_queries = []\\n        gc_ranges = []\\n        for (aggr, attr), op, n in unc_gcs:\\n            gc_query = \"SELECT {aggr}({attr}) FROM memory_representations\".format(aggr=aggr, attr=attr)\\n            if op == operator.le:\\n                # gc_range = (-sys.maxint, n)\\n                gc_range = (-float(\"inf\"), n)\\n            elif op == operator.ge:\\n                # gc_range = (n, sys.maxint)\\n                gc_range = (n, float(\"inf\"))\\n            elif op == operator.eq:\\n                gc_range = (n, n)\\n            else:\\n                raise Exception(\"Operator \\'{}\\' not supported yet.\".format(op))\\n            gc_queries.append(gc_query)\\n            gc_ranges.append(gc_range)\\n\\n        return PackageQuery({\\n            \"bc\": bc_query,\\n            \"gc\": map(lambda x: (x[0], x[1][0], x[1][1]), zip(gc_queries, gc_ranges)),\\n            \"objective\": objective,\\n        })\\n\\n\\n    def get_objective_attributes(self):\\n        attrs = set()\\n        if self.objective is not None:\\n            for attr in self.objective.get_attributes():\\n                if attr != \"*\":\\n                    attrs.add(attr)\\n        return attrs\\n\\n\\n    def get_bcs_attributes(self):\\n        return set(attr for attr in self.coalesced_bcs) - {\"*\"}\\n\\n\\n    def get_gcs_attributes(self):\\n        gcs_attrs = set()\\n        for gc in self.coalesced_gcs:\\n            assert isinstance(gc, CGlobalConstraint)\\n\\n            gcs_attrs.update(gc.get_attributes())\\n\\n        return gcs_attrs\\n\\n\\n    def get_attributes(self):\\n        # FIXME: If this is a relaxed query, you should return all attributes including those of the original query.\\n        return self.get_bcs_attributes() | self.get_gcs_attributes() | self.get_objective_attributes()\\n\\n\\n    def get_data_attributes(self, db):\\n        all_data_attributes = sql_get_all_attributes(db, self.table_name)\\n\\n        # Only pick the data attributes of the allowed data type\\n        data_attributes = set()\\n        for data_attr in all_data_attributes:\\n            attribute_type = sql_table_column_data_type(db, self.table_name, data_attr)\\n            if attribute_type in self.allowed_dbms_data_types:\\n                data_attributes.add(data_attr)\\n        return sorted(data_attributes)\\n\\n\\n\\n    def get_paql_str(self, redo=False, recompute_gcs=True, coalesced=False):\\n        raise NotImplementedError\\n\\n\\n    def abs_ugc_errors(self, gc_scores, attrs=None):\\n        \"\"\"\\n        Returns absolute errors for each (uncoalesced) global constraint.\\n        \"\"\"\\n        if attrs is None:\\n            use_attrs = self.get_attributes()\\n        else:\\n            use_attrs = set(attrs)\\n\\n        return {\\n            (aggr, attr): max(0, c - gc_scores[aggr, attr] if op == operator.ge else gc_scores[aggr, attr] - c)\\n            for (aggr, attr), op, c in self.uncoalesced_gcs if attr == \"*\" or attr in use_attrs\\n        }\\n\\n\\n    def error_mape(self, u_gc_scores, u_bc_scores):\\n        errorsum = .0\\n        n_gcs = 0\\n        n_bcs = 0\\n\\n        for i, ((aggr, attr), op, c) in enumerate(self.uncoalesced_gcs):\\n            score = u_gc_scores[i]\\n            if not op(score, c):\\n                errorsum += abs((c - score) / c)\\n            n_gcs += 1\\n\\n        for bscores in u_bc_scores:\\n            for i, (attr, op, c) in enumerate(self.uncoalesced_bcs):\\n                score = bscores[i]\\n                if not op(score, c):\\n                    errorsum += abs((c - score) / c)\\n                n_bcs += 1\\n\\n        if n_gcs + n_bcs > 0:\\n            return errorsum / (n_gcs + n_bcs)\\n\\n        else:\\n            assert errorsum == 0\\n            return 0\\n\\n\\n    def generate_data_for_selectivity(self, selectivity, n_tuples):\\n        \"\"\"\\n        NOTE: This is currently unused. Not even sure if I completed it. But give a look at it again because\\n        there were some interesting ideas.\\n        \"\"\"\\n        def generate_valid_and_invalid_subsets(n_vars, n_subsets, n_valid):\\n            # TODO: Read again this function. There\\'s some interesting logic\\n            n_subsets = int(math.ceil(n_subsets))\\n            n_valid = int(math.ceil(n_valid))\\n\\n            assert n_valid <= n_subsets == 2**n_vars\\n\\n            valid = []\\n            invalid = []\\n\\n            # This must be always valid (it is the sum of no tuples)\\n            # valid.append( (0,)*n_vars )\\n            valid.append(0)\\n\\n            # Generate half of vars valid and half invalid\\n            for i in range(n_vars):\\n                if len(valid) < n_valid/2.:\\n                    # valid.append(tuple( bit for bit in (\\'{:0%dbms}\\' % n_tuples).format(2**i) ))\\n                    valid.append(2**i)\\n                elif len(invalid) < (n_subsets - n_valid)/2.:\\n                    # invalid.append(tuple( bit for bit in (\\'{:0%dbms}\\' % n_tuples).format(2**i) ))\\n                    invalid.append(2**i)\\n                else:\\n                    valid.append(2**i)\\n\\n            # Generate more invalid (up to n_subsets-n_valid) by combining invalid + invalid\\n            while len(invalid) < n_subsets-n_valid:\\n                found = False\\n                for i in range(len(invalid)):\\n                    for j in range(len(invalid)):\\n                        new_invalid = invalid[i] | invalid[j]\\n                        if new_invalid not in invalid:\\n                            invalid.append(new_invalid)\\n                            found = True\\n                            break\\n                    if found:\\n                        break\\n                if not found:\\n                    break\\n\\n            # If more invalid are needed, generate them by combining invalid + valid\\n            while len(invalid) < n_subsets-n_valid:\\n                found = False\\n                for i in range(len(invalid)):\\n                    for j in range(len(valid)):\\n                        new_invalid = invalid[i] | valid[j]\\n                        if new_invalid not in invalid:\\n                            invalid.append(new_invalid)\\n                            found = True\\n                            break\\n                    if found:\\n                        break\\n                if not found:\\n                    raise Exception\\n\\n            # All the remaining ones are valid\\n            valid = set(range(n_subsets)) - set(invalid)\\n\\n            assert len(valid) == n_valid\\n            assert len(valid) + len(invalid) == n_subsets\\n\\n            if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\\n                debug(\"n invalid = {}\".format(n_subsets - n_valid))\\n                debug(\"{}\".format(valid))\\n                debug(\"{}\".format(invalid))\\n                debug(\"{}\".format([ tuple( bit for bit in (\\'{:0%dbms}\\' % n_tuples).format(i) ) for i in valid ]))\\n                debug(\"{}\".format([ tuple( bit for bit in (\\'{:0%dbms}\\' % n_tuples).format(i) ) for i in invalid ]))\\n\\n            return valid, invalid\\n\\n\\n        def generate_set_of_problems(base_prob, vars, total_n_constraints, n_valid_constraints, a, b):\\n            problems = []\\n\\n            for valid in itertools.combinations(range(total_n_constraints), int(math.ceil(n_valid_constraints))):\\n                valid = set(valid)\\n\\n                invalid = set(range(total_n_constraints)) - valid\\n                assert set(valid) | invalid == set(range(total_n_constraints))\\n\\n                # The empty package must always be valid. TODO: Really?\\n                # valid = [0] + list(valid)\\n\\n                prob = base_prob.copy()\\n\\n                # valid = generate_valid_and_invalid_subsets(n_tuples, total_n_constraints, n_valid_constraints)[0]\\n                # valid = np.random.choice(range(total_n_constraints), size=n_valid_constraints, replace=False)\\n                if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\\n                    debug(\"VALID: {}\".format(valid))\\n                    debug(\"INVALID: {}\".format(sorted(set(range(total_n_constraints)) - set(valid))))\\n\\n                # Add valid constraints to the problem\\n                n_valid_added = 0\\n                for i in valid:\\n                    package_bitmap = [ int(bit) for bit in (\\'{:0%dbms}\\' % n_tuples).format(i) ]\\n                    assert len(package_bitmap) == len(vars)\\n\\n                    # Add a VALID constraint for this combination of tuples\\n                    prob += np.dot(vars, package_bitmap) >= a\\n                    prob += np.dot(vars, package_bitmap) <= b\\n                    n_valid_added += 1\\n                assert n_valid_added == len(valid)\\n\\n                # Add invalid constraints to the problem\\n                n_invalid_added = 0\\n\\n                if float(a) > -float(\"inf\") and float(b) < float(\"inf\"):\\n                    # In this case, we produce 2**(len(invalid)) new sub-problems, each for a different set of ways\\n                    # to break the constraints a <= sum() <= b\\n                    pairs_of_invalid_constraints = []\\n                    for i in invalid:\\n                        package_bitmap = [ int(bit) for bit in (\\'{:0%dbms}\\' % n_tuples).format(i) ]\\n                        pairs_of_invalid_constraints.append((\\n                            (package_bitmap, operator.le, a-1),\\n                            (package_bitmap, operator.ge, b+1),\\n                        ))\\n\\n                    orig_prob = prob.copy()\\n                    for set_of_invalid in itertools.product(*pairs_of_invalid_constraints):\\n                        new_prob = orig_prob.copy()\\n                        for invalid_bitmap, op, c in set_of_invalid:\\n                            new_prob += op(np.dot(vars, invalid_bitmap), c)\\n                        problems.append(new_prob)\\n\\n                else:\\n                    # In this case, we only generate one sub-problem by adding all invalid constraints\\n                    for i in invalid:\\n                        package_bitmap = [ int(bit) for bit in (\\'{:0%dbms}\\' % n_tuples).format(i) ]\\n                        assert len(package_bitmap) == len(vars)\\n\\n                        # Add an INVALID (out of range) constraint for this combination of tuples\\n                        if float(a) > -float(\"inf\") and float(b) < float(\"inf\"):\\n                            raise Exception(\"Should never happen!\")\\n                            # prob += np.dot(vars, package_bitmap) <= a-1\\n                        elif float(a) > -float(\"inf\"):\\n                            prob += np.dot(vars, package_bitmap) <= a-1\\n                        elif float(b) < float(\"inf\"):\\n                            prob += np.dot(vars, package_bitmap) >= b+1\\n                        else:\\n                            raise Exception\\n                    assert n_invalid_added == len(invalid)\\n\\n                    problems.append(prob)\\n\\n            return problems\\n\\n\\n        assert 0 <= selectivity <= 1\\n        assert n_tuples >= 0\\n\\n        table_name_start = self.bc_query.lower().find(\"from \")\\n        table_name_end = self.bc_query[table_name_start+5:].lower().find(\" \")\\n        table_name = self.bc_query[table_name_start+5:table_name_start+5+table_name_end]\\n\\n        attribute_names = []\\n        ranges = []\\n        for j in range(len(self.gc_queries)):\\n            if \\'sum(\\' in self.gc_queries[j].lower():\\n                attr_start = self.gc_queries[j].lower().find(\\'sum(\\')\\n                attr_end = self.gc_queries[j][attr_start+4:].lower().find(\\')\\')\\n                attribute_names.append(self.gc_queries[j][attr_start+4:attr_start+4+attr_end])\\n                ranges.append(self.gc_ranges[j])\\n        debug(\"{} {}\".format(attribute_names, ranges))\\n        assert len(attribute_names) == len(ranges)\\n\\n        # Generate the data via CPLEX\\n        data_columns = []\\n\\n        # Generate one column at a time. Each column is generated with a CPLEX problem\\n        for j in range(len(attribute_names)):\\n            a, b = ranges[j]\\n\\n            total_n_constraints = 2**n_tuples\\n            n_valid_constraints = (1-selectivity) * total_n_constraints\\n\\n            # Check satisfiability of requirements\\n            if n_valid_constraints == 0 and a <= 0 <= b:\\n                warning(\"Since a<=0<=b there is always at least one valid package, i.e. the empty package, \"\\n                        \"therefore selectivity=1 (where no package is valid) is impossible.\")\\n                return None\\n            if n_valid_constraints == total_n_constraints and not a <= 0 <= b:\\n                warning(\"Since not a<=0<=b, the empty package may never be a valid package, \"\\n                        \"therefore selectivity=0 (where all packages are valid) is impossible.\")\\n                return None\\n\\n            # Create the base problem\\n            base_prob = LpProblem(\"package-builder\", LpMinimize)\\n            base_prob += 0 # no objective\\n\\n            # Add constraints to the problem\\n            vars = [\\n                LpVariable(\"{}_{}\".format(attribute_names[j], i), -float(\"inf\"), float(\"inf\"), LpInteger)\\n                for i in range(n_tuples)\\n            ]\\n\\n            # Generate all possible combination of problem constraints\\n            # One of them will be feasible and will give us the dataset\\n            problems = generate_set_of_problems(base_prob, vars, total_n_constraints, n_valid_constraints, a, b)\\n\\n            # Now try to find one feasible problem\\n            for prob in problems:\\n                # Solve the problem\\n                debug(\"{}\".format(prob))\\n                solver = CPLEX(msg=True, timeLimit=None)\\n                solver.solve(prob)\\n\\n                # Check the problem status\\n                if LpStatus[prob.status]==\\'Infeasible\\':\\n                    debug(\"@@@@@@@@@@@@@@@@@ INFEASIBLE: CONTINUE\")\\n                    continue\\n\\n                elif LpStatus[prob.status]==\\'Undefined\\':\\n                    raise Exception(\"Problem is undefined.\")\\n\\n                elif LpStatus[prob.status]==\\'Optimal\\':\\n                    debug(\"################## OPTIMAL\")\\n                    prob.roundSolution()\\n                    sol = [ v.varValue for v in prob.tuple_variables() if type(v.varValue) is float ]\\n                    data_columns.append(sol)\\n                    break\\n\\n                else:\\n                    raise Exception(\"LP status: {}\".format(LpStatus[prob.status]))\\n\\n            else:\\n                raise Exception(\"Could not find feasible combination of constraints \"\\n                                \"for selectivity {} and {} tuples.\".format(selectivity, n_tuples))\\n\\n        tuples = np.array(data_columns).transpose()\\n\\n        return table_name, attribute_names, tuples\\n\\n\\n\\nclass ConjunctivePackageQuery(PackageQuery):\\n    # TODO: later on, move the two staticmethods from_... outside. Make them just functions.\\n    # TODO: IMPORTANT! All base and gc queries MUST be instance of some class SQL_Query instead of just strings\\n\\n    def __init__(self, query_dict):\\n        super(ConjunctivePackageQuery, self).__init__(query_dict)\\n\\n        # Store the base and global constraints as coalesced and un-coalesced constraints\\n        gc_constraint_trees = []\\n        gc_ranges = []\\n        gcs = self.such_that_expr.get_ANDed_gc_list()\\n        for sqlquery_expr, gc_range_a, gc_range_b in gcs:\\n            if isinstance(sqlquery_expr, SQLQueryExpression):\\n                # Note: Technically, you\\'ll get an expression tree of \"constraint trees\" (query plans). So you\\n                # should actually try to combine them into one single constraint tree. Right now I\\'m simplifying\\n                # by assuming that the expression tree is always a simple leaf (so directly a constraint tree).\\n                operator_tree_expr = sqlquery_expr.traverse_leaf_func(leaf_func=\"get_constraint_tree\")\\n                assert isinstance(operator_tree_expr, ArithmeticExpression)\\n            else:\\n                raise Exception\\n            gc_constraint_trees.append(operator_tree_expr)\\n            gc_ranges.append((np.float64(gc_range_a), np.float64(gc_range_b)))\\n        self.coalesced_gcs = get_coalesced_global_constraints(gc_constraint_trees, gc_ranges)\\n        self.uncoalesced_gcs = get_uncoalesced_global_constraints(self.coalesced_gcs)\\n        self.coalesced_bcs = get_coalesced_base_constraints(self.bc_query)\\n        self.uncoalesced_bcs = get_uncoalesced_base_constraints(self.coalesced_bcs)\\n\\n\\n    def __str__(self):\\n        return (\\n            \"/-------------------------------------------- PaQL Query ---------------------------------------------\\\\\\\\\\\\n\"\\n            \"|  PaQL query:\\\\n\"\\n            \"|     \" + str(self._paql_query_str) + \"\\\\n\"\\n            \"|  Base SQL query:\\\\n\"\\n            \"|     \" + str(self.bc_query) + \"\\\\n\"\\n            \"|  Global SQL queries:\\\\n\"\\n            \"|     \" + (\"|     \".join([ str(q) + \"\\\\n\" for q in self.gc_queries ]) if self.gc_queries else \"None\\\\n\") + \"\"\\n            \"|  Glogal constraint ranges:\\\\n\"\\n            \"|     \" + (\"|     \".join([ str(q) + \"\\\\n\" for q in self.gc_ranges ]) if self.gc_ranges else \"None\\\\n\") + \"\"\\n            \"|  Optimization objective:\\\\n\"\\n            \"|     \" + (str(self.objective) if self.objective else \"None\") + \"\\\\n\"\\n            \"\\\\-----------------------------------------------------------------------------------------------------/\"\\n        )\\n\\n\\n    def get_paql_str(self, redo=False, recompute_gcs=True, coalesced=False):\\n        if redo or self._paql_query_str is None or self._paql_query_str_stale:\\n\\n            if recompute_gcs:\\n                self.coalesced_gcs = get_coalesced_global_constraints(self.gc_queries, self.gc_ranges)\\n                self.uncoalesced_gcs = get_uncoalesced_global_constraints(self.coalesced_gcs)\\n                self.coalesced_bcs = get_coalesced_base_constraints(self.bc_query)\\n                self.uncoalesced_bcs = get_uncoalesced_base_constraints(self.coalesced_bcs)\\n\\n            if self.rel_namespace is None:\\n                # raise Exception(\"rel_namespace is None\")\\n                # return \"\"\\n                self.rel_namespace = { \"R\": self.table_name }\\n\\n            bcs_str = []\\n            gcs_str = []\\n            obj_str = None\\n\\n            if not coalesced:\\n                if len(self.uncoalesced_bcs) > 0:\\n                    for attr, op, n in self.uncoalesced_bcs:\\n                        bcs_str.append(\"{} {} {}\".format(attr, op_to_opstr(op), n))\\n\\n                if len(self.uncoalesced_gcs) > 0:\\n                    for (aggr, attr), op, n in self.uncoalesced_gcs:\\n                        gcs_str.append(\"{}({}) {} {}\".format(aggr, attr, op_to_opstr(op), n))\\n\\n            else:\\n                if len(self.coalesced_bcs) > 0:\\n                    for attr, (lb, ub) in self.coalesced_bcs.iteritems():\\n                        if float(lb) == -float(\"inf\") and float(ub) == float(\"inf\"):\\n                            continue\\n                        elif float(ub) == float(\"inf\"):\\n                            bcs_str.append(\"{} {} {}\".format(attr, op_to_opstr(operator.ge), lb))\\n                        elif float(lb) == -float(\"inf\"):\\n                            bcs_str.append(\"{} {} {}\".format(attr, op_to_opstr(operator.le), ub))\\n                        elif lb == ub:\\n                            bcs_str.append(\"{} {} {}\".format(attr, op_to_opstr(operator.eq), ub))\\n                        else:\\n                            bcs_str.append(\"{} BETWEEN {} AND {}\".format(attr, lb, ub))\\n\\n                if len(self.coalesced_gcs) > 0:\\n                    for (aggr, attr), (lb, ub) in self.coalesced_gcs.iteritems():\\n                        if aggr.lower() == \"count\":\\n                            lb, ub = int(lb), int(ub)\\n\\n                        uaggr = aggr.upper()\\n\\n                        if float(lb) == -float(\"inf\") and float(ub) == float(\"inf\"):\\n                            continue\\n                        elif float(ub) == float(\"inf\"):\\n                            gcs_str.append(\"{}({}) {} {}\".format(uaggr, attr, op_to_opstr(operator.ge), lb))\\n                        elif float(lb) == -float(\"inf\"):\\n                            gcs_str.append(\"{}({}) {} {}\".format(uaggr, attr, op_to_opstr(operator.le), ub))\\n                        elif lb == ub:\\n                            gcs_str.append(\"{}({}) {} {}\".format(uaggr, attr, op_to_opstr(operator.eq), ub))\\n                        else:\\n                            gcs_str.append(\"{}({}) BETWEEN {} AND {}\".format(uaggr, attr, lb, ub))\\n\\n            if self.objective is not None:\\n                if self.objective[\"type\"] == \"maximize\":\\n                    obj_str = \"MAXIMIZE \"\\n                elif self.objective[\"type\"] == \"minimize\":\\n                    obj_str = \"MINIMIZE \"\\n                else:\\n                    raise\\n                obj_str += self.objective[\"func\"].get_str()\\n\\n            self._paql_query_str = \\\\\\n                \"SELECT \\\\n\\\\tPACKAGE({pack}) \\\\n\" \\\\\\n                \"FROM \\\\n\\\\t{tables} {bcs}{gcs}{obj};\".format(\\n                pack=\", \".join(self.rel_namespace.keys()),\\n                tables=\", \".join(\"{} {}\".format(name, alias) for alias, name in self.rel_namespace.iteritems()),\\n                bcs=\"\\\\nWHERE \\\\n\\\\t{} \".format(\" AND\\\\n\\\\t\".join(bcs_str)) if bcs_str else \"\",\\n                gcs=\"\\\\nSUCH THAT \\\\n\\\\t{} \".format(\" AND\\\\n\\\\t\".join(gcs_str)) if gcs_str else \"\",\\n                obj=\"\\\\n{}\".format(obj_str) if obj_str is not None else \"\")\\n\\n            self._paql_query_str_stale = False\\n\\n        return self._paql_query_str\\n',\n",
       " '\"\"\"Creates a brownian tree\"\"\"\\nimport random\\nimport os\\nimport numpy as np\\nfrom PIL import Image\\n\\nrandom.seed()\\n\\nclass MyTuple():\\n    \"\"\"Custom tuple with operator overloads\"\"\"\\n    def __init__(self, x, y):\\n        self.x_val = x\\n        self.y_val = y\\n\\n    def __add__(self, rhs):\\n        return MyTuple(self.x_val + rhs.x_val, self.y_val + rhs.y_val)\\n\\n    def __sub__(self, rhs):\\n        return MyTuple(self.x_val - rhs.x_val, self.y_val - rhs.y_val)\\n\\n    def __lt__(self, rhs):\\n        return self.x_val < rhs.x_val and self.y_val < rhs.y_val\\n\\n    def __gt__(self, rhs):\\n        return self.x_val > rhs.x_val and self.y_val > rhs.y_val\\n\\n    def set_vals(self, x, y):\\n        \"\"\"sets both values\"\"\"\\n        self.x_val = x\\n        self.y_val = y\\n\\n    def get_vals(self):\\n        \"\"\"casts to a normal tuple\"\"\"\\n        return (self.x_val, self.y_val)\\n\\n\\nclass Grid():\\n    \"\"\"Stores the data about the grid and tree\"\"\"\\n    def __init__(self, size):\\n        \"\"\"grid constructor\"\"\"\\n        self.size = size\\n        self.data = np.zeros(self.size.get_vals(), dtype=np.uint8)\\n        self.max_steps = 0\\n\\n    def get_val(self, location):\\n        \"\"\"returns the cell value at that location\"\"\"\\n        return self.data[location.x_val, location.y_val]\\n\\n    def is_occupied(self, location):\\n        \"\"\"returns True if the location is occupied\"\"\"\\n        return self.data[location.x_val, location.y_val]\\n\\n    def set_val(self, location, steps):\\n        \"\"\"sets the value at that location\"\"\"\\n        if self.max_steps < steps:\\n            self.max_steps = steps\\n        self.data[location.x_val, location.y_val] = 255\\n\\n    def is_outside_bounds(self, location):\\n        \"\"\"Checks if the location supplied is inside the grid\"\"\"\\n        lower = location > MyTuple(-1, -1)\\n        upper = location < self.size\\n        return upper and lower\\n\\n    def normalise(self):\\n        max = self.max_steps\\n        def normalise(data):\\n            return 255 - (data / (self.max_steps / 255))\\n        np.vectorize(normalise)(self.data)\\n\\nBOUNDS = MyTuple(100, 75)\\n# BOUNDS = MyTuple(160, 120)\\n\\nLEFT = MyTuple(-1, 0)\\nUP = MyTuple(0, 1)\\nRIGHT = MyTuple(1, 0)\\nDOWN = MyTuple(0, -1)\\nNONE = MyTuple(0, 0)\\nDIRECTIONS = [UP, DOWN, LEFT, RIGHT]\\n\\n\\nclass Particle():\\n    \"\"\"Stores data about individual particles\"\"\"\\n    def __init__(self, direction, location):\\n        \"\"\"particle constructor\"\"\"\\n        self.direction = direction\\n        self.location = location\\n        self.is_free = True\\n\\n    def move(self, new_direction):\\n        \"\"\"moves particle in a direction\"\"\"\\n        self.direction = new_direction\\n        self.location += new_direction\\n\\n    def fix_location(self):\\n        \"\"\"sets the location of said particle\"\"\"\\n        self.direction = NONE\\n        self.is_free = False\\n\\n\\ndef random_xy_loc(bounds):\\n    \"\"\"Creates a random xy tuple within bounds\"\"\"\\n    return MyTuple(random.randint(0, bounds.x_val - 1), random.randint(0, bounds.y_val - 1))\\n\\n\\ndef get_next_direction():\\n    return random.choice(DIRECTIONS)\\n\\n\\ndef main():\\n    \"\"\"main init\"\"\"\\n    data_grid = Grid(BOUNDS)\\n\\n    max_particles = int(BOUNDS.x_val * BOUNDS.y_val / 3)\\n\\n    for y in range(BOUNDS.y_val):\\n        for x in range(BOUNDS.x_val):\\n            if x == 0 or x == (BOUNDS.x_val - 1) or y == 0 or y == (BOUNDS.y_val - 1):\\n                data_grid.set_val(MyTuple(x, y), 1)\\n\\n    data_grid.set_val(MyTuple(int(BOUNDS.x_val / 2), int(BOUNDS.y_val / 2)), 1)\\n\\n    is_occupied = data_grid.is_occupied\\n    is_outside_bounds = data_grid.is_outside_bounds\\n\\n    for i in range(max_particles):\\n        count = -1\\n        location = random_xy_loc(BOUNDS)\\n        while is_occupied(location):\\n            location = random_xy_loc(BOUNDS)\\n        new_particle = Particle(direction=NONE, location=location)\\n        while new_particle.is_free:\\n            count += 1\\n            direction = get_next_direction()\\n            new_location = new_particle.location + direction\\n            if not is_outside_bounds(new_location):\\n                continue\\n            if not is_occupied(new_location):\\n                new_particle.move(direction)\\n            else:\\n                new_particle.fix_location()\\n        data_grid.set_val(new_particle.location, count)\\n        print(f\"Finished {i + 1}, loops: {count}\")\\n    output_name = f\"time_data_{max_particles}_{BOUNDS.get_vals()}.bmp\"\\n    full_path = os.path.join(os.path.dirname(__file__), output_name)\\n    Image.fromarray(data_grid.data).save(full_path)\\n\\n\\nif __name__ == \"__main__\":\\n    main()\\n',\n",
       " '\"\"\" Provides named dictionary \"\"\"\\n#-------------------------------------------------------------------------------\\nimport collections\\n\\n#-------------------------------------------------------------------------------\\nclass NamedDict(collections.OrderedDict):\\n  \"\"\" A named dictionary is an ordered dictionary with a name \"\"\"\\n  _name = None\\n  \\n#-------------------------------------------------------------------------------\\n  def __init__(self, name, *args, **kwds):\\n    self.name = name\\n    super().__init__(*args, **kwds)\\n\\n#-------------------------------------------------------------------------------\\n  @property\\n  def name(self):\\n    return self._name\\n\\n  @name.setter\\n  def name(self, name):\\n    assert isinstance(name, str), \\\\\\n        \"Name must be a string, not {}\".format(type(name))\\n    self._name = name\\n\\n#-------------------------------------------------------------------------------\\n  def __repr__(self):\\n    return \"{}: {}\".format(self.name, super().__repr__())\\n\\n#-------------------------------------------------------------------------------\\n',\n",
       " '# Copyright 2019 Google LLC\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     https://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nimport csv\\nimport io\\nimport urllib.request\\n\\noutput_columns = [\\n    \\'Date\\', \\'GeoId\\', \\'CumulativeCount_MedicalTest_ConditionCOVID_19\\',\\n    \\'CumulativeCount_MedicalTest_ConditionCOVID_19_Positive\\',\\n    \\'CumulativeCount_MedicalTest_ConditionCOVID_19_Negative\\',\\n    \\'Count_MedicalTest_ConditionCOVID_19_Pending\\',\\n    \\'CumulativeCount_MedicalConditionIncident_COVID_19_PatientRecovered\\',\\n    \\'CumulativeCount_MedicalConditionIncident_COVID_19_PatientDeceased\\',\\n    \\'Count_MedicalConditionIncident_COVID_19_PatientHospitalized\\',\\n    \\'CumulativeCount_MedicalConditionIncident_COVID_19_PatientHospitalized\\',\\n    \\'Count_MedicalConditionIncident_COVID_19_PatientInICU\\',\\n    \\'CumulativeCount_MedicalConditionIncident_COVID_19_PatientInICU\\',\\n    \\'Count_MedicalConditionIncident_COVID_19_PatientOnVentilator\\',\\n    \\'CumulativeCount_MedicalConditionIncident_COVID_19_PatientOnVentilator\\'\\n]\\nwith open(\\'COVIDTracking_States.csv\\', \\'w\\', newline=\\'\\') as f_out:\\n    writer = csv.DictWriter(f_out,\\n                            fieldnames=output_columns,\\n                            lineterminator=\\'\\\\n\\')\\n    with urllib.request.urlopen(\\n            \\'https://covidtracking.com/api/v1/states/daily.csv\\') as response:\\n        reader = csv.DictReader(io.TextIOWrapper(response))\\n\\n        writer.writeheader()\\n        for row_dict in reader:\\n            processed_dict = {\\n                \\'Date\\':\\n                    \\'%s-%s-%s\\' % (row_dict[\\'date\\'][:4], row_dict[\\'date\\'][4:6],\\n                                  row_dict[\\'date\\'][6:]),\\n                \\'GeoId\\':\\n                    \\'dcid:geoId/%s\\' % row_dict[\\'fips\\'],\\n                \\'CumulativeCount_MedicalTest_ConditionCOVID_19\\':\\n                    row_dict[\\'totalTestResults\\'],\\n                \\'CumulativeCount_MedicalTest_ConditionCOVID_19_Positive\\':\\n                    row_dict[\\'positive\\'],\\n                \\'CumulativeCount_MedicalTest_ConditionCOVID_19_Negative\\':\\n                    row_dict[\\'negative\\'],\\n                \\'Count_MedicalTest_ConditionCOVID_19_Pending\\':\\n                    row_dict[\\'pending\\'],\\n                (\\'CumulativeCount_MedicalConditionIncident\\'\\n                 \\'_COVID_19_PatientRecovered\\'):\\n                    row_dict[\\'recovered\\'],\\n                (\\'CumulativeCount_MedicalConditionIncident\\'\\n                 \\'_COVID_19_PatientDeceased\\'):\\n                    row_dict[\\'death\\'],\\n                \\'Count_MedicalConditionIncident_COVID_19_PatientHospitalized\\':\\n                    row_dict[\\'hospitalizedCurrently\\'],\\n                (\\'CumulativeCount_MedicalConditionIncident\\'\\n                 \\'_COVID_19_PatientHospitalized\\'):\\n                    row_dict[\\'hospitalizedCumulative\\'],\\n                \\'Count_MedicalConditionIncident_COVID_19_PatientInICU\\':\\n                    row_dict[\\'inIcuCurrently\\'],\\n                (\\'CumulativeCount_MedicalConditionIncident\\'\\n                 \\'_COVID_19_PatientInICU\\'):\\n                    row_dict[\\'inIcuCumulative\\'],\\n                \\'Count_MedicalConditionIncident_COVID_19_PatientOnVentilator\\':\\n                    row_dict[\\'onVentilatorCurrently\\'],\\n                (\\'CumulativeCount_MedicalConditionIncident\\'\\n                 \\'_COVID_19_PatientOnVentilator\\'):\\n                    row_dict[\\'onVentilatorCumulative\\'],\\n            }\\n\\n            writer.writerow(processed_dict)\\n\\n# Automate Template MCF generation since there are many Statitical Variables.\\nTEMPLATE_MCF_TEMPLATE = \"\"\"\\nNode: E:COVIDTracking_States->E{index}\\ntypeOf: dcs:StatVarObservation\\nvariableMeasured: dcs:{stat_var}\\nmeasurementMethod: dcs:CovidTrackingProject\\nobservationAbout: C:COVIDTracking_States->GeoId\\nobservationDate: C:COVIDTracking_States->Date\\nvalue: C:COVIDTracking_States->{stat_var}\\n\"\"\"\\n\\nstat_vars = output_columns[2:]\\nwith open(\\'COVIDTracking_States.tmcf\\', \\'w\\', newline=\\'\\') as f_out:\\n    for i in range(len(stat_vars)):\\n        f_out.write(\\n            TEMPLATE_MCF_TEMPLATE.format_map({\\n                \\'index\\': i,\\n                \\'stat_var\\': output_columns[2:][i]\\n            }))\\n',\n",
       " \"from django.urls import path,include\\nfrom . import views\\nfrom django.contrib.auth import views as auth_views\\n# from rest_framework import routers\\nfrom django.conf import settings\\nfrom django.conf.urls.static import static\\n\\nurlpatterns = [\\n    path('index/',views.index, name='index'),\\n    path('register', views.register, name='register'),\\n    path('',auth_views.LoginView.as_view(), name='login'),\\n    path('account/', include('django.contrib.auth.urls')),\\n    path('all-hoods/', views.hoods, name='hood'),\\n    path('new-hood/', views.create_hood, name='new-hood'),\\n    path('profile/<username>', views.profile, name='profile'),\\n    path('profile/<username>/edit/', views.edit_profile, name='edit-profile'),\\n    path('join_hood/<id>', views.join_hood, name='join-hood'),\\n    path('leave_hood/<id>', views.leave_hood, name='leave-hood'),\\n    path('single_hood/<hood_id>', views.single_hood, name='single-hood'),\\n    path('<hood_id>/new-post', views.create_post, name='post'),\\n    path('<hood_id>/members', views.hood_members, name='members'),\\n    path('search/', views.search_business, name='search'),\\n]\\nif settings.DEBUG:\\n    urlpatterns+= static(settings.MEDIA_URL, document_root = settings.MEDIA_ROOT)\",\n",
       " 'from PIL import Image\\nimport numpy as np\\n# from streamlit.logger import update_formatter\\nimport torch\\nfrom matplotlib import cm\\n\\n\\n\\ndef min_max_norm(array):\\n    lim = [array.min(), array.max()]\\n    array = array - lim[0] \\n    array.mul_(1 / (1.e-10+ (lim[1] - lim[0])))\\n    # array = torch.clamp(array, min=0, max=1)\\n    return array\\n\\ndef torch_to_rgba(img):\\n    img = min_max_norm(img)\\n    rgba_im = img.permute(1, 2, 0).cpu()\\n    if rgba_im.shape[2] == 3:\\n        rgba_im = torch.cat((rgba_im, torch.ones(*rgba_im.shape[:2], 1)), dim=2)\\n    assert rgba_im.shape[2] == 4\\n    return rgba_im\\n\\n\\ndef numpy_to_image(img, size):\\n    \"\"\"\\n    takes a [0..1] normalized rgba input and returns resized image as [0...255] rgba image\\n    \"\"\"\\n    resized = Image.fromarray((img*255.).astype(np.uint8)).resize((size, size))\\n    return resized\\n\\ndef upscale_pytorch(img:np.array, size):\\n    torch_img = torch.from_numpy(img).unsqueeze(0).permute(0,3,1,2)\\n    print(torch_img)\\n    upsampler = torch.nn.Upsample(size=size)    \\n    return upsampler(torch_img)[0].permute(1,2,0).cpu().numpy()\\n\\n\\ndef heatmap_helper(image:torch.Tensor, heatmap: torch.Tensor, size=None, alpha=.6):\\n    if not size:\\n        size = image.shape[1]\\n\\n    img = numpy_to_image(min_max_norm(heatmap).numpy(), size)\\n    return np.asarray(img)\\n\\ndef heatmap(image:torch.Tensor, heatmap: torch.Tensor, size=None, alpha=.6):\\n    if not size:\\n        size = image.shape[1]\\n    # print(heatmap)\\n    # print(min_max_norm(heatmap))\\n\\n    img = torch_to_rgba(image).numpy() # [0...1] rgba numpy \"image\"\\n    hm = cm.jet(min_max_norm(heatmap).numpy()) # [0...1] rgba numpy \"image\"\\n\\n    img = np.array(numpy_to_image(img,size))\\n    hm = np.array(numpy_to_image(hm, size))\\n    # hm = upscale_pytorch(hm, size)\\n    # print (hm) \\n\\n    return Image.fromarray((alpha * hm + (1-alpha)*img).astype(np.uint8))\\n    # return Image.fromarray(hm)',\n",
       " '\"\"\"Proto related build rules for fhir.\\n\"\"\"\\n\\nload(\"@rules_proto//proto:defs.bzl\", \"proto_library\")\\nload(\"@rules_cc//cc:defs.bzl\", \"cc_proto_library\")\\nload(\"@io_bazel_rules_go//proto:def.bzl\", \"go_proto_library\")\\nload(\"@com_google_protobuf//:protobuf.bzl\", \"py_proto_library\")\\n\\nWELL_KNOWN_PROTOS = [\"descriptor_proto\", \"any_proto\"]\\nGO_WELL_KNOWN_PROTOS = {\\n    \"descriptor_proto\": \"@org_golang_google_protobuf//types/descriptorpb:go_default_library\",\\n    \"any_proto\": \"@org_golang_google_protobuf//types/known/anypb:go_default_library\",\\n}\\n\\ndef fhir_proto_library(proto_library_prefix, srcs = [], proto_deps = [], **kwargs):\\n    \"\"\"Generates proto_library target, as well as {py,cc,java,go}_proto_library targets.\\n\\n    Args:\\n      proto_library_prefix: Name prefix to be added to various proto libraries.\\n      srcs: Srcs for the proto library.\\n      proto_deps: Deps by the proto_library.\\n      **kwargs: varargs. Passed through to proto rules.\\n    \"\"\"\\n    py_deps = []\\n    cc_deps = []\\n    go_deps = []\\n    has_well_known_dep = False\\n    for x in proto_deps:\\n        tokens = x.split(\":\")\\n        if len(tokens) == 2 and tokens[1] in WELL_KNOWN_PROTOS:\\n            go_deps.append(GO_WELL_KNOWN_PROTOS[tokens[1]])\\n            if not has_well_known_dep:\\n                py_deps.append(tokens[0] + \":protobuf_python\")\\n                cc_deps.append(tokens[0] + \":cc_wkt_protos\")\\n                has_well_known_dep = True\\n        elif x.endswith(\"_proto\"):\\n            py_deps.append(x[:-6] + \"_py_pb2\")\\n            cc_deps.append(x[:-6] + \"_cc_proto\")\\n            go_deps.append(x[:-6] + \"_go_proto\")\\n\\n    proto_library(\\n        name = proto_library_prefix + \"_proto\",\\n        srcs = srcs,\\n        deps = proto_deps,\\n        **kwargs\\n    )\\n\\n    py_proto_library(\\n        name = proto_library_prefix + \"_py_pb2\",\\n        srcs = srcs,\\n        deps = py_deps,\\n        default_runtime = \"@com_google_protobuf//:protobuf_python\",\\n        protoc = \"@com_google_protobuf//:protoc\",\\n        **kwargs\\n    )\\n\\n    cc_proto_library(\\n        name = proto_library_prefix + \"_cc_proto\",\\n        deps = [proto_library_prefix + \"_proto\"],\\n    )\\n\\n    native.java_proto_library(\\n        name = proto_library_prefix + \"_java_proto\",\\n        deps = [\\n            \":\" + proto_library_prefix + \"_proto\",\\n        ],\\n        **kwargs\\n    )\\n\\n    importpath_prefix = \"github.com/google/fhir/go/\"\\n    if native.package_name().startswith(\"go/\"):\\n        importpath_prefix = \"github.com/google/fhir/\"\\n\\n    go_proto_library(\\n        name = proto_library_prefix + \"_go_proto\",\\n        deps = go_deps,\\n        proto = \":\" + proto_library_prefix + \"_proto\",\\n        importpath = importpath_prefix + native.package_name() + \"/\" + proto_library_prefix + \"_go_proto\",\\n        **kwargs\\n    )\\n\\ndef _fhir_individual_resource_rules(resource_files, deps):\\n    for resource_file in resource_files:\\n        fhir_proto_library(\\n            srcs = [resource_file],\\n            proto_deps = deps,\\n            proto_library_prefix = resource_file[:-6],\\n        )\\n\\ndef fhir_resource_rules(resource_files, deps):\\n    resource_files.remove(\"bundle_and_contained_resource.proto\")\\n\\n    _fhir_individual_resource_rules(resource_files, deps)\\n\\n    resources_as_dep = [\":\" + file[:-6] + \"_proto\" for file in resource_files]\\n\\n    fhir_proto_library(\\n        srcs = [\"bundle_and_contained_resource.proto\"],\\n        proto_deps = deps + resources_as_dep,\\n        proto_library_prefix = \"bundle_and_contained_resource\",\\n    )\\n\\ndef fhir_profile_rules(resource_files, deps):\\n    _fhir_individual_resource_rules(resource_files, deps)\\n',\n",
       " \"import pymysql as pymysql\\na = None\\nuser_data = {}\\ndef mysql_db(name,age):\\n\\tconn = pymysql.connect(host='127.0.0.1', user='root', password='mysql', database='zhangqiang_db', charset='utf8')\\n\\tcursor = conn.cursor()\\n\\tsql = '''\\n\\tinsert into USER1(name, age) value(%s, %s);\\n\\t'''\\n\\t# name = name\\n\\t# age = age\\n\\tcursor.execute(sql, [name, age])\\n\\tconn.commit()\\n\\tcursor.close()\\n\\tconn.close()\\n\\n\\ndef serach_db(name):\\n\\tconn = pymysql.connect(host='127.0.0.1', user='root', password='mysql', database='zhangqiang_db', charset='utf8')\\n\\tcursor = conn.cursor()\\n\\tsql = '''\\n\\tselect name from USER1 where name=%s;\\n\\t'''\\n\\t# name = name\\n\\tcursor.execute(sql, [name])\\n\\ta = cursor.fetchone()\\n\\tcursor.close()\\n\\tconn.close()\\n\\tprint(a)\\n\\treturn a\\n\\ndef serach_db_passwd(name):\\n\\tconn = pymysql.connect(host='127.0.0.1', user='root', password='mysql', database='zhangqiang_db', charset='utf8')\\n\\tcursor = conn.cursor()\\n\\tsql = '''\\n\\tselect age from USER1 where age=%s;\\n\\t'''\\n\\t# name = name\\n\\tcursor.execute(sql, [name])\\n\\tpasswd = cursor.fetchone()\\n\\tcursor.close()\\n\\tconn.close()\\n\\tprint(passwd)\\n\\treturn passwd\\n\\ndef new_user():\\n\\tname = input('new_name\\\\t\\\\n')\\n\\twhile 1:\\n\\n\\t\\tif name in user_data:\\n\\t\\t\\tname = input('The user name already exists')\\n\\t\\t\\tcontinue\\n\\t\\telse:\\n\\t\\t\\tbreak\\n\\tpasswd = int(input('password\\\\n'))\\n\\tmysql_db(name, passwd)\\n\\t# user_data[name] = passwd\\n\\tprint('successful')\\n\\n\\ndef old_user():\\n\\n\\tname = input('name')\\n\\twhile 1:\\n\\t\\tusername = serach_db(name)\\n\\t\\t# print(serach_db(name))\\n\\t\\tusername = username[0]\\n\\t\\t# print(a)\\n\\n\\t\\t# if name not in user_data:\\n\\t\\tif username == name:\\n\\t\\t\\tprint('name ok')\\n\\t\\t\\tpassword = input('password\\\\n\\t')\\n\\n\\t\\t\\tpasswd = serach_db_passwd(password)\\n\\t\\t\\tpasswd = str(passwd[0])\\n\\t\\t\\tif passwd == password:\\n\\n\\t\\t\\t\\tprint('登录成功')\\n\\n\\n\\t\\t\\tname = input('name is error,please enter again')\\n\\t\\t\\tcontinue\\n\\t\\telse:\\n\\t\\t\\tbreak\\n\\tpwd= user_data.get(name)\\n\\tif passwd == pwd:\\n\\t\\tprint('successful')\\n\\t\\t\\n\\telse:\\n\\t\\tprint('password is error')\\ndef showmenu():\\n\\tname = ''\\n\\tprint('---new_user:N/n--')\\n\\tprint('---login:E/e------')\\n\\tprint('---quit:Q/q------')\\n\\tprint('---input code----')\\n\\tchoose=input('input code\\\\n')\\t\\n\\twhile 1 :\\n\\t\\tif choose not in 'NEQneq':\\n\\t\\t\\tchoose = input('code is error,try again')\\n\\t\\telif choose == 'q' or choose == 'Q':\\n\\t\\t\\tbreak\\n\\t\\telif  choose == 'N' or choose =='n':\\n\\t\\t\\tnew_user()\\n\\t\\t\\tshowmenu()\\n\\t\\telif  choose =='e' or choose == 'E':\\n\\t\\t\\told_user()\\nshowmenu()\\n# if serach_db('laowang') == ('laowang',):\\n# \\tprint('1')\\n# else:\\n# \\tprint('2')\\n#\\n\\n\\n\\t\\t\\n\",\n",
       " 'import time\\nimport logging\\nimport httplib as http_client\\nimport requests\\nfrom requests.adapters import HTTPAdapter\\nfrom requests.packages.urllib3.poolmanager import PoolManager\\nfrom requests.packages.urllib3.util.retry import Retry\\nimport ssl\\nimport base64\\ntry:\\n    from cStringIO import StringIO\\nexcept BaseException:\\n    from StringIO import StringIO\\nimport zipfile\\n\\n\\nclass MyAdapter(HTTPAdapter):\\n    def init_poolmanager(self, connections, maxsize, block=False):\\n        self.poolmanager = PoolManager(num_pools=connections,\\n                                       maxsize=maxsize,\\n                                       block=block,\\n                                       ssl_version=getattr(ssl, \\'PROTOCOL_TLSv1_2\\', ssl.PROTOCOL_TLSv1))\\n\\n# Retry logic if the API fails to responde\\n\\n\\ndef requests_retry_session(\\n        retries=5,\\n        backoff_factor=0.5,\\n        status_forcelist=(500, 502, 504, 495, 496, 525, 526),\\n        session=None,\\n):\\n    session = session or requests.Session()\\n    retry = Retry(\\n        total=retries,\\n        read=retries,\\n        connect=retries,\\n        backoff_factor=backoff_factor,\\n        status_forcelist=status_forcelist,\\n    )\\n    adapter = MyAdapter(max_retries=retry)\\n    session.mount(\\'http://\\', adapter)\\n    session.mount(\\'https://\\', adapter)\\n    return session\\n\\n# 1and1 Object Classes\\n\\n\\nclass OneAndOneService(object):\\n\\n    # Init Function\\n    def __init__(\\n            self,\\n            api_token,\\n            api_url=\\'https://cloudpanel-api.1and1.com/v1\\',\\n            enable_logs=False):\\n        if api_url == \\'\\' or api_url == \\'default\\':\\n            api_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.api_token = api_token\\n        self.base_url = api_url\\n        self.header = {\\'X-TOKEN\\': self.api_token}\\n        self.success_codes = (200, 201, 202)\\n        if enable_logs:\\n            http_client.HTTPConnection.debuglevel = 1\\n            logging.basicConfig()\\n            logging.getLogger().setLevel(logging.DEBUG)\\n            requests_log = logging.getLogger(\"requests.packages.urllib3\")\\n            requests_log.setLevel(logging.ERROR)\\n            requests_log.propagate = True\\n\\n    def __repr__(self):\\n        return \\'OneAndOneService: api_token=%s, base_url=%s\\' % (self.api_token,\\n                                                                self.base_url)\\n\\n    # Server Functions\\n\\n    # \\'GET\\' methods\\n\\n    def list_servers(self, page=None, per_page=None, sort=None, q=None,\\n                     fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/servers\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def fixed_server_flavors(self):\\n\\n        # Perform Request\\n        url = \\'%s/servers/fixed_instance_sizes\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_fixed_server(self, fixed_server_id=None):\\n\\n        # Error Handling\\n        if(fixed_server_id is None):\\n            raise ValueError(\\'fixed_server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/servers/fixed_instance_sizes/%s\\' %\\n               (self.base_url, fixed_server_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_server(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s\\' % (self.base_url, server_id)\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_server_hardware(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/hardware\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_server_hdds(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/hardware/hdds\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_server_hdd(self, server_id=None, hdd_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(hdd_id is None):\\n            raise ValueError(\\'hdd_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/hardware/hdds/%s\\' %\\n               (self.base_url, server_id, hdd_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_server_image(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/image\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_server_ips(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/ips\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_server_ip(self, server_id=None, ip_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/ips/%s\\' % (self.base_url, server_id, ip_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_ip_firewall_policy(self, server_id=None, ip_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/ips/%s/firewall_policy\\' %\\n               (self.base_url, server_id, ip_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_ip_load_balancers(self, server_id=None, ip_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/ips/%s/load_balancers\\' %\\n               (self.base_url, server_id, ip_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_server_status(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/status\\' % (self.base_url, server_id)\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_server_dvd(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/dvd\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_server_private_networks(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/private_networks\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def private_network_info(self, server_id=None, private_network_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/private_networks/%s\\' %\\n               (self.base_url, server_id, private_network_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_server_snapshots(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/snapshots\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_baremetal_models(\\n            self,\\n            page=None,\\n            per_page=None,\\n            sort=None,\\n            q=None,\\n            fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/servers/baremetal_models\\' % self.base_url\\n\\n        r = requests.get(url, headers=self.header, params=parameters)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def get_baremetal_model(self, model_id=None):\\n\\n        # Error Handling\\n        if (model_id is None):\\n            raise ValueError(\\'model_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/baremetal_models/%s\\' % (self.base_url, model_id)\\n\\n        r = requests.get(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # \\'PUT\\' methods\\n\\n    def modify_server(self, server_id=None, name=None, description=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(name is None):\\n            raise ValueError(\\'name is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description\\n        }\\n\\n        url = \\'%s/servers/%s\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def modify_server_hardware(\\n            self,\\n            server_id=None,\\n            fixed_instance_size_id=None,\\n            vcore=None,\\n            cores_per_processor=None,\\n            ram=None,\\n            test=False):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Use \\'test\\' flag to skip this block when running unit test\\n        if(test == False):\\n\\n            # Prevent hot decreasing of server hardware, allow cold decreasing.\\n            server_specs = self.get_server_hardware(server_id=server_id)\\n\\n            server_status = self.get_server_status(server_id=server_id)\\n\\n            if(server_status[\\'state\\'] == \\'POWERED_ON\\'):\\n                if(vcore is not None):\\n                    if(server_specs[\\'vcore\\'] > vcore):\\n                        raise ValueError((\\'Cannot perform a hot decrease of \\'\\n                                          \\'server CPU.  The new value must be \\'\\n                                          \\'greater than current value.\\'))\\n                if(ram is not None):\\n                    if(server_specs[\\'ram\\'] > ram):\\n                        raise ValueError((\\'Cannot perform a hot decrease of \\'\\n                                          \\'server RAM.  The new value must be \\'\\n                                          \\'greater than current value.\\'))\\n\\n        # Perform Request\\n        data = {\\n            \\'fixed_instance_size_id\\': fixed_instance_size_id,\\n            \\'vcore\\': vcore,\\n            \\'cores_per_processor\\': cores_per_processor,\\n            \\'ram\\': ram\\n        }\\n\\n        url = \\'%s/servers/%s/hardware\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def modify_hdd(self, server_id=None, hdd_id=None, size=None, test=False):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(hdd_id is None):\\n            raise ValueError(\\'hdd_id is a required parameter\\')\\n\\n        # Use \\'test\\' flag to skip this block when running unit test\\n        if(test == False):\\n\\n            # Make sure size argument is valid.  HDD size can\\'t be decreased.\\n            old_hdd = self.get_server_hdd(server_id=server_id, hdd_id=hdd_id)\\n\\n            if(size is not None):\\n                if(old_hdd[\\'size\\'] > size):\\n                    raise ValueError(\\'HDD size can never be decreased. \\'\\n                                     \\'Must be greater than or equal to the \\'\\n                                     \\'current HDD size.\\')\\n\\n        # Perform Request\\n        data = {\\'size\\': size}\\n\\n        url = (\\'%s/servers/%s/hardware/hdds/%s\\' %\\n               (self.base_url, server_id, hdd_id))\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_firewall_policy(\\n            self,\\n            server_id=None,\\n            ip_id=None,\\n            firewall_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\'id\\': firewall_id}\\n\\n        url = (\\'%s/servers/%s/ips/%s/firewall_policy\\' %\\n               (self.base_url, server_id, ip_id))\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def modify_server_status(\\n            self,\\n            server_id=None,\\n            action=None,\\n            method=\\'SOFTWARE\\',\\n            recovery_mode=False,\\n            recovery_image_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(action is None):\\n            raise ValueError(\\'action is a required parameter\\')\\n\\n        # Make sure user is passing in correct arguments\\n        if(action != \\'POWER_ON\\' and action != \\'POWER_OFF\\' and\\n                action != \\'REBOOT\\'):\\n            raise ValueError((\\'action must be set to \"POWER_ON\",\\'\\n                              \\'\"POWER_OFF\", or \"REBOOT\".\\'))\\n\\n        if method != \\'HARDWARE\\' and method != \\'SOFTWARE\\':\\n            raise ValueError((\\'method must be set to either \\'\\n                              \\'\"HARDWARE\" or \"SOFTWARE\".\\'))\\n        if recovery_mode and recovery_image_id is None:\\n            raise ValueError(\\n                (\\'If you want to reboot in recovery mode you must specify an image id recovery_image_id\\'))\\n\\n        # Perform Request\\n        if recovery_mode:\\n            data = {\\n                \\'action\\': action,\\n                \\'method\\': method,\\n                \\'recovery_mode\\': True,\\n                \\'recovery_image_id\\': recovery_image_id\\n            }\\n        else:\\n            data = {\\n                \\'action\\': action,\\n                \\'method\\': method,\\n            }\\n\\n        url = \\'%s/servers/%s/status/action\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def stop_server(self, server_id=None, method=\\'SOFTWARE\\'):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Make sure user is passing in correct arguments\\n        if(method != \\'HARDWARE\\' and method != \\'SOFTWARE\\'):\\n            raise ValueError((\\'method must be set to either \\'\\n                              \\'\"HARDWARE\" or \"SOFTWARE\".\\'))\\n\\n        # Perform Request\\n        data = {\\n            \\'action\\': \\'POWER_OFF\\',\\n            \\'method\\': method\\n        }\\n\\n        url = \\'%s/servers/%s/status/action\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def start_server(self, server_id=None, method=\\'SOFTWARE\\'):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Make sure user is passing in correct arguments\\n        if(method != \\'HARDWARE\\' and method != \\'SOFTWARE\\'):\\n            raise ValueError((\\'method must be set to either \\'\\n                              \\'\"HARDWARE\" or \"SOFTWARE\".\\'))\\n\\n        # Perform Request\\n        data = {\\n            \\'action\\': \\'POWER_ON\\',\\n            \\'method\\': method\\n        }\\n\\n        url = \\'%s/servers/%s/status/action\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def load_dvd(self, server_id=None, dvd_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(dvd_id is None):\\n            raise ValueError(\\'dvd_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\'id\\': dvd_id}\\n\\n        url = \\'%s/servers/%s/dvd\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def restore_snapshot(self, server_id=None, snapshot_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(snapshot_id is None):\\n            raise ValueError(\\'snapshot_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/servers/%s/snapshots/%s\\' %\\n               (self.base_url, server_id, snapshot_id))\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def reinstall_image(self, server_id=None, image_id=None, password=None,\\n                        firewall_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(image_id is None):\\n            raise ValueError(\\'image_id is a required parameter\\')\\n\\n        # Create firewall object, if necessary\\n        firewall_policy = {\\'id\\': firewall_id}\\n\\n        # Perform Request\\n        data = {\\n            \\'id\\': image_id,\\n            \\'password\\': password,\\n            \\'firewall_policy\\': firewall_policy\\n        }\\n\\n        url = \\'%s/servers/%s/image\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' methods\\n\\n    def delete_server(self, server_id=None, keep_ips=None, keep_hdds=True):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n        parameters = {\\'keep_ips\\': keep_ips, \\'keep_hdds\\': keep_hdds}\\n\\n        url = \\'%s/servers/%s\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().delete(\\n                url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_hdd(self, server_id=None, hdd_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(hdd_id is None):\\n            raise ValueError(\\'hdd_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/servers/%s/hardware/hdds/%s\\' %\\n               (self.base_url, server_id, hdd_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_ip(self, server_id=None, ip_id=None, keep_ip=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n        parameters = {\\'keep_ip\\': keep_ip}\\n\\n        url = \\'%s/servers/%s/ips/%s\\' % (self.base_url, server_id, ip_id)\\n\\n        try:\\n            r = requests_retry_session().delete(\\n                url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_load_balancer(self, server_id=None, ip_id=None,\\n                             load_balancer_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/servers/%s/ips/%s/load_balancers/%s\\' %\\n               (self.base_url, server_id, ip_id, load_balancer_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_private_network(self, server_id=None, private_network_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/servers/%s/private_networks/%s\\' %\\n               (self.base_url, server_id, private_network_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def eject_dvd(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/servers/%s/dvd\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def delete_snapshot(self, server_id=None, snapshot_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(snapshot_id is None):\\n            raise ValueError(\\'snapshot_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/servers/%s/snapshots/%s\\' %\\n               (self.base_url, server_id, snapshot_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' methods\\n\\n    def add_new_ip(self, server_id=None, ip_type=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(ip_type is not None) and (ip_type != \\'IPV4\\'):\\n            raise ValueError((\"ip_type.  Only type \\'IPV4\\' is currently \"\\n                              \"supported.\"))\\n\\n        # Perform Request\\n        data = {\\'type\\': ip_type}\\n\\n        url = \\'%s/servers/%s/ips\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_load_balancer(self, server_id=None, ip_id=None,\\n                          load_balancer_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\'load_balancer_id\\': load_balancer_id}\\n\\n        url = (\\'%s/servers/%s/ips/%s/load_balancers\\' %\\n               (self.base_url, server_id, ip_id))\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def assign_private_network(self, server_id=None, private_network_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\'id\\': private_network_id}\\n\\n        url = \\'%s/servers/%s/private_networks\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def create_snapshot(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/servers/%s/snapshots\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def clone_server(self, server_id=None, name=None, datacenter_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(name is None):\\n            raise ValueError(\\'name is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'datacenter_id\\': datacenter_id\\n        }\\n\\n        url = \\'%s/servers/%s/clone\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def create_server(self, server=None, hdds=None):\\n\\n        # Error Handling\\n        if(server is None):\\n            raise ValueError((\\'server is a required parameter. Make \\'\\n                              \\'sure you pass a Server object.\\'))\\n\\n        # Unpack hdds\\n        if hdds:\\n            hdd = []\\n\\n            for value in hdds:\\n                hdd.append(value.specs)\\n\\n            # Add hdds to server object\\n            server.specs[\\'hardware\\'][\\'hdds\\'] = hdd\\n\\n        # Clean dictionary\\n        keys = [k for k, v in server.specs[\\'hardware\\'].items() if\\n                v is None]\\n        for x in keys:\\n            del server.specs[\\'hardware\\'][x]\\n\\n        # Build URL and perform request\\n        url = \\'%s/servers\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=server.specs)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            # Assign new server_id back to calling Server object\\n            response = r.json()\\n\\n            server.specs.update(server_id=response[\\'id\\'])\\n            server.specs.update(api_token=self.header)\\n            server.first_password = response[\\'first_password\\']\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_hdd(self, server_id=None, hdds=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(hdds is None):\\n            raise ValueError((\\'hdds is a required parameter.  Make \\'\\n                              \\'sure you pass a list with at least \\'\\n                              \\'one Hdd object.\\'))\\n\\n        # Unpack hdds\\n        hdd = []\\n\\n        for value in hdds:\\n            hdd.append(value.specs)\\n\\n        # Perform Request\\n        data = {\\'hdds\\': hdd}\\n\\n        url = \\'%s/servers/%s/hardware/hdds\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Image Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_images(self, page=None, per_page=None, sort=None, q=None,\\n                    fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/images\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_image(self, image_id=None):\\n\\n        # Error Handling\\n        if(image_id is None):\\n            raise ValueError(\\'image_id is a required parameter\\')\\n\\n        # Perform Request\\n\\n        url = \\'%s/images/%s\\' % (self.base_url, image_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_image(self, image=None):\\n\\n        # Error Handling\\n        if(image.server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(image.name is None):\\n            raise ValueError(\\'name is a required parameter\\')\\n        if(image.frequency is None):\\n            raise ValueError(\\'frequency is a required parameter\\')\\n        if(image.num_images is None):\\n            raise ValueError(\\'num_images is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'server_id\\': image.server_id,\\n            \\'name\\': image.name,\\n            \\'frequency\\': image.frequency,\\n            \\'num_images\\': image.num_images,\\n            \\'description\\': image.description,\\n            \\'source\\': image.source,\\n            \\'url\\': image.url,\\n            \\'os_id\\': image.os_id,\\n            \\'type\\': image.type\\n        }\\n\\n        url = \\'%s/images\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            # Assign new image_id back to calling Image object\\n            response = r.json()\\n\\n            image.specs.update(image_id=response[\\'id\\'])\\n            image.specs.update(api_token=self.header)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_image(self, image_id=None):\\n\\n        # Error Handling\\n        if(image_id is None):\\n            raise ValueError(\\'image_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/images/%s\\' % (self.base_url, image_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_image(self, image_id=None, name=None, description=None,\\n                     frequency=None):\\n\\n        # Error Handling\\n        if(image_id is None):\\n            raise ValueError(\\'image_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'frequency\\': frequency,\\n            \\'description\\': description\\n        }\\n\\n        url = \\'%s/images/%s\\' % (self.base_url, image_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Shared Storage Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_shared_storages(self, page=None, per_page=None, sort=None,\\n                             q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/shared_storages\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_shared_storage(self, shared_storage_id=None):\\n\\n        # Error Handling\\n        if(shared_storage_id is None):\\n            raise ValueError(\\'shared_storage_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/shared_storages/%s\\' % (self.base_url, shared_storage_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_servers_attached_storage(self, shared_storage_id=None):\\n\\n        # Error Handling\\n        if(shared_storage_id is None):\\n            raise ValueError(\\'shared_storage_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/shared_storages/%s/servers\\' %\\n               (self.base_url, shared_storage_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_shared_storage_server(\\n            self,\\n            shared_storage_id=None,\\n            server_id=None):\\n\\n        # Error Handling\\n        if(shared_storage_id is None):\\n            raise ValueError(\\'shared_storage_id parameter is required\\')\\n        if(server_id is None):\\n            raise ValueError(\\'server_id parameter is required\\')\\n\\n        # Perform Request\\n        url = (\\'%s/shared_storages/%s/servers/%s\\' %\\n               (self.base_url, shared_storage_id, server_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_credentials(self):\\n\\n        # Perform Request\\n        url = \\'%s/shared_storages/access\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_shared_storage(self, shared_storage=None):\\n\\n        # Error Handling\\n        if(shared_storage.name is None):\\n            raise ValueError(\\'name is a required parameter\\')\\n        if(shared_storage.size is None):\\n            raise ValueError(\\'size is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': shared_storage.name,\\n            \\'description\\': shared_storage.description,\\n            \\'size\\': shared_storage.size,\\n            \\'datacenter_id\\': shared_storage.datacenter_id\\n        }\\n\\n        url = \\'%s/shared_storages\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            # Assign new shared_storage_id back to calling SharedStorage object\\n            response = r.json()\\n\\n            shared_storage.specs.update(shared_storage_id=response[\\'id\\'])\\n            shared_storage.specs.update(api_token=self.header)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def attach_server_shared_storage(self, shared_storage_id=None,\\n                                     server_ids=None):\\n\\n        # Error Handling\\n        if(shared_storage_id is None):\\n            raise ValueError(\\'shared_storage_id is a required parameter\\')\\n        if(server_ids is None):\\n            raise ValueError((\\'server_ids is a required parameter.  \\'\\n                              \\'Must attach at least one server\\'))\\n\\n        # Unpack servers\\n        servers = []\\n\\n        for value in server_ids:\\n            servers.append({\\'id\\': value.server_id, \\'rights\\': value.rights})\\n\\n        # Perform Request\\n        data = {\\'servers\\': servers}\\n\\n        url = (\\'%s/shared_storages/%s/servers\\' %\\n               (self.base_url, shared_storage_id))\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_shared_storage(self, shared_storage_id=None, name=None,\\n                              description=None, size=None):\\n\\n        # Error Handling\\n        if(shared_storage_id is None):\\n            raise ValueError(\\'shared_storage_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'size\\': size\\n        }\\n\\n        url = \\'%s/shared_storages/%s\\' % (self.base_url, shared_storage_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def change_password(self, password=None):\\n\\n        # Error Handlong\\n        if(password is None):\\n            raise ValueError((\\'password is a required parameter. \\'\\n                              \\'password must contain at least 8 characters.\\'))\\n\\n        # Perform Request\\n        data = {\\'password\\': password}\\n\\n        url = \\'%s/shared_storages/access\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_shared_storage(self, shared_storage_id=None):\\n\\n        # Error Handling\\n        if(shared_storage_id is None):\\n            raise ValueError(\\'shared_storage_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/shared_storages/%s\\' % (self.base_url, shared_storage_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def detach_server_shared_storage(self, shared_storage_id=None,\\n                                     server_id=None):\\n\\n        # Error Handling\\n        if(shared_storage_id is None):\\n            raise ValueError(\\'shared_storage_id is a required parameter\\')\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/shared_storages/%s/servers/%s\\' %\\n               (self.base_url, shared_storage_id, server_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Firewall Policy Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_firewall_policies(self, page=None, per_page=None, sort=None,\\n                               q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/firewall_policies\\' % self.base_url\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_firewall(self, firewall_id=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/firewall_policies/%s\\' % (self.base_url, firewall_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_firewall_servers(self, firewall_id=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/firewall_policies/%s/server_ips\\' %\\n               (self.base_url, firewall_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_firewall_server(self, firewall_id=None, server_ip_id=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n        if(server_ip_id is None):\\n            raise ValueError(\\'server_ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/firewall_policies/%s/server_ips/%s\\' %\\n               (self.base_url, firewall_id, server_ip_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_firewall_policy_rules(self, firewall_id=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/firewall_policies/%s/rules\\' % (self.base_url, firewall_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_firewall_policy_rule(self, firewall_id=None, rule_id=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n        if(rule_id is None):\\n            raise ValueError(\\'rule_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/firewall_policies/%s/rules/%s\\' %\\n               (self.base_url, firewall_id, rule_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_firewall(self, firewall_id=None, name=None, description=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description\\n        }\\n\\n        url = \\'%s/firewall_policies/%s\\' % (self.base_url, firewall_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_firewall_policy(self, firewall_policy=None,\\n                               firewall_policy_rules=None):\\n\\n        # Error Handling\\n        if(firewall_policy.specs[\\'name\\'] is None):\\n            raise ValueError((\\'Policy name is required.  Make sure your \\'\\n                              \\'FirewallPolicy object was initialized with \\'\\n                              \\'a name parameter\\'))\\n        if(firewall_policy_rules is None):\\n            raise ValueError((\\'firewall_policy_rules is required.  Make sure \\'\\n                              \\'you pass a list with at least one \\'\\n                              \\'FirewallPolicyRule object.\\'))\\n\\n        # Unpack Rules\\n        rules = []\\n\\n        for value in firewall_policy_rules:\\n            rules.append(value.rule_set)\\n\\n        # Attach rules and Perform Request\\n        firewall_policy.specs[\\'rules\\'] = rules\\n\\n        url = \\'%s/firewall_policies\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(\\n                url, headers=self.header, json=firewall_policy.specs)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            # Assign new firewall_id back to calling FirewallPolicy object\\n            response = r.json()\\n\\n            firewall_policy.specs.update(firewall_id=response[\\'id\\'])\\n            firewall_policy.specs.update(api_token=self.header)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_firewall_policy_rule(self, firewall_id=None,\\n                                 firewall_policy_rules=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n        if(firewall_policy_rules is None):\\n            raise ValueError((\\'firewall_policy_rules is required.  Make \\'\\n                              \\'sure you pass a list with at least one \\'\\n                              \\'FirewallPolicyRule object\\'))\\n\\n        # Unpack rules\\n        rules = []\\n\\n        for value in firewall_policy_rules:\\n            rules.append(value.rule_set)\\n\\n        # Perform Request\\n        data = {\\'rules\\': rules}\\n\\n        url = \\'%s/firewall_policies/%s/rules\\' % (self.base_url, firewall_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def attach_server_firewall_policy(self, firewall_id=None, server_ips=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n        if(server_ips is None):\\n            raise ValueError((\\'server_ips is required. Make sure you pass \\'\\n                              \\'a list with at least one AttachServer object\\'))\\n\\n        # Unpack servers\\n        servers = []\\n\\n        for value in server_ips:\\n            servers.append(value.server_ip_id)\\n\\n        # Perform Request\\n        data = {\\'server_ips\\': servers}\\n\\n        url = (\\'%s/firewall_policies/%s/server_ips\\' %\\n               (self.base_url, firewall_id))\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_firewall(self, firewall_id=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/firewall_policies/%s\\' % (self.base_url, firewall_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_firewall_rule(self, firewall_id=None, rule_id=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n        if(rule_id is None):\\n            raise ValueError(\\'rule_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/firewall_policies/%s/rules/%s\\' %\\n               (self.base_url, firewall_id, rule_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Load Balancer Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_load_balancers(self, page=None, per_page=None, sort=None, q=None,\\n                            fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/load_balancers\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_load_balancer(self, load_balancer_id=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/load_balancers/%s\\' % (self.base_url, load_balancer_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_load_balancer_servers(self, load_balancer_id=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/load_balancers/%s/server_ips\\' %\\n               (self.base_url, load_balancer_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_load_balancer_server(self, load_balancer_id=None,\\n                                 server_ip_id=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n        if(server_ip_id is None):\\n            raise ValueError(\\'server_ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/load_balancers/%s/server_ips/%s\\' %\\n               (self.base_url, load_balancer_id, server_ip_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def load_balancer_rules(self, load_balancer_id=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/load_balancers/%s/rules\\' % (self.base_url, load_balancer_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_load_balancer_rule(self, load_balancer_id=None, rule_id=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n        if(rule_id is None):\\n            raise ValueError(\\'rule_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/load_balancers/%s/rules/%s\\' %\\n               (self.base_url, load_balancer_id, rule_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_load_balancer(\\n            self,\\n            load_balancer_id=None,\\n            name=None,\\n            description=None,\\n            health_check_test=None,\\n            health_check_interval=None,\\n            health_check_path=None,\\n            health_check_parse=None,\\n            persistence=None,\\n            persistence_time=None,\\n            method=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n\\n        if(method is not None and method != \\'ROUND_ROBIN\\' and\\n           method != \\'LEAST_CONNECTIONS\\'):\\n            raise ValueError((\\'method must be set to either \"ROUND_ROBIN\" \\'\\n                              \\'or \"LEAST_CONNECTIONS\".\\'))\\n\\n        if(health_check_test is not None and health_check_test != \\'TCP\\'):\\n            raise ValueError((\\'health_check_test must be set to \"TCP\". \\'\\n                              \\'\"HTTP\" is not currently supported.\\'))\\n\\n        if(health_check_interval is not None and health_check_interval < 5 and health_check_interval > 300):\\n            raise ValueError((\\'health_check_interval must be an integer \\'\\n                              \\'between 5 and 300.\\'))\\n\\n        if(persistence_time is not None and persistence_time < 30 and persistence_time > 1200):\\n            raise ValueError((\\'persistence_time must be an integer \\'\\n                              \\'between 30 and 1200.\\'))\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'health_check_test\\': health_check_test,\\n            \\'health_check_interval\\': health_check_interval,\\n            \\'health_check_path\\': health_check_path,\\n            \\'health_check_parse\\': health_check_parse,\\n            \\'persistence\\': persistence,\\n            \\'persistence_time\\': persistence_time,\\n            \\'method\\': method\\n        }\\n\\n        url = \\'%s/load_balancers/%s\\' % (self.base_url, load_balancer_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_load_balancer(self, load_balancer=None,\\n                             load_balancer_rules=None):\\n\\n        # Error Handling\\n        if(load_balancer is None):\\n            raise ValueError((\\'load_balancer parameter is required.  Must \\'\\n                              \\'pass a LoadBalancer object.\\'))\\n        if(load_balancer_rules is None):\\n            raise ValueError((\\'load_balancer_rules parameter is required. \\'\\n                              \\'Must pass a list with at least one \\'\\n                              \\'LoadBalancerRule object.\\'))\\n        if(load_balancer.specs[\\'method\\'] is not None and\\n                load_balancer.specs[\\'method\\'] != \\'ROUND_ROBIN\\' and\\n                load_balancer.specs[\\'method\\'] != \\'LEAST_CONNECTIONS\\'):\\n            raise ValueError((\\'method must be set to either \"ROUND_ROBIN\" \\'\\n                              \\'or \"LEAST_CONNECTIONS\".\\'))\\n\\n        # Unpack rules\\n        rules = []\\n\\n        for value in load_balancer_rules:\\n            rules.append(value.rule_set)\\n\\n        # Perform Request\\n        load_balancer.specs[\\'rules\\'] = rules\\n\\n        url = \\'%s/load_balancers\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(\\n                url, headers=self.header, json=load_balancer.specs)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            # Assign new load_balancer_id back to calling LoadBalancer object\\n            response = r.json()\\n\\n            load_balancer.specs.update(load_balancer_id=response[\\'id\\'])\\n            load_balancer.specs.update(api_token=self.header)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def attach_load_balancer_server(self, load_balancer_id=None,\\n                                    server_ips=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter.\\')\\n        if(server_ips is None):\\n            raise ValueError((\\'server_ips is a required parameter. Must \\'\\n                              \\'pass a list with at least one AttachServer \\'\\n                              \\'object\\'))\\n\\n        # Unpack servers\\n        servers = []\\n\\n        for value in server_ips:\\n            servers.append(value.server_ip_id)\\n\\n        # Perform Request\\n        data = {\\'server_ips\\': servers}\\n\\n        url = (\\'%s/load_balancers/%s/server_ips\\' %\\n               (self.base_url, load_balancer_id))\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_load_balancer_rule(self, load_balancer_id=None,\\n                               load_balancer_rules=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter.\\')\\n        if(load_balancer_rules is None):\\n            raise ValueError((\\'load_balancer_rules is a required \\'\\n                              \\'parameter. Must pass a list with at least one \\'\\n                              \\'LoadBalancerRule object\\'))\\n\\n        # Unpack rules\\n        rules = []\\n\\n        for value in load_balancer_rules:\\n            rules.append(value.rule_set)\\n\\n        # Perform Request\\n        data = {\\'rules\\': rules}\\n\\n        url = (\\'%s/load_balancers/%s/rules\\' %\\n               (self.base_url, load_balancer_id))\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_load_balancer(self, load_balancer_id=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/load_balancers/%s\\' % (self.base_url, load_balancer_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_load_balancer_server(self, load_balancer_id=None,\\n                                    server_ip_id=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter.\\')\\n        if(server_ip_id is None):\\n            raise ValueError(\\'server_ip_id is a required parameter.\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/load_balancers/%s/server_ips/%s\\' %\\n               (self.base_url, load_balancer_id, server_ip_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_load_balancer_rule(self, load_balancer_id=None, rule_id=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter.\\')\\n        if(rule_id is None):\\n            raise ValueError(\\'rule_id is a required parameter.\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/load_balancers/%s/rules/%s\\' %\\n               (self.base_url, load_balancer_id, rule_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Public IP Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_public_ips(self, page=None, per_page=None, sort=None, q=None,\\n                        fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/public_ips\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_public_ip(self, ip_id=None):\\n\\n        # Error Handling\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/public_ips/%s\\' % (self.base_url, ip_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_public_ip(self, reverse_dns=None, ip_type=None,\\n                         datacenter_id=None):\\n\\n        # Error Handling\\n        if(ip_type != \\'IPV4\\' and ip_type is not None):\\n            raise ValueError(\\'ip_type must be set to \"IPV4\".\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'reverse_dns\\': reverse_dns,\\n            \\'type\\': ip_type,\\n            \\'datacenter_id\\': datacenter_id\\n        }\\n\\n        url = \\'%s/public_ips\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_public_ip(self, ip_id=None, reverse_dns=None):\\n\\n        # Error Handling\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\'reverse_dns\\': reverse_dns}\\n\\n        url = \\'%s/public_ips/%s\\' % (self.base_url, ip_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_public_ip(self, ip_id=None):\\n\\n        # Error Handling\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/public_ips/%s\\' % (self.base_url, ip_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Private Network Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_private_networks(self, page=None, per_page=None, sort=None,\\n                              q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/private_networks\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_private_network(self, private_network_id):\\n\\n        # Error Handling\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/private_networks/%s\\' % (self.base_url, private_network_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_private_network_servers(self, private_network_id=None):\\n\\n        # Error Handling\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/private_networks/%s/servers\\' %\\n               (self.base_url, private_network_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_private_network_server(self, private_network_id=None,\\n                                   server_id=None):\\n\\n        # Error Handling\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/private_networks/%s/servers/%s\\' %\\n               (self.base_url, private_network_id, server_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_private_network(self, private_network=None):\\n\\n        # Error Handling\\n        if(private_network.name is None):\\n            raise ValueError(\\'name is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': private_network.name,\\n            \\'description\\': private_network.description,\\n            \\'network_address\\': private_network.network_address,\\n            \\'subnet_mask\\': private_network.subnet_mask,\\n            \\'datacenter_id\\': private_network.datacenter_id\\n        }\\n\\n        url = \\'%s/private_networks\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            # Assign new private_network_id back to calling PrivateNetwork\\n            # object\\n            response = r.json()\\n\\n            private_network.specs.update(private_network_id=response[\\'id\\'])\\n            private_network.specs.update(api_token=self.header)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def attach_private_network_servers(self, private_network_id=None,\\n                                       server_ids=None):\\n\\n        # Error Handling\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n        if(server_ids is None):\\n            raise ValueError((\\'server_ids is a required parameter.  Make \\'\\n                              \\'sure you pass a list with at least one \\'\\n                              \\'server_id string\\'))\\n\\n        # Unpack servers\\n        servers = []\\n\\n        for value in server_ids:\\n            servers.append(value.server_id)\\n\\n        # Perform Request\\n        data = {\\'servers\\': servers}\\n\\n        url = (\\'%s/private_networks/%s/servers\\' %\\n               (self.base_url, private_network_id))\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_private_network(\\n            self,\\n            private_network_id=None,\\n            name=None,\\n            description=None,\\n            network_address=None,\\n            subnet_mask=None):\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'network_address\\': network_address,\\n            \\'subnet_mask\\': subnet_mask\\n        }\\n\\n        url = \\'%s/private_networks/%s\\' % (self.base_url, private_network_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_private_network(self, private_network_id=None):\\n\\n        # Error Handling\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/private_networks/%s\\' % (self.base_url, private_network_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_private_network_server(self, private_network_id=None,\\n                                      server_id=None):\\n\\n        # Error Handling\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/private_networks/%s/servers/%s\\' %\\n               (self.base_url, private_network_id, server_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Monitoring Center Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_server_usages(self, page=None, per_page=None, sort=None,\\n                           q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/monitoring_center\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_usage(self, server_id=None, period=\\'LAST_24H\\',\\n                  start_date=None, end_date=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(period == \\'CUSTOM\\'):\\n            if(start_date is None):\\n                raise ValueError((\\'start_date parameter is required when \\'\\n                                  \\'using CUSTOM period\\'))\\n            if(end_date is None):\\n                raise ValueError((\\'end_date parameter is required when \\'\\n                                  \\'using CUSTOM period\\'))\\n\\n        # Perform Request\\n        parameters = {\\n            \\'period\\': period,\\n            \\'start_date\\': start_date,\\n            \\'end_date\\': end_date\\n        }\\n\\n        url = \\'%s/monitoring_center/%s\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Monitoring Policy Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_monitoring_policies(self, page=None, per_page=None,\\n                                 sort=None, q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/monitoring_policies\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_monitoring_policy(self, monitoring_policy_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s\\' %\\n               (self.base_url, monitoring_policy_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_monitoring_policy_ports(self, monitoring_policy_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/ports\\' %\\n               (self.base_url, monitoring_policy_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_monitoring_policy_port(self, monitoring_policy_id=None,\\n                                   port_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(port_id is None):\\n            raise ValueError(\\'port_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/ports/%s\\' %\\n               (self.base_url, monitoring_policy_id, port_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_monitoring_policy_processes(self, monitoring_policy_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/processes\\' %\\n               (self.base_url, monitoring_policy_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_monitoring_policy_process(self, monitoring_policy_id=None,\\n                                      process_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(process_id is None):\\n            raise ValueError(\\'process_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/processes/%s\\' %\\n               (self.base_url, monitoring_policy_id, process_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_monitoring_policy_servers(self, monitoring_policy_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/servers\\' %\\n               (self.base_url, monitoring_policy_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_monitoring_policy_server(self, monitoring_policy_id=None,\\n                                     server_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/servers/%s\\' %\\n               (self.base_url, monitoring_policy_id, server_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_monitoring_policy(self, monitoring_policy_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/monitoring_policies/%s\\' %\\n               (self.base_url, monitoring_policy_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def delete_monitoring_policy_port(self, monitoring_policy_id=None,\\n                                      port_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(port_id is None):\\n            raise ValueError(\\'port_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/monitoring_policies/%s/ports/%s\\' %\\n               (self.base_url, monitoring_policy_id, port_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def delete_monitoring_policy_process(self, monitoring_policy_id=None,\\n                                         process_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(process_id is None):\\n            raise ValueError(\\'process_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/monitoring_policies/%s/processes/%s\\' %\\n               (self.base_url, monitoring_policy_id, process_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def detach_monitoring_policy_server(self, monitoring_policy_id=None,\\n                                        server_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/monitoring_policies/%s/servers/%s\\' %\\n               (self.base_url, monitoring_policy_id, server_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_monitoring_policy(self, monitoring_policy=None,\\n                                 thresholds=None, ports=None, processes=None):\\n\\n        # Error Handling\\n        if(monitoring_policy is None):\\n            raise ValueError((\\'monitoring_policy is a required parameter. \\'\\n                              \\'Make sure you pass a MonitoringPolicy object.\\'))\\n        if(thresholds is None):\\n            raise ValueError((\\'thresholds is a required parameter.  Make \\'\\n                              \\'sure you pass a list with all 5 Threshold \\'\\n                              \\'objects(cpu, ram, disk, transfer, \\'\\n                              \\'internal_ping).\\'))\\n        if(ports is None):\\n            raise ValueError(\\n                (\\'ports is a required parameter.  Make sure \\'\\n                 \\'you pass a list with at least one Port object.\\'))\\n        if(processes is None):\\n            raise ValueError((\\'processes is a required parameter.  Make \\'\\n                              \\'sure you pass a list with at least one \\'\\n                              \\'Process object.\\'))\\n\\n        # Unpack Thresholds\\n        new_thresholds = {}\\n\\n        for value in thresholds:\\n            new_thresholds[value.entity] = {\\n                \\'warning\\': {\\n                    \\'value\\': value.warning_value,\\n                    \\'alert\\': value.warning_alert\\n                },\\n                \\'critical\\': {\\n                    \\'value\\': value.critical_value,\\n                    \\'alert\\': value.critical_alert\\n                }\\n            }\\n\\n        # Unpack Ports\\n        new_ports = []\\n\\n        for value in ports:\\n            new_ports.append(value.specs)\\n\\n        # Unpack Processes\\n        new_processes = []\\n\\n        for value in processes:\\n            new_processes.append(value.process_set)\\n\\n        # Add Ports, Processes, and Thresholds to Monitoring Policy object\\n        monitoring_policy.specs[\\'thresholds\\'] = new_thresholds\\n        monitoring_policy.specs[\\'ports\\'] = new_ports\\n        monitoring_policy.specs[\\'processes\\'] = new_processes\\n\\n        # Perform Request\\n        url = \\'%s/monitoring_policies\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header,\\n                                              json=monitoring_policy.specs)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            # Assign new monitoring_policy_id back to calling MonitoringPolicy\\n            # object\\n            response = r.json()\\n\\n            monitoring_policy.specs.update(monitoring_policy_id=response[\\'id\\'])\\n            monitoring_policy.specs.update(api_token=self.header)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_port(self, monitoring_policy_id=None, ports=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(ports is None):\\n            raise ValueError((\\'ports is a required parameter. Make sure you \\'\\n                              \\'send in a list with at least one Port object\\'))\\n\\n        # Unpack ports\\n        new_ports = []\\n\\n        for value in ports:\\n            new_ports.append(value.specs)\\n\\n        # Perform Request\\n        data = {\\'ports\\': new_ports}\\n\\n        url = (\\'%s/monitoring_policies/%s/ports\\' %\\n               (self.base_url, monitoring_policy_id))\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_process(self, monitoring_policy_id=None, processes=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(processes is None):\\n            raise ValueError((\\'processes is a required parameter. Make \\'\\n                              \\'sure you send in a list with at least one \\'\\n                              \\'Process object\\'))\\n\\n        # Unpack processes\\n        new_processes = []\\n\\n        for value in processes:\\n            new_processes.append(value.process_set)\\n\\n        # Perform Request\\n        data = {\\'processes\\': new_processes}\\n\\n        url = (\\'%s/monitoring_policies/%s/processes\\' %\\n               (self.base_url, monitoring_policy_id))\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def attach_monitoring_policy_server(self, monitoring_policy_id=None,\\n                                        servers=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(servers is None):\\n            raise ValueError((\\'servers is a required parameter. Make sure \\'\\n                              \\'you send in a list with at least one \\'\\n                              \\'AttachServer object\\'))\\n\\n        # Unpack servers\\n        add_servers = []\\n\\n        for value in servers:\\n            add_servers.append(value.server_id)\\n\\n        # Perform Request\\n        data = {\\'servers\\': add_servers}\\n\\n        url = (\\'%s/monitoring_policies/%s/servers\\' %\\n               (self.base_url, monitoring_policy_id))\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_monitoring_policy(\\n            self,\\n            monitoring_policy_id=None,\\n            monitoring_policy=None,\\n            thresholds=None,\\n            test=False):\\n\\n        try:\\n            # Error Handling\\n            if(monitoring_policy_id is None):\\n                raise ValueError(\\n                    \\'monitoring_policy_id is a required parameter\\')\\n\\n            # Use flag to skip this live API call when running unit test\\n            if(test == False):\\n                # Make request for existing monitoring policy object\\n                json = self.get_monitoring_policy(\\n                    monitoring_policy_id=monitoring_policy_id)\\n\\n                # Update policy specs with new values, if necessary.\\n                if(monitoring_policy):\\n                    if(json[\\'name\\'] != monitoring_policy.specs[\\'name\\']):\\n                        if(monitoring_policy.specs[\\'name\\'] is not None):\\n                            json[\\'name\\'] = monitoring_policy.specs[\\'name\\']\\n\\n                    if(json[\\'description\\'] !=\\n                            monitoring_policy.specs[\\'description\\']):\\n                        if(monitoring_policy.specs[\\'description\\'] is not None):\\n                            json[\\'description\\'] = monitoring_policy.specs[\\'description\\']\\n\\n                    if(json[\\'email\\'] != monitoring_policy.specs[\\'email\\']):\\n                        if(monitoring_policy.specs[\\'email\\'] is not None):\\n                            json[\\'email\\'] = monitoring_policy.specs[\\'email\\']\\n\\n                # Unpack thresholds\\n                if(thresholds):\\n                    new_thresholds = {}\\n\\n                    for value in thresholds:\\n                        new_thresholds[value.entity] = {\\n                            \\'warning\\': {\\n                                \\'value\\': value.warning_value,\\n                                \\'alert\\': value.warning_alert\\n                            },\\n                            \\'critical\\': {\\n                                \\'value\\': value.critical_value,\\n                                \\'alert\\': value.critical_alert\\n                            }\\n                        }\\n\\n                    # Compare all threshold values and update, if necessary.\\n                    threshold_entities = [\\'cpu\\', \\'ram\\', \\'disk\\', \\'transfer\\',\\n                                          \\'internal_ping\\']\\n\\n                    for value in threshold_entities:\\n\\n                        if(value in new_thresholds.keys()):\\n                            if(json[\\'thresholds\\'][value][\\'warning\\'][\\'value\\'] !=\\n                                    new_thresholds[value][\\'warning\\'][\\'value\\']):\\n                                json[\\'thresholds\\'][value][\\'warning\\'][\\'value\\'] = new_thresholds[value][\\'warning\\'][\\'value\\']\\n\\n                            if(json[\\'thresholds\\'][value][\\'warning\\'][\\'alert\\'] !=\\n                                    new_thresholds[value][\\'warning\\'][\\'alert\\']):\\n                                json[\\'thresholds\\'][value][\\'warning\\'][\\'alert\\'] = new_thresholds[value][\\'warning\\'][\\'alert\\']\\n\\n                            if(json[\\'thresholds\\'][value][\\'critical\\'][\\'value\\'] !=\\n                                    new_thresholds[value][\\'critical\\'][\\'value\\']):\\n                                json[\\'thresholds\\'][value][\\'critical\\'][\\'value\\'] = new_thresholds[value][\\'critical\\'][\\'value\\']\\n\\n                            if(json[\\'thresholds\\'][value][\\'critical\\'][\\'alert\\'] !=\\n                                    new_thresholds[value][\\'critical\\'][\\'alert\\']):\\n                                json[\\'thresholds\\'][value][\\'critical\\'][\\'alert\\'] = new_thresholds[value][\\'critical\\'][\\'alert\\']\\n\\n                # Perform Request\\n                data = {\\n                    \\'name\\': json[\\'name\\'],\\n                    \\'description\\': json[\\'description\\'],\\n                    \\'email\\': json[\\'email\\'],\\n                    \\'thresholds\\': json[\\'thresholds\\']\\n                }\\n\\n                url = (\\'%s/monitoring_policies/%s\\' %\\n                       (self.base_url, monitoring_policy_id))\\n\\n                r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            else:\\n                # Mock Request for Unit Testing\\n                r = requests_retry_session().put(\\n                    self.base_url + \\'/monitoring_policies/%s\\' %\\n                    (monitoring_policy_id), headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def modify_port(self, monitoring_policy_id=None, port_id=None, port=None,\\n                    test=False):\\n\\n        try:\\n            # Error Handling\\n            if(monitoring_policy_id is None):\\n                raise ValueError(\\n                    \\'monitoring_policy_id is a required parameter\\')\\n            if(port_id is None):\\n                raise ValueError(\\'port_id is a required parameter\\')\\n\\n            # Use flag to skip this live API call when running unit test\\n            if(test == False):\\n                # Make request for existing port object\\n                json = self.get_monitoring_policy_port(\\n                    monitoring_policy_id=monitoring_policy_id, port_id=port_id)\\n                del json[\\'id\\']\\n\\n                # Update port object with new values, if necessary.\\n                if(json[\\'alert_if\\'] != port.specs[\\'alert_if\\']):\\n                    if(port.specs[\\'alert_if\\'] is not None):\\n                        json[\\'alert_if\\'] = port.specs[\\'alert_if\\']\\n\\n                if(json[\\'email_notification\\'] != port.specs[\\'email_notification\\']):\\n                    if(port.specs[\\'email_notification\\'] is not None):\\n                        json[\\'email_notification\\'] = port.specs[\\'email_notification\\']\\n\\n                # Perform Request\\n                data = {\\'ports\\': json}\\n\\n                url = (\\'%s/monitoring_policies/%s/ports/%s\\' %\\n                       (self.base_url, monitoring_policy_id, port_id))\\n\\n                r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            else:\\n                # Mock Request for Unit Testing\\n                r = requests_retry_session().put(\\n                    self.base_url + \\'/monitoring_policies/%s/ports/%s\\' %\\n                    (monitoring_policy_id, port_id), headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def modify_process(self, monitoring_policy_id=None, process_id=None,\\n                       process=None, test=False):\\n\\n        try:\\n            # Error Handling\\n            if(monitoring_policy_id is None):\\n                raise ValueError(\\n                    \\'monitoring_policy_id is a required parameter\\')\\n            if(process_id is None):\\n                raise ValueError(\\'process_id is a required parameter\\')\\n\\n            # Use flag to skip this live API call when running unit test\\n            if(test == False):\\n                # Make request for existing process object\\n                json = self.get_monitoring_policy_process(\\n                    monitoring_policy_id=monitoring_policy_id,\\n                    process_id=process_id)\\n                del json[\\'id\\']\\n\\n                # Update process object with new values, if necessary.\\n                if(json[\\'alert_if\\'] != process.process_set[\\'alert_if\\']):\\n                    if(process.process_set[\\'alert_if\\'] is not None):\\n                        json[\\'alert_if\\'] = process.process_set[\\'alert_if\\']\\n\\n                if(json[\\'email_notification\\'] !=\\n                        process.process_set[\\'email_notification\\']):\\n                    if(process.process_set[\\'email_notification\\'] is not None):\\n                        json[\\'email_notification\\'] = process.process_set[\\'email_notification\\']\\n\\n                # Perform Request\\n                data = {\\'processes\\': json}\\n\\n                url = (\\'%s/monitoring_policies/%s/processes/%s\\' %\\n                       (self.base_url, monitoring_policy_id, process_id))\\n\\n                r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            else:\\n                # Mock Request for Unit Testing\\n                r = requests_retry_session().put(\\n                    self.base_url + \\'/monitoring_policies/%s/processes/%s\\' %\\n                    (monitoring_policy_id, process_id), headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Log Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_logs(\\n            self,\\n            page=None,\\n            per_page=None,\\n            sort=None,\\n            q=None,\\n            fields=None,\\n            period=\\'LAST_24H\\',\\n            start_date=None,\\n            end_date=None):\\n\\n        # Error Handling\\n        if(period == \\'CUSTOM\\'):\\n            if(start_date is None):\\n                raise ValueError((\\'start_date parameter is required when \\'\\n                                  \\'using CUSTOM period\\'))\\n            if(end_date is None):\\n                raise ValueError((\\'end_date parameter is required when \\'\\n                                  \\'using CUSTOM period\\'))\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields,\\n            \\'period\\': period,\\n            \\'start_date\\': start_date,\\n            \\'end_date\\': end_date\\n        }\\n\\n        url = \\'%s/logs\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_log(self, log_id=None):\\n\\n        # Error Handling\\n        if(log_id is None):\\n            raise ValueError(\\'log_id parameter is required\\')\\n\\n        # Perform Request\\n        url = \\'%s/logs/%s\\' % (self.base_url, log_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # User Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_users(self, page=None, per_page=None, sort=None, q=None,\\n                   fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/users\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_user(self, user_id=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/users/%s\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def api_info(self, user_id=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/users/%s/api\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def show_api_key(self, user_id=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/users/%s/api/key\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def show_user_permissions(self):\\n\\n        # Perform Request\\n        url = \\'%s/users/current_user_permissions\\' % (self.base_url)\\n\\n        r = requests.get(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def ips_api_access_allowed(self, user_id=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/users/%s/api/ips\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_user(self, name=None, password=None, email=None,\\n                    description=None):\\n\\n        # Error Handling\\n        if(name is None):\\n            raise ValueError(\\'name is a required parameter\\')\\n        if(password is None):\\n            raise ValueError(\\'password is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'password\\': password,\\n            \\'email\\': email,\\n            \\'description\\': description\\n        }\\n\\n        url = \\'%s/users\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_user_ip(self, user_id=None, user_ips=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n        if(user_ips is None):\\n            raise ValueError((\\'user_ips is a required parameter. Make \\'\\n                              \\'sure you pass a list with at least \\'\\n                              \\'one IP string.\\'))\\n\\n        # Unpack IPs\\n        ips = []\\n\\n        for value in user_ips:\\n            ips.append(value)\\n\\n        # Perform Request\\n        data = {\\'ips\\': ips}\\n\\n        url = \\'%s/users/%s/api/ips\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_user(self, user_id=None, description=None, email=None,\\n                    password=None, state=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n        if(password is not None) and (len(password) < 8):\\n            raise ValueError(\\'password must be at least 8 characters long\\')\\n        if(state is not None) and (state != \\'ACTIVE\\') and (state != \\'DISABLE\\'):\\n            raise ValueError(\\'state should be set to \"ACTIVE\" or \"DISABLE\".\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'description\\': description,\\n            \\'email\\': email,\\n            \\'password\\': password,\\n            \\'state\\': state\\n        }\\n\\n        url = \\'%s/users/%s\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def modify_user_api(self, user_id=None, active=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n        if(active is not None) and (active != True) and (active != False):\\n            raise ValueError(\\'active parameter only accepts a boolean value\\')\\n\\n        # Perform Request\\n        data = {\\'active\\': active}\\n\\n        url = \\'%s/users/%s/api\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def change_api_key(self, user_id=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/users/%s/api/key\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_user(self, user_id=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/users/%s\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_user_ip(self, user_id=None, ip=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n        if(ip is None):\\n            raise ValueError(\\'ip is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/users/%s/api/ips/%s\\' % (self.base_url, user_id, ip)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Usage Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_usages(\\n            self,\\n            page=None,\\n            per_page=None,\\n            sort=None,\\n            q=None,\\n            fields=None,\\n            period=\\'LAST_24H\\',\\n            start_date=None,\\n            end_date=None):\\n\\n        # Error Handling\\n        if(period == \\'CUSTOM\\'):\\n            if(start_date is None):\\n                raise ValueError((\\'start_date parameter is required when \\'\\n                                  \\'using CUSTOM period\\'))\\n            if(end_date is None):\\n                raise ValueError((\\'end_date parameter is required when \\'\\n                                  \\'using CUSTOM period\\'))\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields,\\n            \\'period\\': period,\\n            \\'start_date\\': start_date,\\n            \\'end_date\\': end_date\\n        }\\n\\n        url = \\'%s/usages\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_pricing(self):\\n\\n        # Perform Request\\n        url = \\'%s/pricing\\' % (self.base_url)\\n\\n        r = requests.get(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # Recovery images\\n\\n    # \\'GET\\' Methods\\n\\n    def list_recovery_images(self, page=None, per_page=None, sort=None,\\n                             q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/recovery_appliances\\' % self.base_url\\n\\n        r = requests.get(url, headers=self.header, params=parameters)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def get_recovery_image(self, image_id=None):\\n\\n        # Error Handling\\n        if(image_id is None):\\n            raise ValueError(\\'appliance_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/recovery_appliances/%s\\' % (self.base_url, image_id)\\n\\n        r = requests.get(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # Server Appliance Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_appliances(self, page=None, per_page=None, sort=None,\\n                        q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/server_appliances\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_appliance(self, appliance_id=None):\\n\\n        # Error Handling\\n        if(appliance_id is None):\\n            raise ValueError(\\'appliance_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/server_appliances/%s\\' % (self.base_url, appliance_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # DVD Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_dvds(self, page=None, per_page=None, sort=None,\\n                  q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/dvd_isos\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_dvd(self, iso_id=None):\\n\\n        # Error Handling\\n        if(iso_id is None):\\n            raise ValueError(\\'iso_id parameter is required\\')\\n\\n        # Perform Request\\n        url = \\'%s/dvd_isos/%s\\' % (self.base_url, iso_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Datacenter Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_datacenters(self, page=None, per_page=None, sort=None,\\n                         q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/datacenters\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_datacenter(self, datacenter_id=None):\\n\\n        # Error Handling\\n        if(datacenter_id is None):\\n            raise ValueError(\\'datacenter_id parameter is required\\')\\n\\n        # Perform Request\\n        url = \\'%s/datacenters/%s\\' % (self.base_url, datacenter_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Pricing Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def pricing(self):\\n\\n        # Perform Request\\n        url = \\'%s/pricing\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Ping Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def ping(self):\\n\\n        # Perform Request\\n        url = \\'%s/ping\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Ping Auth Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def ping_auth(self):\\n\\n        # Perform Request\\n        url = \\'%s/ping_auth\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # VPN Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_vpns(self, page=None, per_page=None, sort=None, q=None,\\n                  fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/vpns\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_vpn(self, vpn_id=None):\\n\\n        # Error Handling\\n        if(vpn_id is None):\\n            raise ValueError(\\'vpn_id is a required parameter\\')\\n\\n        # Perform Request\\n\\n        url = \\'%s/vpns/%s\\' % (self.base_url, vpn_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def download_config(self, vpn_id=None, file_path=None):\\n        # Error Handling\\n        if(vpn_id is None):\\n            raise ValueError(\\'vpn_id is a required parameter\\')\\n\\n        # Perform Request\\n\\n        url = \\'%s/vpns/%s/configuration_file\\' % (self.base_url, vpn_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n                body = r.json()\\n                filestring = base64.b64decode(body[\"config_zip_file\"])\\n                zipPath = file_path + \\'.zip\\'\\n                with open(zipPath, \\'wb\\') as zipFile:\\n                    zipFile.write(filestring)\\n\\n                    return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_vpn(self, vpn=None):\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': vpn.name,\\n            \\'description\\': vpn.description,\\n            \\'datacenter_id\\': vpn.datacenter_id\\n        }\\n\\n        url = \\'%s/vpns\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            # Assign new image_id back to calling Image object\\n            response = r.json()\\n\\n            vpn.specs.update(vpn_id=response[\\'id\\'])\\n            vpn.specs.update(api_token=self.header)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_vpn(self, vpn_id=None):\\n\\n        # Error Handling\\n        if(vpn_id is None):\\n            raise ValueError(\\'vpn_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/vpns/%s\\' % (self.base_url, vpn_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_vpn(self, vpn_id=None, name=None, description=None):\\n\\n        # Error Handling\\n        if(vpn_id is None):\\n            raise ValueError(\\'vpn_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description\\n        }\\n\\n        url = \\'%s/vpns/%s\\' % (self.base_url, vpn_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Role Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_roles(self, page=None, per_page=None, sort=None, q=None,\\n                   fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/roles\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_role(self, role_id=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n\\n        url = \\'%s/roles/%s\\' % (self.base_url, role_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def permissions(self, role_id=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n\\n        url = \\'%s/roles/%s/permissions\\' % (self.base_url, role_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def current_user_permissions(self):\\n\\n        # Perform Request\\n\\n        url = \\'%s/users/current_user_permissions\\' % (self.base_url)\\n\\n        r = requests_retry_session().get(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def role_users(self, role_id=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n\\n        url = \\'%s/roles/%s/users\\' % (self.base_url, role_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_role_user(self, role_id=None, user_id=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/roles/%s/users/%s\\' % (self.base_url, role_id, user_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_role(self, name=None):\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name\\n        }\\n\\n        url = \\'%s/roles\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_users(self, role_id=None, users=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'users\\': users\\n        }\\n\\n        url = \\'%s/roles/%s/users\\' % (self.base_url, role_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def clone_role(self, role_id=None, name=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name\\n        }\\n\\n        url = \\'%s/roles/%s/clone\\' % (self.base_url, role_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_role(self, role_id=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/roles/%s\\' % (self.base_url, role_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_user(self, role_id=None, user_id=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/roles/%s/users/%s\\' % (self.base_url, role_id, user_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_role(self, role_id=None, name=None, description=None,\\n                    state=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'state\\': state\\n        }\\n\\n        url = \\'%s/roles/%s\\' % (self.base_url, role_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def modify_permissions(\\n            self,\\n            role_id=None,\\n            servers=None,\\n            images=None,\\n            shared_storages=None,\\n            firewalls=None,\\n            load_balancers=None,\\n            ips=None,\\n            private_networks=None,\\n            vpns=None,\\n            monitoring_centers=None,\\n            monitoring_policies=None,\\n            backups=None,\\n            logs=None,\\n            users=None,\\n            roles=None,\\n            usages=None,\\n            interactive_invoices=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'servers\\': servers,\\n            \\'images\\': images,\\n            \\'sharedstorages\\': shared_storages,\\n            \\'firewalls\\': firewalls,\\n            \\'loadbalancers\\': load_balancers,\\n            \\'ips\\': ips,\\n            \\'privatenetwork\\': private_networks,\\n            \\'vpn\\': vpns,\\n            \\'monitoringcenter\\': monitoring_centers,\\n            \\'monitoringpolicies\\': monitoring_policies,\\n            \\'backups\\': backups,\\n            \\'logs\\': logs,\\n            \\'users\\': users,\\n            \\'roles\\': roles,\\n            \\'usages\\': usages,\\n            \\'interactiveinvoice\\': interactive_invoices\\n        }\\n\\n        url = \\'%s/roles/%s/permissions\\' % (self.base_url, role_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Block Storage Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_block_storages(self, page=None, per_page=None, sort=None,\\n                            q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/block_storages\\' % self.base_url\\n\\n        r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def get_block_storage(self, block_storage_id=None):\\n\\n        # Error Handling\\n        if(block_storage_id is None):\\n            raise ValueError(\\'block_storage_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/block_storages/%s\\' % (self.base_url, block_storage_id)\\n\\n        r = requests_retry_session().get(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # \\'POST\\' Methods\\n\\n    def create_block_storage(self, block_storage=None):\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': block_storage.name,\\n            \\'description\\': block_storage.description,\\n            \\'size\\': block_storage.size,\\n            \\'server\\': block_storage.server_id,\\n            \\'datacenter_id\\': block_storage.datacenter_id,\\n            \\'execution_group\\': block_storage.execution_group\\n        }\\n\\n        url = \\'%s/block_storages\\' % self.base_url\\n\\n        r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        # Assign new block_storage_id back to calling BlockStorage object\\n        response = r.json()\\n\\n        block_storage.specs.update(block_storage_id=response[\\'id\\'])\\n        block_storage.specs.update(api_token=self.header)\\n\\n        return r.json()\\n\\n    def attach_block_storage(self, block_storage_id=None,\\n                             server_id=None):\\n\\n        # Error Handling\\n        if(block_storage_id is None):\\n            raise ValueError(\\'block_storage_id is a required parameter\\')\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter.\\')\\n\\n        # Perform Request\\n        data = {\\'server\\': server_id}\\n\\n        url = (\\'%s/block_storages/%s/server\\' %\\n               (self.base_url, block_storage_id))\\n\\n        r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_block_storage(self, block_storage_id=None, name=None,\\n                             description=None, size=None):\\n\\n        # Error Handling\\n        if(block_storage_id is None):\\n            raise ValueError(\\'block_storage_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'size\\': size\\n        }\\n\\n        url = \\'%s/block_storages/%s\\' % (self.base_url, block_storage_id)\\n\\n        r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_block_storage(self, block_storage_id=None):\\n\\n        # Error Handling\\n        if(block_storage_id is None):\\n            raise ValueError(\\'block_storage_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/block_storages/%s\\' % (self.base_url, block_storage_id)\\n\\n        r = requests_retry_session().delete(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def detach_block_storage(self, block_storage_id=None):\\n\\n        # Error Handling\\n        if(block_storage_id is None):\\n            raise ValueError(\\'block_storage_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/block_storages/%s/server\\' %\\n               (self.base_url, block_storage_id))\\n\\n        r = requests_retry_session().delete(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # Ssh Key Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_ssh_keys(self, page=None, per_page=None, sort=None, q=None,\\n                      fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/ssh_keys\\' % self.base_url\\n\\n        r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def get_ssh_key(self, ssh_key_id=None):\\n\\n        # Error Handling\\n        if(ssh_key_id is None):\\n            raise ValueError(\\'ssh_key_id is a required parameter\\')\\n\\n        # Perform Request\\n\\n        url = \\'%s/ssh_keys/%s\\' % (self.base_url, ssh_key_id)\\n\\n        r = requests_retry_session().get(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # \\'POST\\' Methods\\n\\n    def create_ssh_key(self, ssh_key=None):\\n\\n        # Perform Request\\n        url = \\'%s/ssh_keys\\' % self.base_url\\n\\n        r = requests_retry_session().post(url, headers=self.header, json=ssh_key.specs)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        # Assign new ssh_key_id back to calling SshKey object\\n        response = r.json()\\n\\n        ssh_key.specs.update(ssh_key_id=response[\\'id\\'])\\n        ssh_key.specs.update(api_token=self.header)\\n\\n        return r.json()\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_ssh_key(self, ssh_key_id=None):\\n\\n        # Error Handling\\n        if(ssh_key_id is None):\\n            raise ValueError(\\'ssh_key_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/ssh_keys/%s\\' % (self.base_url, ssh_key_id)\\n\\n        r = requests_retry_session().delete(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_ssh_key(self, ssh_key_id=None, name=None, description=None):\\n\\n        # Error Handling\\n        if(ssh_key_id is None):\\n            raise ValueError(\\'ssh_key_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description\\n        }\\n\\n        url = \\'%s/ssh_keys/%s\\' % (self.base_url, ssh_key_id)\\n\\n        r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n\\n# Utility Classes\\n\\nclass Server(object):\\n\\n    # Init Function\\n    def __init__(\\n            self,\\n            name=None,\\n            description=None,\\n            fixed_instance_size_id=None,\\n            vcore=None,\\n            cores_per_processor=None,\\n            ram=None,\\n            appliance_id=None,\\n            password=None,\\n            power_on=None,\\n            firewall_policy_id=None,\\n            ip_id=None,\\n            load_balancer_id=None,\\n            monitoring_policy_id=None,\\n            datacenter_id=None,\\n            rsa_key=None,\\n            private_network_id=None,\\n            server_type=None,\\n            public_key=None,\\n            baremetal_model_id=None,\\n            ipv6_range=None,\\n            hostname=None,\\n            execution_group=None):\\n\\n        self.first_password = None\\n        self.first_ip = None\\n\\n        self.specs = {\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'hardware\\': {\\n                \\'fixed_instance_size_id\\': fixed_instance_size_id,\\n                \\'vcore\\': vcore,\\n                \\'cores_per_processor\\': cores_per_processor,\\n                \\'ram\\': ram,\\n                \\'baremetal_model_id\\': baremetal_model_id\\n            },\\n            \\'appliance_id\\': appliance_id,\\n            \\'password\\': password,\\n            \\'power_on\\': power_on,\\n            \\'firewall_policy_id\\': firewall_policy_id,\\n            \\'ip_id\\': ip_id,\\n            \\'load_balancer_id\\': load_balancer_id,\\n            \\'monitoring_policy_id\\': monitoring_policy_id,\\n            \\'datacenter_id\\': datacenter_id,\\n            \\'rsa_key\\': rsa_key,\\n            \\'private_network_id\\': private_network_id,\\n            \\'server_type\\': server_type,\\n            \\'public_key\\': public_key,\\n            \\'ipv6_range\\': ipv6_range,\\n            \\'hostname\\': hostname,\\n            \\'execution_group\\': execution_group\\n        }\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\n            \\'ACTIVE\\',\\n            \\'ENABLED\\',\\n            \\'POWERED_ON\\',\\n            \\'POWERED_OFF\\',\\n            \\'ON RECOVERY\\')\\n\\n    def __repr__(self):\\n        return (\\n            \\'Server: name=%s, description=%s, fixed_instance_size_id=%s, \\'\\n            \\'vcore=%s, cores_per_processor=%s, ram=%s, baremetal_model_id=%s, appliance_id=%s, \\'\\n            \\'password=%s, power_on=%s, firewall_policy_id=%s, ip_id=%s, \\'\\n            \\'load_balancer_id=%s, monitoring_policy_id=%s, \\'\\n            \\'rsa_key=%s, datacenter_id=%s, first_password=%s, \\'\\n            \\'first_ip=%s, public_key=%s, server_type=%s, ipv6_range=%s, execution_group=%s, hostname=%s\\' %\\n            (self.specs[\\'name\\'],\\n             self.specs[\\'description\\'],\\n             self.specs[\\'hardware\\'][\\'fixed_instance_size_id\\'],\\n             self.specs[\\'hardware\\'][\\'vcore\\'],\\n             self.specs[\\'hardware\\'][\\'cores_per_processor\\'],\\n             self.specs[\\'hardware\\'][\\'ram\\'],\\n             self.specs[\\'hardware\\'][\\'baremetal_model_id\\'],\\n             self.specs[\\'appliance_id\\'],\\n             self.specs[\\'password\\'],\\n             self.specs[\\'power_on\\'],\\n             self.specs[\\'firewall_policy_id\\'],\\n             self.specs[\\'ip_id\\'],\\n             self.specs[\\'load_balancer_id\\'],\\n             self.specs[\\'monitoring_policy_id\\'],\\n             self.specs[\\'rsa_key\\'],\\n             self.specs[\\'datacenter_id\\'],\\n             self.first_password,\\n             self.first_ip,\\n             self.specs[\\'server_type\\'],\\n             self.specs[\\'ipv6_range\\'],\\n             self.specs[\\'execution_group\\'],\\n             self.specs[\\'hostname\\'],\\n             ))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def hardware(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/hardware\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def hdds(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/hardware/hdds\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def image(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/image\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def ips(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/ips\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def status(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/status\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def dvd(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/dvd\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def private_networks(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/private_networks\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def snapshots(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/snapshots\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=15):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial server status\\n        url = \\'%s/servers/%s\\' % (self.base_url, self.specs[\\'server_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial server state and percent values\\n        server_state = response[\\'status\\'][\\'state\\']\\n        percent = response[\\'status\\'][\\'percent\\']\\n\\n        # Keep polling the server\\'s status until good\\n        while (server_state not in self.good_states) or (percent is not None):\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update server state and percent values\\n            server_state = response[\\'status\\'][\\'state\\']\\n            percent = response[\\'status\\'][\\'percent\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n            # Parse for first IP address\\n            if len(response[\\'ips\\']) == 1:\\n                self.first_ip = response[\\'ips\\'][0]\\n\\n        return {\\'duration\\': duration}\\n\\n    def wait_deleted(self, timeout=25, interval=15):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial server status\\n        url = \\'%s/servers/%s\\' % (self.base_url, self.specs[\\'server_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Keep polling the server\\'s status until got 404\\n        while r.status_code != 404 :\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n        return {\\'duration\\': duration}\\n\\n\\nclass Hdd(object):\\n\\n    # Init Function\\n    def __init__(self, size=None, is_main=None):\\n        self.specs = {\\n            \\'size\\': size,\\n            \\'is_main\\': is_main\\n        }\\n\\n    def __repr__(self):\\n        return (\\'HDD: size=%s, is_main=%s\\' %\\n                (self.specs[\\'size\\'], self.specs[\\'is_main\\']))\\n\\n\\nclass AttachServer(object):\\n\\n    # Init Function\\n    def __init__(self, server_id=None, rights=None, server_ip_id=None):\\n        self.server_id = server_id\\n        self.rights = rights\\n        self.server_ip_id = server_ip_id\\n\\n    def __repr__(self):\\n        return (\\'AttachServer: server_id=%s, rights=%s, server_ip_id=%s\\' %\\n                (self.server_id, self.rights, self.server_ip_id))\\n\\n\\nclass Image(object):\\n\\n    # Init Function\\n    def __init__(\\n            self,\\n            server_id=None,\\n            name=None,\\n            description=None,\\n            frequency=None,\\n            num_images=None,\\n            source=\\'server\\',\\n            url=None,\\n            os_id=None,\\n            isotype=None,\\n            type=None):\\n\\n        self.server_id = server_id\\n        self.name = name\\n        self.description = description\\n        self.frequency = frequency\\n        self.num_images = num_images\\n        self.source = source\\n        self.url = url\\n        self.os_id = os_id\\n        self.type = isotype\\n\\n        self.specs = {}\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\n            \\'Image: server_id=%s, name=%s, description=%s, \\'\\n            \\'frequency=%s, num_images=%s, source=%s, url=%s\\'\\n            \\'os_id=%s, type=%s\\' %\\n            (self.server_id,\\n             self.name,\\n             self.description,\\n             self.frequency,\\n             self.num_images,\\n             self.source,\\n             self.url,\\n             self.os_id,\\n             self.type))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/images/%s\\' %\\n               (self.base_url, self.specs[\\'image_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=15):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial image status\\n        url = \\'%s/images/%s\\' % (self.base_url, self.specs[\\'image_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial server state and percent values\\n        image_state = response[\\'state\\']\\n\\n        # Keep polling the server\\'s status until good\\n        while image_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update server state and percent values\\n            image_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n\\n\\nclass SharedStorage(object):\\n\\n    # Init Function\\n    def __init__(self, name=None, description=None, size=None,\\n                 datacenter_id=None):\\n\\n        self.name = name\\n        self.description = description\\n        self.size = size\\n        self.datacenter_id = datacenter_id\\n\\n        self.specs = {}\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\'Shared Storage: name=%s, description=%s, size=%s\\' %\\n                (self.name, self.description, self.size, self.datacenter_id))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/shared_storages/%s\\' %\\n               (self.base_url, self.specs[\\'shared_storage_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def servers(self):\\n\\n        # Perform Request\\n        url = (\\'%s/shared_storages/%s/servers\\' %\\n               (self.base_url, self.specs[\\'shared_storage_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=5):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial image status\\n        url = \\'%s/shared_storages/%s\\' % (self.base_url,\\n                                         self.specs[\\'shared_storage_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial server state and percent values\\n        shared_storage_state = response[\\'state\\']\\n\\n        # Keep polling the server\\'s status until good\\n        while shared_storage_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update server state and percent values\\n            shared_storage_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n\\n\\nclass FirewallPolicyRule(object):\\n\\n    # Init Function\\n    def __init__(self, protocol=None, port_from=None, port_to=None,\\n                 source=None, action=None, description=None, port=None):\\n\\n        self.rule_set = {\\n            \\'protocol\\': protocol,\\n            \\'port_from\\': port_from,\\n            \\'port_to\\': port_to,\\n            \\'source\\': source,\\n            \\'action\\': action,\\n            \\'description\\': description,\\n            \\'port\\': port\\n        }\\n\\n    def __repr__(self):\\n        return (\\'FirewallPolicyRule: protocol=%s, port_from=%s, \\'\\n                \\'port_to=%s, source=%s, action=%s, description=%s, port=%s\\' %\\n                (self.rule_set[\\'protocol\\'], self.rule_set[\\'port_from\\'],\\n                    self.rule_set[\\'port_to\\'], self.rule_set[\\'source\\'], self.rule_set[\\'action\\'], self.rule_set[\\'description\\'], self.rule_set[\\'port\\']))\\n\\n\\nclass FirewallPolicy(object):\\n\\n    # Init Function\\n    def __init__(self, name=None, description=None):\\n        self.specs = {\\n            \\'name\\': name,\\n            \\'description\\': description\\n        }\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\'FirewallPolicy: name=%s, description=%s\\' %\\n                (self.specs[\\'name\\'], self.specs[\\'description\\']))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/firewall_policies/%s\\' %\\n               (self.base_url, self.specs[\\'firewall_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def ips(self):\\n\\n        # Perform Request\\n        url = (\\'%s/firewall_policies/%s/server_ips\\' %\\n               (self.base_url, self.specs[\\'firewall_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def rules(self):\\n\\n        # Perform Request\\n        url = (\\'%s/firewall_policies/%s/rules\\' %\\n               (self.base_url, self.specs[\\'firewall_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=5):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial image status\\n        url = \\'%s/firewall_policies/%s\\' % (self.base_url,\\n                                           self.specs[\\'firewall_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial server state and percent values\\n        firewall_state = response[\\'state\\']\\n\\n        # Keep polling the server\\'s status until good\\n        while firewall_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update server state and percent values\\n            firewall_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n\\n\\nclass LoadBalancerRule(object):\\n\\n    # Init Function\\n    def __init__(self, protocol=None, port_balancer=None, port_server=None,\\n                 source=None):\\n\\n        self.rule_set = {\\n            \\'protocol\\': protocol,\\n            \\'port_balancer\\': port_balancer,\\n            \\'port_server\\': port_server,\\n            \\'source\\': source\\n        }\\n\\n    def __repr__(self):\\n        return (\\n            \\'LoadBalancerRule: protocol=%s, port_balancer=%s, \\'\\n            \\'port_server=%s, source=%s\\' %\\n            (self.rule_set[\\'protocol\\'],\\n             self.rule_set[\\'port_balancer\\'],\\n             self.rule_set[\\'port_server\\'],\\n             self.rule_set[\\'source\\']))\\n\\n\\nclass LoadBalancer(object):\\n\\n    # Init Function\\n    def __init__(self, health_check_path=None, health_check_parse=None,\\n                 name=None, description=None, health_check_test=None,\\n                 health_check_interval=None, persistence=None,\\n                 persistence_time=None, method=None, datacenter_id=None):\\n\\n        self.specs = {\\n            \\'health_check_path\\': health_check_path,\\n            \\'health_check_parse\\': health_check_parse,\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'health_check_test\\': health_check_test,\\n            \\'health_check_interval\\': health_check_interval,\\n            \\'persistence\\': persistence,\\n            \\'persistence_time\\': persistence_time,\\n            \\'method\\': method,\\n            \\'datacenter_id\\': datacenter_id\\n        }\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\'LoadBalancer: health_check_path=%s, health_check_parse=%s, \\'\\n                \\'name=%s, description=%s, health_check_test=%s, \\'\\n                \\'health_check_interval=%s, persistence=%s, \\'\\n                \\'persistence_time=%s, method=%s, datacenter_id=%s\\' %\\n                (self.specs[\\'health_check_path\\'],\\n                    self.specs[\\'health_check_parse\\'], self.specs[\\'name\\'],\\n                    self.specs[\\'description\\'], self.specs[\\'health_check_test\\'],\\n                    self.specs[\\'health_check_interval\\'],\\n                    self.specs[\\'persistence\\'], self.specs[\\'persistence_time\\'],\\n                    self.specs[\\'method\\'], self.datacenter_id))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/load_balancers/%s\\' %\\n               (self.base_url, self.specs[\\'load_balancer_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def ips(self):\\n\\n        # Perform Request\\n        url = (\\'%s/load_balancers/%s/server_ips\\' %\\n               (self.base_url, self.specs[\\'load_balancer_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def rules(self):\\n\\n        # Perform Request\\n        url = (\\'%s/load_balancers/%s/rules\\' %\\n               (self.base_url, self.specs[\\'load_balancer_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=5):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial image status\\n        url = \\'%s/load_balancers/%s\\' % (self.base_url,\\n                                        self.specs[\\'load_balancer_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial server state and percent values\\n        load_balancer_state = response[\\'state\\']\\n\\n        # Keep polling the server\\'s status until good\\n        while load_balancer_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update server state and percent values\\n            load_balancer_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n\\n\\nclass PrivateNetwork(object):\\n\\n    # Init Function\\n    def __init__(self, name=None, description=None, network_address=None,\\n                 subnet_mask=None, datacenter_id=None):\\n\\n        self.name = name\\n        self.description = description\\n        self.network_address = network_address\\n        self.subnet_mask = subnet_mask\\n        self.datacenter_id = datacenter_id\\n\\n        self.specs = {}\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\n            \\'Private Network: name=%s, description=%s, network_address=%s, \\'\\n            \\'subnet_mask=%s\\' %\\n            (self.name, self.description, self.network_address, self.subnet_mask))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/private_networks/%s\\' %\\n               (self.base_url, self.specs[\\'private_network_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def servers(self):\\n\\n        # Perform Request\\n        url = (\\'%s/private_networks/%s/servers\\' %\\n               (self.base_url, self.specs[\\'private_network_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=5):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial image status\\n        url = \\'%s/private_networks/%s\\' % (self.base_url,\\n                                          self.specs[\\'private_network_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial server state and percent values\\n        private_network_state = response[\\'state\\']\\n\\n        # Keep polling the server\\'s status until good\\n        while private_network_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update server state and percent values\\n            private_network_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n\\n\\nclass MonitoringPolicy(object):\\n\\n    # Init Function\\n    def __init__(self, name=None, description=None, email=None, agent=None):\\n        self.specs = {\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'email\\': email,\\n            \\'agent\\': agent\\n        }\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\'MonitoringPolicy: name=%s, description=%s, email=%s, \\'\\n                \\'agent=%s\\' %\\n                (self.specs[\\'name\\'], self.specs[\\'description\\'],\\n                    self.specs[\\'email\\'], self.specs[\\'agent\\']))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s\\' %\\n               (self.base_url, self.specs[\\'monitoring_policy_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def ports(self):\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/ports\\' %\\n               (self.base_url, self.specs[\\'monitoring_policy_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def processes(self):\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/processes\\' %\\n               (self.base_url, self.specs[\\'monitoring_policy_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def servers(self):\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/servers\\' %\\n               (self.base_url, self.specs[\\'monitoring_policy_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=5):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial image status\\n        url = \\'%s/monitoring_policies/%s\\' % (self.base_url,\\n                                             self.specs[\\'monitoring_policy_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial server state and percent values\\n        mp_state = response[\\'state\\']\\n\\n        # Keep polling the server\\'s status until good\\n        while mp_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update server state and percent values\\n            mp_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n\\n\\nclass Threshold(object):\\n\\n    # Init Function\\n    def __init__(self, entity=None, warning_value=None, warning_alert=None,\\n                 critical_value=None, critical_alert=None):\\n\\n        self.entity = entity\\n        self.warning_value = warning_value\\n        self.warning_alert = warning_alert\\n        self.critical_value = critical_value\\n        self.critical_alert = critical_alert\\n\\n    def __repr__(self):\\n        return (\\n            \\'Threshold: entity=%s, warning_value=%s, warning_alert=%s, \\'\\n            \\'critical_value=%s, critical_alert=%s\\' %\\n            (self.entity,\\n             self.warning_value,\\n             self.warning_alert,\\n             self.critical_value,\\n             self.critical_alert))\\n\\n\\nclass Port(object):\\n\\n    # Init Function\\n    def __init__(self, protocol=None, port=None, alert_if=None,\\n                 email_notification=None):\\n\\n        self.specs = {\\n            \\'protocol\\': protocol,\\n            \\'port\\': port,\\n            \\'alert_if\\': alert_if,\\n            \\'email_notification\\': email_notification\\n        }\\n\\n    def __repr__(self):\\n        return (\\n            \\'Port: protocol=%s, port=%s, alert_if=%s, \\'\\n            \\'email_notification=%s\\' %\\n            (self.specs[\\'protocol\\'],\\n             self.specs[\\'port\\'],\\n             self.specs[\\'alert_if\\'],\\n             self.specs[\\'email_notification\\']))\\n\\n\\nclass Process(object):\\n\\n    # Init Function\\n    def __init__(self, process=None, alert_if=None, email_notification=None):\\n        self.process_set = {\\n            \\'process\\': process,\\n            \\'alert_if\\': alert_if,\\n            \\'email_notification\\': email_notification\\n        }\\n\\n    def __repr__(self):\\n        return (\\'Process: process=%s, alert_if=%s, email_notification=%s\\' %\\n                (self.process_set[\\'process\\'], self.process_set[\\'alert_if\\'],\\n                    self.process_set[\\'email_notification\\']))\\n\\n\\nclass Vpn(object):\\n\\n    # Init Function\\n    def __init__(self, name=None, description=None, datacenter_id=None):\\n\\n        self.name = name\\n        self.description = description\\n        self.datacenter_id = datacenter_id\\n\\n        self.specs = {}\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\'Vpn: name=%s, description=%s, datacenter_id=%s\\' %\\n                (self.name, self.description, self.datacenter_id))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/vpns/%s\\' %\\n               (self.base_url, self.specs[\\'vpn_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=15):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial image status\\n        url = \\'%s/vpns/%s\\' % (self.base_url, self.specs[\\'vpn_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial server state and percent values\\n        vpn_state = response[\\'state\\']\\n\\n        # Keep polling the server\\'s status until good\\n        while vpn_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update server state and percent values\\n            vpn_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n\\n\\nclass BlockStorage(object):\\n\\n    # Init Function\\n    def __init__(self, name=None, description=None, size=None,\\n                 datacenter_id=None, server_id=None, execution_group=None):\\n\\n        self.name = name\\n        self.description = description\\n        self.size = size\\n        self.datacenter_id = datacenter_id\\n        self.server_id = server_id\\n        self.execution_group = execution_group\\n\\n        self.specs = {}\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\'Block Storage: name=%s, description=%s, size=%s, execution_group=%s, server_id=%s\\' % (\\n            self.name, self.description, self.size, self.datacenter_id, self.execution_group, self.server_id))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/block_storages/%s\\' %\\n               (self.base_url, self.specs[\\'block_storage_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def server(self):\\n\\n        # Perform Request\\n        url = (\\'%s/block_storages/%s/server\\' %\\n               (self.base_url, self.specs[\\'block_storage_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=5):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial block storage status\\n        url = \\'%s/block_storages/%s\\' % (self.base_url,\\n                                        self.specs[\\'block_storage_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial block storage state and percent values\\n        block_storage_state = response[\\'state\\']\\n\\n        # Keep polling the block storage\\'s status until good\\n        while block_storage_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check block storage status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update block storage state and percent values\\n            block_storage_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n\\n\\nclass SshKey(object):\\n\\n    # Init Function\\n    def __init__(self, name=None, description=None,\\n                 state=None, servers=None, md5=None,\\n                 public_key=None, creation_date=None):\\n\\n        self.specs = {\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'state\\': state,\\n            \\'servers\\': servers,\\n            \\'md5\\': md5,\\n            \\'public_key\\': public_key,\\n            \\'creation_date\\': creation_date\\n        }\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\'SshKey: name=%s, description=%s, \\'\\n                \\'state=%s, servers=%s, md5=%s, \\'\\n                \\'public_key=%s, creation_date=%s, \\' %\\n                (self.specs[\\'name\\'], self.specs[\\'description\\'],\\n                 self.specs[\\'state\\'], self.specs[\\'servers\\'],\\n                 self.specs[\\'md5\\'], self.specs[\\'public_key\\'],\\n                 self.specs[\\'creation_date\\']))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/ssh_keys/%s\\' %\\n               (self.base_url, self.specs[\\'ssh_key_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=5):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial ssh_key status\\n        url = \\'%s/ssh_keys/%s\\' % (self.base_url,\\n                                  self.specs[\\'ssh_key_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial ssh_key state and percent values\\n        ssh_key_state = response[\\'state\\']\\n\\n        # Keep polling the ssh_key\\'s status until good\\n        while ssh_key_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check ssh_key status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update ssh_key state and percent values\\n            ssh_key_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n',\n",
       " '\"\"\"\\nTests for DragonflyBackend class.\\n\"\"\"\\n\\nfrom tuun.backend import DragonflyBackend\\n\\n\\ndef test_initialize():\\n    \"\"\"Test initialize DragonflyBackend.\"\"\"\\n    domain_config = {\\'name\\': \\'real\\', \\'min_max\\': [[-5, 5]]}\\n    opt_config = {\\'name\\': \\'real\\'}\\n    dragonfly_config = {\\'acq_str\\': \\'ucb-ei\\', \\'n_init_rs\\': 0}\\n    db = DragonflyBackend(domain_config, opt_config, dragonfly_config)\\n    assert getattr(db, \\'domain_config\\', None)\\n\\ndef test_suggest_to_minimize():\\n    \"\"\"Test DragonflyBackend suggest_to_minimize on a dataset.\"\"\"\\n    domain_config = {\\'name\\': \\'real\\', \\'min_max\\': [[0.0, 2.0]]}\\n    opt_config = {\\'name\\': \\'real\\'}\\n    dragonfly_config = {\\'acq_str\\': \\'ucb-ei\\', \\'n_init_rs\\': 0}\\n    db = DragonflyBackend(domain_config, opt_config, dragonfly_config)\\n\\n    data = {\\n        \\'x\\': [[0.5], [1.0], [1.5]],\\n        \\'y\\': [6.0, 1.0, 4.0],\\n    }\\n\\n    suggestion = db.suggest_to_minimize(data)\\n    assert 0.75 < suggestion[0] < 1.25\\n',\n",
       " '# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nimport sys\\nsys.path.append(\\'../\\')\\n\\nfrom auto_scan_test import AutoScanTest, IgnoreReasons\\nfrom program_config import TensorConfig, ProgramConfig, OpConfig, CxxConfig, TargetType, PrecisionType, DataLayoutType, Place\\nimport unittest\\n\\nimport hypothesis\\nfrom hypothesis import given, settings, seed, example, assume\\nimport hypothesis.strategies as st\\nimport argparse\\nimport numpy as np\\nfrom functools import partial\\nimport copy\\n\\n\\nclass TestInverseOp(AutoScanTest):\\n    def __init__(self, *args, **kwargs):\\n        AutoScanTest.__init__(self, *args, **kwargs)\\n        self.enable_testing_on_place(\\n            TargetType.Host,\\n            PrecisionType.FP32,\\n            DataLayoutType.NCHW,\\n            thread=[1, 2])\\n\\n    def is_program_valid(self,\\n                         program_config: ProgramConfig,\\n                         predictor_config: CxxConfig) -> bool:\\n        return True\\n\\n    def sample_program_configs(self, draw):\\n        in_shape = draw(\\n            st.lists(\\n                st.integers(\\n                    min_value=1, max_value=32), min_size=1, max_size=2))\\n\\n        def generate_input(*args, **kwargs):\\n            last_dim = np.random.randint(\\n                low=1, high=16, size=[1]).astype(np.int32)\\n            input_dim = copy.deepcopy(in_shape)\\n            input_dim.append(last_dim[0])  #last 2 dim must be equal\\n            input_dim.append(last_dim[0])\\n            return np.random.random(input_dim).astype(np.float32)\\n\\n        build_ops = OpConfig(\\n            type=\"inverse\",\\n            inputs={\"Input\": [\"input_data\"], },\\n            outputs={\"Output\": [\"output_data\"], },\\n            attrs={})\\n\\n        program_config = ProgramConfig(\\n            ops=[build_ops],\\n            weights={},\\n            inputs={\\n                \"input_data\": TensorConfig(data_gen=partial(generate_input)),\\n            },\\n            outputs=[\"output_data\"])\\n        return program_config\\n\\n    def sample_predictor_configs(self):\\n        return self.get_predictor_configs(), [\"inverse\"], (5e-5, 5e-5)\\n\\n    def add_ignore_pass_case(self):\\n        pass\\n\\n    def test(self, *args, **kwargs):\\n        self.run_and_statis(quant=False, max_examples=25)\\n\\n\\nif __name__ == \"__main__\":\\n    unittest.main(argv=[\\'\\'])\\n',\n",
       " 'import re\\n\\nimport sqlparse\\n\\nfrom django.db.backends.base.introspection import (\\n    BaseDatabaseIntrospection, FieldInfo, TableInfo,\\n)\\nfrom django.db.models.indexes import Index\\n\\nfield_size_re = re.compile(r\\'^\\\\s*(?:var)?char\\\\s*\\\\(\\\\s*(\\\\d+)\\\\s*\\\\)\\\\s*$\\')\\n\\n\\ndef get_field_size(name):\\n    \"\"\" Extract the size number from a \"varchar(11)\" type name \"\"\"\\n    m = field_size_re.search(name)\\n    return int(m.group(1)) if m else None\\n\\n\\n# This light wrapper \"fakes\" a dictionary interface, because some SQLite data\\n# types include variables in them -- e.g. \"varchar(30)\" -- and can\\'t be matched\\n# as a simple dictionary lookup.\\nclass FlexibleFieldLookupDict:\\n    # Maps SQL types to Django Field types. Some of the SQL types have multiple\\n    # entries here because SQLite allows for anything and doesn\\'t normalize the\\n    # field type; it uses whatever was given.\\n    base_data_types_reverse = {\\n        \\'bool\\': \\'BooleanField\\',\\n        \\'boolean\\': \\'BooleanField\\',\\n        \\'smallint\\': \\'SmallIntegerField\\',\\n        \\'smallint unsigned\\': \\'PositiveSmallIntegerField\\',\\n        \\'smallinteger\\': \\'SmallIntegerField\\',\\n        \\'int\\': \\'IntegerField\\',\\n        \\'integer\\': \\'IntegerField\\',\\n        \\'bigint\\': \\'BigIntegerField\\',\\n        \\'integer unsigned\\': \\'PositiveIntegerField\\',\\n        \\'decimal\\': \\'DecimalField\\',\\n        \\'real\\': \\'FloatField\\',\\n        \\'text\\': \\'TextField\\',\\n        \\'char\\': \\'CharField\\',\\n        \\'blob\\': \\'BinaryField\\',\\n        \\'date\\': \\'DateField\\',\\n        \\'datetime\\': \\'DateTimeField\\',\\n        \\'time\\': \\'TimeField\\',\\n    }\\n\\n    def __getitem__(self, key):\\n        key = key.lower()\\n        try:\\n            return self.base_data_types_reverse[key]\\n        except KeyError:\\n            size = get_field_size(key)\\n            if size is not None:\\n                return (\\'CharField\\', {\\'max_length\\': size})\\n            raise KeyError\\n\\n\\nclass DatabaseIntrospection(BaseDatabaseIntrospection):\\n    data_types_reverse = FlexibleFieldLookupDict()\\n\\n    def get_table_list(self, cursor):\\n        \"\"\"Return a list of table and view names in the current database.\"\"\"\\n        # Skip the sqlite_sequence system table used for autoincrement key\\n        # generation.\\n        cursor.execute(\"\"\"\\n            SELECT name, type FROM sqlite_master\\n            WHERE type in (\\'table\\', \\'view\\') AND NOT name=\\'sqlite_sequence\\'\\n            ORDER BY name\"\"\")\\n        return [TableInfo(row[0], row[1][0]) for row in cursor.fetchall()]\\n\\n    def get_table_description(self, cursor, table_name):\\n        \"\"\"\\n        Return a description of the table with the DB-API cursor.description\\n        interface.\\n        \"\"\"\\n        return [\\n            FieldInfo(\\n                info[\\'name\\'],\\n                info[\\'type\\'],\\n                None,\\n                info[\\'size\\'],\\n                None,\\n                None,\\n                info[\\'null_ok\\'],\\n                info[\\'default\\'],\\n            ) for info in self._table_info(cursor, table_name)\\n        ]\\n\\n    def get_sequences(self, cursor, table_name, table_fields=()):\\n        pk_col = self.get_primary_key_column(cursor, table_name)\\n        return [{\\'table\\': table_name, \\'column\\': pk_col}]\\n\\n    def get_relations(self, cursor, table_name):\\n        \"\"\"\\n        Return a dictionary of {field_name: (field_name_other_table, other_table)}\\n        representing all relationships to the given table.\\n        \"\"\"\\n        # Dictionary of relations to return\\n        relations = {}\\n\\n        # Schema for this table\\n        cursor.execute(\\n            \"SELECT sql, type FROM sqlite_master \"\\n            \"WHERE tbl_name = %s AND type IN (\\'table\\', \\'view\\')\",\\n            [table_name]\\n        )\\n        create_sql, table_type = cursor.fetchone()\\n        if table_type == \\'view\\':\\n            # It might be a view, then no results will be returned\\n            return relations\\n        results = create_sql[create_sql.index(\\'(\\') + 1:create_sql.rindex(\\')\\')]\\n\\n        # Walk through and look for references to other tables. SQLite doesn\\'t\\n        # really have enforced references, but since it echoes out the SQL used\\n        # to create the table we can look for REFERENCES statements used there.\\n        for field_desc in results.split(\\',\\'):\\n            field_desc = field_desc.strip()\\n            if field_desc.startswith(\"UNIQUE\"):\\n                continue\\n\\n            m = re.search(r\\'references (\\\\S*) ?\\\\([\"|]?(.*)[\"|]?\\\\)\\', field_desc, re.I)\\n            if not m:\\n                continue\\n            table, column = [s.strip(\\'\"\\') for s in m.groups()]\\n\\n            if field_desc.startswith(\"FOREIGN KEY\"):\\n                # Find name of the target FK field\\n                m = re.match(r\\'FOREIGN KEY\\\\s*\\\\(([^\\\\)]*)\\\\).*\\', field_desc, re.I)\\n                field_name = m.groups()[0].strip(\\'\"\\')\\n            else:\\n                field_name = field_desc.split()[0].strip(\\'\"\\')\\n\\n            cursor.execute(\"SELECT sql FROM sqlite_master WHERE tbl_name = %s\", [table])\\n            result = cursor.fetchall()[0]\\n            other_table_results = result[0].strip()\\n            li, ri = other_table_results.index(\\'(\\'), other_table_results.rindex(\\')\\')\\n            other_table_results = other_table_results[li + 1:ri]\\n\\n            for other_desc in other_table_results.split(\\',\\'):\\n                other_desc = other_desc.strip()\\n                if other_desc.startswith(\\'UNIQUE\\'):\\n                    continue\\n\\n                other_name = other_desc.split(\\' \\', 1)[0].strip(\\'\"\\')\\n                if other_name == column:\\n                    relations[field_name] = (other_name, table)\\n                    break\\n\\n        return relations\\n\\n    def get_key_columns(self, cursor, table_name):\\n        \"\"\"\\n        Return a list of (column_name, referenced_table_name, referenced_column_name)\\n        for all key columns in given table.\\n        \"\"\"\\n        key_columns = []\\n\\n        # Schema for this table\\n        cursor.execute(\"SELECT sql FROM sqlite_master WHERE tbl_name = %s AND type = %s\", [table_name, \"table\"])\\n        results = cursor.fetchone()[0].strip()\\n        results = results[results.index(\\'(\\') + 1:results.rindex(\\')\\')]\\n\\n        # Walk through and look for references to other tables. SQLite doesn\\'t\\n        # really have enforced references, but since it echoes out the SQL used\\n        # to create the table we can look for REFERENCES statements used there.\\n        for field_index, field_desc in enumerate(results.split(\\',\\')):\\n            field_desc = field_desc.strip()\\n            if field_desc.startswith(\"UNIQUE\"):\\n                continue\\n\\n            m = re.search(r\\'\"(.*)\".*references (.*) \\\\([\"|](.*)[\"|]\\\\)\\', field_desc, re.I)\\n            if not m:\\n                continue\\n\\n            # This will append (column_name, referenced_table_name, referenced_column_name) to key_columns\\n            key_columns.append(tuple(s.strip(\\'\"\\') for s in m.groups()))\\n\\n        return key_columns\\n\\n    def get_primary_key_column(self, cursor, table_name):\\n        \"\"\"Return the column name of the primary key for the given table.\"\"\"\\n        # Don\\'t use PRAGMA because that causes issues with some transactions\\n        cursor.execute(\\n            \"SELECT sql, type FROM sqlite_master \"\\n            \"WHERE tbl_name = %s AND type IN (\\'table\\', \\'view\\')\",\\n            [table_name]\\n        )\\n        row = cursor.fetchone()\\n        if row is None:\\n            raise ValueError(\"Table %s does not exist\" % table_name)\\n        create_sql, table_type = row\\n        if table_type == \\'view\\':\\n            # Views don\\'t have a primary key.\\n            return None\\n        fields_sql = create_sql[create_sql.index(\\'(\\') + 1:create_sql.rindex(\\')\\')]\\n        for field_desc in fields_sql.split(\\',\\'):\\n            field_desc = field_desc.strip()\\n            m = re.match(r\\'(?:(?:[\"`\\\\[])(.*)(?:[\"`\\\\]])|(\\\\w+)).*PRIMARY KEY.*\\', field_desc)\\n            if m:\\n                return m.group(1) if m.group(1) else m.group(2)\\n        return None\\n\\n    def _table_info(self, cursor, name):\\n        cursor.execute(\\'PRAGMA table_info(%s)\\' % self.connection.ops.quote_name(name))\\n        # cid, name, type, notnull, default_value, pk\\n        return [{\\n            \\'name\\': field[1],\\n            \\'type\\': field[2],\\n            \\'size\\': get_field_size(field[2]),\\n            \\'null_ok\\': not field[3],\\n            \\'default\\': field[4],\\n            \\'pk\\': field[5],  # undocumented\\n        } for field in cursor.fetchall()]\\n\\n    def _get_foreign_key_constraints(self, cursor, table_name):\\n        constraints = {}\\n        cursor.execute(\\'PRAGMA foreign_key_list(%s)\\' % self.connection.ops.quote_name(table_name))\\n        for row in cursor.fetchall():\\n            # Remaining on_update/on_delete/match values are of no interest.\\n            id_, _, table, from_, to = row[:5]\\n            constraints[\\'fk_%d\\' % id_] = {\\n                \\'columns\\': [from_],\\n                \\'primary_key\\': False,\\n                \\'unique\\': False,\\n                \\'foreign_key\\': (table, to),\\n                \\'check\\': False,\\n                \\'index\\': False,\\n            }\\n        return constraints\\n\\n    def get_constraints(self, cursor, table_name):\\n        \"\"\"\\n        Retrieve any constraints or keys (unique, pk, fk, check, index) across\\n        one or more columns.\\n        \"\"\"\\n        constraints = {}\\n        # Find inline check constraints.\\n        try:\\n            table_schema = cursor.execute(\\n                \"SELECT sql FROM sqlite_master WHERE type=\\'table\\' and name=%s\" % (\\n                    self.connection.ops.quote_name(table_name),\\n                )\\n            ).fetchone()[0]\\n        except TypeError:\\n            # table_name is a view.\\n            pass\\n        else:\\n            # Check constraint parsing is based of SQLite syntax diagram.\\n            # https://www.sqlite.org/syntaxdiagrams.html#table-constraint\\n            def next_ttype(ttype):\\n                for token in tokens:\\n                    if token.ttype == ttype:\\n                        return token\\n\\n            statement = sqlparse.parse(table_schema)[0]\\n            tokens = statement.flatten()\\n            for token in tokens:\\n                name = None\\n                if token.match(sqlparse.tokens.Keyword, \\'CONSTRAINT\\'):\\n                    # Table constraint\\n                    name_token = next_ttype(sqlparse.tokens.Literal.String.Symbol)\\n                    name = name_token.value[1:-1]\\n                    token = next_ttype(sqlparse.tokens.Keyword)\\n                if token.match(sqlparse.tokens.Keyword, \\'UNIQUE\\'):\\n                    constraints[name] = {\\n                        \\'unique\\': True,\\n                        \\'columns\\': [],\\n                        \\'primary_key\\': False,\\n                        \\'foreign_key\\': False,\\n                        \\'check\\': False,\\n                        \\'index\\': False,\\n                    }\\n                if token.match(sqlparse.tokens.Keyword, \\'CHECK\\'):\\n                    # Column check constraint\\n                    if name is None:\\n                        column_token = next_ttype(sqlparse.tokens.Literal.String.Symbol)\\n                        column = column_token.value[1:-1]\\n                        name = \\'__check__%s\\' % column\\n                        columns = [column]\\n                    else:\\n                        columns = []\\n                    constraints[name] = {\\n                        \\'check\\': True,\\n                        \\'columns\\': columns,\\n                        \\'primary_key\\': False,\\n                        \\'unique\\': False,\\n                        \\'foreign_key\\': False,\\n                        \\'index\\': False,\\n                    }\\n        # Get the index info\\n        cursor.execute(\"PRAGMA index_list(%s)\" % self.connection.ops.quote_name(table_name))\\n        for row in cursor.fetchall():\\n            # Sqlite3 3.8.9+ has 5 columns, however older versions only give 3\\n            # columns. Discard last 2 columns if there.\\n            number, index, unique = row[:3]\\n            # Get the index info for that index\\n            cursor.execute(\\'PRAGMA index_info(%s)\\' % self.connection.ops.quote_name(index))\\n            for index_rank, column_rank, column in cursor.fetchall():\\n                if index not in constraints:\\n                    constraints[index] = {\\n                        \"columns\": [],\\n                        \"primary_key\": False,\\n                        \"unique\": bool(unique),\\n                        \"foreign_key\": False,\\n                        \"check\": False,\\n                        \"index\": True,\\n                    }\\n                constraints[index][\\'columns\\'].append(column)\\n            # Add type and column orders for indexes\\n            if constraints[index][\\'index\\'] and not constraints[index][\\'unique\\']:\\n                # SQLite doesn\\'t support any index type other than b-tree\\n                constraints[index][\\'type\\'] = Index.suffix\\n                cursor.execute(\\n                    \"SELECT sql FROM sqlite_master \"\\n                    \"WHERE type=\\'index\\' AND name=%s\" % self.connection.ops.quote_name(index)\\n                )\\n                orders = []\\n                # There would be only 1 row to loop over\\n                for sql, in cursor.fetchall():\\n                    order_info = sql.split(\\'(\\')[-1].split(\\')\\')[0].split(\\',\\')\\n                    orders = [\\'DESC\\' if info.endswith(\\'DESC\\') else \\'ASC\\' for info in order_info]\\n                constraints[index][\\'orders\\'] = orders\\n        # Get the PK\\n        pk_column = self.get_primary_key_column(cursor, table_name)\\n        if pk_column:\\n            # SQLite doesn\\'t actually give a name to the PK constraint,\\n            # so we invent one. This is fine, as the SQLite backend never\\n            # deletes PK constraints by name, as you can\\'t delete constraints\\n            # in SQLite; we remake the table with a new PK instead.\\n            constraints[\"__primary__\"] = {\\n                \"columns\": [pk_column],\\n                \"primary_key\": True,\\n                \"unique\": False,  # It\\'s not actually a unique constraint.\\n                \"foreign_key\": False,\\n                \"check\": False,\\n                \"index\": False,\\n            }\\n        constraints.update(self._get_foreign_key_constraints(cursor, table_name))\\n        return constraints\\n',\n",
       " 'class Solution:\\n    def minSwaps(self, nums: List[int]) -> int:\\n        ones, N = sum(nums), len(nums)\\n        min_swap = s = ones - sum(nums[:ones])\\n        for i in range(N):\\n            s += nums[i] - nums[(i + ones) % N]\\n            min_swap = min(s, min_swap)\\n        return min_swap',\n",
       " '\"\"\"Module for the LoadEnvironment class.\"\"\"\\n\\nimport os\\nimport sys\\nfrom pathlib import Path\\n\\n\\nclass LoadEnvironment:\\n    \"\"\"This class is used for loading the environment from .env and .env.* files.\"\"\"\\n\\n    def __init__(self, environment=None, override=True, only=None):\\n        \"\"\"LoadEnvironment constructor.\\n\\n        Keyword Arguments:\\n            env {string} -- An additional environment file that you want to load. (default: {None})\\n            override {bool} -- Whether or not the environment variables found should overwrite existing ones. (default: {False})\\n            only {string} -- If this is set then it will only load that environment. (default: {None})\\n        \"\"\"\\n        from dotenv import load_dotenv\\n\\n        self.env = load_dotenv\\n\\n        if only:\\n            self._load_environment(only, override=override)\\n            return\\n\\n        env_path = str(Path(\".\") / \".env\")\\n        self.env(env_path, override=override)\\n\\n        if os.environ.get(\"APP_ENV\"):\\n            self._load_environment(os.environ.get(\"APP_ENV\"), override=override)\\n        if environment:\\n            self._load_environment(environment, override=override)\\n\\n        if \"pytest\" in sys.modules:\\n            self._load_environment(\"testing\", override=override)\\n\\n    def _load_environment(self, environment, override=False):\\n        \"\"\"Load the environment depending on the env file.\\n\\n        Arguments:\\n            environment {string} -- Name of the environment file to load from\\n\\n        Keyword Arguments:\\n            override {bool} -- Whether the environment file should overwrite existing environment keys. (default: {False})\\n        \"\"\"\\n        env_path = str(Path(\".\") / \".env.{}\".format(environment))\\n        self.env(dotenv_path=env_path, override=override)\\n\\n\\ndef env(value, default=\"\", cast=True):\\n    env_var = os.getenv(value, default)\\n\\n    if not cast:\\n        return env_var\\n\\n    if env_var == \"\":\\n        env_var = default\\n\\n    if isinstance(env_var, bool):\\n        return env_var\\n    elif env_var is None:\\n        return None\\n    elif env_var.isnumeric():\\n        return int(env_var)\\n    elif env_var in (\"false\", \"False\"):\\n        return False\\n    elif env_var in (\"true\", \"True\"):\\n        return True\\n    else:\\n        return env_var\\n',\n",
       " '\\nfrom pl_bolts.transforms.self_supervised import RandomTranslateWithReflect, Patchify\\n\\ntry:\\n    from torchvision import transforms\\nexcept ImportError:\\n    warn(\\'You want to use `torchvision` which is not installed yet,\\'  # pragma: no-cover\\n                      \\' install it with `pip install torchvision`.\\')\\n\\n\\nclass CPCTrainTransformsCIFAR10:\\n\\n    def __init__(self, patch_size=8, overlap=4):\\n        \"\"\"\\n        Transforms used for CPC:\\n\\n        Args:\\n\\n            patch_size: size of patches when cutting up the image into overlapping patches\\n            overlap: how much to overlap patches\\n\\n        Transforms::\\n\\n            random_flip\\n            img_jitter\\n            col_jitter\\n            rnd_gray\\n            transforms.ToTensor()\\n            normalize\\n            Patchify(patch_size=patch_size, overlap_size=patch_size // 2)\\n\\n        Example::\\n\\n            # in a regular dataset\\n            CIFAR10(..., transforms=CPCTrainTransformsCIFAR10())\\n\\n            # in a DataModule\\n            module = CIFAR10DataModule(PATH)\\n            train_loader = module.train_dataloader(batch_size=32, transforms=CPCTrainTransformsCIFAR10())\\n\\n        \"\"\"\\n        self.patch_size = patch_size\\n        self.overlap = overlap\\n        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)\\n\\n        normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\\n                                         std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\\n        col_jitter = transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.2)], p=0.8)\\n        img_jitter = transforms.RandomApply([RandomTranslateWithReflect(4)], p=0.8)\\n        rnd_gray = transforms.RandomGrayscale(p=0.25)\\n\\n        self.transforms = transforms.Compose([\\n            img_jitter,\\n            col_jitter,\\n            rnd_gray,\\n            transforms.ToTensor(),\\n            normalize,\\n            Patchify(patch_size=patch_size, overlap_size=overlap),\\n        ])\\n\\n    def __call__(self, inp):\\n        inp = self.flip_lr(inp)\\n        out1 = self.transforms(inp)\\n        return out1\\n\\n\\nclass CPCEvalTransformsCIFAR10:\\n\\n    def __init__(self, patch_size=8, overlap=4):\\n        \"\"\"\\n        Transforms used for CPC:\\n\\n        Args:\\n\\n            patch_size: size of patches when cutting up the image into overlapping patches\\n            overlap: how much to overlap patches\\n\\n        Transforms::\\n\\n            random_flip\\n            transforms.ToTensor()\\n            normalize\\n            Patchify(patch_size=patch_size, overlap_size=overlap)\\n\\n        Example::\\n\\n            # in a regular dataset\\n            CIFAR10(..., transforms=CPCEvalTransformsCIFAR10())\\n\\n            # in a DataModule\\n            module = CIFAR10DataModule(PATH)\\n            train_loader = module.train_dataloader(batch_size=32, transforms=CPCEvalTransformsCIFAR10())\\n\\n        \"\"\"\\n\\n        # flipping image along vertical axis\\n        self.patch_size = patch_size\\n        self.overlap = overlap\\n        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)\\n\\n        normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\\n                                         std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\\n\\n        self.transforms = transforms.Compose([\\n            transforms.ToTensor(),\\n            normalize,\\n            Patchify(patch_size=patch_size, overlap_size=overlap),\\n        ])\\n\\n    def __call__(self, inp):\\n        out1 = self.transforms(inp)\\n        return out1\\n\\n\\nclass CPCTrainTransformsSTL10:\\n\\n    def __init__(self, patch_size=16, overlap=8):\\n        \"\"\"\\n        Transforms used for CPC:\\n\\n        Args:\\n\\n            patch_size: size of patches when cutting up the image into overlapping patches\\n            overlap: how much to overlap patches\\n\\n        Transforms::\\n\\n            random_flip\\n            img_jitter\\n            col_jitter\\n            rnd_gray\\n            transforms.ToTensor()\\n            normalize\\n            Patchify(patch_size=patch_size, overlap_size=patch_size // 2)\\n\\n        Example::\\n\\n            # in a regular dataset\\n            STL10(..., transforms=CPCTrainTransformsSTL10())\\n\\n            # in a DataModule\\n            module = STL10DataModule(PATH)\\n            train_loader = module.train_dataloader(batch_size=32, transforms=CPCTrainTransformsSTL10())\\n\\n\\n        \"\"\"\\n        # flipping image along vertical axis\\n        self.patch_size = patch_size\\n        self.overlap = overlap\\n        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)\\n        normalize = transforms.Normalize(mean=(0.43, 0.42, 0.39), std=(0.27, 0.26, 0.27))\\n\\n        # image augmentation functions\\n        col_jitter = transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.2)], p=0.8)\\n        rnd_gray = transforms.RandomGrayscale(p=0.25)\\n        rand_crop = transforms.RandomResizedCrop(64, scale=(0.3, 1.0), ratio=(0.7, 1.4), interpolation=3)\\n\\n        self.transforms = transforms.Compose([\\n            rand_crop,\\n            col_jitter,\\n            rnd_gray,\\n            transforms.ToTensor(),\\n            normalize,\\n            Patchify(patch_size=patch_size, overlap_size=overlap)\\n        ])\\n\\n    def __call__(self, inp):\\n        inp = self.flip_lr(inp)\\n        out1 = self.transforms(inp)\\n        return out1\\n\\n\\nclass CPCEvalTransformsSTL10:\\n\\n    def __init__(self, patch_size=16, overlap=8):\\n        \"\"\"\\n        Transforms used for CPC:\\n\\n        Args:\\n\\n            patch_size: size of patches when cutting up the image into overlapping patches\\n            overlap: how much to overlap patches\\n\\n        Transforms::\\n\\n            random_flip\\n            transforms.ToTensor()\\n            normalize\\n            Patchify(patch_size=patch_size, overlap_size=patch_size // 2)\\n\\n        Example::\\n\\n            # in a regular dataset\\n            STL10(..., transforms=CPCEvalTransformsSTL10())\\n\\n            # in a DataModule\\n            module = STL10DataModule(PATH)\\n            train_loader = module.train_dataloader(batch_size=32, transforms=CPCEvalTransformsSTL10())\\n\\n        \"\"\"\\n        # flipping image along vertical axis\\n        self.patch_size = patch_size\\n        self.overlap = overlap\\n        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)\\n        normalize = transforms.Normalize(mean=(0.43, 0.42, 0.39), std=(0.27, 0.26, 0.27))\\n\\n        self.transforms = transforms.Compose([\\n            transforms.Resize(70, interpolation=3),\\n            transforms.CenterCrop(64),\\n            transforms.ToTensor(),\\n            normalize,\\n            Patchify(patch_size=patch_size, overlap_size=overlap)\\n        ])\\n\\n    def __call__(self, inp):\\n        out1 = self.transforms(inp)\\n        return out1\\n\\n\\nclass CPCTrainTransformsImageNet128:\\n    def __init__(self, patch_size=32, overlap=16):\\n        \"\"\"\\n        Transforms used for CPC:\\n\\n        Args:\\n\\n            patch_size: size of patches when cutting up the image into overlapping patches\\n            overlap: how much to overlap patches\\n\\n        Transforms::\\n\\n            random_flip\\n            transforms.ToTensor()\\n            normalize\\n            Patchify(patch_size=patch_size, overlap_size=patch_size // 2)\\n\\n        Example::\\n\\n            # in a regular dataset\\n            Imagenet(..., transforms=CPCTrainTransformsImageNet128())\\n\\n            # in a DataModule\\n            module = ImagenetDataModule(PATH)\\n            train_loader = module.train_dataloader(batch_size=32, transforms=CPCTrainTransformsImageNet128())\\n        \"\"\"\\n        # image augmentation functions\\n        self.patch_size = patch_size\\n        self.overlap = overlap\\n        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)\\n        rand_crop = transforms.RandomResizedCrop(128, scale=(0.3, 1.0), ratio=(0.7, 1.4), interpolation=3)\\n        col_jitter = transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8)\\n        rnd_gray = transforms.RandomGrayscale(p=0.25)\\n\\n        post_transform = transforms.Compose([\\n            transforms.ToTensor(),\\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\\n                                 std=[0.229, 0.224, 0.225]),\\n            Patchify(patch_size=patch_size, overlap_size=overlap),\\n        ])\\n\\n        self.transforms = transforms.Compose([\\n            rand_crop,\\n            col_jitter,\\n            rnd_gray,\\n            post_transform\\n        ])\\n\\n    def __call__(self, inp):\\n        inp = self.flip_lr(inp)\\n        out1 = self.transforms(inp)\\n        return out1\\n\\n\\nclass CPCEvalTransformsImageNet128:\\n    def __init__(self, patch_size=32, overlap=16):\\n        \"\"\"\\n        Transforms used for CPC:\\n\\n        Args:\\n\\n            patch_size: size of patches when cutting up the image into overlapping patches\\n            overlap: how much to overlap patches\\n\\n        Transforms::\\n\\n            random_flip\\n            transforms.ToTensor()\\n            normalize\\n            Patchify(patch_size=patch_size, overlap_size=patch_size // 2)\\n\\n        Example::\\n\\n            # in a regular dataset\\n            Imagenet(..., transforms=CPCEvalTransformsImageNet128())\\n\\n            # in a DataModule\\n            module = ImagenetDataModule(PATH)\\n            train_loader = module.train_dataloader(batch_size=32, transforms=CPCEvalTransformsImageNet128())\\n        \"\"\"\\n        # image augmentation functions\\n        self.patch_size = patch_size\\n        self.overlap = overlap\\n        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)\\n        post_transform = transforms.Compose([\\n            transforms.ToTensor(),\\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\\n                                 std=[0.229, 0.224, 0.225]),\\n            Patchify(patch_size=patch_size, overlap_size=overlap),\\n        ])\\n        self.transforms = transforms.Compose([\\n            transforms.Resize(146, interpolation=3),\\n            transforms.CenterCrop(128),\\n            post_transform\\n        ])\\n\\n    def __call__(self, inp):\\n        inp = self.flip_lr(inp)\\n        out1 = self.transforms(inp)\\n        return out1\\n',\n",
       " \"import pyspark.sql.types\\n\\nfirmware_cpes_schema = pyspark.sql.types.StructType([\\n    pyspark.sql.types.StructField('cpe', pyspark.sql.types.StringType()),\\n    pyspark.sql.types.StructField('firmware_hash', pyspark.sql.types.StringType()),\\n    pyspark.sql.types.StructField('evidence', pyspark.sql.types.StructType([\\n        pyspark.sql.types.StructField('type', pyspark.sql.types.StringType()),\\n        pyspark.sql.types.StructField('firmware_hash', pyspark.sql.types.StringType()),\\n        pyspark.sql.types.StructField('product', pyspark.sql.types.StringType()),\\n        pyspark.sql.types.StructField('version', pyspark.sql.types.StringType()),\\n    ]))\\n])\\n\\n\",\n",
       " '\"\"\"\\nThis module contains the class for detecting\\nthe presence of keywords in an audio stream\\n\"\"\"\\nimport logging\\nimport os\\n\\nimport numpy as np  # type: ignore\\n\\nfrom spokestack.context import SpeechContext\\nfrom spokestack.models.tensorflow import TFLiteModel\\nfrom spokestack.ring_buffer import RingBuffer\\n\\nfrom pydub import AudioSegment\\nfrom pydub.playback import play\\n\\n_LOG = logging.getLogger(__name__)\\n\\n\\nclass WakewordTrigger:\\n    \"\"\"Detects the presence of a wakeword in the audio input\\n\\n    Args:\\n            pre_emphasis (float): The value of the pre-emmphasis filter\\n            sample_rate (int): The number of audio samples per second of audio (kHz)\\n            fft_window_type (str): The type of fft window. (only support for hann)\\n            fft_hop_length (int): Audio sliding window for STFT calculation (ms)\\n            model_dir (str): Path to the directory containing .tflite models\\n            posterior_threshold (float): Probability threshold for if a wakeword\\n                                         was detected\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        pre_emphasis: float = 0.0,\\n        sample_rate: int = 16000,\\n        fft_window_type: str = \"hann\",\\n        fft_hop_length: int = 10,\\n        model_dir: str = \"\",\\n        model_type: str = \"\",\\n        posterior_threshold: float = 0.5,\\n        **kwargs,\\n    ) -> None:\\n\\n        self.pre_emphasis: float = pre_emphasis\\n        self.hop_length: int = int(fft_hop_length * sample_rate / 1000)\\n\\n        if fft_window_type != \"hann\":\\n            raise ValueError(\"Invalid fft_window_type\")\\n\\n        self.filter_model: TFLiteModel = TFLiteModel(\\n            model_path=os.path.join(model_dir, \"filter.tflite\")\\n        )\\n        self.encode_model: TFLiteModel = TFLiteModel(\\n            model_path=os.path.join(model_dir, \"encode.tflite\")\\n        )\\n        self.detect_model: TFLiteModel = TFLiteModel(\\n            model_path=os.path.join(model_dir, \"detect.tflite\")\\n        )\\n\\n        # Model architecture \\n        self.model_type = model_type.upper()\\n\\n        # window size calculated based on fft\\n        # the filter inputs are (fft_size - 1) / 2\\n        # which makes the window size (post_fft_size - 1) * 2\\n        self._window_size = (self.filter_model.input_details[0][\"shape\"][-1] - 1) * 2\\n        self._fft_window = np.hanning(self._window_size)\\n\\n        # retrieve the mel_length and mel_width based on the encoder model metadata\\n        # these allocate the buffer to the correct size\\n        if self.model_type == \\'CRNN\\':\\n            self.mel_length: int = self.encode_model.input_details[0][\"shape\"][2]\\n            self.mel_width: int = self.encode_model.input_details[0][\"shape\"][1]\\n        elif self.model_type == \\'WAVENET\\':\\n            self.mel_length: int = self.encode_model.input_details[0][\"shape\"][1]\\n            self.mel_width: int = self.encode_model.input_details[0][\"shape\"][2]\\n\\n        # initialize the first state input for autoregressive encoder model\\n        # retrieve the encode_lenth and encode_width from the model detect_model\\n        # metadata. We get the dimensions from the detect_model inputs because the\\n        # encode_model runs autoregressively and outputs a single encoded sample.\\n        # the detect_model input is a collection of these samples.\\n\\n        if self.model_type == \\'CRNN\\':\\n            self.encode_length: int = self.detect_model.input_details[0][\"shape\"][0]\\n            self.encode_width: int = self.detect_model.input_details[0][\"shape\"][-1]\\n        elif self.model_type == \\'WAVENET\\':\\n            self.encode_length: int = self.detect_model.input_details[0][\"shape\"][1]\\n            self.encode_width: int = self.detect_model.input_details[0][\"shape\"][-1]\\n\\n        self.sample_window: RingBuffer = RingBuffer(shape=[self._window_size])\\n        self.frame_window: RingBuffer = RingBuffer(\\n            shape=[self.mel_length, self.mel_width]\\n        )\\n        self.encode_window: RingBuffer = RingBuffer(\\n            shape=[1, self.encode_length, self.encode_width]\\n        )\\n\\n        # initialize the frame and encode windows with zeros\\n        # this minimizes the delay caused by filling the buffer\\n        self.frame_window.fill(0.0)\\n        self.encode_window.fill(-1.0)\\n\\n        self._posterior_threshold: float = posterior_threshold\\n        self._posterior_max: float = 0.0\\n        self._prev_sample: float = 0.0\\n        self._is_speech: bool = False\\n\\n        # audio segments for reponse on wake\\n        self.load_awake_responses(\\'audio_responses\\')\\n\\n    def load_awake_responses(self, audio_path):\\n        # load all mp3\\'s from audio_path for wake responses\\n        segs = []\\n        for f in os.listdir(audio_path):\\n            f_path = os.path.join(audio_path, f)\\n            if os.path.isfile(f_path) and \\'.mp3\\' in f_path:\\n                segs.append(AudioSegment.from_mp3(f_path))\\n\\n        self.audio_responses = np.array(segs, dtype=object)\\n\\n    def __call__(self, context: SpeechContext, frame) -> None:\\n        \"\"\"Entry point of the trigger\\n\\n        Args:\\n            context (SpeechContext): current state of the speech pipeline\\n            frame (np.ndarray): a single frame of an audio signal\\n\\n        Returns: None\\n\\n        \"\"\"\\n\\n        # detect vad edges for wakeword deactivation\\n        vad_fall = self._is_speech and not context.is_speech\\n        self._is_speech = context.is_speech\\n\\n        # sample frame to detect the presence of wakeword\\n        if not context.is_active:\\n            self._sample(context, frame)\\n\\n        # reset on vad fall deactivation\\n        if vad_fall:\\n            if not context.is_active:\\n                _LOG.info(f\"wake: {self._posterior_max}\")\\n            self.reset()\\n\\n    def _sample(self, context: SpeechContext, frame) -> None:\\n        # convert the PCM-16 audio to float32 in (-1.0, 1.0)\\n        frame = frame.astype(np.float32) / (2 ** 15 - 1)\\n        frame = np.clip(frame, -1.0, 1.0)\\n\\n        # pull out a single value from the frame and apply pre-emphasis\\n        # with the previous sample then cache the previous sample\\n        # to be use in the next iteration\\n        prev_sample = frame[-1]\\n        frame -= self.pre_emphasis * np.append(self._prev_sample, frame[:-1])\\n        self._prev_sample = prev_sample\\n\\n        # fill the sample window to analyze speech containing samples\\n        # after each window fill the buffer advances by the hop length\\n        # to produce an overlapping window\\n        for sample in frame:\\n            self.sample_window.write(sample)\\n            if self.sample_window.is_full:\\n                if context.is_speech:\\n                    self._analyze(context)\\n                self.sample_window.rewind().seek(self.hop_length)\\n\\n    def _analyze(self, context: SpeechContext) -> None:\\n        # read the full contents of the sample window to calculate a single frame\\n        # of the STFT by applying the DFT to a real-valued input and\\n        # taking the magnitude of the complex DFT\\n        frame = self.sample_window.read_all()\\n        frame = np.fft.rfft(frame * self._fft_window, n=self._window_size)\\n        frame = np.abs(frame).astype(np.float32)\\n\\n        # compute mel spectrogram\\n        self._filter(context, frame)\\n\\n    def _filter(self, context: SpeechContext, frame) -> None:\\n        # add the batch dimension and compute the mel spectrogram with filter model\\n        frame = np.expand_dims(frame, 0)\\n        frame = self.filter_model(frame)[0]\\n\\n        # advance the window by 1 and write mel frame to the frame buffer\\n        self.frame_window.rewind().seek(1)\\n        self.frame_window.write(frame)\\n\\n        # encode the mel spectrogram\\n        self._encode(context)\\n\\n    def _encode(self, context: SpeechContext) -> None:\\n        # read the full contents of the frame window and add the batch dimension\\n        # run the encoder and save the output state for autoregression\\n        frame = self.frame_window.read_all()\\n\\n        # different architectures have different input requirements \\n        if self.model_type == \\'CRNN\\':\\n            # swap timesteps and features\\n            frame = np.expand_dims(frame.T, 0)\\n            # add channel dimension\\n            frame = np.expand_dims(frame, -1)\\n\\n        elif self.model_type == \\'WAVENET\\':\\n            frame = np.expand_dims(frame, 0)\\n\\n        # inference for encoder\\n        frame = self.encode_model(frame)\\n\\n        # accumulate encoded samples until size of detection window\\n        self.encode_window.rewind().seek(1)\\n        self.encode_window.write(np.squeeze(frame))\\n        #self.encode_window.write(frame)\\n        self._detect(context)\\n\\n    def _detect(self, context: SpeechContext) -> None:\\n        # read the full contents of the encode window and add the batch dimension\\n        # calculate a scalar probability of if the frame contains the wakeword\\n        # with the detect model\\n        frame = self.encode_window.read_all()\\n\\n        # frame is (batch,timesteps,features), same as Wavenet\\n        # CRNN detect input is (batch,features)\\n        if self.model_type == \\'CRNN\\':\\n            frame = frame.squeeze(0)\\n\\n        if self.model_type == \"CRNN\":\\n            posterior = self.detect_model(frame)[0][0][0]\\n        else:\\n            posterior = self.detect_model(frame)[0][0][1]\\n\\n        if posterior > self._posterior_max:\\n            self._posterior_max = posterior\\n        if posterior > self._posterior_threshold:\\n            if not context.is_active:\\n                _LOG.info(f\"AWAKE!: {self._posterior_max}\")\\n                play(np.random.choice(self.audio_responses))\\n                context.is_active = True\\n\\n    def reset(self) -> None:\\n        \"\"\" Resets the currect WakewordDetector state \"\"\"\\n        self.sample_window.reset()\\n        self.frame_window.reset().fill(0.0)\\n        self.encode_window.reset().fill(-1.0)\\n        self._posterior_max = 0.0\\n\\n    def close(self) -> None:\\n        \"\"\" Close interface for use in the pipeline \"\"\"\\n        self.reset()\\n',\n",
       " \"import torch.nn as nn\\nimport torch\\nfrom torch.autograd import Variable\\nimport torch.nn.functional as F\\n\\n\\nclass AngleLoss(nn.Module):  # 设置loss，超参数gamma，最小比例，和最大比例\\n    def __init__(self, gamma=0, lambda_min=5, lambda_max=1500):\\n        super(AngleLoss, self).__init__()\\n        self.gamma = gamma\\n        self.it = 0\\n        self.lambda_min = lambda_min\\n        self.lambda_max = lambda_max\\n\\n    def forward(self, x, y):  # 分别是output和target\\n        self.it += 1\\n        cos_theta, phi_theta = x  # output包括上面的[cos_theta, phi_theta]\\n        y = y.view(-1, 1)\\n\\n        index = cos_theta.data * 0.0\\n        index.scatter_(1, y.data.view(-1, 1), 1)  # 将label存成稀疏矩阵\\n        index = index.byte()\\n        # index = Variable(index)   # warning occurs, change to following line. see link blew:\\n        # https://github.com/pytorch/pytorch/issues/29365\\n        #index = torch.tensor(index, dtype=torch.bool)\\n        index = index.clone().detach().bool()\\n\\n        lamb = max(self.lambda_min, self.lambda_max / (1 + 0.1 * self.it))  # 动态调整lambda，来调整cos(\\\\theta)和\\\\phi(\\\\theta)的比例\\n        output = cos_theta * 1.0\\n        output[index] -= cos_theta[index]*(1.0+0)/(1 + lamb)  # 减去目标\\\\cos(\\\\theta)的部分\\n        output[index] += phi_theta[index]*(1.0+0)/(1 + lamb)  # 加上目标\\\\phi(\\\\theta)的部分\\n\\n        logpt = F.log_softmax(output, dim=1)\\n        logpt = logpt.gather(1, y)\\n        logpt = logpt.view(-1)\\n        pt = Variable(logpt.data.exp())\\n\\n        loss = -1 * (1-pt)**self.gamma * logpt\\n        loss = loss.mean()\\n\\n        return loss\\n\\n\\nclass AngleLossWithCE(nn.Module):\\n    def __init__(self, lambda_min=5, lambda_max=1500, weight=[1, 1]):\\n        super().__init__()\\n        self.embeddingLoss = AngleLoss(lambda_min, lambda_max)\\n        self.clsLoss = nn.CrossEntropyLoss()\\n        self.weight = weight\\n\\n    def forward(self, x1, x2, label):\\n        embeddingLoss = self.embeddingLoss(x1, label)\\n        clsLoss = self.clsLoss(x2, label)\\n        total_loss = embeddingLoss * self.weight[0] + clsLoss * self.weight[1]\\n        return total_loss\\n\\n\\n# http://papers.nips.cc/paper/6653-learning-with-average-top-k-loss.pdf\\n# not sure working or not\\nclass TopKLossWithBCE(nn.Module):\\n    def __init__(self, p=0.7):\\n        super().__init__()\\n        self.p = p\\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\\n\\n    def forward(self, pred, gt):\\n        k = int(pred.shape[0] * self.p)\\n        loss = self.bce(pred, gt)\\n        loss = loss.topk(k, dim=0)[0]\\n        loss = loss.mean()\\n        return loss\\n\\n\\n# https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354\\nclass F1_Loss(nn.Module):\\n    '''Calculate F1 score. Can work with gpu tensors\\n\\n    The original implmentation is written by Michal Haltuf on Kaggle.\\n\\n    Returns\\n    -------\\n    torch.Tensor\\n        `ndim` == 1. epsilon <= val <= 1\\n\\n    Reference\\n    ---------\\n    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\\n    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\\n    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\\n    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\\n    '''\\n\\n    def __init__(self, classes=2, epsilon=1e-7):\\n        super().__init__()\\n        self.epsilon = epsilon\\n        self.classes = classes\\n\\n    def forward(self, y_pred, y_true, ):\\n        assert y_pred.ndim == 2\\n        assert y_true.ndim == 1\\n        y_true = F.one_hot(y_true, self.classes).to(torch.float32)\\n        y_pred = F.softmax(y_pred, dim=1)\\n\\n        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\\n        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\\n        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\\n        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\\n\\n        precision = tp / (tp + fp + self.epsilon)\\n        recall = tp / (tp + fn + self.epsilon)\\n\\n        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\\n        f1 = f1.clamp(min=self.epsilon, max=1 - self.epsilon)\\n        return 1 - f1.mean()\\n\\n\\nclass F1LossWithBCE(nn.Module):\\n    def __init__(self, classes=264, weights=[1, 1]):\\n        super().__init__()\\n        self.classes = classes\\n        self.weights = weights\\n        self.bce = nn.BCEWithLogitsLoss()\\n        self.f1 = F1_Loss(classes=self.classes)\\n\\n    def forward(self, pred, gt):\\n        bce = self.bce(pred, gt)\\n        f1 = self.f1(pred, gt)\\n        loss = self.weights[0] * bce + self.weights[1] * f1\\n        return loss\\n\\n\\n\",\n",
       " '# -*- coding: utf-8 -*-\\n\"\"\"\\nThis file is part of pyCMBS.\\n(c) 2012- Alexander Loew\\nFor COPYING and LICENSE details, please refer to the LICENSE file\\n\"\"\"\\n\\n\\nclass Koeppen(object):\\n    \"\"\"\\n    KOEPPEN CLASS\\n    class to generate koeppen plot\\n    \"\"\"\\n\\n    def __init__(self, temp=None, precip=None, lsm=None):\\n        \"\"\"\\n        Koeppen class\\n        This class implements the functionality to generate koeppen plots.\\n\\n        Parameters\\n        ----------\\n        temp : Data\\n            data objekt of temperature\\n        precip : Data\\n            data objekt of precipitation\\n        lsm : Data\\n            data objekt of land-sea-mask (0.0 to 1.0)\\n\\n        EXAMPLES\\n        ========\\n\\n        \"\"\"\\n\\n        # check consistency\\n        if temp is None:\\n            raise ValueError(\\'No temperature given\\')\\n        if precip is None:\\n            raise ValueError(\\'No precipitation given\\')\\n        if lsm is None:\\n            raise ValueError(\\'No land-sea-mask given\\')\\n\\n        # set values of class\\n        self.temp = temp\\n        self.precip = precip\\n        self.lsm = lsm\\n\\n        if not self._check_resolution():\\n            raise ValueError(\\'ERROR:The three array differe in the resolution\\')\\n        if not self._check_units():\\n            raise ValueError(\\'ERROR:The units of one value is wrong\\')\\n\\n        # Create new koeppen Color map\\n        self.koeppen_cmap()\\n        self.cmap = cm.get_cmap(\\'koeppen\\')\\n        # convert from [kg m-2 s-1] to [kg m-2 day-1] (= [mm day-1])\\n         # ??? Unklar warum nicht \\'precip.mulc(60. * 60. * 24. * 365.)\\'\\n        self.precip = precip.mulc(60. * 60. * 24. * 365. / 12., copy=True)\\n        self.temp = temp.subc(273.15, copy=True)  # ??? Unklar warum nicht \\'temp.subc(273.15)\\'\\n\\n        Psum = self.precip.timsum(return_object=True)            # Berechnet die Summe der Jahresniederschlag\\n\\n        nt, ny, nx = self.temp.shape\\n        nlat = ny\\n        nlon = nx\\n\\n        Pmin = self.precip.data.min(axis=0)\\n        Pmax = self.precip.data.max(axis=0)\\n\\n        precipHS = self.precip.copy()\\n        precipHS.data[(0, 1, 2, 3, 4, 5), 0:(nlat / 2 - 1), :] \\\\\\n            = self.precip.data[(3, 4, 5, 6, 7, 8), 0:(nlat / 2 - 1), :]\\n        precipHS.data[(6, 7, 8, 9, 10, 11), 0:(nlat / 2 - 1), :] \\\\\\n            = self.precip.data[(3, 4, 5, 6, 7, 8), 0:(nlat / 2 - 1), :]\\n        precipHS.data[(0, 1, 2, 3, 4, 5), (nlat / 2):(nlat - 1), :] \\\\\\n            = self.precip.data[(0, 1, 2, 9, 10, 11), (nlat / 2):(nlat - 1), :]\\n        precipHS.data[(6, 7, 8, 9, 10, 11), (nlat / 2):(nlat - 1), :] \\\\\\n            = self.precip.data[(0, 1, 2, 9, 10, 11), (nlat / 2):(nlat - 1), :]\\n\\n        precipHW = self.precip.copy()\\n        precipHW.data[(0, 1, 2, 3, 4, 5), 0:(nlat / 2 - 1), :] = self.precip.data[(0, 1, 2, 9, 10, 11), 0:(nlat / 2 - 1), :]\\n        precipHW.data[(6, 7, 8, 9, 10, 11), 0:(nlat / 2 - 1), :] = self.precip.data[(0, 1, 2, 9, 10, 11), 0:(nlat / 2 - 1), :]\\n        precipHW.data[(0, 1, 2, 3, 4, 5), (nlat / 2):(nlat - 1), :] = self.precip.data[(3, 4, 5, 6, 7, 8), (nlat / 2):(nlat - 1), :]\\n        precipHW.data[(6, 7, 8, 9, 10, 11), (nlat / 2):(nlat - 1), :] = self.precip.data[(3, 4, 5, 6, 7, 8), (nlat / 2):(nlat - 1), :]\\n\\n        PminHS = precipHS.data.min(axis=0)   # Bestimmt den minimalen Monastniederschlag aus PmaxHS\\n        PmaxHS = precipHS.data.max(axis=0)   # Bestimmt den maximalen Monastniederschlag aus PmaxHS\\n        PminHW = precipHW.data.min(axis=0)   # Bestimmt den minimalen Monastniederschlag aus PminHW\\n        PmaxHW = precipHW.data.max(axis=0)   # Bestimmt den maximalen Monastniederschlag aus PminHW\\n\\n        Tavg = self.temp.data.mean(axis=0)   # Bestimmt die mittlere Jahrestemperatur\\n        Tmin = self.temp.data.min(axis=0)     # Bestimmt die minimale Monatstemperatur\\n        Tmax = self.temp.data.max(axis=0)     # Bestimmt die maximale Jahrestemperatur\\n\\n        self.Clim = self.precip.timmean(return_object=True)\\n        self.Clim.units = \"climate type\"\\n\\n        for lat in range(0, nlat):\\n            for lon in range(0, nlon):\\n                psum = Psum.data.data[lat][lon]\\n                pmin = Pmin[lat][lon]\\n                pminhs = PminHS[lat][lon]\\n                pminhw = PminHW[lat][lon]\\n                pmaxhs = PmaxHS[lat][lon]\\n                pmaxhw = PmaxHW[lat][lon]\\n                tavg = Tavg[lat][lon]\\n                tmin = Tmin[lat][lon]\\n                tmax = Tmax[lat][lon]\\n                self.Clim.data.data[lat][lon] = self.set_clim(psum, pmin, pminhs, pminhw, pmaxhs, pmaxhw, tavg, tmin, tmax)\\n\\n        self.Clim.data.mask[less(self.lsm.data, 0.5)] = True\\n\\n    def koeppen_cmap(self):\\n        \"\"\"\\n        Create a colormap with 14 discrete colors and register it\\n        \"\"\"\\n        # define individual colors as hex values\\n        cpool = [\\'#7f0000\\', \\'#ff0000\\', \\'#ff4c4c\\', \\'#ff9999\\', \\'#ffa500\\',\\n                 \\'#ffff4c\\', \\'#009900\\', \\'#00ff00\\', \\'#99ff99\\', \\'#990099\\',\\n                 \\'#e500e5\\', \\'#ff66ff\\', \\'#0000ff\\', \\'#9999ff\\', \\'#000000\\']\\n        cmap3 = col.ListedColormap(cpool[0:14], \\'koeppen\\')\\n#       plt.cm.register_cmap(cmap=cmap3,name=\\'koeppen\\',lut=15)\\n        plt.cm.register_cmap(cmap=cmap3, name=\\'koeppen\\')\\n        return cmap3\\n\\n    def set_clim(self, psum, pmin, pminhs, pminhw, pmaxhs, pmaxhw, tavg, tmin, tmax):\\n        clim = -999\\n\\n        if tmin > 18:\\n            if pmin > 60:                 # A(B)\\n                clim = 1                    # Af\\n            else:\\n                if pmin > (0.04 * (2500 - psum)):      # A(B)-msw\\n                    clim = 2                  # Am\\n                else:\\n                    if (pminhs < 40) and (pminhs < (pmaxhw / 3)):   # A(B)-sw\\n                        if (psum / 10) < (2 * tavg):          # A(B)-s\\n                            if (psum / 10) < (tavg):            # B\\n                                clim = 6                    # BW\\n                            else:\\n                                clim = 5                    # BS\\n                        else:\\n                            clim = 3                      # As\\n                    else:\\n                        if (psum / 10) < (2 * (tavg + 14)):       # A(B)-w\\n                            if (psum / 10) < (tavg + 14):       # B\\n                                clim = 6                    # BW\\n                            else:\\n                                clim = 5                    # BS\\n                        else:\\n                            clim = 4                      # Aw\\n        else:\\n            if (pminhs < 40) and (pminhs < (pmaxhw / 3)):   # CDE(B)\\n                if (psum / 10) < (2 * tavg):          # CDE(B)-s\\n                    if (psum / 10) < (tavg):            # B\\n                        clim = 6                    # BW\\n                    else:\\n                        clim = 5                    # BS\\n                else:\\n                    if tmax < 10:                # CDE-s\\n                        if tmax < 0:                # E\\n                            clim = 14                 # EF\\n                        else:\\n                            clim = 13                 # ET\\n                    else:\\n                        if (tmin > -3):             # CD-s\\n                            clim = 8                  # Cs\\n                        else:\\n                            clim = 11                 # Ds\\n            else:\\n                if pminhw < (pmaxhs / 10):            # CDE(B)-fw\\n                    if (psum / 10) < (2 * (tavg + 14)):     # CDE(B)-w\\n                        if (psum / 10) < (tavg + 14):         # B\\n                            clim = 6                  # BW\\n                        else:\\n                            clim = 5                  # BS\\n                    else:\\n                        if tmax < 10:               # CDE-w\\n\\n                            if (tmax < 0):                # E\\n                                clim = 14               # EF\\n                            else:\\n                                clim = 13              # ET\\n                        else:\\n                            if (tmin > -3):               # CD-w\\n                                clim = 9                # Cw\\n                            else:\\n                                clim = 12               # Dw\\n                else:\\n                    if (psum / 10) < (2 * (tavg + 7)):      # CDE(B)-f\\n                        if (psum / 10) < (tavg + 7):          # B\\n                            clim = 6                  # BW\\n                        else:\\n                            clim = 5                  # BS\\n                    else:\\n                        if (tmax < 10):             # CDE-f\\n                            if (tmax < 0):                # E\\n                                clim = 14               # EF\\n                            else:\\n                                clim = 13              # ET\\n                        else:\\n                            if (tmin > -3):               # CD-f\\n                                clim = 7                # Cf\\n                            else:\\n                                clim = 10              # Df\\n        return clim\\n\\n    def _check_resolution(self):\\n        \"\"\"\\n        This routine just checks if all three array have a equal number of ny and nx values\\n        \"\"\"\\n        nt_t, ny_t, nx_t = self.temp.shape\\n        nt_p, ny_p, nx_p = self.precip.shape\\n        ny_l, nx_l = self.lsm.shape\\n\\n        if (ny_t != ny_p) or (ny_t != ny_l):\\n            sys.exit(\\'ERROR: The resolution ot the three arrays differ in \\\\\\n       Y-dimension: \\\\n\\' + str(ny_t) + \"(temp)  \" + str(ny_p)\\n                     + \"(precip) \" + str(ny_l) + \"(lsm) \")\\n            return False\\n\\n        if (nx_t != nx_p) or (nx_t != nx_l):\\n            sys.exit(\\'ERROR: The resolution ot the three arrays differ in \\\\\\n       X-dimension: \\\\n\\' + str(nx_t) + \"(temp)  \" + str(nx_p)\\n                     + \"(precip) \" + str(nx_l) + \"(lsm) \")\\n            return False\\n\\n        return True\\n\\n    def _check_units(self):\\n        \"\"\"\\n        This routine just checks if all three array have a equal number of ny and nx values\\n        \"\"\"\\n        if self.precip.unit != \"kg/m^2s\":\\n            raise ValueError(\\'ERROR: The unit of the precip is not [kg/m^2s] its set to [\\' + self.precip.unit + \"]\")\\n\\n        if self.temp.unit != \"K\":\\n            raise ValueError(\\'ERROR: The unit of the temperature is not [K] its set to [\\' + self.temp.unit + \"]\")\\n\\n        if self.lsm.unit != \"fractional\":\\n            raise ValueError(\\'ERROR: The unit of the temperature is not [fractional] its set to [\\' + self.temp.unit + \"]\")\\n\\n        return True\\n\\n    def copy(self):\\n        \"\"\"\\n        Returns the Clim Data as an Data variable\\n        \"\"\"\\n        return self.Clim\\n\\n    def climfrac(self):\\n        \"\"\"\\n        This routine calculats the fraction of each type in per centum.\\n        ToDo:\\n        Unclear id the print is OK or if the values should given back as an array.\\n        \"\"\"\\n        climfrac = [0] * 14\\n\\n        ny, nx = self.Clim.data.data.shape\\n        for ny in range(0, ny - 1):\\n            Aweight = cos(self.Clim.lat[ny][0] / 180 * 3.14159265359)\\n            for nx in range(0, nx - 1):\\n                clim = int(self.Clim.data.data[ny][nx])\\n                climfrac[clim - 1] = climfrac[clim - 1] + Aweight\\n\\n        s = sum(climfrac)\\n        climfrac[:] = [x / s for x in climfrac]\\n\\n        print \"Af: \" + str(climfrac[0])\\n        print \"Am: \" + str(climfrac[1])\\n        print \"As: \" + str(climfrac[2])\\n        print \"Aw: \" + str(climfrac[3])\\n        print \"BS: \" + str(climfrac[4])\\n        print \"BW: \" + str(climfrac[5])\\n        print \"Cf: \" + str(climfrac[6])\\n        print \"Cs: \" + str(climfrac[7])\\n        print \"Cw: \" + str(climfrac[8])\\n        print \"Df: \" + str(climfrac[9])\\n        print \"Ds: \" + str(climfrac[10])\\n        print \"Dw: \" + str(climfrac[11])\\n        print \"ET: \" + str(climfrac[12])\\n        print \"EF: \" + str(climfrac[13])\\n\\n    def legend(self):\\n        \"\"\"\\n        This routine prints a legend of the geiger-koeppen types.\\n        The description is taken from:\\n        MARKUS KOTTEK, JUERGEN GRIESER, CHRISTOPH BECK , BRUNO RUDOLF and FRANZ RUBEL\\n        World Map of the Koeppen-Geiger climate classification updated\\n        Meteorologische Zeitschrift, Vol. 15, No. 3, 259-263 (June 2006)\\n\\n        \"\"\"\\n\\n        print \"|================= Class legend =================|\"\\n        print \"| Af: Equatorial rainforest, fully humid         |\"\\n        print \"| Am: Equatorial climates                        |\"\\n        print \"| As: Equatorial monsoon                         |\"\\n        print \"| Aw: Equatorial savannah with dry winter        |\"\\n        print \"|------------------------------------------------|\"\\n        print \"| BS: Steppe climate                             |\"\\n        print \"| BW: Desert climate                             |\"\\n        print \"|------------------------------------------------|\"\\n        print \"| Cf: Warm temperate climate, fully humid        |\"\\n        print \"| Cs: Warm temperate climate with dry summer     |\"\\n        print \"| Cw: Warm temperate climate with dry winter     |\"\\n        print \"|------------------------------------------------|\"\\n        print \"| Df: Snow climate, fully humid                  |\"\\n        print \"| Ds: Snow climate with dry summer               |\"\\n        print \"| Dw: Snow climate with dry winter               |\"\\n        print \"|------------------------------------------------|\"\\n        print \"| ET: Tundra climate                             |\"\\n        print \"| EF: Frost climate                              |\"\\n        print \"|================================================|\"\\n\\n    def plot(self, **kwargs):\\n        \"\"\"\\n        This routine plots the data of his own geiger-koeppen data by\\n        using the plot-routine map_plot.\\n        It use the own created color-map and sets the color-bar to a\\n        horizontal orientation.\\n        It set the range of values between 0.5 and 14.5. Which are the\\n        possible values of geiger-koeppen.\\n        ToDo:\\n        At the moment the label of the geiger-koeppen types are missing\\n        at the color-bar\\n        \"\"\"\\n        map_plot(self.Clim, cmap_data=self.cmap, colorbar_orientation=\\'horizontal\\', vmin=0.5, vmax=14.5,\\n                 cticks=[1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14.],\\n                 cticklabels=[\"Af\", \"Am\", \"As\", \"Aw\", \"BS\", \"BW\", \"Cf\",\\n                              \"Cs\", \"Cw\", \"Df\", \"Ds\", \"Dw\", \"ET\", \"EF\"],\\n                 nclasses=15, **kwargs)\\n',\n",
       " '##########################################################################\\n#\\n#  Copyright (c) 2011-2012, Image Engine Design Inc. All rights reserved.\\n#  Copyright (c) 2011-2012, John Haddon. All rights reserved.\\n#\\n#  Redistribution and use in source and binary forms, with or without\\n#  modification, are permitted provided that the following conditions are\\n#  met:\\n#\\n#      * Redistributions of source code must retain the above\\n#        copyright notice, this list of conditions and the following\\n#        disclaimer.\\n#\\n#      * Redistributions in binary form must reproduce the above\\n#        copyright notice, this list of conditions and the following\\n#        disclaimer in the documentation and/or other materials provided with\\n#        the distribution.\\n#\\n#      * Neither the name of John Haddon nor the names of\\n#        any other contributors to this software may be used to endorse or\\n#        promote products derived from this software without specific prior\\n#        written permission.\\n#\\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\\n#  IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\\n#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\\n#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR\\n#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\\n#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\\n#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\\n#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\\n#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\\n#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\n#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n#\\n##########################################################################\\n\\nfrom __future__ import with_statement\\n\\nimport IECore\\n\\nimport GafferUI\\nimport GafferCortexUI\\n\\n## Supported parameter userData entries :\\n#\\n# [\"UI\"][\"collapsible\"]\\n# [\"UI\"][\"collapsed\"]\\n#\\n# Supported child userData entries :\\n#\\n# [\"UI\"][\"visible\"]\\nclass CompoundParameterValueWidget( GafferCortexUI.ParameterValueWidget ) :\\n\\n\\t## If collapsible is not None then it overrides any [\"UI][\"collapsible\"] userData the parameter might have.\\n\\tdef __init__( self, parameterHandler, collapsible=None, _plugValueWidgetClass=None, **kw ) :\\n\\n\\t\\tif collapsible is None :\\n\\t\\t\\tcollapsible = True\\n\\t\\t\\twith IECore.IgnoredExceptions( KeyError ) :\\n\\t\\t\\t\\tcollapsible = parameterHandler.parameter().userData()[\"UI\"][\"collapsible\"].value\\n\\n\\t\\tcollapsed = None\\n\\t\\tif collapsible :\\n\\t\\t\\tcollapsed = True\\n\\t\\t\\twith IECore.IgnoredExceptions( KeyError ) :\\n\\t\\t\\t\\tcollapsed = parameterHandler.parameter().userData()[\"UI\"][\"collapsed\"].value\\n\\n\\t\\tif _plugValueWidgetClass is None :\\n\\t\\t\\t_plugValueWidgetClass = _PlugValueWidget\\n\\n\\t\\tGafferCortexUI.ParameterValueWidget.__init__(\\n\\t\\t\\tself,\\n\\t\\t\\t_plugValueWidgetClass( parameterHandler, collapsed ),\\n\\t\\t\\tparameterHandler,\\n\\t\\t\\t**kw\\n\\t\\t)\\n\\n# CompoundParameterValueWidget is simply a lightweight wrapper around this CompoundPlugValueWidget\\n# derived class. This allows us to take advantage of all the code in CompoundPlugValueWidget that\\n# deals with dynamically adding and removing children etc.\\nclass _PlugValueWidget( GafferUI.CompoundPlugValueWidget ) :\\n\\n\\tdef __init__( self, parameterHandler, collapsed ) :\\n\\n\\t\\tGafferUI.CompoundPlugValueWidget.__init__( self, parameterHandler.plug(), collapsed )\\n\\n\\t\\tself.__parameterHandler = parameterHandler\\n\\n\\tdef _childPlugs( self ) :\\n\\n\\t\\tplug = self.getPlug()\\n\\t\\torderedChildren = []\\n\\t\\tfor childName in self.__parameterHandler.parameter().keys() :\\n\\t\\t\\tif childName in plug :\\n\\t\\t\\t\\torderedChildren.append( plug[childName] )\\n\\n\\t\\treturn orderedChildren\\n\\n\\tdef _childPlugWidget( self, childPlug ) :\\n\\n\\t\\tchildParameter = self.__parameterHandler.parameter()[childPlug.getName()]\\n\\n\\t\\twith IECore.IgnoredExceptions( KeyError ) :\\n\\t\\t\\tif not childParameter.userData()[\"UI\"][\"visible\"].value :\\n\\t\\t\\t\\treturn None\\n\\n\\t\\tchildParameterHandler = self.__parameterHandler.childParameterHandler( childParameter )\\n\\t\\tvalueWidget = GafferCortexUI.ParameterValueWidget.create( childParameterHandler )\\n\\t\\tif not valueWidget :\\n\\t\\t\\treturn None\\n\\n\\t\\tif isinstance( valueWidget, CompoundParameterValueWidget ) :\\n\\t\\t\\treturn valueWidget\\n\\n\\t\\treturn GafferUI.PlugWidget( valueWidget )\\n\\n\\tdef _parameter( self ) :\\n\\n\\t\\treturn self.__parameterHandler.parameter()\\n\\n\\tdef _parameterHandler( self ) :\\n\\n\\t\\treturn self.__parameterHandler\\n\\n\\tdef _parameterLabelText( self, parameterHandler ) :\\n\\n \\t\\treturn IECore.CamelCase.toSpaced( parameterHandler.plug().getName() )\\n\\n\\tdef _parameterToolTip( self, parameterHandler ) :\\n\\n\\t\\tplug = parameterHandler.plug()\\n\\n\\t\\tresult = \"<h3>\" + plug.relativeName( plug.node() ) + \"</h3>\"\\n\\t\\tif parameterHandler.parameter().description :\\n\\t\\t\\tresult += \"\\\\n\\\\n\" + parameterHandler.parameter().description\\n\\n\\t\\treturn result\\n\\n# install implementation class as a protected member, so it can be used by\\n# derived classes.\\nCompoundParameterValueWidget._PlugValueWidget = _PlugValueWidget\\n\\nGafferCortexUI.ParameterValueWidget.registerType( IECore.CompoundParameter, CompoundParameterValueWidget )\\n',\n",
       " '#\\n# Copyright (c) 2013 Juniper Networks, Inc. All rights reserved.\\n#\\n\\nfrom setuptools import setup, find_packages\\n\\nsetup(\\n    name=\\'nodemgr\\',\\n    version=\\'0.1dev\\',\\n    packages=[\\'nodemgr\\'],\\n    package_data={\\'\\': [\\'*.html\\', \\'*.css\\', \\'*.xml\\']},\\n    zip_safe=False,\\n    long_description=\"Nodemgr Implementation\",\\n    entry_points={\\n        \\'console_scripts\\': [\\n            \\'contrail-nodemgr = nodemgr.main:main\\',\\n        ],\\n    },\\n)\\n',\n",
       " 'from collections import defaultdict\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\nfrom hyperopt_synthetic import run_one_exp as hyperopt_synthetic_opt\\nfrom xbbo_synthetic import run_one_exp as xbbo_synthetic_opt\\n\\nmax_call = 50\\nif __name__ == \"__main__\":\\n    rng = np.random.RandomState(42)\\n    result_opts = defaultdict(list)\\n    for i in range(3):\\n        seed = rng.randint(1e5)\\n        # result_opts[\\'hyperopt-rand\\'].append(hyperopt_synthetic_opt(\\'rand\\', max_call,seed))\\n        result_opts[\\'hyperopt-tpe\\'].append(hyperopt_synthetic_opt(\\'tpe\\', max_call,seed))\\n        # result_opts[\\'hyperopt-atpe\\'].append(hyperopt_synthetic_opt(\\'atpe\\', max_call,seed))\\n        # result_opts[\\'hyperopt-mix\\'].append(hyperopt_synthetic_opt(\\'mix\\', max_call,seed))\\n        result_opts[\\'hyperopt-anneal\\'].append(hyperopt_synthetic_opt(\\'anneal\\', max_call,seed))\\n        result_opts[\\'XBBO-tpe\\'].append(xbbo_synthetic_opt(\\'tpe\\', max_call,seed))\\n        result_opts[\\'XBBO-anneal\\'].append(xbbo_synthetic_opt(\\'anneal\\',max_call,seed))\\n    plt.figure()\\n    for key in result_opts:\\n        plt.plot(range(1,max_call+1), np.mean(np.minimum.accumulate(np.asarray(result_opts[key]), axis=1),axis=0)[:], label=key)\\n    plt.ylim([-0.1,1000])\\n    plt.xlabel(\\'# of Evaluate\\')\\n    plt.ylabel(\\'OBJ\\')\\n    plt.title(\\'Average of cumulate best on 3 seeds\\')\\n    plt.legend()\\n    plt.savefig(\\'./out/comp_with_hyperopt.png\\')\\n    plt.show()\\n\\n',\n",
       " '#!/usr/bin/env python\\nimport json\\nimport os\\nimport sys\\nfrom textwrap import dedent\\n\\n\\ndef get_hashed_filenames(static_path):\\n    json_file = \\'{}/staticfiles.json\\'.format(static_path)\\n    with open(json_file) as jsonf:\\n        staticfiles = json.load(jsonf)\\n\\n    return list(staticfiles[\\'paths\\'].values())\\n\\n\\ndef move_hashed_files(static_path, hashed_path):\\n    filenames = get_hashed_filenames(static_path)\\n    moved_count = 0\\n    for filename in filenames:\\n        # some filenames in the file are in the form\\n        # fontawesome/fonts/fontawesome-webfont.f7c2b4b747b1.eot?v=4.3.0\\n        # we can skip these as they\\'re duplicated\\n        if \\'?\\' in filename:\\n            continue\\n\\n        src_fn = os.path.join(static_path, filename)\\n        dst_fn = os.path.join(hashed_path, filename)\\n        if not os.path.exists(os.path.dirname(dst_fn)):\\n            os.makedirs(os.path.dirname(dst_fn))\\n\\n        os.rename(src_fn, dst_fn)\\n        moved_count += 1\\n\\n    return moved_count\\n\\n\\ndef main(static_path, hashed_path):\\n    moved = move_hashed_files(static_path, hashed_path)\\n    print(\\'Successfully moved {} files\\'.format(moved))\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        main(sys.argv[1], sys.argv[2])\\n    except IndexError:\\n        sys.exit(dedent(\"\"\"\\\\\\n            ERROR: source and destination directory arguments required.\\n\\n            Usage: move_hashed_staticfiles.py <source_dir> <dest_dir>\\n\\n            Moves hashed static files from source_dir to dest_dir based on the\\n            map of staticfiles in `source_dir/staticfiles.json`.\\n        \"\"\"))\\n',\n",
       " '#!/usr/bin/env python\\n# encoding: utf-8\\n# Carlos Rafael Giani, 2007 (dv)\\n# Thomas Nagy, 2007-2008 (ita)\\n\\nimport os, sys, re, optparse\\nimport ccroot # <- leave this\\nimport TaskGen, Utils, Task, Configure, Logs, Build\\nfrom Logs import debug, error\\nfrom TaskGen import taskgen, feature, after, before, extension\\nfrom Configure import conftest\\n\\nEXT_D = [\\'.d\\', \\'.di\\', \\'.D\\']\\nD_METHS = [\\'apply_core\\', \\'apply_vnum\\', \\'apply_objdeps\\'] # additional d methods\\n\\ndef filter_comments(filename):\\n\\ttxt = Utils.readf(filename)\\n\\tbuf = []\\n\\n\\ti = 0\\n\\tmax = len(txt)\\n\\twhile i < max:\\n\\t\\tc = txt[i]\\n\\t\\t# skip a string\\n\\t\\tif c == \\'\"\\':\\n\\t\\t\\ti += 1\\n\\t\\t\\tc = \\'\\'\\n\\t\\t\\twhile i < max:\\n\\t\\t\\t\\tp = c\\n\\t\\t\\t\\tc = txt[i]\\n\\t\\t\\t\\ti += 1\\n\\t\\t\\t\\tif i == max: return buf\\n\\t\\t\\t\\tif c == \\'\"\\':\\n\\t\\t\\t\\t\\tcnt = 0\\n\\t\\t\\t\\t\\twhile i < cnt and i < max:\\n\\t\\t\\t\\t\\t\\t#print \"cntcnt = \", str(cnt), self.txt[self.i-2-cnt]\\n\\t\\t\\t\\t\\t\\tif txt[i-2-cnt] == \\'\\\\\\\\\\': cnt+=1\\n\\t\\t\\t\\t\\t\\telse: break\\n\\t\\t\\t\\t\\t#print \"cnt is \", str(cnt)\\n\\t\\t\\t\\t\\tif (cnt%2)==0: break\\n\\t\\t\\ti += 1\\n\\t\\t# skip a char\\n\\t\\telif c == \"\\'\":\\n\\t\\t\\ti += 1\\n\\t\\t\\tif i == max: return buf\\n\\t\\t\\tc = txt[i]\\n\\t\\t\\tif c == \\'\\\\\\\\\\':\\n\\t\\t\\t\\ti += 1\\n\\t\\t\\t\\tif i == max: return buf\\n\\t\\t\\t\\tc = txt[i]\\n\\t\\t\\t\\tif c == \\'x\\':\\n\\t\\t\\t\\t\\ti += 2 # skip two chars\\n\\t\\t\\t\\telif c == \\'u\\':\\n\\t\\t\\t\\t\\ti += 4 # skip unicode chars\\n\\t\\t\\ti += 1\\n\\t\\t\\tif i == max: return buf\\n\\t\\t\\tc = txt[i]\\n\\t\\t\\tif c != \\'\\\\\\'\\': error(\"uh-oh, invalid character\")\\n\\n\\t\\t# skip a comment\\n\\t\\telif c == \\'/\\':\\n\\t\\t\\tif i == max: break\\n\\t\\t\\tc = txt[i+1]\\n\\t\\t\\t# eat /+ +/ comments\\n\\t\\t\\tif c == \\'+\\':\\n\\t\\t\\t\\ti += 1\\n\\t\\t\\t\\tnesting = 1\\n\\t\\t\\t\\tprev = 0\\n\\t\\t\\t\\twhile i < max:\\n\\t\\t\\t\\t\\tc = txt[i]\\n\\t\\t\\t\\t\\tif c == \\'+\\':\\n\\t\\t\\t\\t\\t\\tprev = 1\\n\\t\\t\\t\\t\\telif c == \\'/\\':\\n\\t\\t\\t\\t\\t\\tif prev:\\n\\t\\t\\t\\t\\t\\t\\tnesting -= 1\\n\\t\\t\\t\\t\\t\\t\\tif nesting == 0: break\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tif i < max:\\n\\t\\t\\t\\t\\t\\t\\t\\ti += 1\\n\\t\\t\\t\\t\\t\\t\\t\\tc = txt[i]\\n\\t\\t\\t\\t\\t\\t\\t\\tif c == \\'+\\':\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tnesting += 1\\n\\t\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\t\\treturn buf\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tprev = 0\\n\\t\\t\\t\\t\\ti += 1\\n\\t\\t\\t# eat /* */ comments\\n\\t\\t\\telif c == \\'*\\':\\n\\t\\t\\t\\ti += 1\\n\\t\\t\\t\\twhile i < max:\\n\\t\\t\\t\\t\\tc = txt[i]\\n\\t\\t\\t\\t\\tif c == \\'*\\':\\n\\t\\t\\t\\t\\t\\tprev = 1\\n\\t\\t\\t\\t\\telif c == \\'/\\':\\n\\t\\t\\t\\t\\t\\tif prev: break\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tprev = 0\\n\\t\\t\\t\\t\\ti += 1\\n\\t\\t\\t# eat // comments\\n\\t\\t\\telif c == \\'/\\':\\n\\t\\t\\t\\ti += 1\\n\\t\\t\\t\\tc = txt[i]\\n\\t\\t\\t\\twhile i < max and c != \\'\\\\n\\':\\n\\t\\t\\t\\t\\ti += 1\\n\\t\\t\\t\\t\\tc = txt[i]\\n\\t\\t# a valid char, add it to the buffer\\n\\t\\telse:\\n\\t\\t\\tbuf.append(c)\\n\\t\\ti += 1\\n\\treturn buf\\n\\nclass d_parser(object):\\n\\tdef __init__(self, env, incpaths):\\n\\t\\t#self.code = \\'\\'\\n\\t\\t#self.module = \\'\\'\\n\\t\\t#self.imports = []\\n\\n\\t\\tself.allnames = []\\n\\n\\t\\tself.re_module = re.compile(\"module\\\\s+([^;]+)\")\\n\\t\\tself.re_import = re.compile(\"import\\\\s+([^;]+)\")\\n\\t\\tself.re_import_bindings = re.compile(\"([^:]+):(.*)\")\\n\\t\\tself.re_import_alias = re.compile(\"[^=]+=(.+)\")\\n\\n\\t\\tself.env = env\\n\\n\\t\\tself.nodes = []\\n\\t\\tself.names = []\\n\\n\\t\\tself.incpaths = incpaths\\n\\n\\tdef tryfind(self, filename):\\n\\t\\tfound = 0\\n\\t\\tfor n in self.incpaths:\\n\\t\\t\\tfound = n.find_resource(filename.replace(\\'.\\', \\'/\\') + \\'.d\\')\\n\\t\\t\\tif found:\\n\\t\\t\\t\\tself.nodes.append(found)\\n\\t\\t\\t\\tself.waiting.append(found)\\n\\t\\t\\t\\tbreak\\n\\t\\tif not found:\\n\\t\\t\\tif not filename in self.names:\\n\\t\\t\\t\\tself.names.append(filename)\\n\\n\\tdef get_strings(self, code):\\n\\t\\t#self.imports = []\\n\\t\\tself.module = \\'\\'\\n\\t\\tlst = []\\n\\n\\t\\t# get the module name (if present)\\n\\n\\t\\tmod_name = self.re_module.search(code)\\n\\t\\tif mod_name:\\n\\t\\t\\tself.module = re.sub(\\'\\\\s+\\', \\'\\', mod_name.group(1)) # strip all whitespaces\\n\\n\\t\\t# go through the code, have a look at all import occurrences\\n\\n\\t\\t# first, lets look at anything beginning with \"import\" and ending with \";\"\\n\\t\\timport_iterator = self.re_import.finditer(code)\\n\\t\\tif import_iterator:\\n\\t\\t\\tfor import_match in import_iterator:\\n\\t\\t\\t\\timport_match_str = re.sub(\\'\\\\s+\\', \\'\\', import_match.group(1)) # strip all whitespaces\\n\\n\\t\\t\\t\\t# does this end with an import bindings declaration?\\n\\t\\t\\t\\t# (import bindings always terminate the list of imports)\\n\\t\\t\\t\\tbindings_match = self.re_import_bindings.match(import_match_str)\\n\\t\\t\\t\\tif bindings_match:\\n\\t\\t\\t\\t\\timport_match_str = bindings_match.group(1)\\n\\t\\t\\t\\t\\t# if so, extract the part before the \":\" (since the module declaration(s) is/are located there)\\n\\n\\t\\t\\t\\t# split the matching string into a bunch of strings, separated by a comma\\n\\t\\t\\t\\tmatches = import_match_str.split(\\',\\')\\n\\n\\t\\t\\t\\tfor match in matches:\\n\\t\\t\\t\\t\\talias_match = self.re_import_alias.match(match)\\n\\t\\t\\t\\t\\tif alias_match:\\n\\t\\t\\t\\t\\t\\t# is this an alias declaration? (alias = module name) if so, extract the module name\\n\\t\\t\\t\\t\\t\\tmatch = alias_match.group(1)\\n\\n\\t\\t\\t\\t\\tlst.append(match)\\n\\t\\treturn lst\\n\\n\\tdef start(self, node):\\n\\t\\tself.waiting = [node]\\n\\t\\t# while the stack is not empty, add the dependencies\\n\\t\\twhile self.waiting:\\n\\t\\t\\tnd = self.waiting.pop(0)\\n\\t\\t\\tself.iter(nd)\\n\\n\\tdef iter(self, node):\\n\\t\\tpath = node.abspath(self.env) # obtain the absolute path\\n\\t\\tcode = \"\".join(filter_comments(path)) # read the file and filter the comments\\n\\t\\tnames = self.get_strings(code) # obtain the import strings\\n\\t\\tfor x in names:\\n\\t\\t\\t# optimization\\n\\t\\t\\tif x in self.allnames: continue\\n\\t\\t\\tself.allnames.append(x)\\n\\n\\t\\t\\t# for each name, see if it is like a node or not\\n\\t\\t\\tself.tryfind(x)\\n\\ndef scan(self):\\n\\t\"look for .d/.di the .d source need\"\\n\\tenv = self.env\\n\\tgruik = d_parser(env, env[\\'INC_PATHS\\'])\\n\\tgruik.start(self.inputs[0])\\n\\n\\tif Logs.verbose:\\n\\t\\tdebug(\\'deps: nodes found for %s: %s %s\\' % (str(self.inputs[0]), str(gruik.nodes), str(gruik.names)))\\n\\t\\t#debug(\"deps found for %s: %s\" % (str(node), str(gruik.deps)), \\'deps\\')\\n\\treturn (gruik.nodes, gruik.names)\\n\\ndef get_target_name(self):\\n\\t\"for d programs and libs\"\\n\\tv = self.env\\n\\ttp = \\'program\\'\\n\\tfor x in self.features:\\n\\t\\tif x in [\\'dshlib\\', \\'dstaticlib\\']:\\n\\t\\t\\ttp = x.lstrip(\\'d\\')\\n\\treturn v[\\'D_%s_PATTERN\\' % tp] % self.target\\n\\nd_params = {\\n\\'dflags\\': \\'\\',\\n\\'importpaths\\':\\'\\',\\n\\'libs\\':\\'\\',\\n\\'libpaths\\':\\'\\',\\n\\'generate_headers\\':False,\\n}\\n\\n@feature(\\'d\\')\\n@before(\\'apply_type_vars\\')\\ndef init_d(self):\\n\\tfor x in d_params:\\n\\t\\tsetattr(self, x, getattr(self, x, d_params[x]))\\n\\nclass d_taskgen(TaskGen.task_gen):\\n\\tdef __init__(self, *k, **kw):\\n\\t\\tTaskGen.task_gen.__init__(self, *k, **kw)\\n\\n\\t\\t# COMPAT\\n\\t\\tif len(k) > 1:\\n\\t\\t\\tself.features.append(\\'d\\' + k[1])\\n\\n# okay, we borrow a few methods from ccroot\\nTaskGen.bind_feature(\\'d\\', D_METHS)\\n\\n@feature(\\'d\\')\\n@before(\\'apply_d_libs\\')\\ndef init_d(self):\\n\\tUtils.def_attrs(self,\\n\\t\\tdflags=\\'\\',\\n\\t\\timportpaths=\\'\\',\\n\\t\\tlibs=\\'\\',\\n\\t\\tlibpaths=\\'\\',\\n\\t\\tuselib=\\'\\',\\n\\t\\tuselib_local=\\'\\',\\n\\t\\tgenerate_headers=False, # set to true if you want .di files as well as .o\\n\\t\\tcompiled_tasks=[],\\n\\t\\tadd_objects=[],\\n\\t\\tlink_task=None)\\n\\n@feature(\\'d\\')\\n@after(\\'apply_d_link\\', \\'init_d\\')\\n@before(\\'apply_vnum\\')\\ndef apply_d_libs(self):\\n\\t\"\"\"after apply_link because of \\'link_task\\'\\n\\tafter default_cc because of the attribute \\'uselib\\'\"\"\"\\n\\tenv = self.env\\n\\n\\t# 1. the case of the libs defined in the project (visit ancestors first)\\n\\t# the ancestors external libraries (uselib) will be prepended\\n\\tself.uselib = self.to_list(self.uselib)\\n\\tnames = self.to_list(self.uselib_local)\\n\\n\\tseen = set([])\\n\\ttmp = Utils.deque(names) # consume a copy of the list of names\\n\\twhile tmp:\\n\\t\\tlib_name = tmp.popleft()\\n\\t\\t# visit dependencies only once\\n\\t\\tif lib_name in seen:\\n\\t\\t\\tcontinue\\n\\n\\t\\ty = self.name_to_obj(lib_name)\\n\\t\\tif not y:\\n\\t\\t\\traise Utils.WafError(\\'object %r was not found in uselib_local (required by %r)\\' % (lib_name, self.name))\\n\\t\\ty.post()\\n\\t\\tseen.add(lib_name)\\n\\n\\t\\t# object has ancestors to process (shared libraries): add them to the end of the list\\n\\t\\tif getattr(y, \\'uselib_local\\', None):\\n\\t\\t\\tlst = y.to_list(y.uselib_local)\\n\\t\\t\\tif \\'dshlib\\' in y.features or \\'cprogram\\' in y.features:\\n\\t\\t\\t\\tlst = [x for x in lst if not \\'cstaticlib\\' in self.name_to_obj(x).features]\\n\\t\\t\\ttmp.extend(lst)\\n\\n\\t\\t# link task and flags\\n\\t\\tif getattr(y, \\'link_task\\', None):\\n\\n\\t\\t\\tlink_name = y.target[y.target.rfind(os.sep) + 1:]\\n\\t\\t\\tif \\'dstaticlib\\' in y.features or \\'dshlib\\' in y.features:\\n\\t\\t\\t\\tenv.append_unique(\\'DLINKFLAGS\\', env.DLIB_ST % link_name)\\n\\t\\t\\t\\tenv.append_unique(\\'DLINKFLAGS\\', env.DLIBPATH_ST % y.link_task.outputs[0].parent.bldpath(env))\\n\\n\\t\\t\\t# the order\\n\\t\\t\\tself.link_task.set_run_after(y.link_task)\\n\\n\\t\\t\\t# for the recompilation\\n\\t\\t\\tdep_nodes = getattr(self.link_task, \\'dep_nodes\\', [])\\n\\t\\t\\tself.link_task.dep_nodes = dep_nodes + y.link_task.outputs\\n\\n\\t\\t# add ancestors uselib too - but only propagate those that have no staticlib\\n\\t\\tfor v in self.to_list(y.uselib):\\n\\t\\t\\tif not v in self.uselib:\\n\\t\\t\\t\\tself.uselib.insert(0, v)\\n\\n\\t\\t# if the library task generator provides \\'export_incdirs\\', add to the include path\\n\\t\\t# the export_incdirs must be a list of paths relative to the other library\\n\\t\\tif getattr(y, \\'export_incdirs\\', None):\\n\\t\\t\\tfor x in self.to_list(y.export_incdirs):\\n\\t\\t\\t\\tnode = y.path.find_dir(x)\\n\\t\\t\\t\\tif not node:\\n\\t\\t\\t\\t\\traise Utils.WafError(\\'object %r: invalid folder %r in export_incdirs\\' % (y.target, x))\\n\\t\\t\\t\\tself.env.append_unique(\\'INC_PATHS\\', node)\\n\\n@feature(\\'dprogram\\', \\'dshlib\\', \\'dstaticlib\\')\\n@after(\\'apply_core\\')\\ndef apply_d_link(self):\\n\\tlink = getattr(self, \\'link\\', None)\\n\\tif not link:\\n\\t\\tif \\'dstaticlib\\' in self.features: link = \\'static_link\\'\\n\\t\\telse: link = \\'d_link\\'\\n\\n\\toutputs = [t.outputs[0] for t in self.compiled_tasks]\\n\\tself.link_task = self.create_task(link, outputs, self.path.find_or_declare(get_target_name(self)))\\n\\n@feature(\\'d\\')\\n@after(\\'apply_core\\')\\ndef apply_d_vars(self):\\n\\tenv = self.env\\n\\tdpath_st   = env[\\'DPATH_ST\\']\\n\\tlib_st     = env[\\'DLIB_ST\\']\\n\\tlibpath_st = env[\\'DLIBPATH_ST\\']\\n\\n\\timportpaths = self.to_list(self.importpaths)\\n\\tlibpaths = []\\n\\tlibs = []\\n\\tuselib = self.to_list(self.uselib)\\n\\n\\tfor i in uselib:\\n\\t\\tif env[\\'DFLAGS_\\' + i]:\\n\\t\\t\\tenv.append_unique(\\'DFLAGS\\', env[\\'DFLAGS_\\' + i])\\n\\n\\tfor x in self.features:\\n\\t\\tif not x in [\\'dprogram\\', \\'dstaticlib\\', \\'dshlib\\']:\\n\\t\\t\\tcontinue\\n\\t\\tx.lstrip(\\'d\\')\\n\\t\\td_shlib_dflags = env[\\'D_\\' + x + \\'_DFLAGS\\']\\n\\t\\tif d_shlib_dflags:\\n\\t\\t\\tenv.append_unique(\\'DFLAGS\\', d_shlib_dflags)\\n\\n\\t# add import paths\\n\\tfor i in uselib:\\n\\t\\tif env[\\'DPATH_\\' + i]:\\n\\t\\t\\tfor entry in self.to_list(env[\\'DPATH_\\' + i]):\\n\\t\\t\\t\\tif not entry in importpaths:\\n\\t\\t\\t\\t\\timportpaths.append(entry)\\n\\n\\t# now process the import paths\\n\\tfor path in importpaths:\\n\\t\\tif os.path.isabs(path):\\n\\t\\t\\tenv.append_unique(\\'_DIMPORTFLAGS\\', dpath_st % path)\\n\\t\\telse:\\n\\t\\t\\tnode = self.path.find_dir(path)\\n\\t\\t\\tself.env.append_unique(\\'INC_PATHS\\', node)\\n\\t\\t\\tenv.append_unique(\\'_DIMPORTFLAGS\\', dpath_st % node.srcpath(env))\\n\\t\\t\\tenv.append_unique(\\'_DIMPORTFLAGS\\', dpath_st % node.bldpath(env))\\n\\n\\t# add library paths\\n\\tfor i in uselib:\\n\\t\\tif env[\\'LIBPATH_\\' + i]:\\n\\t\\t\\tfor entry in self.to_list(env[\\'LIBPATH_\\' + i]):\\n\\t\\t\\t\\tif not entry in libpaths:\\n\\t\\t\\t\\t\\tlibpaths.append(entry)\\n\\tlibpaths = self.to_list(self.libpaths) + libpaths\\n\\n\\t# now process the library paths\\n\\t# apply same path manipulation as used with import paths\\n\\tfor path in libpaths:\\n\\t\\tenv.append_unique(\\'DLINKFLAGS\\', libpath_st % path)\\n\\n\\t# add libraries\\n\\tfor i in uselib:\\n\\t\\tif env[\\'LIB_\\' + i]:\\n\\t\\t\\tfor entry in self.to_list(env[\\'LIB_\\' + i]):\\n\\t\\t\\t\\tif not entry in libs:\\n\\t\\t\\t\\t\\tlibs.append(entry)\\n\\tlibs.extend(self.to_list(self.libs))\\n\\n\\t# process user flags\\n\\tfor flag in self.to_list(self.dflags):\\n\\t\\tenv.append_unique(\\'DFLAGS\\', flag)\\n\\n\\t# now process the libraries\\n\\tfor lib in libs:\\n\\t\\tenv.append_unique(\\'DLINKFLAGS\\', lib_st % lib)\\n\\n\\t# add linker flags\\n\\tfor i in uselib:\\n\\t\\tdlinkflags = env[\\'DLINKFLAGS_\\' + i]\\n\\t\\tif dlinkflags:\\n\\t\\t\\tfor linkflag in dlinkflags:\\n\\t\\t\\t\\tenv.append_unique(\\'DLINKFLAGS\\', linkflag)\\n\\n@feature(\\'dshlib\\')\\n@after(\\'apply_d_vars\\')\\ndef add_shlib_d_flags(self):\\n\\tfor linkflag in self.env[\\'D_shlib_LINKFLAGS\\']:\\n\\t\\tself.env.append_unique(\\'DLINKFLAGS\\', linkflag)\\n\\n@extension(EXT_D)\\ndef d_hook(self, node):\\n\\t# create the compilation task: cpp or cc\\n\\ttask = self.create_task(self.generate_headers and \\'d_with_header\\' or \\'d\\')\\n\\ttry: obj_ext = self.obj_ext\\n\\texcept AttributeError: obj_ext = \\'_%d.o\\' % self.idx\\n\\n\\ttask.inputs = [node]\\n\\ttask.outputs = [node.change_ext(obj_ext)]\\n\\tself.compiled_tasks.append(task)\\n\\n\\tif self.generate_headers:\\n\\t\\theader_node = node.change_ext(self.env[\\'DHEADER_ext\\'])\\n\\t\\ttask.outputs += [header_node]\\n\\nd_str = \\'${D_COMPILER} ${DFLAGS} ${_DIMPORTFLAGS} ${D_SRC_F}${SRC} ${D_TGT_F}${TGT}\\'\\nd_with_header_str = \\'${D_COMPILER} ${DFLAGS} ${_DIMPORTFLAGS} \\\\\\n${D_HDR_F}${TGT[1].bldpath(env)} \\\\\\n${D_SRC_F}${SRC} \\\\\\n${D_TGT_F}${TGT[0].bldpath(env)}\\'\\nlink_str = \\'${D_LINKER} ${DLNK_SRC_F}${SRC} ${DLNK_TGT_F}${TGT} ${DLINKFLAGS}\\'\\n\\ndef override_exec(cls):\\n\\t\"\"\"stupid dmd wants -of stuck to the file name\"\"\"\\n\\told_exec = cls.exec_command\\n\\tdef exec_command(self, *k, **kw):\\n\\t\\tif isinstance(k[0], list):\\n\\t\\t\\tlst = k[0]\\n\\t\\t\\tfor i in xrange(len(lst)):\\n\\t\\t\\t\\tif lst[i] == \\'-of\\':\\n\\t\\t\\t\\t\\tdel lst[i]\\n\\t\\t\\t\\t\\tlst[i] = \\'-of\\' + lst[i]\\n\\t\\t\\t\\t\\tbreak\\n\\t\\treturn old_exec(self, *k, **kw)\\n\\tcls.exec_command = exec_command\\n\\ncls = Task.simple_task_type(\\'d\\', d_str, \\'GREEN\\', before=\\'static_link d_link\\', shell=False)\\ncls.scan = scan\\noverride_exec(cls)\\n\\ncls = Task.simple_task_type(\\'d_with_header\\', d_with_header_str, \\'GREEN\\', before=\\'static_link d_link\\', shell=False)\\noverride_exec(cls)\\n\\ncls = Task.simple_task_type(\\'d_link\\', link_str, color=\\'YELLOW\\', shell=False)\\noverride_exec(cls)\\n\\n# for feature request #104\\n@taskgen\\ndef generate_header(self, filename, install_path):\\n\\tif not hasattr(self, \\'header_lst\\'): self.header_lst = []\\n\\tself.meths.append(\\'process_header\\')\\n\\tself.header_lst.append([filename, install_path])\\n\\n@before(\\'apply_core\\')\\ndef process_header(self):\\n\\tenv = self.env\\n\\tfor i in getattr(self, \\'header_lst\\', []):\\n\\t\\tnode = self.path.find_resource(i[0])\\n\\n\\t\\tif not node:\\n\\t\\t\\traise Utils.WafError(\\'file not found on d obj \\'+i[0])\\n\\n\\t\\ttask = self.create_task(\\'d_header\\')\\n\\t\\ttask.set_inputs(node)\\n\\t\\ttask.set_outputs(node.change_ext(\\'.di\\'))\\n\\nd_header_str = \\'${D_COMPILER} ${D_HEADER} ${SRC}\\'\\nTask.simple_task_type(\\'d_header\\', d_header_str, color=\\'BLUE\\', shell=False)\\n\\n@conftest\\ndef d_platform_flags(conf):\\n\\tv = conf.env\\n\\tbinfmt = v.DEST_BINFMT or Utils.unversioned_sys_platform_to_binary_format(\\n\\t\\tv.DEST_OS or Utils.unversioned_sys_platform())\\n\\tif binfmt == \\'pe\\':\\n\\t\\tv[\\'D_program_PATTERN\\']   = \\'%s.exe\\'\\n\\t\\tv[\\'D_shlib_PATTERN\\']     = \\'lib%s.dll\\'\\n\\t\\tv[\\'D_staticlib_PATTERN\\'] = \\'lib%s.a\\'\\n\\telse:\\n\\t\\tv[\\'D_program_PATTERN\\']   = \\'%s\\'\\n\\t\\tv[\\'D_shlib_PATTERN\\']     = \\'lib%s.so\\'\\n\\t\\tv[\\'D_staticlib_PATTERN\\'] = \\'lib%s.a\\'\\n\\n# quick test #\\nif __name__ == \"__main__\":\\n\\t#Logs.verbose = 2\\n\\n\\ttry: arg = sys.argv[1]\\n\\texcept IndexError: arg = \"file.d\"\\n\\n\\tprint(\"\".join(filter_comments(arg)))\\n\\t# TODO\\n\\tpaths = [\\'.\\']\\n\\n\\t#gruik = filter()\\n\\t#gruik.start(arg)\\n\\n\\t#code = \"\".join(gruik.buf)\\n\\n\\t#print \"we have found the following code\"\\n\\t#print code\\n\\n\\t#print \"now parsing\"\\n\\t#print \"-------------------------------------------\"\\n\\t\"\"\"\\n\\tparser_ = d_parser()\\n\\tparser_.start(arg)\\n\\n\\tprint \"module: %s\" % parser_.module\\n\\tprint \"imports: \",\\n\\tfor imp in parser_.imports:\\n\\t\\tprint imp + \" \",\\n\\tprint\\n\"\"\"\\n\\n',\n",
       " '# -*- coding: utf-8 -*-\\n#\\n# Configuration file for the Sphinx documentation builder.\\n#\\n# This file does only contain a selection of the most common options. For a\\n# full list see the documentation:\\n# http://www.sphinx-doc.org/en/stable/config\\n\\n# -- Path setup --------------------------------------------------------------\\n\\n# If extensions (or modules to document with autodoc) are in another directory,\\n# add these directories to sys.path here. If the directory is relative to the\\n# documentation root, use os.path.abspath to make it absolute, like shown here.\\n#\\n# import os\\n# import sys\\n# sys.path.insert(0, os.path.abspath(\\'.\\'))\\n\\n\\n# -- Project information -----------------------------------------------------\\n\\nproject = \\'analyze pairing and clustering of molecular systems\\'\\ncopyright = \"2018, Matthew W. Thompson\"\\nauthor = \\'Matthew W. Thompson\\'\\n\\n# The short X.Y version\\nversion = \\'\\'\\n# The full version, including alpha/beta/rc tags\\nrelease = \\'\\'\\n\\n\\n# -- General configuration ---------------------------------------------------\\n\\n# If your documentation needs a minimal Sphinx version, state it here.\\n#\\n# needs_sphinx = \\'1.0\\'\\n\\n# Add any Sphinx extension module names here, as strings. They can be\\n# extensions coming with Sphinx (named \\'sphinx.ext.*\\') or your custom\\n# ones.\\nextensions = [\\n    \\'sphinx.ext.autodoc\\',\\n    \\'sphinx.ext.mathjax\\',\\n]\\n\\n# Add any paths that contain templates here, relative to this directory.\\ntemplates_path = [\\'_templates\\']\\n\\n# The suffix(es) of source filenames.\\n# You can specify multiple suffix as a list of string:\\n#\\n# source_suffix = [\\'.rst\\', \\'.md\\']\\nsource_suffix = \\'.rst\\'\\n\\n# The master toctree document.\\nmaster_doc = \\'index\\'\\n\\n# The language for content autogenerated by Sphinx. Refer to documentation\\n# for a list of supported languages.\\n#\\n# This is also used if you do content translation via gettext catalogs.\\n# Usually you set \"language\" from the command line for these cases.\\nlanguage = None\\n\\n# List of patterns, relative to source directory, that match files and\\n# directories to ignore when looking for source files.\\n# This pattern also affects html_static_path and html_extra_path .\\nexclude_patterns = [\\'_build\\', \\'Thumbs.db\\', \\'.DS_Store\\']\\n\\n# The name of the Pygments (syntax highlighting) style to use.\\npygments_style = \\'sphinx\\'\\n\\n\\n# -- Options for HTML output -------------------------------------------------\\n\\n# The theme to use for HTML and HTML Help pages.  See the documentation for\\n# a list of builtin themes.\\n#\\nhtml_theme = \\'sphinx_rtd_theme\\'\\n\\n# Theme options are theme-specific and customize the look and feel of a theme\\n# further.  For a list of options available for each theme, see the\\n# documentation.\\n#\\n# html_theme_options = {}\\n\\n# Add any paths that contain custom static files (such as style sheets) here,\\n# relative to this directory. They are copied after the builtin static files,\\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\\nhtml_static_path = [\\'_static\\']\\n\\n# Custom sidebar templates, must be a dictionary that maps document names\\n# to template names.\\n#\\n# The default sidebars (for documents that don\\'t match any pattern) are\\n# defined by theme itself.  Builtin themes are using these templates by\\n# default: ``[\\'localtoc.html\\', \\'relations.html\\', \\'sourcelink.html\\',\\n# \\'searchbox.html\\']``.\\n#\\n# html_sidebars = {}\\n\\n\\n# -- Options for HTMLHelp output ---------------------------------------------\\n\\n# Output file base name for HTML help builder.\\nhtmlhelp_basename = \\'pairingdoc\\'\\n\\n\\n# -- Options for LaTeX output ------------------------------------------------\\n\\nlatex_elements = {\\n    # The paper size (\\'letterpaper\\' or \\'a4paper\\').\\n    #\\n    # \\'papersize\\': \\'letterpaper\\',\\n\\n    # The font size (\\'10pt\\', \\'11pt\\' or \\'12pt\\').\\n    #\\n    # \\'pointsize\\': \\'10pt\\',\\n\\n    # Additional stuff for the LaTeX preamble.\\n    #\\n    # \\'preamble\\': \\'\\',\\n\\n    # Latex figure (float) alignment\\n    #\\n    # \\'figure_align\\': \\'htbp\\',\\n}\\n\\n# Grouping the document tree into LaTeX files. List of tuples\\n# (source start file, target name, title,\\n#  author, documentclass [howto, manual, or own class]).\\nlatex_documents = [\\n    (master_doc, \\'pairing.tex\\', \\'pairing Documentation\\',\\n     \\'pairing\\', \\'manual\\'),\\n]\\n\\n\\n# -- Options for manual page output ------------------------------------------\\n\\n# One entry per manual page. List of tuples\\n# (source start file, name, description, authors, manual section).\\nman_pages = [\\n    (master_doc, \\'pairing\\', \\'pairing Documentation\\',\\n     [author], 1)\\n]\\n\\n\\n# -- Options for Texinfo output ----------------------------------------------\\n\\n# Grouping the document tree into Texinfo files. List of tuples\\n# (source start file, target name, title, author,\\n#  dir menu entry, description, category)\\ntexinfo_documents = [\\n    (master_doc, \\'pairing\\', \\'pairing Documentation\\',\\n     author, \\'pairing\\', \\'analyze pairing and clustering of molecular systems\\',\\n     \\'Miscellaneous\\'),\\n]\\n\\n\\n# -- Extension configuration -------------------------------------------------\\n',\n",
       " 'v = \\'2.1\\'\\n\\ntry:\\n    import UEManifestReader\\n    import coloredlogs\\n    import aioconsole\\n    import webbrowser\\n    import subprocess\\n    import crayons\\n    import logging\\n    import asyncio\\n    import psutil\\n    import json\\n    import time\\n    import sys\\n    import os\\nexcept:\\n    print(\\'It seems that some modules are missing. Run \"INSTALL.bat\" and try again.\\')\\n    input(\\'Press ENTER to exit\\')\\n\\nfrom os import kill\\nfrom modules import http\\n\\nlog = logging.getLogger(\\'FortniteLauncher\\')\\n\\nconfiguration = json.load(open(\\'config.json\\', \\'r\\', encoding = \\'utf-8\\'))\\nauths = json.load(open(\\'auths.json\\', \\'r\\', encoding = \\'utf-8\\'))\\n\\ndef get_colored_box(color, text):\\n\\n    return f\\'{color(\"[\")}{text}{color(\"]\")}\\'\\n\\nasync def get_other_clients():\\n\\n    log.debug(\\'Looking for other running clients...\\')\\n\\n    clients = []\\n\\n    for p in psutil.process_iter([\\'name\\', \\'pid\\']):\\n        if p.info[\\'name\\'] == \\'FortniteClient-Win64-Shipping.exe\\':\\n            clients.append(p.info[\\'pid\\'])\\n\\n    log.debug(f\\'Found {len(clients)} clients.\\')\\n\\n    return clients\\n\\nasync def wait_for_game_spawn(process: psutil.Process, ignore: list):\\n\\n    log.debug(f\\'Waiting for game to spawn...\\')\\n\\n    while True:\\n        if process.is_running() == False:\\n            return False\\n        for p in psutil.process_iter([\\'name\\', \\'pid\\']):\\n            if p.info[\\'name\\'] == \\'FortniteClient-Win64-Shipping.exe\\':\\n                if p.info[\\'pid\\'] in ignore:\\n                    continue\\n                return True\\n\\n\\nasync def add_account():\\n\\n    log.debug(\\'add_account flow started.\\')\\n\\n    print()\\n    print(crayons.green(\\'Add Account\\', bold=True))\\n\\n    auth_type = configuration[\\'auth_type\\']\\n\\n    LAUNCHER_AUTHORIZATION_URL = \\'https://www.epicgames.com/id/api/redirect?clientId=34a02cf8f4414e29b15921876da36f9a&responseType=code\\'\\n    LAUNCHER_AUTHORIZATION_URL_LOGIN = \\'https://www.epicgames.com/id/logout?redirectUrl=https%3A%2F%2Fwww.epicgames.com%2Fid%2Flogin%3FredirectUrl%3Dhttps%253A%252F%252Fwww.epicgames.com%252Fid%252Fapi%252Fredirect%253FclientId%253D34a02cf8f4414e29b15921876da36f9a%2526responseType%253Dcode\\'\\n\\n    IOS_AUTHORIZATION_URL = \\'https://www.epicgames.com/id/api/redirect?clientId=3446cd72694c4a4485d81b77adbb2141&responseType=code\\'\\n    IOS_AUTHORIZATION_URL_LOGIN = \\'https://www.epicgames.com/id/logout?redirectUrl=https%3A%2F%2Fwww.epicgames.com%2Fid%2Flogin%3FredirectUrl%3Dhttps%253A%252F%252Fwww.epicgames.com%252Fid%252Fapi%252Fredirect%253FclientId%253D3446cd72694c4a4485d81b77adbb2141%2526responseType%253Dcode\\'\\n\\n    while True:\\n        user_selection = await aioconsole.ainput(f\\'Are you logged in to the required account in your web browser?\\\\nType {crayons.white(\"1\", bold=True)} if yes.\\\\nType {crayons.white(\"2\", bold=True)} if no.\\\\n\\')\\n\\n        user_logged = user_selection.strip(\\' \\')\\n\\n        if user_logged == \\'1\\':\\n\\n            if auth_type == \\'refresh\\':\\n\\n                choosen_url = LAUNCHER_AUTHORIZATION_URL\\n            else:\\n                choosen_url = IOS_AUTHORIZATION_URL\\n\\n        elif user_logged == \\'2\\':\\n\\n            if auth_type == \\'refresh\\':\\n                choosen_url = LAUNCHER_AUTHORIZATION_URL_LOGIN\\n            else:\\n                choosen_url = IOS_AUTHORIZATION_URL_LOGIN\\n\\n        else:\\n\\n            print(\\'Select a valid option! Try again\\\\n\\')\\n\\n            continue\\n        break\\n\\n    webbrowser.open_new_tab(choosen_url)\\n\\n    print(choosen_url)\\n    if user_logged == \\'1\\':\\n        print(\\'An epic games page should be opened in your web brower. Paste the authorizationCode here:\\')\\n    else:\\n        print(\\'An epic games page should be opened in your web brower. Login on the required account and then paste the authorizationCode here:\\')\\n    \\n    user_code = await aioconsole.ainput(\\'> \\')\\n\\n    code = user_code.strip(\\' \\')\\n\\n    if code in [\\'cancel\\', \\'c\\']:\\n        log.debug(\\'add_account flow stopped. User cancelled\\')\\n        print(\\'Account add cancelled\\')\\n        return False\\n\\n    if len(code) != 32:\\n        log.debug(\\'add_account flow stopped. The code from the user was invalid.\\')\\n        print(f\\'Failed account add. The code\\\\\\'s lenght is invalid. A valid authorization code is 32 characters long.\\')\\n        return False\\n\\n    Auth = http.EpicAPI()\\n\\n    if auth_type == \\'refresh\\':\\n\\n        auth_request = await Auth.authorization_code_auth(code)\\n\\n        if \\'errorCode\\' not in auth_request.text:\\n\\n            oauth_json = auth_request.json()\\n\\n            credentials = {}\\n\\n            credentials[\\'auth_type\\'] = \\'refresh\\'\\n            credentials[\\'refresh_token\\'] = str(oauth_json[\\'refresh_token\\'])\\n            credentials[\\'refresh_expires\\'] = int(time.time()) + oauth_json[\\'refresh_expires\\']\\n\\n            auths[oauth_json[\\'displayName\\']] = credentials\\n\\n            with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n            log.debug(\\'add_account flow completed without errors.\\')\\n\\n            return f\\'Account \"{oauth_json[\"displayName\"]}\" added successfully! (Note: this login will expire after 23 days of inactivity)\\'\\n\\n        else:\\n            print(f\\'Authentication failed. {auth_request.json()[\"errorMessage\"]}\\')\\n            log.debug(\\'add_account flow stopped. The authentication failed.\\')\\n            return False\\n    \\n    elif auth_type == \\'device\\':\\n\\n        auth_request = await Auth.authorization_code_auth(code, client = http.Clients.fortniteIOSGameClient)\\n\\n        if \\'errorCode\\' not in auth_request.text:\\n\\n            oauth_json = auth_request.json()\\n\\n            device_create = await Auth.create_device_auth(oauth_json[\\'access_token\\'], oauth_json[\\'account_id\\'])\\n\\n            if \\'errorCode\\' not in device_create.text:\\n\\n                device_json = device_create.json()\\n\\n                credentials = {}\\n                \\n                credentials[\\'auth_type\\'] = \\'device\\'\\n                credentials[\\'account_id\\'] = device_json[\\'accountId\\']\\n                credentials[\\'device_id\\'] = device_json[\\'deviceId\\']\\n                credentials[\\'secret\\'] = device_json[\\'secret\\']\\n\\n\\n                auths[oauth_json[\\'displayName\\']] = credentials\\n\\n                with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                    json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n                await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n\\n                return f\\'Account \"{oauth_json[\"displayName\"]}\" added successfully!\\'\\n\\n            else:\\n                print(f\\'Device auth creation failed. {auth_request.json()[\"errorMessage\"]}\\')\\n                log.debug(\\'add_account flow stopped. The authentication failed.\\')\\n                return False\\n\\n        else:\\n            print(f\\'Authentication failed. {auth_request.json()[\"errorMessage\"]}\\')\\n            log.debug(\\'add_account flow stopped. The authentication failed.\\')\\n            return False\\n\\n\\nasync def remove_account():\\n\\n    log.debug(\\'remove_account flow started.\\')\\n\\n    print()\\n    print(crayons.red(\\'Remove Account\\', bold=True))\\n\\n    while True:\\n\\n        account_list = list(auths.keys())\\n        countlist = []\\n        count = 0\\n\\n        for account in account_list:\\n            count += 1\\n            countlist.append(count)\\n            print(f\\'{get_colored_box(crayons.red, str(count))} {account}\\')\\n\\n        print(f\\'{get_colored_box(crayons.green, \"C\")} Cancel\\\\n\\')\\n\\n\\n        user_selection = await aioconsole.ainput(f\\'Select an account: \\')\\n\\n        try:\\n            user_selection.strip(\\' \\')\\n\\n            if user_selection.lower() in [\\'c\\', \\'cancel\\']:\\n                print(crayons.red(\\'Account remove cancelled.\\'))\\n                log.debug(\\'remove_account flow cancelled by user.\\')\\n                return False\\n\\n            if int(user_selection) not in countlist:\\n                print(crayons.red(\\'Invalid selection\\\\n\\'))\\n                continue\\n\\n            else:\\n                break\\n        except:\\n            print(crayons.red(\\'Select a valid option\\\\n\\'))\\n            continue\\n\\n    credentials = auths[account_list[int(user_selection) - 1]]\\n\\n    if credentials[\\'auth_type\\'] == \\'refresh\\':\\n\\n        if int(time.time()) > credentials[\\'refresh_expires\\']:\\n\\n            del auths[account_list[int(user_selection) - 1]]\\n\\n            with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n            log.debug(\\'remove_account flow completed. The saved refresh wasn\\\\\\'t valid and removed from auths.json file\\')\\n            print(\\'Account removed successfully.\\')\\n            return True\\n        \\n        else:\\n\\n            Auth = http.EpicAPI()\\n            auth_request = await Auth.refresh_token_auth(refresh_token = credentials[\\'refresh_token\\'])\\n\\n            if \\'errorCode\\' not in auth_request.text:\\n\\n                oauth_json = auth_request.json()\\n                \\n                kill_request = await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n\\n                if kill_request not in [401, 403]:\\n\\n                    del auths[account_list[int(user_selection) - 1]]\\n\\n                    with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                        json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n                    log.debug(\\'remove_account flow completed without errors\\')\\n                    print(\\'Account removed successfully.\\')\\n                    return True\\n\\n            else:\\n\\n                print(f\\'Authentication failed. {auth_request.json()[\"errorMessage\"]}\\')\\n                print(\\'Removing account from auths.json file anyway.\\')\\n\\n                del auths[account_list[int(user_selection) - 1]]\\n\\n                with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                    json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n                log.debug(\\'remove_account flow failed successfully. Authentication failed but removed from auths.json anyways\\')\\n\\n                print(\\'Account removed.\\') # task failed successfully\\n                return True\\n\\n    elif credentials[\\'auth_type\\'] == \\'device\\':\\n\\n        Auth = http.EpicAPI()\\n\\n        auth_request = await Auth.device_auths_auth(credentials)\\n\\n        if \\'errorCode\\' not in auth_request.text:\\n\\n            oauth_json = auth_request.json()\\n\\n            kill_device = await Auth.delete_device_auth(oauth_json[\\'access_token\\'], account_id=credentials[\\'account_id\\'], device_id=credentials[\\'device_id\\'])\\n\\n            if kill_device.status_code not in [401, 403]:\\n\\n                del auths[account_list[int(user_selection) - 1]]\\n\\n                with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                    json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n                await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n\\n                log.debug(\\'remove_account flow completed without errors\\')\\n                print(\\'Account removed successfully.\\')\\n                return True\\n\\n            else:\\n\\n                print(f\\'Device auth delete failed. {kill_device.json()[\"errorMessage\"]}\\')\\n                print(\\'Removing account from auths.json anyway. Change the account password to make sure you kill the device auth.\\')\\n\\n                del auths[account_list[int(user_selection) - 1]]\\n\\n                with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                    json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n                log.debug(\\'remove_account flow failed successfully. Device delete failed but removed from auths.json anyways\\')\\n\\n                await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n\\n                print(\\'Account removed.\\') # task failed successfully\\n                return True\\n\\n        else:\\n\\n                print(f\\'Authentication failed. {auth_request.json()[\"errorMessage\"]}\\')\\n                print(\\'Removing account from auths.json anyway.\\')\\n\\n                del auths[account_list[int(user_selection) - 1]]\\n\\n                with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                    json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n                log.debug(\\'remove_account flow failed successfully. Authentication failed but removed from auths.json anyways\\')\\n\\n                print(\\'Account removed.\\') # task failed successfully\\n                return True\\n\\n    \\nasync def launch_game(exchange_code: str, launch_command: str):\\n\\n    log.debug(\\'Launching game...\\')\\n\\n    fortnite_path = configuration[\\'fortnite_path\\']\\n    executable_args = launch_command\\n    additional_args = configuration[\"commandline_arguments\"]\\n\\n    log.debug(\\'Preparing command line arguments.\\')\\n\\n    args = [\\n        executable_args,\\n        \\'-AUTH_LOGIN=unused\\',\\n        f\\'-AUTH_PASSWORD={exchange_code}\\',\\n        \\'-AUTH_TYPE=exchangecode\\',\\n        \\'-epicapp=Fortnite\\',\\n        \\'-epicenv=Prod\\',\\n        \\'-EpicPortal\\',\\n    ]\\n\\n    for i in additional_args:\\n        if i.startswith(\\'-\\'):\\n            args.append(i)\\n\\n    ignore_list = await get_other_clients()\\n\\n    log.debug(f\\'Starting FortniteLauncher.exe with args {args}...\\')\\n\\n    FortniteLauncher = subprocess.Popen([f\\'{fortnite_path}/FortniteGame/Binaries/Win64/FortniteLauncher.exe\\'] + args, cwd=f\\'{fortnite_path}/FortniteGame/Binaries/Win64/\\', stdout=subprocess.DEVNULL)\\n    process = psutil.Process(pid = FortniteLauncher.pid)\\n\\n    wait_spawn = await wait_for_game_spawn(process, ignore_list)\\n\\n    if wait_spawn == True:\\n\\n        log.debug(\\'Game launched correctly.\\')\\n        return True\\n\\n    else:\\n\\n        log.debug(\\'Game did\\\\\\'nt launch.\\')\\n        return False\\n\\n\\nasync def start():\\n\\n    if \\'--debug\\' in sys.argv:\\n        coloredlogs.install(\\n            level=\\'DEBUG\\'\\n        )\\n\\n    while True:\\n\\n        print()\\n\\n        print(f\\'\\\\n{crayons.cyan(\"Fortnite Launcher\", bold=True)} | {crayons.white(f\"Beta v{v}\", bold=True)}\\\\n\\')\\n\\n        try:\\n            configuration = json.load(open(\\'config.json\\', \\'r\\', encoding = \\'utf-8\\'))\\n\\n            if configuration[\\'auth_type\\'] not in [\\'refresh\\', \\'device\\']:\\n                print(\\'Error, the choosen auth type in configuration file isn\\\\\\'t valid. Auth type must be \"refresh\" or \"device\".\\')\\n                await aioconsole.ainput(\\'Press ENTER to exit\\')\\n                exit()\\n\\n        except Exception as e:\\n            print(f\\'An error ocurred loading config.json file. {e}\\')\\n            await aioconsole.ainput(\\'Press ENTER to exit\\')\\n            exit()\\n\\n        try:\\n            auths = json.load(open(\\'auths.json\\', \\'r\\', encoding = \\'utf-8\\'))\\n        except Exception as e:\\n            print(f\\'An error ocurred loading auths.json file. {e}\\')\\n            await aioconsole.ainput(\\'Press ENTER to exit\\')\\n            exit()\\n\\n        account_list = list(auths.keys())\\n        countlist = []\\n        count = 0\\n\\n        for account in account_list:\\n            count += 1\\n            countlist.append(count)\\n            print(f\\'{get_colored_box(crayons.green, str(count))} {account}\\')\\n\\n        print(f\\'\\\\n{get_colored_box(crayons.blue, \"A\")} Add an account\\')\\n        print(f\\'{get_colored_box(crayons.blue, \"R\")} Remove an account\\\\n\\')\\n        print(f\\'{get_colored_box(crayons.red, \"X\")} Exit\\\\n\\')\\n\\n        user_selection = await aioconsole.ainput(f\\'Select an option: \\')\\n\\n        try:\\n            user_selection.strip(\\' \\')\\n\\n            if user_selection.lower() == \\'x\\':\\n                exit()\\n\\n            if user_selection.lower() == \\'a\\':\\n                add = await add_account()\\n                if isinstance(add, str):\\n                    print(add)\\n                continue\\n\\n            if user_selection.lower() == \\'r\\':\\n                if len(account_list) == 0:\\n                    print(\\'There is no accounts to remove!\\\\n\\')\\n                    continue\\n                \\n                else:\\n                    await remove_account()\\n                    continue\\n\\n            if int(user_selection) not in countlist:\\n                print(crayons.red(\\'Invalid selection\\\\n\\'))\\n                continue\\n\\n        except:\\n            print(crayons.red(\\'Select a valid option\\\\n\\'))\\n            continue\\n\\n        selected_account = int(user_selection) - 1\\n\\n        game_folder = configuration[\\'fortnite_path\\']\\n\\n        if os.path.isdir(game_folder) == False:\\n            print(\\'Seems like the fortnite path in configuration is not valid. Check it and try again\\')\\n            await aioconsole.ainput(\\'Press ENTER to exit\\')\\n\\n        else:\\n\\n            credentials = auths[account_list[selected_account]]\\n\\n            auth_type = credentials[\\'auth_type\\']\\n\\n            if auth_type == \\'refresh\\':\\n\\n                if int(time.time()) > credentials[\\'refresh_expires\\']:\\n                    print(\\'The credentials of this account have expired. Re-add the account and try again\\')\\n\\n                Auth = http.EpicAPI()\\n                auth_request = await Auth.refresh_token_auth(refresh_token = credentials[\\'refresh_token\\'])\\n\\n                if \\'errorCode\\' not in auth_request.text:\\n\\n                    oauth_json = auth_request.json()\\n\\n                    credentials[\\'refresh_token\\'] = str(oauth_json[\\'refresh_token\\'])\\n                    credentials[\\'refresh_expires\\'] = int(time.time()) + oauth_json[\\'refresh_expires\\']\\n\\n                    auths[account_list[selected_account]] = credentials\\n\\n                    with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                        json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n                    exchange_request = await Auth.get_exchange_code(oauth_json[\\'access_token\\'])\\n\\n                    if \\'errorCode\\' not in exchange_request.text:\\n\\n                        exchange_json = exchange_request.json()\\n                        launch_command = \\'\\'\\n\\n                        launch_info = await Auth.get_launch_info()\\n                        if launch_info.status_code == 200:\\n                            log.debug(\\'Using baydev api launch args.\\')\\n                            launch_command = launch_info.json()[\\'data\\'][\\'launch_args\\']\\n                            log.debug(f\\'Launch args for build {launch_info.json()[\"data\"][\"build\"]}\\')\\n\\n                        else:\\n                            log.debug(\\'Using epicgames manifest launch args.\\')\\n                            Reader = UEManifestReader.UEManifestReader()\\n                            manifest = await Reader.download_manifest()\\n                            launch_command = manifest[\\'LaunchCommand\\']\\n\\n                        print(\\'Launching...\\')\\n\\n                        launch_try = await launch_game(exchange_json[\\'code\\'], launch_command)\\n\\n                        if launch_try == False:\\n                            print(\\'Failed game launch.\\')\\n                            await asyncio.sleep(2)\\n                            continue\\n\\n                        else:\\n\\n                            print(\\'Launched.\\')\\n                            await asyncio.sleep(3)\\n                            exit()\\n\\n                    else:\\n                        print(f\\'Exchange code request failed. {exchange_request.json()[\"errorMessage\"]}\\')\\n                        continue\\n\\n                else:\\n                    print(f\\'Authentication failed. {auth_request.json()[\"errorMessage\"]}\\')\\n                    continue\\n\\n            else:\\n\\n                Auth = http.EpicAPI()\\n                auth_request = await Auth.device_auths_auth(credentials)\\n\\n                if \\'errorCode\\' not in auth_request.text:\\n\\n                    oauth_json = auth_request.json()\\n\\n                    exchange_request = await Auth.get_exchange_code(oauth_json[\\'access_token\\'])\\n\\n                    if \\'errorCode\\' not in exchange_request.text:\\n\\n                        exchange_auth = exchange_request.json()\\n\\n                        launcher_auth_request = await Auth.exchange_code_auth(exchange_auth[\\'code\\'])\\n\\n                        if \\'errorCode\\' not in launcher_auth_request.text:\\n\\n                            launcher_auth = launcher_auth_request.json()\\n\\n                            launcher_exchange_request = await Auth.get_exchange_code(launcher_auth[\\'access_token\\'])\\n\\n                            if \\'errorCode\\' not in launcher_exchange_request.text:\\n\\n                                final_exchange_json = launcher_exchange_request.json()\\n\\n                                launch_command = \\'\\'\\n\\n                                launch_info = await Auth.get_launch_info()\\n                                if launch_info.status_code == 200:\\n                                    log.debug(\\'Using baydev api launch args.\\')\\n                                    launch_command = launch_info.json()[\\'data\\'][\\'launch_args\\']\\n\\n                                else:\\n                                    print(f\\'Baydev api returned status {launch_info.status_code}. Downloading and parsing manifest manually. (This may take a while)\\')\\n                                    log.debug(\\'Using epicgames manifest launch args. (This may take a while)\\')\\n                                    Reader = UEManifestReader.UEManifestReader()\\n                                    manifest = await Reader.download_manifest()\\n                                    launch_command = manifest[\\'LaunchCommand\\']\\n\\n                                print(\\'Launching...\\')\\n\\n                                launch_try = await launch_game(final_exchange_json[\\'code\\'], launch_command)\\n\\n                                if launch_try == False:\\n                                    print(\\'Failed game launch.\\')\\n                                    await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n                                    await Auth.kill_oauth_session(launcher_auth[\\'access_token\\'])\\n                                    await asyncio.sleep(2)\\n                                    continue\\n\\n                                else:\\n\\n                                    print(\\'Launched.\\')\\n                                    await asyncio.sleep(3)\\n                                    await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n                                    await Auth.kill_oauth_session(launcher_auth[\\'access_token\\'])\\n                                    exit()\\n                            \\n                            else:\\n                                print(f\\'Launcher exchange code generate failed. {launcher_exchange_request.json()[\"errorMessage\"]}\\')\\n                                await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n                                await Auth.kill_oauth_session(launcher_auth[\\'access_token\\'])\\n                                continue\\n\\n                        else:\\n                            print(f\\'Launcher exchange auth failed. {launcher_auth_request.json()[\"errorMessage\"]}\\')\\n                            await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n                            continue\\n\\n                    else:\\n                        print(f\\'Exchange code request failed. {exchange_request.json()[\"errorMessage\"]}\\')\\n                        await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n                        continue\\n                \\n                else:\\n                    print(f\\'Authentication failed. {auth_request.json()[\"errorMessage\"]}\\')\\n                    continue\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n\\n    loop = asyncio.get_event_loop()\\n    loop.run_until_complete(start())',\n",
       " 'import io\\nimport numpy as np\\nimport json\\nimport requests\\nimport h5py\\n\\n\\nseq_len = 100\\nfile_name = \"output_video.mp4\"\\ndata_file = \"flame_params.hdf5\"\\n\\n\\ndef byteify(x):\\n    memfile = io.BytesIO()\\n    np.save(memfile, x)\\n    memfile.seek(0)\\n    return memfile.read().decode(\"latin-1\")\\n\\n\\ndef get_face(x, seq_len):\\n    return {\\n        \"expression\": byteify(x[\"tf_exp\"][:seq_len]),\\n        \"pose\": byteify(x[\"tf_pose\"][:seq_len]),\\n        \"shape\": byteify(x[\"tf_shape\"][:seq_len]),\\n        \"rotation\": byteify(x[\"tf_rot\"][:seq_len]),\\n    }\\n\\n\\nwith h5py.File(data_file, \"r\") as f:\\n    p1 = f[\"sessions/1/participants/P1\"]\\n    p2 = f[\"sessions/1/participants/P2\"]\\n\\n    serialized = json.dumps(\\n        {\\n            \"seqs\": [get_face(p1, seq_len), get_face(p2, seq_len)],\\n            \"file_name\": file_name,\\n            \"fps\": 25,\\n        }\\n    )\\ntry:\\n    resp = requests.post(\"http://localhost:8000/render\", data=serialized, timeout=600)\\n    resp.raise_for_status()\\n    print(resp.json())\\nexcept requests.exceptions.HTTPError:\\n    print(\"render request: failed on the server..\")\\nexcept requests.exceptions.Timeout:\\n    print(\"render request: timed out\")\\nexcept requests.exceptions.ConnectionError:\\n    print(\"render request: connection error\")\\n',\n",
       " \"# Generated by Django 3.2.2 on 2021-05-18 15:01\\n\\nfrom django.db import migrations, models\\nimport django.db.models.deletion\\n\\n\\nclass Migration(migrations.Migration):\\n\\n    dependencies = [\\n        ('usd_rest_api', '0006_alter_lesson_group'),\\n    ]\\n\\n    operations = [\\n        migrations.AddField(\\n            model_name='event',\\n            name='account',\\n            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, to='usd_rest_api.account'),\\n        ),\\n    ]\\n\",\n",
       " '#!/usr/bin/env python\\n\\n\"\"\"\\nA basic tool for working with the Cards corpus transcripts.\\n\"\"\"\\n\\n__author__ = \"Christopher Potts\"\\n__date__ = \"2012-03-03\"\\n__credits__ = \"Thanks to Karl Schultz for designing the data collecting program, and \\\\\\n               to David Clausen, Alex Djalali, Sven Lauer, Victoria Schwanda, Chriz Czyzewicz, \\\\\\n               and the rest of the SUBTLE team for their help with data collection. \\\\\\n               This research was supported in part by ONR grant No. N00014-10-1-0109 and \\\\\\n               ARO grant No. W911NF-07-1-0216.\"\\n__license__ = \"Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License: \\\\\\n               http://creativecommons.org/licenses/by-nc-sa/3.0/\"\\n__version__ = \"2.0\"\\n__maintainer__ = \"Christopher Potts\"\\n__email__ = \"See the author\\'s website\"\\n\\n######################################################################\\n\\nimport re\\nimport os\\nimport sys\\nimport csv\\nimport codecs\\nimport datetime\\n#import pytz\\nfrom glob import iglob\\n\\n######################################################################\\n###### ACTIONS CAPTURED BY PRAGBOT TRANSCRIPTS\\n#\\n# These variables are useful to have named, and this also serves as a\\n# reference for the complete list of meta-data values.\\n\\n# Official names for the two players, as written in the transcripts.\\nPLAYER1 = \"Player 1\"\\nPLAYER2 = \"Player 2\"\\n\\n# META-DATA ABOUT THE GAME\\nORIGINAL_FILENAME  = r\"ORIGINAL_FILENAME\"   # Filename as created by the program; included for book-keeping purposes.\\nCOLLECTION_SITE    = r\"COLLECTION_SITE\"     # \\'Amazon Mechanical Turk\\' or \\'Penn\\'\\nTASK_COMPLETED     = r\"TASK_COMPLETED\"      # Completion time - also encode in ORIGINAL_FILENAME\\nPLAYER             = r\"PLAYER_[12]\"         # Player Id in the format A[0-9]{5}, else \\'UNKNOWN\\'\\nP1_MAX_LINEOFSIGHT = r\"P1_MAX_LINEOFSIGHT\"  # Distance the players could \\'see\\'\\nP2_MAX_LINEOFSIGHT = r\"P2_MAX_LINEOFSIGHT\"\\nENVIRONMENT        = r\"CREATE_ENVIRONMENT\"  # ASCII representation of the world\\nP1_MAX_TURNS       = r\"P1_MAX_TURNS\"        # Player\\'s maximum allowed turns\\nP2_MAX_TURNS       = r\"P2_MAX_TURNS\"\\nP1_MAX_CARDS       = r\"P1_MAX_CARDS\"        # Number of cards the player could hold at any given time\\nP2_MAX_CARDS       = r\"P2_MAX_CARDS\"\\nPLAYER_2_TASK_ID   = r\"PLAYER_2_TASK_ID\"\\nPLAYER_1_TASK_ID   = r\"PLAYER_1_TASK_ID\"\\nGOAL_DESCRIPTION   = r\"GOAL_DESCRIPTION\"    # High-level goal (always the same in the v1 release)\\n# ACTIONS\\n## utterances\\nUTTERANCE          = r\"CHAT_MESSAGE_PREFIX\"\\n## locations\\nINITIAL_LOCATION   = r\"PLAYER_INITIAL_LOCATION\"\\nMOVE               = r\"PLAYER_MOVE\"\\n## card movements\\nPICKUP             = r\"PLAYER_PICKUP_CARD\"\\nDROP               = r\"PLAYER_DROP_CARD\"\\n# finish\\nTASK_COMPLETE      = r\"TASK_COMPLETE_CLICKED\"\\nCLOSE_SOCKETS      = r\"CLOSE_SOCKETS\"\\n\\n######################################################################\\n\\nclass Corpus:\\n    \"\"\"\\n    Corpus instances are built from the directory name of the\\n    corpus. Thus, if your program is in the same directory as the root\\n    of the corpus, you can use\\n\\n    corpus = Corpus(\\'transcripts\\')\\n\\n    to build a corpus object.  Relative and full absolute paths also\\n    work.\\n\\n    Corpus objects exist mainly as iterators. The methods\\n    iter_transcripts() and iter_events() allow you to move through the\\n    entire corpus efficiently.\\n    \"\"\"\\n        \\n    def __init__(self, dirname):\\n        \"\"\"\\n        Argument:\\n        dirname -- the root of the corpus transcripts\\n        \"\"\"\\n        self.dirname = dirname\\n\\n    def iter_transcripts(self, display_progress=True):\\n        \"\"\"\\n        Iterate through the transcripts, by yielding Transcript objects one-by-one.\\n\\n        Keyword argument:\\n        display_progress -- should create an overwriting progress bar to stderr if set to True (default: True)        \\n        \"\"\"\\n        trans_no = 1\\n        for filename in iglob(os.path.join(self.dirname, \\'*/*.csv\\')):\\n            if display_progress:\\n                sys.stderr.write(\\'\\\\r\\') ; sys.stderr.write(\\'transcript %s\\' % trans_no) ; sys.stderr.flush() ; trans_no += 1\\n            yield Transcript(filename)\\n        if display_progress:\\n            sys.stderr.write(\\'\\\\n\\')\\n\\n    def iter_events(self, display_progress=True):\\n        \"\"\"\\n        Iterate through the events, by yielding Event objects\\n        one-by-one.  This is useful if you don\\'t need to rely on the\\n        transcripts as a unit --- say, because you\\'re just counting\\n        words for the whole corpus.\\n\\n        Keyword argument:\\n        display_progress -- should create an overwriting progress bar to stderr if set to True (default: True)        \\n        \"\"\"\\n        for trans in self.iter_transcripts(display_progress=display_progress):\\n            for event in trans.iter_events():\\n                yield event\\n\\n######################################################################\\n\\nclass Transcript:\\n    \"\"\"\\n    Transcript objects correspond to individual files.\\n    You can build a Transcript object directly with\\n\\n    trans = Transcript(filename)\\n\\n    where filename is the absolute or relative path to the file you\\n    want to study.\\n    \"\"\"\\n    \\n    def __init__(self, filename):\\n        \"\"\"\\n        Argument:\\n        filename -- the source filename\\n\\n        At intialization, the code turns the filename contents into a\\n        CSV reader and then turns each row into an Event instance. The\\n        attribute self.events is an ordered list of those Event\\n        instances.\\n        \"\"\"\\n        self.filename = filename\\n        csvreader = csv.reader(codecs.open(self.filename, \\'r\\', \\'utf8\\'))\\n        self.events = []\\n        for row in csvreader:                \\n            self.events.append(Event(row))\\n\\n    def iter_events(self):\\n        \"\"\"\\n        Iterate through self.events.\\n        \"\"\"\\n        for event in iter(self.events):\\n            yield event\\n\\n######################################################################\\n\\nclass Event:\\n    \"\"\"\\n    Events are the basic unit of the corpus. Event objects are not\\n    really designed to be instantiated directly, but rather only as\\n    part of a Trancript or Corpus instance. However, you can build\\n    then directly from a 4-membered list of strings. Here, I\\'ve\\n    copied and pasted a row from one of the CSV files and turned\\n    it into a list:\\n\\n    event = Event([\\'Player 1\\',\\'4555\\',\\'PLAYER_INITIAL_LOCATION\\',\\'16,25\\']\\n    \"\"\"\\n    def __init__(self, row):\\n        \"\"\"\\n        Argument:\\n        row -- a row of a csvreader (or a list)\\n\\n        The attributes created:\\n\\n        self.agent (str): Server, Player 1, or Player 2\\n\\n        self.time (int): an integer representing the time of the\\n                         event relative to the start of the game\\n\\n        self.action (str): a string capturing the type of action; see the\\n                           top of this file for details\\n\\n        self.contents (str): the actual contents of the event; the structure\\n                             depends on self.action; see self.parse_contents()\\n                             for details\\n        \"\"\"\\n        self.agent, time, self.action, self.contents = row\\n        self.time = int(time)\\n\\n    def parse_contents(self):\\n        \"\"\"\\n        This method seeks to do something useful with the contents of\\n        the event. Summary:\\n\\n        -- utterances: return the list of strings as tokenized by Tokenizer().tokenize() [see below]\\n        -- pickup, drop: return a triple (x-coord, y-coord, card), where the coordinates are the location of the action\\n        -- move, initial location: return (x-coord, y-coord) of the resulting position\\n        -- task completed: return parsed date\\n        -- max cards, max turns, max line-of-sight: return int() of the value\\n        -- all else: return self.contents\\n        \"\"\"\\n        # Utterances are tokenized using a basic card-aware tokenizer:\\n        if self.action == UTTERANCE:\\n            return Tokenizer().tokenize(self.contents)\\n        # Card manipulations return a trip (x-coord, y-coord, card)\\n        elif self.action in (PICKUP, DROP):\\n            loc, card = self.contents.split(\":\")\\n            lr, tb = loc.split(\",\")\\n            return (int(lr), int(tb), card)\\n        # Locations: pairs (x-coord, y-coord)\\n        elif self.action in (MOVE, INITIAL_LOCATION):\\n            lr, tb = self.contents.split(\",\")\\n            return (int(lr), int(tb))\\n        # Completion times are parsed as dates:\\n        elif self.action == TASK_COMPLETED:\\n            time_format = \"%Y-%m-%d %H:%M:%S\"\\n            dt = datetime.datetime.strptime(self.contents.replace(\\' EDT\\', \\'\\'), time_format)\\n            ##### Uncomment if localization is important:\\n            # eastern = pytz.timezone(\\'US/Eastern\\')\\n            # dt = eastern.localize(dt)\\n            return dt\\n        # These values are parsed as integers:\\n        elif self.action in (P1_MAX_CARDS, P2_MAX_CARDS, P1_MAX_TURNS, P2_MAX_TURNS, P1_MAX_LINEOFSIGHT, P2_MAX_LINEOFSIGHT):\\n            return int(self.contents)\\n        # All other values are returned as strings:\\n        else:\\n            return self.contents\\n  \\n    def __repr__(self):\\n        \"\"\"Computable representation of the object.\"\"\"\\n        return \\'[%s]\\' % \\',\\'.join(map((lambda x : \\'\"%s\"\\' % x), (self.agent, self.time, self.action, self.contents)))        \\n\\n######################################################################    \\n\\nclass Tokenizer:\\n    \"\"\"\\n    This is a very basic tokenizer that seeks to keep intact emoticons\\n    and card references in a basic way. The class-level variables are\\n    put into a regular expression word_re (order matters) and then the\\n    input string is parsed with token_re.findall(). The output list\\n    treats things like \\'4 H\\', \\'four of hearts\\', and >:( as single\\n    tokens. All characters are retained except whitespace not deemed\\n    to be word-internal.\\n\\n    The tokenizer can be used on any string:\\n\\n    words = Tokenizer().tokenize(s)\\n\\n    where s is a str or unicode instance. The return value is a list\\n    of strings or unicode instances.\\n    \"\"\"\\n\\n    # Emoticon identification:\\n    emoticons = r\"\"\"\\n        (?:\\n            [<>]?\\n            [:;=8] # eyes\\n            [\\\\-o\\\\*\\\\\\']? # optional nose\\n            [\\\\)\\\\]\\\\(\\\\[dDpP/\\\\:\\\\}\\\\{@\\\\|\\\\\\\\] # mouth      \\n            |\\n            [\\\\)\\\\]\\\\(\\\\[dDpP/\\\\:\\\\}\\\\{@\\\\|\\\\\\\\] # mouth\\n            [\\\\-o\\\\*\\\\\\']? # optional nose\\n            [:;=8] # eyes\\n           [<>]?\\n        )\"\"\"\\n\\n    # A few final kinds of plain words, and a last resort:\\n    words = r\"\"\"\\n        (?:[a-zA-Z][a-zA-Z\\'\\\\-_]+[a-zA-Z]) # Words with apostrophes or dashes.\\n        |\\n        (?:[\\\\w_]+)                        # Words without apostrophes or dashes.\\n        \"\"\"\\n\\n    # Ranks:\\n    ranks = r\"\"\"[Tt]wo|[Tt]hree|[Ff]our|[Ff]ive|[Ss]ix|[Ss]even|[Ee]ight|[Nn]ine|[Tt]en|[Jj]ack|[Qq]ueen|[Kk]ing|[Aa]ce\\n                |\\n                2|3|4|5|6|7|8|9|10|[Jj]|[Qq]|[Kk]|A\"\"\"\\n\\n    # Suits:\\n    suits = r\"[Hh]earts?|[Dd]iamonds?|[Ss]pades?|[Cc]lubs?|[Hh]|[Ss]|[Dd]|[Cc]\"\\n\\n    # Last-ditch effort to create tokens; finall splits on whitespace:\\n    misc_punc = r\"\"\"\\n        (?:[+\\\\-]?\\\\d+[,/.:-]\\\\d+[+\\\\-]?)     # Numbers, including fractions, decimals.\\n        |\\n        (?:\\\\.(?:\\\\s*\\\\.){1,})               # Ellipsis dots. \\n        |\\n        (?:\\\\S)                            # Everything else that isn\\'t whitespace.\\n        \"\"\"\\n\\n    # The actual tokenizing regular expression:\\n    token_re = re.compile(r\"(%s)\" % \"|\".join((emoticons, words, ranks, suits, misc_punc)), re.VERBOSE)\\n   \\n    def tokenize(self, s):\\n        \"\"\"\\n        Tokenize the string s using token_re.findall(). Return value\\n        is a list of strings or unicode instances.\\n        \"\"\"\\n        return Tokenizer.token_re.findall(s)\\n',\n",
       " 'from fpssh.clients.base_parallel import BaseParallelSSHClient\\nfrom gevent.lock import RLock\\nfrom fpssh.clients.native.single import SSHClient\\n\\n\\nclass ParallelSSHClient(BaseParallelSSHClient):\\n\\n    def __init__(self, hosts, user, password, port):\\n        BaseParallelSSHClient.__init__(self, hosts, user, password, port)\\n        self._clients_lock = RLock()\\n\\n    def _run_command(self, host, command):\\n        self._make_ssh_client(host)\\n        return self.host_clients[host].run_command(command)\\n\\n    def _make_ssh_client(self, host):\\n        # with 相当于 acquire release\\n        with self._clients_lock:\\n            self.host_clients[host] = SSHClient(host, self.user, self.password, self.port)\\n\\n\\nif \"__main__\" == __name__:\\n    fake_cmd = \"echo foo\"\\n    fake_res = \"foo\\\\n\"\\n    hosts = [\"127.0.0.1\", \"127.0.0.1\"]\\n    port = 2222\\n    user = \"foo\"\\n    password = \"foo\"\\n    client = ParallelSSHClient(hosts, user, password, port)\\n\\n    def test_run_command():\\n        outputs = client.run_command(fake_cmd)\\n        for host, output in outputs.items():\\n            print(host, list(output[0]))\\n\\n\\n    test_run_command()',\n",
       " '# -*- coding: utf-8 -*-\\n\\n# This code is part of Qiskit.\\n#\\n# (C) Copyright IBM 2017, 2019.\\n#\\n# This code is licensed under the Apache License, Version 2.0. You may\\n# obtain a copy of this license in the LICENSE.txt file in the root directory\\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\\n#\\n# Any modifications or derivative works of this code must retain this\\n# copyright notice, and modified files need to carry a notice indicating\\n# that they have been altered from the originals.\\n\\n\"\"\"Assemble function for converting a list of circuits into a qobj\"\"\"\\nimport uuid\\nimport copy\\n\\nfrom qiskit.circuit import QuantumCircuit\\nfrom qiskit.exceptions import QiskitError\\nfrom qiskit.pulse import ScheduleComponent, LoConfig\\nfrom qiskit.assembler.run_config import RunConfig\\nfrom qiskit.assembler import assemble_circuits, assemble_schedules\\nfrom qiskit.qobj import QobjHeader\\nfrom qiskit.validation.exceptions import ModelValidationError\\nfrom qiskit.qobj.utils import MeasLevel, MeasReturnType\\n\\n\\n# TODO: parallelize over the experiments (serialize each separately, then add global header/config)\\ndef assemble(experiments,\\n             backend=None,\\n             qobj_id=None, qobj_header=None,\\n             shots=1024, memory=False, max_credits=None, seed_simulator=None,\\n             qubit_lo_freq=None, meas_lo_freq=None,\\n             qubit_lo_range=None, meas_lo_range=None,\\n             schedule_los=None, meas_level=MeasLevel.CLASSIFIED,\\n             meas_return=MeasReturnType.AVERAGE, meas_map=None,\\n             memory_slot_size=100, rep_time=None, parameter_binds=None,\\n             **run_config):\\n    \"\"\"Assemble a list of circuits or pulse schedules into a Qobj.\\n\\n    This function serializes the payloads, which could be either circuits or schedules,\\n    to create Qobj \"experiments\". It further annotates the experiment payload with\\n    header and configurations.\\n\\n    Args:\\n        experiments (QuantumCircuit or list[QuantumCircuit] or Schedule or list[Schedule]):\\n            Circuit(s) or pulse schedule(s) to execute\\n\\n        backend (BaseBackend):\\n            If set, some runtime options are automatically grabbed from\\n            backend.configuration() and backend.defaults().\\n            If any other option is explicitly set (e.g., rep_rate), it\\n            will override the backend\\'s.\\n            If any other options is set in the run_config, it will\\n            also override the backend\\'s.\\n\\n        qobj_id (str):\\n            String identifier to annotate the Qobj\\n\\n        qobj_header (QobjHeader or dict):\\n            User input that will be inserted in Qobj header, and will also be\\n            copied to the corresponding Result header. Headers do not affect the run.\\n\\n        shots (int):\\n            Number of repetitions of each circuit, for sampling. Default: 1024\\n\\n        memory (bool):\\n            If True, per-shot measurement bitstrings are returned as well\\n            (provided the backend supports it). For OpenPulse jobs, only\\n            measurement level 2 supports this option. Default: False\\n\\n        max_credits (int):\\n            Maximum credits to spend on job. Default: 10\\n\\n        seed_simulator (int):\\n            Random seed to control sampling, for when backend is a simulator\\n\\n        qubit_lo_freq (list):\\n            List of default qubit LO frequencies in Hz. Will be overridden by\\n            `schedule_los` if set.\\n\\n        meas_lo_freq (list):\\n            List of default measurement LO frequencies in Hz. Will be overridden\\n            by `schedule_los` if set.\\n\\n        qubit_lo_range (list):\\n            List of drive LO ranges each of form `[range_min, range_max]` in Hz.\\n            Used to validate the supplied qubit frequencies.\\n\\n        meas_lo_range (list):\\n            List of measurement LO ranges each of form `[range_min, range_max]` in Hz.\\n            Used to validate the supplied qubit frequencies.\\n\\n        schedule_los (None or list[Union[Dict[PulseChannel, float], LoConfig]] or \\\\\\n                      Union[Dict[PulseChannel, float], LoConfig]):\\n            Experiment LO configurations, frequencies are given in Hz.\\n\\n        meas_level (int or MeasLevel):\\n            Set the appropriate level of the measurement output for pulse experiments.\\n\\n        meas_return (str or MeasReturn):\\n            Level of measurement data for the backend to return.\\n\\n            For `meas_level` 0 and 1:\\n                * \"single\" returns information from every shot.\\n                * \"avg\" returns average measurement output (averaged over number of shots).\\n\\n        meas_map (list):\\n            List of lists, containing qubits that must be measured together.\\n\\n        memory_slot_size (int):\\n            Size of each memory slot if the output is Level 0.\\n\\n        rep_time (int): repetition time of the experiment in μs.\\n            The delay between experiments will be rep_time.\\n            Must be from the list provided by the device.\\n\\n        parameter_binds (list[dict{Parameter: Value}]):\\n            List of Parameter bindings over which the set of experiments will be\\n            executed. Each list element (bind) should be of the form\\n            {Parameter1: value1, Parameter2: value2, ...}. All binds will be\\n            executed across all experiments; e.g., if parameter_binds is a\\n            length-n list, and there are m experiments, a total of m x n\\n            experiments will be run (one for each experiment/bind pair).\\n\\n        **run_config (dict):\\n            extra arguments used to configure the run (e.g., for Aer configurable\\n            backends). Refer to the backend documentation for details on these\\n            arguments.\\n\\n    Returns:\\n            Qobj: a qobj that can be run on a backend. Depending on the type of input,\\n            this will be either a QasmQobj or a PulseQobj.\\n\\n    Raises:\\n        QiskitError: if the input cannot be interpreted as either circuits or schedules\\n    \"\"\"\\n    experiments = experiments if isinstance(experiments, list) else [experiments]\\n    qobj_id, qobj_header, run_config_common_dict = _parse_common_args(backend, qobj_id, qobj_header,\\n                                                                      shots, memory, max_credits,\\n                                                                      seed_simulator, **run_config)\\n\\n    # assemble either circuits or schedules\\n    if all(isinstance(exp, QuantumCircuit) for exp in experiments):\\n        run_config = _parse_circuit_args(parameter_binds, **run_config_common_dict)\\n\\n        # If circuits are parameterized, bind parameters and remove from run_config\\n        bound_experiments, run_config = _expand_parameters(circuits=experiments,\\n                                                           run_config=run_config)\\n        return assemble_circuits(circuits=bound_experiments, qobj_id=qobj_id,\\n                                 qobj_header=qobj_header, run_config=run_config)\\n\\n    elif all(isinstance(exp, ScheduleComponent) for exp in experiments):\\n        run_config = _parse_pulse_args(backend, qubit_lo_freq, meas_lo_freq,\\n                                       qubit_lo_range, meas_lo_range,\\n                                       schedule_los, meas_level, meas_return,\\n                                       meas_map, memory_slot_size, rep_time,\\n                                       **run_config_common_dict)\\n\\n        return assemble_schedules(schedules=experiments, qobj_id=qobj_id,\\n                                  qobj_header=qobj_header, run_config=run_config)\\n\\n    else:\\n        raise QiskitError(\"bad input to assemble() function; \"\\n                          \"must be either circuits or schedules\")\\n\\n\\n# TODO: rework to return a list of RunConfigs (one for each experiments), and a global one\\ndef _parse_common_args(backend, qobj_id, qobj_header, shots,\\n                       memory, max_credits, seed_simulator,\\n                       **run_config):\\n    \"\"\"Resolve the various types of args allowed to the assemble() function through\\n    duck typing, overriding args, etc. Refer to the assemble() docstring for details on\\n    what types of inputs are allowed.\\n\\n    Here the args are resolved by converting them to standard instances, and prioritizing\\n    them in case a run option is passed through multiple args (explicitly setting an arg\\n    has more priority than the arg set by backend)\\n\\n    Returns:\\n        RunConfig: a run config, which is a standardized object that configures the qobj\\n            and determines the runtime environment.\\n\\n    Raises:\\n        QiskitError: if the memory arg is True and the backend does not support\\n        memory.\\n    \"\"\"\\n    # grab relevant info from backend if it exists\\n    backend_config = None\\n    if backend:\\n        backend_config = backend.configuration()\\n        # check for memory flag applied to backend that does not support memory\\n        if memory and not backend_config.memory:\\n            raise QiskitError(\"memory not supported by backend {}\"\\n                              .format(backend_config.backend_name))\\n\\n    # an identifier for the Qobj\\n    qobj_id = qobj_id or str(uuid.uuid4())\\n\\n    # The header that goes at the top of the Qobj (and later Result)\\n    # we process it as dict, then write entries that are not None to a QobjHeader object\\n    qobj_header = qobj_header or {}\\n    if isinstance(qobj_header, QobjHeader):\\n        qobj_header = qobj_header.to_dict()\\n    backend_name = getattr(backend_config, \\'backend_name\\', None)\\n    backend_version = getattr(backend_config, \\'backend_version\\', None)\\n    qobj_header = {**dict(backend_name=backend_name, backend_version=backend_version),\\n                   **qobj_header}\\n    qobj_header = QobjHeader(**{k: v for k, v in qobj_header.items() if v is not None})\\n\\n    # create run configuration and populate\\n    run_config_dict = dict(shots=shots,\\n                           memory=memory,\\n                           max_credits=max_credits,\\n                           seed_simulator=seed_simulator,\\n                           **run_config)\\n\\n    return qobj_id, qobj_header, run_config_dict\\n\\n\\ndef _parse_pulse_args(backend, qubit_lo_freq, meas_lo_freq, qubit_lo_range,\\n                      meas_lo_range, schedule_los, meas_level,\\n                      meas_return, meas_map,\\n                      memory_slot_size, rep_time,\\n                      **run_config):\\n    \"\"\"Build a pulse RunConfig replacing unset arguments with defaults derived from the `backend`.\\n    See `assemble` for more information on the required arguments.\\n\\n    Returns:\\n        RunConfig: a run config, which is a standardized object that configures the qobj\\n            and determines the runtime environment.\\n    \"\"\"\\n    # grab relevant info from backend if it exists\\n    backend_config = None\\n    backend_default = None\\n    if backend:\\n        backend_config = backend.configuration()\\n        # TODO : Remove usage of config.defaults when backend.defaults() is updated.\\n        try:\\n            backend_default = backend.defaults()\\n        except (ModelValidationError, AttributeError):\\n            from collections import namedtuple\\n            backend_config_defaults = getattr(backend_config, \\'defaults\\', {})\\n            BackendDefault = namedtuple(\\'BackendDefault\\', (\\'qubit_freq_est\\', \\'meas_freq_est\\'))\\n            backend_default = BackendDefault(\\n                qubit_freq_est=backend_config_defaults.get(\\'qubit_freq_est\\'),\\n                meas_freq_est=backend_config_defaults.get(\\'meas_freq_est\\')\\n            )\\n\\n    meas_map = meas_map or getattr(backend_config, \\'meas_map\\', None)\\n\\n    schedule_los = schedule_los or []\\n    if isinstance(schedule_los, (LoConfig, dict)):\\n        schedule_los = [schedule_los]\\n\\n    # Convert to LoConfig if LO configuration supplied as dictionary\\n    schedule_los = [lo_config if isinstance(lo_config, LoConfig) else LoConfig(lo_config)\\n                    for lo_config in schedule_los]\\n\\n    if not qubit_lo_freq and hasattr(backend_default, \\'qubit_freq_est\\'):\\n        qubit_lo_freq = backend_default.qubit_freq_est\\n    if not meas_lo_freq and hasattr(backend_default, \\'meas_freq_est\\'):\\n        meas_lo_freq = backend_default.meas_freq_est\\n\\n    qubit_lo_range = qubit_lo_range or getattr(backend_config, \\'qubit_lo_range\\', None)\\n    meas_lo_range = meas_lo_range or getattr(backend_config, \\'meas_lo_range\\', None)\\n\\n    rep_time = rep_time or getattr(backend_config, \\'rep_times\\', None)\\n    if isinstance(rep_time, list):\\n        rep_time = rep_time[0]\\n\\n    # create run configuration and populate\\n    run_config_dict = dict(qubit_lo_freq=qubit_lo_freq,\\n                           meas_lo_freq=meas_lo_freq,\\n                           qubit_lo_range=qubit_lo_range,\\n                           meas_lo_range=meas_lo_range,\\n                           schedule_los=schedule_los,\\n                           meas_level=meas_level,\\n                           meas_return=meas_return,\\n                           meas_map=meas_map,\\n                           memory_slot_size=memory_slot_size,\\n                           rep_time=rep_time,\\n                           **run_config)\\n    run_config = RunConfig(**{k: v for k, v in run_config_dict.items() if v is not None})\\n\\n    return run_config\\n\\n\\ndef _parse_circuit_args(parameter_binds, **run_config):\\n    \"\"\"Build a circuit RunConfig replacing unset arguments with defaults derived from the `backend`.\\n    See `assemble` for more information on the required arguments.\\n\\n    Returns:\\n        RunConfig: a run config, which is a standardized object that configures the qobj\\n            and determines the runtime environment.\\n    \"\"\"\\n    parameter_binds = parameter_binds or []\\n\\n    # create run configuration and populate\\n    run_config_dict = dict(parameter_binds=parameter_binds, **run_config)\\n    run_config = RunConfig(**{k: v for k, v in run_config_dict.items() if v is not None})\\n\\n    return run_config\\n\\n\\ndef _expand_parameters(circuits, run_config):\\n    \"\"\"Verifies that there is a single common set of parameters shared between\\n    all circuits and all parameter binds in the run_config. Returns an expanded\\n    list of circuits (if parameterized) with all parameters bound, and a copy of\\n    the run_config with parameter_binds cleared.\\n\\n    If neither the circuits nor the run_config specify parameters, the two are\\n    returned unmodified.\\n\\n    Raises:\\n        QiskitError: if run_config parameters are not compatible with circuit parameters\\n\\n    Returns:\\n        Tuple(List[QuantumCircuit], RunConfig):\\n          - List of input circuits expanded and with parameters bound\\n          - RunConfig with parameter_binds removed\\n    \"\"\"\\n\\n    parameter_binds = run_config.parameter_binds\\n    if parameter_binds or \\\\\\n       any(circuit.parameters for circuit in circuits):\\n\\n        all_bind_parameters = [bind.keys()\\n                               for bind in parameter_binds]\\n        all_circuit_parameters = [circuit.parameters for circuit in circuits]\\n\\n        # Collect set of all unique parameters across all circuits and binds\\n        unique_parameters = {param\\n                             for param_list in all_bind_parameters + all_circuit_parameters\\n                             for param in param_list}\\n\\n        # Check that all parameters are common to all circuits and binds\\n        if not all_bind_parameters \\\\\\n           or not all_circuit_parameters \\\\\\n           or any(unique_parameters != bind_params for bind_params in all_bind_parameters) \\\\\\n           or any(unique_parameters != parameters for parameters in all_circuit_parameters):\\n            raise QiskitError(\\n                (\\'Mismatch between run_config.parameter_binds and all circuit parameters. \\' +\\n                 \\'Parameter binds: {} \\' +\\n                 \\'Circuit parameters: {}\\').format(all_bind_parameters, all_circuit_parameters))\\n\\n        circuits = [circuit.bind_parameters(binds)\\n                    for circuit in circuits\\n                    for binds in parameter_binds]\\n\\n        # All parameters have been expanded and bound, so remove from run_config\\n        run_config = copy.deepcopy(run_config)\\n        run_config.parameter_binds = []\\n\\n    return circuits, run_config\\n',\n",
       " '# coding: utf-8\\n\\n\"\"\"\\n    Buy Marketing API\\n\\n    The Marketing API retrieves eBay products based on a metric, such as Best Selling, as well as products that were also bought and also viewed.  # noqa: E501\\n\\n    OpenAPI spec version: v1_beta.1.0\\n    \\n    Generated by: https://github.com/swagger-api/swagger-codegen.git\\n\"\"\"\\n\\nimport pprint\\nimport re  # noqa: F401\\n\\nimport six\\n\\nclass Image(object):\\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\\n\\n    Do not edit the class manually.\\n    \"\"\"\\n    \"\"\"\\n    Attributes:\\n      swagger_types (dict): The key is attribute name\\n                            and the value is attribute type.\\n      attribute_map (dict): The key is attribute name\\n                            and the value is json key in definition.\\n    \"\"\"\\n    swagger_types = {\\n        \\'height\\': \\'int\\',\\n        \\'image_url\\': \\'str\\',\\n        \\'width\\': \\'int\\'\\n    }\\n\\n    attribute_map = {\\n        \\'height\\': \\'height\\',\\n        \\'image_url\\': \\'imageUrl\\',\\n        \\'width\\': \\'width\\'\\n    }\\n\\n    def __init__(self, height=None, image_url=None, width=None):  # noqa: E501\\n        \"\"\"Image - a model defined in Swagger\"\"\"  # noqa: E501\\n        self._height = None\\n        self._image_url = None\\n        self._width = None\\n        self.discriminator = None\\n        if height is not None:\\n            self.height = height\\n        if image_url is not None:\\n            self.image_url = image_url\\n        if width is not None:\\n            self.width = width\\n\\n    @property\\n    def height(self):\\n        \"\"\"Gets the height of this Image.  # noqa: E501\\n\\n        Reserved for future use.  # noqa: E501\\n\\n        :return: The height of this Image.  # noqa: E501\\n        :rtype: int\\n        \"\"\"\\n        return self._height\\n\\n    @height.setter\\n    def height(self, height):\\n        \"\"\"Sets the height of this Image.\\n\\n        Reserved for future use.  # noqa: E501\\n\\n        :param height: The height of this Image.  # noqa: E501\\n        :type: int\\n        \"\"\"\\n\\n        self._height = height\\n\\n    @property\\n    def image_url(self):\\n        \"\"\"Gets the image_url of this Image.  # noqa: E501\\n\\n        The URL of the image.  # noqa: E501\\n\\n        :return: The image_url of this Image.  # noqa: E501\\n        :rtype: str\\n        \"\"\"\\n        return self._image_url\\n\\n    @image_url.setter\\n    def image_url(self, image_url):\\n        \"\"\"Sets the image_url of this Image.\\n\\n        The URL of the image.  # noqa: E501\\n\\n        :param image_url: The image_url of this Image.  # noqa: E501\\n        :type: str\\n        \"\"\"\\n\\n        self._image_url = image_url\\n\\n    @property\\n    def width(self):\\n        \"\"\"Gets the width of this Image.  # noqa: E501\\n\\n        Reserved for future use.  # noqa: E501\\n\\n        :return: The width of this Image.  # noqa: E501\\n        :rtype: int\\n        \"\"\"\\n        return self._width\\n\\n    @width.setter\\n    def width(self, width):\\n        \"\"\"Sets the width of this Image.\\n\\n        Reserved for future use.  # noqa: E501\\n\\n        :param width: The width of this Image.  # noqa: E501\\n        :type: int\\n        \"\"\"\\n\\n        self._width = width\\n\\n    def to_dict(self):\\n        \"\"\"Returns the model properties as a dict\"\"\"\\n        result = {}\\n\\n        for attr, _ in six.iteritems(self.swagger_types):\\n            value = getattr(self, attr)\\n            if isinstance(value, list):\\n                result[attr] = list(map(\\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\\n                    value\\n                ))\\n            elif hasattr(value, \"to_dict\"):\\n                result[attr] = value.to_dict()\\n            elif isinstance(value, dict):\\n                result[attr] = dict(map(\\n                    lambda item: (item[0], item[1].to_dict())\\n                    if hasattr(item[1], \"to_dict\") else item,\\n                    value.items()\\n                ))\\n            else:\\n                result[attr] = value\\n        if issubclass(Image, dict):\\n            for key, value in self.items():\\n                result[key] = value\\n\\n        return result\\n\\n    def to_str(self):\\n        \"\"\"Returns the string representation of the model\"\"\"\\n        return pprint.pformat(self.to_dict())\\n\\n    def __repr__(self):\\n        \"\"\"For `print` and `pprint`\"\"\"\\n        return self.to_str()\\n\\n    def __eq__(self, other):\\n        \"\"\"Returns true if both objects are equal\"\"\"\\n        if not isinstance(other, Image):\\n            return False\\n\\n        return self.__dict__ == other.__dict__\\n\\n    def __ne__(self, other):\\n        \"\"\"Returns true if both objects are not equal\"\"\"\\n        return not self == other\\n',\n",
       " 'import logging\\nimport time\\nimport os\\nfrom collections import OrderedDict\\nfrom copy import deepcopy\\nfrom typing import Tuple\\n\\nimport numpy as np\\nfrom tqdm.auto import tqdm\\n\\nfrom .reporter import FakeReporter\\nfrom ..searcher import searcher_factory\\nfrom ..searcher.local_searcher import LocalSearcher\\nfrom ..utils import EasyDict\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass LocalReporter:\\n    \"\"\"\\n    Reporter implementation for LocalSequentialScheduler\\n    \"\"\"\\n\\n    def __init__(self, trial, searcher_config, training_history: dict, config_history: dict):\\n        self.trial = trial\\n        self.training_history = training_history\\n        self.training_history[trial] = []\\n        self.searcher_config = deepcopy(searcher_config)\\n        self.config_history = config_history\\n        self.trial_started = time.time()\\n        self.last_reported_time = self.trial_started\\n        self.last_result = None\\n\\n    def __call__(self, *args, **kwargs):\\n        result = deepcopy(kwargs)\\n        if \\'done\\' not in result:\\n            result[\\'trial\\'] = self.trial\\n\\n            now = time.time()\\n            result[\\'time_this_iter\\'] = now - self.last_reported_time\\n            result[\\'time_since_start\\'] = now - self.trial_started\\n            self.last_reported_time = now\\n\\n            self.training_history[self.trial].append(result)\\n\\n            if self.trial not in self.config_history:\\n                self.config_history[self.trial] = self.searcher_config\\n                if \\'util_args\\' in self.searcher_config:\\n                    self.searcher_config.pop(\\'util_args\\')\\n\\n            self.last_result = result\\n\\n    def terminate(self):\\n        pass  # compatibility\\n\\n\\nclass LocalSequentialScheduler(object):\\n    \"\"\" Simple scheduler which schedules all HPO jobs in sequence without any parallelism.\\n    The next trial scheduling will be decided based on the available time left withing `time_out` setting\\n    and average time required for a trial to complete multiplied by the fill_factor (0.95) by default to\\n    accommodate variance in runtimes per HPO run.\\n\\n    Parameters\\n    ----------\\n    train_fn : callable\\n        A task launch function for training.\\n    resource : dict\\n        Computation resources. For example, `{\\'num_cpus\\':2, \\'num_gpus\\':1}`\\n    searcher : str\\n        Searcher (get_config decisions). If str, this is passed to\\n        searcher_factory along with search_options.\\n    search_options : dict\\n        If searcher is str, these arguments are passed to searcher_factory.\\n    num_trials : int\\n        Maximum number of jobs run in experiment. One of `num_trials`,\\n        `time_out` must be given.\\n    time_out : float\\n        If given, jobs are started only until this time_out (wall clock time).\\n        One of `num_trials`, `time_out` must be given.\\n    reward_attr : str\\n        Name of reward (i.e., metric to maximize) attribute in data obtained\\n        from reporter\\n    time_attr : str\\n        Name of resource (or time) attribute in data obtained from reporter.\\n        This attribute is optional for FIFO scheduling, but becomes mandatory\\n        in multi-fidelity scheduling (e.g., Hyperband).\\n        Note: The type of resource must be int.\\n    \"\"\"\\n\\n    def __init__(self, train_fn, search_space, util_args=None, searcher=\\'auto\\', reward_attr=\\'reward\\', resource=None, **kwargs):\\n        self.train_fn = train_fn\\n        self.training_history = None\\n        self.config_history = None\\n        self._reward_attr = reward_attr\\n        self.time_attr = kwargs.get(\\'time_attr\\', None)\\n        self.resource = resource\\n        self.max_reward = kwargs.get(\\'max_reward\\', None)\\n        self.searcher: LocalSearcher = self.get_searcher_(searcher, train_fn, search_space=search_space, **kwargs)\\n        self.init_limits_(kwargs)\\n        self.util_args = util_args\\n        self.metadata = {\\n            \\'search_space\\': search_space,\\n            \\'search_strategy\\': self.searcher,\\n            \\'stop_criterion\\': {\\n                \\'time_limits\\': self.time_out,\\n                \\'max_reward\\': self.max_reward},\\n            \\'resources_per_trial\\': self.resource\\n        }\\n\\n    def init_limits_(self, kwargs):\\n        if kwargs.get(\\'num_trials\\', None) is None:\\n            assert kwargs.get(\\'time_out\\', None) is not None, \"Need stopping criterion: Either num_trials or time_out\"\\n        self.num_trials = kwargs.get(\\'num_trials\\', 9999)\\n        self.time_out = kwargs.get(\\'time_out\\', None)\\n        if self.num_trials is None:\\n            assert self.time_out is not None, \"Need stopping criterion: Either num_trials or time_out\"\\n\\n    def get_searcher_(self, searcher, train_fn, search_space, **kwargs) -> LocalSearcher:\\n        scheduler_opts = {}\\n        if searcher == \\'auto\\':\\n            searcher = \\'local_random\\'\\n            scheduler_opts = {\\'scheduler\\': \\'local\\'}\\n        elif searcher == \\'random\\':\\n            # FIXME: Hack to be compatible with gluoncv\\n            searcher = \\'local_random\\'\\n\\n        search_options = kwargs.get(\\'search_options\\', None)\\n        if isinstance(searcher, str):\\n            if search_options is None:\\n                search_options = dict()\\n            _search_options = search_options.copy()\\n            if searcher.startswith(\\'local_\\'):\\n                _search_options[\\'search_space\\'] = search_space\\n            else:\\n                _search_options[\\'configspace\\'] = train_fn.cs\\n                _search_options[\\'resource_attribute\\'] = kwargs.get(\\'time_attr\\', None)\\n            _search_options[\\'reward_attribute\\'] = self._reward_attr\\n            # Adjoin scheduler info to search_options, if not already done by\\n            # subclass\\n            if \\'scheduler\\' not in _search_options:\\n                _search_options[\\'scheduler\\'] = \\'local\\'\\n            searcher = searcher_factory(searcher, **{**scheduler_opts, **_search_options})\\n        else:\\n            assert isinstance(searcher, LocalSearcher)\\n        return searcher\\n\\n    def run(self, **kwargs):\\n        \"\"\"Run multiple trials given specific time and trial numbers limits.\\n        \"\"\"\\n        self.searcher.configure_scheduler(self)\\n\\n        self.training_history = OrderedDict()\\n        self.config_history = OrderedDict()\\n\\n        trial_run_times = []\\n        time_start = time.time()\\n\\n        r = range(self.num_trials)\\n        for i in (tqdm(r) if self.num_trials < 1000 else r):\\n            trial_start_time = time.time()\\n            try:\\n                is_failed, result = self.run_trial(task_id=i)\\n            except Exception:\\n                # TODO: Add special exception type when there are no more new configurations to try (exhausted search space)\\n                logger.log(30, f\\'\\\\tWARNING: Encountered unexpected exception during trial {i}, stopping HPO early.\\')\\n                logger.exception(\\'Detailed Traceback:\\')  # TODO: Avoid logging if verbosity=0\\n                break\\n            trial_end_time = time.time()\\n            trial_run_times.append(np.NaN if is_failed else (trial_end_time - trial_start_time))\\n\\n            if self.max_reward and self.get_best_reward() >= self.max_reward:\\n                logger.log(20, f\\'\\\\tMax reward is reached\\')\\n                break\\n\\n            if self.time_out is not None:\\n                avg_trial_run_time = np.nanmean(trial_run_times)\\n                avg_trial_run_time = 0 if np.isnan(avg_trial_run_time) else avg_trial_run_time\\n                if not self.has_enough_time_for_trial_(self.time_out, time_start, trial_start_time, trial_end_time, avg_trial_run_time):\\n                    logger.log(20, f\\'\\\\tTime limit exceeded\\')\\n                    break\\n\\n    @classmethod\\n    def has_enough_time_for_trial_(cls, time_out, time_start, trial_start_time, trial_end_time, avg_trial_run_time, fill_factor=0.95):\\n        \"\"\"\\n        Checks if the remaining time is enough to run another trial.\\n\\n        Parameters\\n        ----------\\n        time_out total\\n            timeout in m\\n        time_start\\n            trials start time\\n        trial_start_time\\n            last trial start time\\n        trial_end_time\\n            last trial end time\\n        avg_trial_run_time\\n            running average of all trial runs\\n        fill_factor: float\\n            discount of `avg_trial_run_time` allowed for a next trial. Default is 0.95 of `avg_trial_run_time`\\n\\n        Returns\\n        -------\\n            True if there is enough time to run another trial give runs statistics and remaining time\\n\\n        \"\"\"\\n        time_spent = trial_end_time - time_start\\n        is_timeout_exceeded = time_spent >= time_out\\n        time_left = time_start + time_out - trial_end_time\\n        is_enough_time_for_another_trial = True\\n        if avg_trial_run_time:\\n            is_enough_time_for_another_trial = time_left > avg_trial_run_time * fill_factor\\n        return is_enough_time_for_another_trial and not is_timeout_exceeded\\n\\n    @classmethod\\n    def get_average_trial_time_(cls, i, avg_trial_run_time, trial_start_time, time_end):\\n        trial_time = time_end - trial_start_time\\n        if avg_trial_run_time is None:\\n            avg_trial_run_time = trial_time\\n        else:\\n            avg_trial_run_time = ((avg_trial_run_time * i) + trial_time) / (i + 1)\\n        return avg_trial_run_time\\n\\n    def run_trial(self, task_id=0) -> Tuple[bool, dict]:\\n        \"\"\"\\n        Start a trial with a given task_id\\n\\n        Parameters\\n        ----------\\n        task_id\\n            task\\n\\n        Returns\\n        -------\\n        is_failed: bool\\n            True if task completed successfully\\n        trial_start_time\\n            Trial start time\\n        trial_end_time\\n            Trial end time\\n\\n        \"\"\"\\n        new_searcher_config = self.searcher.get_config()\\n        searcher_config = deepcopy(self.metadata[\\'search_space\\'])\\n        searcher_config.update(new_searcher_config)\\n        reporter = LocalReporter(task_id, searcher_config, self.training_history, self.config_history)\\n        return self.run_job_(task_id, searcher_config, reporter)\\n\\n    def run_job_(self, task_id, searcher_config, reporter):\\n        args = dict()\\n        if self.util_args is not None:\\n            args[\\'util_args\\'] = deepcopy(self.util_args)\\n        args.update(searcher_config)\\n\\n        args[\\'task_id\\'] = task_id\\n        args = EasyDict(args)  # TODO: Remove, currently used for compatibility with gluoncv\\n        self.searcher.register_pending(searcher_config)\\n        is_failed = False\\n        try:\\n            result = self.train_fn(args, reporter=reporter)\\n            if type(reporter) is not FakeReporter and reporter.last_result:\\n                self.searcher.update(config=searcher_config, **reporter.last_result)\\n        except Exception as e:\\n            logger.error(f\\'Exception during a trial: {e}\\')\\n            self.searcher.evaluation_failed(config=searcher_config)\\n            reporter(traceback=e)\\n            is_failed = True\\n            result = {\\'traceback\\': str(e)}\\n        return is_failed, result\\n\\n    def run_with_config(self, config):\\n        \"\"\"Run with config for final fit.\\n        It launches a single training trial under any fixed values of the hyperparameters.\\n        For example, after HPO has identified the best hyperparameter values based on a hold-out dataset,\\n        one can use this function to retrain a model with the same hyperparameters on all the available labeled data\\n        (including the hold out set). It can also returns other objects or states.\\n        \"\"\"\\n        is_failed, result = self.run_job_(\\'run_with_config\\', config, FakeReporter())\\n        return result\\n\\n    def join_jobs(self, timeout=None):\\n        pass  # Compatibility\\n\\n    def get_best_config(self):\\n        \"\"\"Get the best configuration from the finished jobs.\\n        \"\"\"\\n        return self.searcher.get_best_config()\\n\\n    def get_best_reward(self):\\n        \"\"\"Get the best reward from the finished jobs.\\n        \"\"\"\\n        return self.searcher.get_best_reward()\\n\\n    def get_training_curves(self, filename=None, plot=False, use_legend=True):\\n        \"\"\"Get Training Curves\\n        \"\"\"\\n        if filename is None and not plot:\\n            logger.warning(\\'Please either provide filename or allow plot in get_training_curves\\')\\n        import matplotlib.pyplot as plt\\n\\n        eval_metric = self.__get_training_history_metric(\\'eval_metric\\', default=\\'validation_performance\\')\\n        sign_mult = int(self.__get_training_history_metric(\\'greater_is_better\\', default=True)) * 2 - 1\\n\\n        plt.ylabel(eval_metric)\\n        plt.xlabel(self.time_attr)\\n        plt.title(\"Performance vs Training-Time in each HPO Trial\")\\n        for task_id, task_res in self.training_history.items():\\n            rewards = [x[self._reward_attr] * sign_mult for x in task_res]\\n            x = [x[self.time_attr] for x in task_res]\\n            plt.plot(x, rewards, label=f\\'task {task_id}\\')\\n        if use_legend:\\n            plt.legend(loc=\\'best\\')\\n        if filename:\\n            logger.info(f\\'Saving Training Curve in {filename}\\')\\n            file_dir = os.path.split(os.path.abspath(filename))[0]\\n            if not os.path.exists(file_dir):\\n                os.makedirs(file_dir)\\n            plt.savefig(filename)\\n        if plot:\\n            plt.show()\\n\\n    def __get_training_history_metric(self, metric, default=None):\\n        for _, task_res in self.training_history.items():\\n            if task_res and metric in task_res[0]:\\n                return task_res[0][metric]\\n        return default\\n\\n    def get_best_task_id(self):\\n        \"\"\"Get the task id that results in the best configuration/best reward.\\n\\n        If there are duplicated configurations, we return the id of the first one.\\n        \"\"\"\\n        best_config = self.get_best_config()\\n        for task_id, config in self.config_history.items():\\n            if best_config == config:\\n                return task_id\\n        raise RuntimeError(\\'The best config {} is not found in config history = {}. \\'\\n                           \\'This should never happen!\\'.format(best_config, self.config_history))\\n',\n",
       " 'from copy import deepcopy\\nfrom ditk import logging\\nfrom ding.model import DQN\\nfrom ding.policy import DQNPolicy\\nfrom ding.envs import DingEnvWrapper, SubprocessEnvManagerV2\\nfrom ding.data import DequeBuffer\\nfrom ding.config import compile_config\\nfrom ding.framework import task\\nfrom ding.framework.context import OnlineRLContext\\nfrom ding.framework.middleware import OffPolicyLearner, StepCollector, interaction_evaluator, data_pusher, \\\\\\n    eps_greedy_handler, CkptSaver, nstep_reward_enhancer, termination_checker\\nfrom ding.utils import set_pkg_seed\\nfrom dizoo.atari.envs.atari_env import AtariEnv\\nfrom dizoo.atari.config.serial.pong.pong_dqn_config import main_config, create_config\\n\\n\\ndef main():\\n    logging.getLogger().setLevel(logging.INFO)\\n    cfg = compile_config(main_config, create_cfg=create_config, auto=True)\\n    with task.start(async_mode=False, ctx=OnlineRLContext()):\\n        collector_cfg = deepcopy(cfg.env)\\n        collector_cfg.is_train = True\\n        evaluator_cfg = deepcopy(cfg.env)\\n        evaluator_cfg.is_train = False\\n        collector_env = SubprocessEnvManagerV2(\\n            env_fn=[lambda: AtariEnv(collector_cfg) for _ in range(cfg.env.collector_env_num)], cfg=cfg.env.manager\\n        )\\n        evaluator_env = SubprocessEnvManagerV2(\\n            env_fn=[lambda: AtariEnv(evaluator_cfg) for _ in range(cfg.env.evaluator_env_num)], cfg=cfg.env.manager\\n        )\\n\\n        set_pkg_seed(cfg.seed, use_cuda=cfg.policy.cuda)\\n\\n        model = DQN(**cfg.policy.model)\\n        buffer_ = DequeBuffer(size=cfg.policy.other.replay_buffer.replay_buffer_size)\\n        policy = DQNPolicy(cfg.policy, model=model)\\n\\n        task.use(interaction_evaluator(cfg, policy.eval_mode, evaluator_env))\\n        task.use(eps_greedy_handler(cfg))\\n        task.use(StepCollector(cfg, policy.collect_mode, collector_env))\\n        task.use(nstep_reward_enhancer(cfg))\\n        task.use(data_pusher(cfg, buffer_))\\n        task.use(OffPolicyLearner(cfg, policy.learn_mode, buffer_))\\n        task.use(CkptSaver(cfg, policy, train_freq=1000))\\n        task.use(termination_checker(max_train_iter=int(1e7)))\\n        task.run()\\n\\n\\nif __name__ == \"__main__\":\\n    main()\\n',\n",
       " '\\'\\'\\'\\nModels (mostly base classes) for the various kinds of renderer\\ntypes that Bokeh supports.\\n\\n\\'\\'\\'\\n#-----------------------------------------------------------------------------\\n# Boilerplate\\n#-----------------------------------------------------------------------------\\nimport logging # isort:skip\\nlog = logging.getLogger(__name__)\\n\\n#-----------------------------------------------------------------------------\\n# Imports\\n#-----------------------------------------------------------------------------\\n\\n# Standard library imports\\nfrom difflib import get_close_matches\\n\\n# Bokeh imports\\nfrom ..core.enums import RenderLevel\\nfrom ..core.has_props import abstract\\nfrom ..core.properties import (\\n    Auto,\\n    Bool,\\n    Either,\\n    Enum,\\n    Float,\\n    Instance,\\n    Override,\\n    String,\\n)\\nfrom ..core.validation import error\\nfrom ..core.validation.errors import (\\n    BAD_COLUMN_NAME,\\n    CDSVIEW_FILTERS_WITH_CONNECTED,\\n    CDSVIEW_SOURCE_DOESNT_MATCH,\\n    MALFORMED_GRAPH_SOURCE,\\n    MISSING_GLYPH,\\n    NO_SOURCE_FOR_GLYPH,\\n)\\nfrom ..model import Model\\nfrom .glyphs import Circle, ConnectedXYGlyph, Glyph, MultiLine\\nfrom .graphs import GraphHitTestPolicy, LayoutProvider, NodesOnly\\nfrom .sources import CDSView, ColumnDataSource, DataSource, WebSource\\nfrom .tiles import TileSource, WMTSTileSource\\n\\n#-----------------------------------------------------------------------------\\n# Globals and constants\\n#-----------------------------------------------------------------------------\\n\\n__all__ = (\\n    \\'DataRenderer\\',\\n    \\'GlyphRenderer\\',\\n    \\'GraphRenderer\\',\\n    \\'GuideRenderer\\',\\n    \\'Renderer\\',\\n    \\'TileRenderer\\',\\n)\\n\\n#-----------------------------------------------------------------------------\\n# General API\\n#-----------------------------------------------------------------------------\\n\\n#-----------------------------------------------------------------------------\\n# Dev API\\n#-----------------------------------------------------------------------------\\n\\n@abstract\\nclass Renderer(Model):\\n    \\'\\'\\'\\n    An abstract base class for renderer types.\\n\\n    \\'\\'\\'\\n\\n    level = Enum(RenderLevel, help=\"\"\"\\n    Specifies the level in which to paint this renderer.\\n    \"\"\")\\n\\n    visible = Bool(default=True, help=\"\"\"\\n    Is the renderer visible.\\n    \"\"\")\\n\\n    x_range_name = String(\\'default\\', help=\"\"\"\\n    A particular (named) x-range to use for computing screen locations when\\n    rendering glyphs on the plot. If unset, use the default x-range.\\n    \"\"\")\\n\\n    y_range_name = String(\\'default\\', help=\"\"\"\\n    A particular (named) y-range to use for computing screen locations when\\n    rendering glyphs on the plot. If unset, use the default y-range.\\n    \"\"\")\\n\\n\\n@abstract\\nclass DataRenderer(Renderer):\\n    \\'\\'\\'\\n    An abstract base class for data renderer types (e.g. ``GlyphRenderer``, ``TileRenderer``, ``GraphRenderer``).\\n\\n    \\'\\'\\'\\n\\n    level = Override(default=\"glyph\")\\n\\nclass TileRenderer(DataRenderer):\\n    \\'\\'\\'\\n\\n    \\'\\'\\'\\n\\n    tile_source = Instance(TileSource, default=lambda: WMTSTileSource(), help=\"\"\"\\n    Local data source to use when rendering glyphs on the plot.\\n    \"\"\")\\n\\n    alpha = Float(1.0, help=\"\"\"\\n    tile opacity 0.0 - 1.0\\n    \"\"\")\\n\\n    smoothing = Bool(default=True, help=\"\"\"\\n    Enable image smoothing for the rendered tiles.\\n    \"\"\")\\n\\n    render_parents = Bool(default=True, help=\"\"\"\\n    Flag enable/disable drawing of parent tiles while waiting for new tiles to arrive. Default value is True.\\n    \"\"\")\\n\\nclass GlyphRenderer(DataRenderer):\\n    \\'\\'\\'\\n\\n    \\'\\'\\'\\n\\n    def __str__(self):\\n        return f\"GlyphRenderer(id={self.id}, glyph={str(self.glyph)}, ...)\"\\n\\n    @error(CDSVIEW_FILTERS_WITH_CONNECTED)\\n    def _check_cdsview_filters_with_connected(self):\\n        if isinstance(self.glyph, ConnectedXYGlyph) and len(self.view.filters) > 0:\\n            return str(self)\\n\\n    @error(MISSING_GLYPH)\\n    def _check_missing_glyph(self):\\n        if not self.glyph: return str(self)\\n\\n    @error(NO_SOURCE_FOR_GLYPH)\\n    def _check_no_source_for_glyph(self):\\n        if not self.data_source: return str(self)\\n\\n    @error(CDSVIEW_SOURCE_DOESNT_MATCH)\\n    def _check_cdsview_source(self):\\n        if self.data_source is not self.view.source: return str(self)\\n\\n    @error(BAD_COLUMN_NAME)\\n    def _check_bad_column_name(self):\\n        if not self.glyph: return\\n        if not self.data_source: return\\n        if isinstance(self.data_source, WebSource): return\\n        missing_values = set()\\n        specs = self.glyph.dataspecs()\\n        for name, item in self.glyph.properties_with_values(include_defaults=False).items():\\n            if name not in specs: continue\\n            if not isinstance(item, dict): continue\\n            if not isinstance(self.data_source, ColumnDataSource): continue\\n            if \\'field\\' in item and item[\\'field\\'] not in self.data_source.column_names:\\n                missing_values.add((item[\\'field\\'], name))\\n        if missing_values:\\n            suggestions = [\\'\" (closest match: \"%s\")\\' % s[0] if s else \\'\"\\' for s in [\\n                get_close_matches(term[0], self.data_source.column_names, n=1) for term in missing_values]]\\n            missing_values = [(\"\".join([m[0], s]), m[1]) for m, s in zip(missing_values, suggestions)]\\n            missing = [\\'key \"%s\" value \"%s\\' % (k, v) for v, k in missing_values]\\n            return \"%s [renderer: %s]\" % (\", \".join(sorted(missing)), self)\\n\\n    def __init__(self, **kw):\\n        super().__init__(**kw)\\n        if \"view\" not in kw:\\n            self.view = CDSView(source=self.data_source)\\n\\n    data_source = Instance(DataSource, help=\"\"\"\\n    Local data source to use when rendering glyphs on the plot.\\n    \"\"\")\\n\\n    view = Instance(CDSView, help=\"\"\"\\n    A view into the data source to use when rendering glyphs. A default view\\n    of the entire data source is created when a view is not passed in during\\n    initialization.\\n\\n    .. note:\\n        Only the default (filterless) CDSView is compatible with glyphs that\\n        have connected topology, such as Line and Patch. Setting filters on\\n        views for these glyphs will result in a warning and undefined behavior.\\n    \"\"\")\\n\\n    glyph = Instance(Glyph, help=\"\"\"\\n    The glyph to render, in conjunction with the supplied data source\\n    and ranges.\\n    \"\"\")\\n\\n    selection_glyph = Either(Auto, Instance(Glyph), default=\"auto\", help=\"\"\"\\n    An optional glyph used for selected points.\\n\\n    If set to \"auto\" then the standard glyph will be used for selected\\n    points.\\n    \"\"\")\\n\\n    nonselection_glyph = Either(Auto, Instance(Glyph), default=\"auto\", help=\"\"\"\\n    An optional glyph used for explicitly non-selected points\\n    (i.e., non-selected when there are other points that are selected,\\n    but not when no points at all are selected.)\\n\\n    If set to \"auto\" then a glyph with a low alpha value (0.1) will\\n    be used for non-selected points.\\n    \"\"\")\\n\\n    hover_glyph = Instance(Glyph, help=\"\"\"\\n    An optional glyph used for inspected points, e.g., those that are\\n    being hovered over by a ``HoverTool``.\\n    \"\"\")\\n\\n    muted_glyph = Instance(Glyph, help=\"\"\"\\n    \"\"\")\\n\\n    muted = Bool(False, help=\"\"\"\\n    \"\"\")\\n\\n_DEFAULT_NODE_RENDERER = lambda: GlyphRenderer(\\n    glyph=Circle(), data_source=ColumnDataSource(data=dict(index=[]))\\n)\\n\\n_DEFAULT_EDGE_RENDERER = lambda: GlyphRenderer(\\n    glyph=MultiLine(), data_source=ColumnDataSource(data=dict(start=[], end=[]))\\n)\\n\\nclass GraphRenderer(DataRenderer):\\n    \\'\\'\\'\\n\\n    \\'\\'\\'\\n\\n    @error(MALFORMED_GRAPH_SOURCE)\\n    def _check_malformed_graph_source(self):\\n        missing = []\\n        if \"index\" not in self.node_renderer.data_source.column_names:\\n            missing.append(\"Column \\'index\\' is missing in GraphSource.node_renderer.data_source\")\\n        if \"start\" not in self.edge_renderer.data_source.column_names:\\n            missing.append(\"Column \\'start\\' is missing in GraphSource.edge_renderer.data_source\")\\n        if \"end\" not in self.edge_renderer.data_source.column_names:\\n            missing.append(\"Column \\'end\\' is missing in GraphSource.edge_renderer.data_source\")\\n        if missing:\\n            return \" ,\".join(missing) + \" [%s]\" % self\\n\\n    layout_provider = Instance(LayoutProvider, help=\"\"\"\\n    An instance of a ``LayoutProvider`` that supplies the layout of the network\\n    graph in cartesian space.\\n    \"\"\")\\n\\n    node_renderer = Instance(GlyphRenderer, default=_DEFAULT_NODE_RENDERER, help=\"\"\"\\n    Instance of a ``GlyphRenderer`` containing an ``XYGlyph`` that will be rendered\\n    as the graph nodes.\\n    \"\"\")\\n\\n    edge_renderer = Instance(GlyphRenderer, default=_DEFAULT_EDGE_RENDERER, help=\"\"\"\\n    Instance of a ``GlyphRenderer`` containing an ``MultiLine`` Glyph that will be\\n    rendered as the graph edges.\\n    \"\"\")\\n\\n    selection_policy = Instance(GraphHitTestPolicy, default=lambda: NodesOnly(), help=\"\"\"\\n    An instance of a ``GraphHitTestPolicy`` that provides the logic for selection\\n    of graph components.\\n    \"\"\")\\n\\n    inspection_policy = Instance(GraphHitTestPolicy, default=lambda: NodesOnly(), help=\"\"\"\\n    An instance of a ``GraphHitTestPolicy`` that provides the logic for inspection\\n    of graph components.\\n    \"\"\")\\n\\n@abstract\\nclass GuideRenderer(Renderer):\\n    \\'\\'\\' A base class for all guide renderer types. ``GuideRenderer`` is\\n    not generally useful to instantiate on its own.\\n\\n    \\'\\'\\'\\n\\n    level = Override(default=\"guide\")\\n\\n#-----------------------------------------------------------------------------\\n# Private API\\n#-----------------------------------------------------------------------------\\n\\n#-----------------------------------------------------------------------------\\n# Code\\n#-----------------------------------------------------------------------------\\n',\n",
       " \"# model settings\\ntemperature = 0.2\\nwith_norm = True\\nquery_dim = 128\\nmodel = dict(\\n    type='SimSiamBaseTracker',\\n    backbone=dict(\\n        type='ResNet',\\n        pretrained=None,\\n        depth=18,\\n        out_indices=(3, ),\\n        # strides=(1, 2, 1, 1),\\n        norm_cfg=dict(type='SyncBN', requires_grad=True),\\n        norm_eval=False,\\n        zero_init_residual=True),\\n    # cls_head=None,\\n    # patch_head=None,\\n    img_head=dict(\\n        type='SimSiamHead',\\n        in_channels=512,\\n        norm_cfg=dict(type='SyncBN'),\\n        num_projection_fcs=3,\\n        projection_mid_channels=512,\\n        projection_out_channels=512,\\n        num_predictor_fcs=2,\\n        predictor_mid_channels=128,\\n        predictor_out_channels=512,\\n        with_norm=True,\\n        loss_feat=dict(type='CosineSimLoss', negative=False),\\n        spatial_type='avg'))\\n# model training and testing settings\\ntrain_cfg = dict(intra_video=False, transpose_temporal=True)\\ntest_cfg = dict(\\n    precede_frames=20,\\n    topk=10,\\n    temperature=0.2,\\n    strides=(1, 2, 1, 1),\\n    out_indices=(2, 3),\\n    neighbor_range=24,\\n    with_first=True,\\n    with_first_neighbor=True,\\n    output_dir='eval_results')\\n# dataset settings\\ndataset_type = 'VideoDataset'\\ndataset_type_val = 'DavisDataset'\\ndata_prefix = 'data/kinetics400/videos_train'\\nann_file_train = 'data/kinetics400/kinetics400_train_list_videos.txt'\\ndata_prefix_val = 'data/davis/DAVIS/JPEGImages/480p'\\nanno_prefix_val = 'data/davis/DAVIS/Annotations/480p'\\ndata_root_val = 'data/davis/DAVIS'\\nann_file_val = 'data/davis/DAVIS/ImageSets/davis2017_val_list_rawframes.txt'\\nimg_norm_cfg = dict(\\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\\ntrain_pipeline = [\\n    dict(type='DecordInit'),\\n    dict(type='SampleFrames', clip_len=2, frame_interval=8, num_clips=1),\\n    # dict(type='DuplicateFrames', times=2),\\n    dict(type='DecordDecode'),\\n    dict(\\n        type='RandomResizedCrop',\\n        area_range=(0.2, 1.),\\n        same_across_clip=False,\\n        same_on_clip=False),\\n    dict(type='Resize', scale=(224, 224), keep_ratio=False),\\n    dict(\\n        type='Flip',\\n        flip_ratio=0.5,\\n        same_across_clip=False,\\n        same_on_clip=False),\\n    # dict(\\n    #     type='ColorJitter',\\n    #     brightness=0.4,\\n    #     contrast=0.4,\\n    #     saturation=0.4,\\n    #     hue=0.1,\\n    #     p=0.8,\\n    #     same_across_clip=False,\\n    #     same_on_clip=False),\\n    # dict(\\n    #     type='RandomGrayScale',\\n    #     p=0.2,\\n    #     same_across_clip=False,\\n    #     same_on_clip=False),\\n    # dict(\\n    #     type='RandomGaussianBlur',\\n    #     p=0.5,\\n    #     same_across_clip=False,\\n    #     same_on_clip=False),\\n    dict(type='Normalize', **img_norm_cfg),\\n    dict(type='FormatShape', input_format='NCTHW'),\\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\\n    dict(type='ToTensor', keys=['imgs', 'label'])\\n]\\nval_pipeline = [\\n    dict(type='SequentialSampleFrames', frame_interval=1),\\n    dict(type='RawFrameDecode'),\\n    dict(type='Resize', scale=(-1, 480), keep_ratio=True),\\n    dict(type='Flip', flip_ratio=0),\\n    dict(type='Normalize', **img_norm_cfg),\\n    dict(type='FormatShape', input_format='NCTHW'),\\n    dict(\\n        type='Collect',\\n        keys=['imgs', 'ref_seg_map'],\\n        meta_keys=('frame_dir', 'frame_inds', 'original_shape', 'seg_map')),\\n    dict(type='ToTensor', keys=['imgs', 'ref_seg_map'])\\n]\\ndata = dict(\\n    videos_per_gpu=128,\\n    workers_per_gpu=16,\\n    val_workers_per_gpu=1,\\n    train=dict(\\n        type='RepeatDataset',\\n        times=5,\\n        dataset=dict(\\n            type=dataset_type,\\n            ann_file=ann_file_train,\\n            data_prefix=data_prefix,\\n            pipeline=train_pipeline)),\\n    val=dict(\\n        type=dataset_type_val,\\n        ann_file=ann_file_val,\\n        data_prefix=data_prefix_val,\\n        data_root=data_root_val,\\n        anno_prefix=anno_prefix_val,\\n        pipeline=val_pipeline,\\n        test_mode=True),\\n    test=dict(\\n        type=dataset_type_val,\\n        ann_file=ann_file_val,\\n        data_prefix=data_prefix_val,\\n        data_root=data_root_val,\\n        anno_prefix=anno_prefix_val,\\n        pipeline=val_pipeline,\\n        test_mode=True))\\n# optimizer\\n# optimizer = dict(type='Adam', lr=1e-4)\\noptimizer = dict(type='SGD', lr=0.05, momentum=0.9, weight_decay=0.0001)\\noptimizer_config = dict(grad_clip=None)\\n# learning policy\\nlr_config = dict(policy='CosineAnnealing', min_lr=0, by_epoch=False)\\n# lr_config = dict(policy='Fixed')\\n# lr_config = dict(\\n#     policy='step',\\n#     warmup='linear',\\n#     warmup_iters=100,\\n#     warmup_ratio=0.001,\\n#     step=[1, 2])\\ntotal_epochs = 100\\ncheckpoint_config = dict(interval=1)\\nevaluation = dict(\\n    interval=1,\\n    metrics='davis',\\n    key_indicator='feat_1.J&F-Mean',\\n    rule='greater')\\nlog_config = dict(\\n    interval=50,\\n    hooks=[\\n        dict(type='TextLoggerHook'),\\n        # dict(type='TensorboardLoggerHook'),\\n        dict(\\n            type='WandbLoggerHook',\\n            init_kwargs=dict(\\n                project='mmaction2',\\n                name='{{fileBasenameNoExtension}}',\\n                resume=True,\\n                tags=['ssb'],\\n                dir='wandb/{{fileBasenameNoExtension}}',\\n                config=dict(\\n                    model=model,\\n                    train_cfg=train_cfg,\\n                    test_cfg=test_cfg,\\n                    data=data))),\\n    ])\\n# runtime settings\\ndist_params = dict(backend='nccl')\\nlog_level = 'INFO'\\nload_from = None\\nresume_from = None\\nworkflow = [('train', 1)]\\nfind_unused_parameters = False\\n\",\n",
       " \"import radiate\\nimport numpy as np\\nimport os\\n\\n# path to the sequence\\nroot_path = 'data/radiate/'\\nsequence_name = 'tiny_foggy'\\n\\n# time (s) to retrieve next frame\\ndt = 0.25\\n\\n# load sequence\\nseq = radiate.Sequence(os.path.join(root_path, sequence_name))\\n\\n# play sequence\\nfor t in np.arange(seq.init_timestamp, seq.end_timestamp, dt):\\n    output = seq.get_from_timestamp(t)\\n    seq.vis_all(output, 0)\",\n",
       " '\"\"\"The :mod:`sklearn.kernel_regressor` module implements the Kernel Regressor.\\n\"\"\"\\n# Author: Jan Hendrik Metzen <janmetzen@mailbox.de>\\n#\\n# License: BSD 3 clause\\n\\nimport numpy as np\\n\\nfrom sklearn.metrics.pairwise import pairwise_kernels\\nfrom sklearn.base import BaseEstimator, RegressorMixin\\nimport gc\\n\\nclass KernelRegression(BaseEstimator, RegressorMixin):\\n    \"\"\"Nadaraya-Watson kernel regression with automatic bandwidth selection.\\n\\n    This implements Nadaraya-Watson kernel regression with (optional) automatic\\n    bandwith selection of the kernel via leave-one-out cross-validation. Kernel\\n    regression is a simple non-parametric kernelized technique for learning\\n    a non-linear relationship between input variable(s) and a target variable.\\n\\n    Parameters\\n    ----------\\n    kernel : string or callable, default=\"rbf\"\\n        Kernel map to be approximated. A callable should accept two arguments\\n        and the keyword arguments passed to this object as kernel_params, and\\n        should return a floating point number.\\n\\n    gamma : float, default=None\\n        Gamma parameter for the RBF (\"bandwidth\"), polynomial,\\n        exponential chi2 and sigmoid kernels. Interpretation of the default\\n        value is left to the kernel; see the documentation for\\n        sklearn.metrics.pairwise. Ignored by other kernels. If a sequence of\\n        values is given, one of these values is selected which minimizes\\n        the mean-squared-error of leave-one-out cross-validation.\\n\\n    See also\\n    --------\\n\\n    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\\n    \"\"\"\\n\\n    def __init__(self, kernel=\"rbf\", gamma=None):\\n        self.kernel = kernel\\n        self.gamma = gamma\\n\\n    def fit(self, X, y):\\n        \"\"\"Fit the model\\n\\n        Parameters\\n        ----------\\n        X : array-like of shape = [n_samples, n_features]\\n            The training input samples.\\n\\n        y : array-like, shape = [n_samples]\\n            The target values\\n\\n        Returns\\n        -------\\n        self : object\\n            Returns self.\\n        \"\"\"\\n        self.X = X\\n        self.y = y\\n\\n        if hasattr(self.gamma, \"__iter__\"):\\n            self.gamma = self._optimize_gamma(self.gamma)\\n\\n        return self\\n\\n    def predict(self, X):\\n        \"\"\"Predict target values for X.\\n\\n        Parameters\\n        ----------\\n        X : array-like of shape = [n_samples, n_features]\\n            The input samples.\\n\\n        Returns\\n        -------\\n        y : array of shape = [n_samples]\\n            The predicted target value.\\n        \"\"\"\\n        K = pairwise_kernels(self.X, X, metric=self.kernel, gamma=self.gamma)\\n        try:\\n            ret = (K * self.y[:, None]).sum(axis=0) / K.sum(axis=0)\\n        except MemoryError:\\n            gc.collect()  # gc and retry\\n            ret = (K * self.y[:, None]).sum(axis=0) / K.sum(axis=0)\\n\\n        return ret\\n\\n    def _optimize_gamma(self, gamma_values):\\n        # Select specific value of gamma from the range of given gamma_values\\n        # by minimizing mean-squared error in leave-one-out cross validation\\n        mse = np.empty_like(gamma_values, dtype=np.float)\\n        for i, gamma in enumerate(gamma_values):\\n            K = pairwise_kernels(self.X, self.X, metric=self.kernel,\\n                                 gamma=gamma)\\n            np.fill_diagonal(K, 0)  # leave-one-out\\n            Ky = K * self.y[:, np.newaxis]\\n            y_pred = Ky.sum(axis=0) / K.sum(axis=0)\\n            mse[i] = ((y_pred - self.y) ** 2).mean()\\n\\n        return gamma_values[np.nanargmin(mse)]\\n',\n",
       " '#!/usr/bin/env python\\n\\n# summary: physical touchscreen buttons listener\\n\\n# deps: sudo apt-get install python-dev python-rpi.gpio\\n\\nimport RPi.GPIO as GPIO\\nfrom time import sleep\\nimport signal, os, subprocess, sys\\n\\nbuttons = [24, 23, 18]\\n\\n\\ndef button_pressed(channel):\\n    if channel == 23:\\n        print(\"[buttons.python] MIDDLE: restart all processes...\")\\n        subprocess.Popen([\\'/bin/sh\\', \\'/home/pi/pdac/AUTOSTART.sh\\'])\\n    elif channel == 24:\\n        print(\"[buttons.python] BOTTOM: toggle browser...\")\\n        subprocess.Popen([\\'/bin/sh\\', \\'/home/pi/pdac/system/utilityToggleBrowser.sh\\'])\\ndef unregister_events():\\n    for pin in buttons:\\n        GPIO.remove_event_detect(pin)\\n\\nif __name__ == \\'__main__\\':\\n    signal.signal(signal.SIGINT, unregister_events)\\n    try:\\n        GPIO.setmode(GPIO.BCM)\\n        for pin in buttons:\\n            GPIO.setup(pin, GPIO.IN, pull_up_down=GPIO.PUD_UP)\\n            GPIO.add_event_detect(pin, GPIO.RISING, callback=button_pressed, bouncetime=200)\\n        while True:\\n            sleep(10)\\n    except Exception as e:\\n        print(\"Caught exception:\", e)\\n        unregister_events()',\n",
       " \"# Generated by Django 3.1.13 on 2021-09-19 08:36\\n\\nfrom django.db import migrations, models\\nimport django.db.models.deletion\\nimport taggit.managers\\n\\n\\nclass Migration(migrations.Migration):\\n\\n    initial = True\\n\\n    dependencies = [\\n        ('wagtailimages', '0023_add_choose_permissions'),\\n        ('taggit', '0003_taggeditem_add_unique_index'),\\n    ]\\n\\n    operations = [\\n        migrations.CreateModel(\\n            name='Volunteer',\\n            fields=[\\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\\n                ('name', models.CharField(max_length=255, verbose_name='name')),\\n                ('email', models.EmailField(blank=True, max_length=254, null=True, verbose_name='email')),\\n                ('bio', models.TextField(verbose_name='biography')),\\n                ('affiliation', models.CharField(max_length=128, verbose_name='Affiliation')),\\n                ('website', models.URLField(blank=True, verbose_name='Website')),\\n                ('twitter_profile', models.URLField(blank=True, null=True, verbose_name='Twitter Profile')),\\n                ('linkedin_profile', models.URLField(blank=True, null=True, verbose_name='LinkedIn Profile')),\\n                ('orcid_profile', models.URLField(blank=True, null=True, verbose_name='OrcID Link')),\\n                ('creation_date', models.DateTimeField(auto_now_add=True)),\\n                ('last_updated', models.DateTimeField(auto_now_add=True)),\\n                ('active_since', models.DateTimeField(default=None, null=True)),\\n                ('areas_expertise', taggit.managers.TaggableManager(help_text='A comma-separated list of tags.', through='taggit.TaggedItem', to='taggit.Tag', verbose_name='areas of expertise')),\\n                ('picture', models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='+', to='wagtailimages.image')),\\n            ],\\n            options={\\n                'ordering': ['name'],\\n            },\\n        ),\\n    ]\\n\",\n",
       " '#! /usr/bin/env python\\nfrom __future__ import print_function\\n\\nimport pytest\\nimport sys\\nimport os\\nimport subprocess\\n\\n\\nPYTEST_ARGS = {\\n    \\'default\\': [\\'tests\\'],\\n    \\'fast\\': [\\'tests\\', \\'-q\\'],\\n}\\n\\nFLAKE8_ARGS = [\\'rest_framework_tracking\\', \\'tests\\', \\'--ignore=E501\\']\\n\\n\\nsys.path.append(os.path.dirname(__file__))\\n\\n\\ndef exit_on_failure(ret, message=None):\\n    if ret:\\n        sys.exit(ret)\\n\\n\\ndef flake8_main(args):\\n    print(\\'Running flake8 code linting\\')\\n    ret = subprocess.call([\\'flake8\\'] + args)\\n    print(\\'flake8 failed\\' if ret else \\'flake8 passed\\')\\n    return ret\\n\\n\\ndef split_class_and_function(string):\\n    class_string, function_string = string.split(\\'.\\', 1)\\n    return \"%s and %s\" % (class_string, function_string)\\n\\n\\ndef is_function(string):\\n    # `True` if it looks like a test function is included in the string.\\n    return string.startswith(\\'test_\\') or \\'.test_\\' in string\\n\\n\\ndef is_class(string):\\n    # `True` if first character is uppercase - assume it\\'s a class name.\\n    return string[0] == string[0].upper()\\n\\n\\nif __name__ == \"__main__\":\\n    try:\\n        sys.argv.remove(\\'--nolint\\')\\n    except ValueError:\\n        run_flake8 = True\\n    else:\\n        run_flake8 = False\\n\\n    try:\\n        sys.argv.remove(\\'--lintonly\\')\\n    except ValueError:\\n        run_tests = True\\n    else:\\n        run_tests = False\\n\\n    try:\\n        sys.argv.remove(\\'--fast\\')\\n    except ValueError:\\n        style = \\'default\\'\\n    else:\\n        style = \\'fast\\'\\n        run_flake8 = False\\n\\n    if len(sys.argv) > 1:\\n        pytest_args = sys.argv[1:]\\n        first_arg = pytest_args[0]\\n        if first_arg.startswith(\\'-\\'):\\n            # `runtests.py [flags]`\\n            pytest_args = [\\'tests\\'] + pytest_args\\n        elif is_class(first_arg) and is_function(first_arg):\\n            # `runtests.py TestCase.test_function [flags]`\\n            expression = split_class_and_function(first_arg)\\n            pytest_args = [\\'tests\\', \\'-k\\', expression] + pytest_args[1:]\\n        elif is_class(first_arg) or is_function(first_arg):\\n            # `runtests.py TestCase [flags]`\\n            # `runtests.py test_function [flags]`\\n            pytest_args = [\\'tests\\', \\'-k\\', pytest_args[0]] + pytest_args[1:]\\n    else:\\n        pytest_args = PYTEST_ARGS[style]\\n\\n    if run_tests:\\n        exit_on_failure(pytest.main(pytest_args))\\n        # ipdb support: comment the previous line and uncomment the nextone.\\n        # pytest.main(pytest_args + [\\'-s\\'])\\n    if run_flake8:\\n        exit_on_failure(flake8_main(FLAKE8_ARGS))\\n',\n",
       " '#!/usr/bin/env python3\\n# Copyright (c) 2014-2017 The Bitcoin Core developers\\n# Distributed under the MIT software license, see the accompanying\\n# file COPYING or http://www.opensource.org/licenses/mit-license.php.\\n\"\"\"Test running bitcoind with -reindex and -reindex-chainstate options.\\n\\n- Start a single node and generate 3 blocks.\\n- Stop the node and restart it with -reindex. Verify that the node has reindexed up to block 3.\\n- Stop the node and restart it with -reindex-chainstate. Verify that the node has reindexed up to block 3.\\n\"\"\"\\n\\nfrom test_framework.test_framework import UnoTestFramework\\nfrom test_framework.util import assert_equal\\nimport time\\n\\nclass ReindexTest(UnoTestFramework):\\n\\n    def set_test_params(self):\\n        self.setup_clean_chain = True\\n        self.num_nodes = 1\\n\\n    def reindex(self):\\n        self.nodes[0].generate(3)\\n        blockcount = self.nodes[0].getblockcount()\\n        self.stop_nodes()\\n        extra_args = [[\"-reindex\", \"-checkblockindex=1\"]]\\n        self.start_nodes(extra_args)\\n        assert_equal(self.nodes[0].getblockcount(), blockcount)  # start_node is blocking on reindex\\n        self.log.info(\"Success\")\\n\\n    def run_test(self):\\n        self.reindex()\\n\\nif __name__ == \\'__main__\\':\\n    ReindexTest().main()\\n',\n",
       " '# bottom-up dp: traverses iteratively starting from the smallest subset (bottom) going up\\n# ex: fib(1), fib(2), fib(3), fib(4), ... , fib(n)\\ndef knapsack_bottom_up_dp(weights, values, W):\\n    # generating array for storing optimal values\\n    n = len(weights)\\n    opt_vals = [[0 for _ in range(W + 1)] for _ in range(n + 1)]\\n    \\n    # computing possible optimal values\\n    for i in range(1, n + 1):\\n        for w in range(1, W + 1):\\n            wi = weights[i - 1]\\n            # if weight of the current item is greater than the current weight\\n            # take the previous optimal value from previous top slot (i - 1)\\n            if wi > w:\\n                opt_vals[i][w] = opt_vals[i - 1][w]\\n            # otherwise, take the maximum between:\\n            # putting the current item into the knapsack or not\\n            else:\\n                opt_vals[i][w] = max(values[i - 1] + opt_vals[i - 1][w - wi], \\n                                     opt_vals[i - 1][w])\\n\\n    # backtracking\\n    opt_subset = backtrack(n, W, weights, values, opt_vals)\\n\\n    # for i in opt_vals: print(i)\\n    return (opt_vals[-1][-1], opt_subset)\\n\\n# top-down: recursively computes values starting from the biggest (top) going down\\n# ex: fib(n), fib(n-1), fib(n-2), ... , fib(1)\\ndef knapsack_top_down_dp(weights, values, W):\\n    # generating array for storing optimal values with 0 in edges and -1 elsewhere\\n    n = len(weights)\\n    opt_vals = [[0 for _ in range(W + 1)]]\\n    [opt_vals.append([0 if j == 0 else -1 for j in range(W + 1)]) for _ in range(n)]\\n\\n    # run recursion\\n    max_val = helper(weights, values, opt_vals, n, W)\\n\\n    # backtracking\\n    opt_subset = backtrack(n, W, weights, values, opt_vals)\\n\\n    # for i in opt_vals: print(i)\\n    return (max_val, opt_subset)\\n  \\ndef helper(weights, values, opt_vals, i, w):\\n    # base case\\n    if opt_vals[i][w] >= 0:\\n        return opt_vals[i][w]\\n\\n    # skip the item if the wieght of item is bigger than the remaining weight in the knapsack\\n    if weights[i - 1] > w :\\n        max_val = helper(weights, values, opt_vals, i - 1, w)\\n    # otherwise, recursively compute maximum between picking the item or not picking the item\\n    else:\\n        max_val = max(values[i - 1] + helper(weights, values, opt_vals, i - 1, w - weights[i - 1]),\\n                      helper(weights, values, opt_vals, i - 1, w))\\n    \\n    # memorize the computed maximum value\\n    opt_vals[i][w] = max_val\\n    return max_val\\n\\ndef backtrack(n, W, weights, values, opt_vals):\\n    opt_subset = [0 for i in range(n)]\\n    i, w = n, W\\n    while i > 0 and w > 0:\\n        wi = weights[i - 1]\\n        if w - wi >= 0 and opt_vals[i][w] == values[i - 1] + opt_vals[i - 1][w - wi]:\\n            opt_subset[i - 1] = 1\\n            w -= wi\\n        i -= 1 \\n    return opt_subset\\n\\n# brute force: generate all possible 2^n variants and determine the maximum optimal value\\n# brute force: without bit manipulation\\nimport itertools\\ndef knapsack_brute_force(weights, values, W):\\n    # initializing length, maximum total value and optimal subset of selected items\\n    n, max_val, opt_subset = len(weights), 0, []\\n\\n    # creating a generator, that traveses through all possible combinations of selecting n items\\n    combinations = map(list, itertools.product([0, 1], repeat=n))\\n\\n    # iterating over all combinations\\n    for cmb in combinations:\\n        # calcualting total weight and total value for current combination\\n        tot_weights = sum([a*b for a,b in zip(weights, cmb)])\\n        tot_values = sum([a*b for a,b in zip(values, cmb)])\\n\\n        # updating maximum total value and optimal subset to current \\n        if tot_weights <= W and tot_values > max_val:\\n            max_val = tot_values\\n            opt_subset = cmb\\n    \\n    return (max_val, opt_subset)\\n\\n# brute force: with bit manipulation\\ndef knapsack_brute_force_bm(weights, values, W):\\n    # initializing length, maximum total value and optimal subset of selected items\\n    n, max_val = len(weights), 0\\n    opt_subset = [0]*n\\n    i, m = 1, 2**n\\n\\n    # iterating over all combinations\\n    while i < m:\\n        j, tot_weights, tot_values, cur = i, 0, 0, 0\\n        cur_subset = [0]*n\\n        while j:\\n            if j & 1:\\n                tot_weights += weights[cur]\\n                tot_values += values[cur]\\n                cur_subset[cur] = 1\\n            j >>= 1\\n            cur += 1\\n        i+=1\\n        \\n        # updating maximum total value and optimal subset to current \\n        if tot_weights <= W and tot_values > max_val:\\n            max_val = tot_values\\n            opt_subset = cur_subset\\n    \\n    return (max_val, opt_subset)\\n\\n# correctness testing\\ndef corr_test():\\n    functions = [\\n        (\"BOTTOM UP:\", knapsack_bottom_up_dp), \\n        (\"TOP DOWN:\", knapsack_top_down_dp),\\n        (\"BRUTE FORCE:\", knapsack_brute_force),\\n        (\"BRUTE FORCE (bit manip.):\", knapsack_brute_force_bm)\\n    ]\\n\\n    # source of the test cases: http://people.sc.fsu.edu/~jburkardt/datasets/knapsack_01/knapsack_01.html\\n    test_cases = [\\n        [([2,2,3], [2,3,4], 6), [0, 1, 1]],\\n        [([2,2,3], [7,2,1], 6), [1, 1, 0]],\\n        [([23,31,29,44,53,38,63,85,89,82], [92,57,49,68,60,43,67,84,87,72], 165), [1,1,1,1,0,1,0,0,0,0]],\\n        [([12,7,11,8,9], [24,13,23,15,16], 26), [0,1,1,1,0]],\\n        [([56,59,80,64,75,17], [50,50,64,46,50,5], 190), [1,1,0,0,1,0]],\\n        [([31,10,20,19,4,3,6], [70,20,39,37,7,5,10], 50), [1,0,0,1,0,0,0]],\\n        [([25,35,45,5,25,3,2,2], [350,400,450,20,70,8,5,5], 104), [1,0,1,1,1,0,1,1]],\\n        [([41,50,49,59,55,57,60], [442,525,511,593,546,564,617], 170), [0,1,0,1,0,0,1]]\\n    ]\\n    for fn in functions:\\n        for tc in test_cases:\\n            max_val, opt_subset = fn[1](*tc[0])\\n            is_correct = opt_subset == tc[1]\\n            print(fn[0], max_val)\\n            print(\"Correct:\", is_correct)\\n            print(\"Output:\", opt_subset)\\n            print(\"Answer:\", tc[1])\\n\\nimport random\\nimport time\\nimport numpy as np\\nimport pandas as pd\\n\\ndef main():\\n    # Brute force vs. DP bottom-up vs. DP top-down\\n    test(*get_inputs_BF_vs_DPbu_vs_DPtd())\\n\\n    # DP bottom-up vs. DP top-down\\n    test(*get_inputs_DPbu_vs_DPtd())\\n    \\n# generate inputs for testing DP bottom-up vs. DP top-down\\ndef get_inputs_BF_vs_DPbu_vs_DPtd():\\n    # N = np.arange(1, 26, 1)         #[1, 2, ..., 25]\\n    # K = np.arange(0.2, 1.1, 0.2)    #[0.1, 0.2, ..., 1]\\n    # wi_vi_pow = np.arange(3, 10, 2)  #[3, 5, 7, 9]\\n    N = np.arange(1, 3, 1)         #[1, 2, ..., 25]\\n    K = np.arange(0.2, 0.3, 0.2)    #[0.1, 0.2, ..., 1]\\n    wi_vi_pow = np.arange(3, 4, 2)  #[3, 5, 7, 9]\\n    algorithms = [\\n        (\"Brute force\", knapsack_brute_force), \\n        (\"DP bottom-up\",knapsack_bottom_up_dp),\\n        (\"DP top-down\", knapsack_top_down_dp)\\n    ]\\n    filename = \"DP bottom-up vs. DP top-down\"\\n    return (N, K, wi_vi_pow, algorithms, filename)\\n\\n# generate inputs for testing DP bottom-up vs. DP top-down\\ndef get_inputs_DPbu_vs_DPtd():\\n    # N = [2**i for i in range(0,32)]\\n    # K = np.arange(0.2, 1.1, 0.2)\\n    # wi_vi_pow = np.arange(3, 10, 2)\\n    N = [2**i for i in range(0,2)]\\n    K = np.arange(0.2, 0.3, 0.2)\\n    wi_vi_pow = np.arange(3, 4, 2)\\n    algorithms = [\\n        (\"DP bottom-up\", knapsack_bottom_up_dp),\\n        (\"DP top-down\", knapsack_top_down_dp)\\n    ]\\n    filename = \"DP bottom-up vs. DP top-down\"\\n    return (N, K, wi_vi_pow, algorithms, filename)\\n    \\n# full testing\\ndef test(N, K, wi_vi_pow, algorithms, filename):\\n    # arrays to store columns of the output table\\n    runtimes = [[] for _ in algorithms]\\n    n_arr = []\\n    W_arr = []\\n    wi_vi_range_arr = []\\n\\n    # run over all combinations of the inputs\\n    # different number of input items \\n    for ni in N:\\n        # different range of weights and values (ni = n) => 1,2,3,...,n\\n        for wi_vi in wi_vi_pow:\\n            # generate weights and values and compute sum of weights \\n            # (range = (1, 10^wi_vi)) => (1, 10^3),...,(1, 10^m)\\n            weights = np.random.randint(10**wi_vi, size=ni)\\n            values = np.random.randint(10**wi_vi, size=ni)\\n            sum_weights = sum(weights)\\n            # different capacity of the knapsack \\n            # (W = sum(weights) * ki) => W*1,W*0.8,...,W*0.2\\n            for ki in K:\\n                # add inputs values into the table columns           \\n                n_arr.append(ni)\\n                W_arr.append(int(sum_weights * ki))\\n                wi_vi_range_arr.append(10**wi_vi)\\n                # run algorithms and time performance\\n                print(\"Inputs: n={}, W={}\".format(ni, W_arr[-1]))\\n                for i in range(len(algorithms)):\\n                    print(\"Running: {} with wi_vi_range: 1-{}\".format(algorithms[i][0], wi_vi_range_arr[-1]))\\n                    start = time.clock()\\n                    algorithms[i][1](weights, values, int(sum_weights * ki))\\n                    end = time.clock()\\n                    runtimes[i].append(end - start)\\n        # save table as csv            \\n        save(runtimes, algorithms, n_arr, W_arr, wi_vi_range_arr, filename)\\n    # save table as csv    \\n    save(runtimes, algorithms, n_arr, W_arr, wi_vi_range_arr, filename)                \\n\\n# save table as csv\\ndef save(runtimes, algorithms, n_arr, W_arr, wi_vi_range_arr, filename):\\n    total_runtime = sum([sum(rn) for rn in runtimes])\\n    print(\"Saving to csv\\\\nTotal runtime: {}\".format(total_runtime))\\n    df_algorithms = pd.concat([pd.Series(rn) for rn in runtimes], keys=[alg[0] for alg in algorithms], axis=1)\\n    df_inputs = pd.DataFrame({\"n\": n_arr, \"W\": W_arr, \"wi, vi range\": wi_vi_range_arr})\\n    df_concat = pd.concat([df_algorithms, df_inputs], axis = 1)\\n    df_concat.to_csv(filename+\".csv\")\\n\\nif __name__ == \"__main__\":\\n    main()',\n",
       " '# Author    : Andrzej Wojciechowski (AAWO)\\n# Copyright : Andrzej Wojciechowski (AAWO)\\n# --------------------------------------------\\nfrom sys import argv, stdout\\nfrom random import randrange\\n\\nif len(argv) == 3:\\n   stdout.write(str(randrange(int(argv[1]), int(argv[2])+1)))\\nelif len(argv) == 4:\\n   stdout.write(str(randrange(int(argv[1]), int(argv[2])+1, int(argv[3]))))\\nelse:\\n   argv_num = (len(argv)-1)\\n   raise TypeError(\"Wrong number of arguments. Expected 2 or 3 - received %d\" % argv_num)\\n',\n",
       " '#!/usr/bin/env python2\\n# -*- coding: utf-8 -*-\\n##################################################\\n# GNU Radio Python Flow Graph\\n# Title: IP3\\n# Author: Alexandros-Apostolos A. Boulogeorgos\\n# Generated: Mon Aug 12 09:13:37 2019\\n##################################################\\n\\nif __name__ == \\'__main__\\':\\n    import ctypes\\n    import sys\\n    if sys.platform.startswith(\\'linux\\'):\\n        try:\\n            x11 = ctypes.cdll.LoadLibrary(\\'libX11.so\\')\\n            x11.XInitThreads()\\n        except:\\n            print \"Warning: failed to XInitThreads()\"\\n\\nfrom PyQt4 import Qt\\nfrom gnuradio import analog\\nfrom gnuradio import blocks\\nfrom gnuradio import channels\\nfrom gnuradio import eng_notation\\nfrom gnuradio import gr\\nfrom gnuradio import qtgui\\nfrom gnuradio.eng_option import eng_option\\nfrom gnuradio.filter import firdes\\nfrom gnuradio.qtgui import Range, RangeWidget\\nfrom optparse import OptionParser\\nimport math\\nimport sip\\nimport sys\\n\\n\\nclass amplifiers_nonlinearities(gr.top_block, Qt.QWidget):\\n\\n    def __init__(self):\\n        gr.top_block.__init__(self, \"IP3\")\\n        Qt.QWidget.__init__(self)\\n        self.setWindowTitle(\"IP3\")\\n        try:\\n            self.setWindowIcon(Qt.QIcon.fromTheme(\\'gnuradio-grc\\'))\\n        except:\\n            pass\\n        self.top_scroll_layout = Qt.QVBoxLayout()\\n        self.setLayout(self.top_scroll_layout)\\n        self.top_scroll = Qt.QScrollArea()\\n        self.top_scroll.setFrameStyle(Qt.QFrame.NoFrame)\\n        self.top_scroll_layout.addWidget(self.top_scroll)\\n        self.top_scroll.setWidgetResizable(True)\\n        self.top_widget = Qt.QWidget()\\n        self.top_scroll.setWidget(self.top_widget)\\n        self.top_layout = Qt.QVBoxLayout(self.top_widget)\\n        self.top_grid_layout = Qt.QGridLayout()\\n        self.top_layout.addLayout(self.top_grid_layout)\\n\\n        self.settings = Qt.QSettings(\"GNU Radio\", \"amplifiers_nonlinearities\")\\n        self.restoreGeometry(self.settings.value(\"geometry\").toByteArray())\\n\\n        ##################################################\\n        # Variables\\n        ##################################################\\n        self.samp_rate = samp_rate = 100000\\n        self.signal_amp = signal_amp = 0\\n        self.sigfreq = sigfreq = samp_rate*1.0247385/21.0\\n        self.ip3 = ip3 = 0\\n\\n        ##################################################\\n        # Blocks\\n        ##################################################\\n        self._signal_amp_range = Range(-150, 10, 5, 0, 200)\\n        self._signal_amp_win = RangeWidget(self._signal_amp_range, self.set_signal_amp, \\'Singal Power\\', \"counter_slider\", float)\\n        self.top_grid_layout.addWidget(self._signal_amp_win, 2,0,1,1)\\n        self._sigfreq_range = Range(0, samp_rate/2, 1000, samp_rate*1.0247385/21.0, 200)\\n        self._sigfreq_win = RangeWidget(self._sigfreq_range, self.set_sigfreq, \\'Signal Freq\\', \"counter_slider\", float)\\n        self.top_grid_layout.addWidget(self._sigfreq_win, 3,0,1,1)\\n        self._ip3_range = Range(0, 2, 0.01, 0, 200)\\n        self._ip3_win = RangeWidget(self._ip3_range, self.set_ip3, \\'IP3\\', \"counter_slider\", float)\\n        self.top_grid_layout.addWidget(self._ip3_win, 3,1,1,1)\\n        self.qtgui_freq_sink_x_0 = qtgui.freq_sink_c(\\n        \\t2048, #size\\n        \\tfirdes.WIN_FLATTOP, #wintype\\n        \\t0, #fc\\n        \\tsamp_rate, #bw\\n        \\t\\'\\', #name\\n        \\t2 #number of inputs\\n        )\\n        self.qtgui_freq_sink_x_0.set_update_time(0.10)\\n        self.qtgui_freq_sink_x_0.set_y_axis(-200, 0)\\n        self.qtgui_freq_sink_x_0.set_y_label(\\'Relative Gain\\', \\'dB\\')\\n        self.qtgui_freq_sink_x_0.set_trigger_mode(qtgui.TRIG_MODE_FREE, 0.0, 0, \"\")\\n        self.qtgui_freq_sink_x_0.enable_autoscale(False)\\n        self.qtgui_freq_sink_x_0.enable_grid(False)\\n        self.qtgui_freq_sink_x_0.set_fft_average(1.0)\\n        self.qtgui_freq_sink_x_0.enable_axis_labels(True)\\n        self.qtgui_freq_sink_x_0.enable_control_panel(False)\\n        \\n        if not True:\\n          self.qtgui_freq_sink_x_0.disable_legend()\\n        \\n        if \"complex\" == \"float\" or \"complex\" == \"msg_float\":\\n          self.qtgui_freq_sink_x_0.set_plot_pos_half(not True)\\n        \\n        labels = [\\'Input\\', \\'With IP3\\', \\'\\', \\'\\', \\'\\',\\n                  \\'\\', \\'\\', \\'\\', \\'\\', \\'\\']\\n        widths = [2, 2, 1, 1, 1,\\n                  1, 1, 1, 1, 1]\\n        colors = [\"blue\", \"red\", \"green\", \"black\", \"cyan\",\\n                  \"magenta\", \"yellow\", \"dark red\", \"dark green\", \"dark blue\"]\\n        alphas = [0.5, 0.5, 1.0, 1.0, 1.0,\\n                  1.0, 1.0, 1.0, 1.0, 1.0]\\n        for i in xrange(2):\\n            if len(labels[i]) == 0:\\n                self.qtgui_freq_sink_x_0.set_line_label(i, \"Data {0}\".format(i))\\n            else:\\n                self.qtgui_freq_sink_x_0.set_line_label(i, labels[i])\\n            self.qtgui_freq_sink_x_0.set_line_width(i, widths[i])\\n            self.qtgui_freq_sink_x_0.set_line_color(i, colors[i])\\n            self.qtgui_freq_sink_x_0.set_line_alpha(i, alphas[i])\\n        \\n        self._qtgui_freq_sink_x_0_win = sip.wrapinstance(self.qtgui_freq_sink_x_0.pyqwidget(), Qt.QWidget)\\n        self.top_grid_layout.addWidget(self._qtgui_freq_sink_x_0_win, 0,0,1,2)\\n        self.channels_distortion_3_gen_0 = channels.distortion_3_gen(ip3)\\n        self.blocks_throttle_0 = blocks.throttle(gr.sizeof_gr_complex*1, samp_rate,True)\\n        self.blocks_add_xx_0 = blocks.add_vcc(1)\\n        self.analog_sig_source_x_0_0 = analog.sig_source_c(samp_rate, analog.GR_COS_WAVE, 2.45*sigfreq, pow(10.0,signal_amp/20.0), 0)\\n        self.analog_sig_source_x_0 = analog.sig_source_c(samp_rate, analog.GR_COS_WAVE, sigfreq, 1, 0)\\n\\n        ##################################################\\n        # Connections\\n        ##################################################\\n        self.connect((self.analog_sig_source_x_0, 0), (self.blocks_add_xx_0, 1))    \\n        self.connect((self.analog_sig_source_x_0_0, 0), (self.blocks_add_xx_0, 0))    \\n        self.connect((self.blocks_add_xx_0, 0), (self.blocks_throttle_0, 0))    \\n        self.connect((self.blocks_throttle_0, 0), (self.channels_distortion_3_gen_0, 0))    \\n        self.connect((self.blocks_throttle_0, 0), (self.qtgui_freq_sink_x_0, 0))    \\n        self.connect((self.channels_distortion_3_gen_0, 0), (self.qtgui_freq_sink_x_0, 1))    \\n\\n    def closeEvent(self, event):\\n        self.settings = Qt.QSettings(\"GNU Radio\", \"amplifiers_nonlinearities\")\\n        self.settings.setValue(\"geometry\", self.saveGeometry())\\n        event.accept()\\n\\n    def get_samp_rate(self):\\n        return self.samp_rate\\n\\n    def set_samp_rate(self, samp_rate):\\n        self.samp_rate = samp_rate\\n        self.set_sigfreq(self.samp_rate*1.0247385/21.0)\\n        self.qtgui_freq_sink_x_0.set_frequency_range(0, self.samp_rate)\\n        self.blocks_throttle_0.set_sample_rate(self.samp_rate)\\n        self.analog_sig_source_x_0_0.set_sampling_freq(self.samp_rate)\\n        self.analog_sig_source_x_0.set_sampling_freq(self.samp_rate)\\n\\n    def get_signal_amp(self):\\n        return self.signal_amp\\n\\n    def set_signal_amp(self, signal_amp):\\n        self.signal_amp = signal_amp\\n        self.analog_sig_source_x_0_0.set_amplitude(pow(10.0,self.signal_amp/20.0))\\n\\n    def get_sigfreq(self):\\n        return self.sigfreq\\n\\n    def set_sigfreq(self, sigfreq):\\n        self.sigfreq = sigfreq\\n        self.analog_sig_source_x_0_0.set_frequency(2.45*self.sigfreq)\\n        self.analog_sig_source_x_0.set_frequency(self.sigfreq)\\n\\n    def get_ip3(self):\\n        return self.ip3\\n\\n    def set_ip3(self, ip3):\\n        self.ip3 = ip3\\n        self.channels_distortion_3_gen_0.set_beta(self.ip3)\\n\\n\\ndef main(top_block_cls=amplifiers_nonlinearities, options=None):\\n\\n    from distutils.version import StrictVersion\\n    if StrictVersion(Qt.qVersion()) >= StrictVersion(\"4.5.0\"):\\n        style = gr.prefs().get_string(\\'qtgui\\', \\'style\\', \\'raster\\')\\n        Qt.QApplication.setGraphicsSystem(style)\\n    qapp = Qt.QApplication(sys.argv)\\n\\n    tb = top_block_cls()\\n    tb.start()\\n    tb.show()\\n\\n    def quitting():\\n        tb.stop()\\n        tb.wait()\\n    qapp.connect(qapp, Qt.SIGNAL(\"aboutToQuit()\"), quitting)\\n    qapp.exec_()\\n\\n\\nif __name__ == \\'__main__\\':\\n    main()\\n',\n",
       " '#    ____  ____\\n#   /   /\\\\/   /\\n#  /___/  \\\\  /   Copyright (c) 2021, Xilinx®.\\n#  \\\\   \\\\   \\\\/    Author: Víctor Mayoral Vilches <victorma@xilinx.com>\\n#   \\\\   \\\\\\n#   /   /\\n#  /___/   /\\\\\\n#  \\\\   \\\\  /  \\\\\\n#   \\\\___\\\\/\\\\___\\\\\\n#\\n# Licensed under the Apache License, Version 2.0\\n#\\n__version__ = \"0.2.0\"\\n',\n",
       " '# -*- coding: utf-8 -*-\\n\\n\"\"\"Top-level package for Easy graphviz.\"\"\"\\n\\n__author__ = \"\"\"Gus Dunn\"\"\"\\n__email__ = \\'w.gus.dunn@gmail.com\\'\\n__version__ = \\'0.1.1\\'\\n',\n",
       " 'from django.apps import AppConfig\\r\\n\\r\\n\\r\\nclass LogicConfig(AppConfig):\\r\\n    default_auto_field = \"django.db.models.BigAutoField\"\\r\\n    name = \"Logic\"\\r\\n',\n",
       " '# Auto generated configuration file\\n# using: \\n# Revision: 1.19 \\n# Source: /local/reps/CMSSW/CMSSW/Configuration/Applications/python/ConfigBuilder.py,v \\n# with command line options: l1Ntuple -s RAW2DIGI --python_filename=mc_L1TReEmulMCFromRAW_L1NtupleEMU.py -n 2 --no_output --era=Run3 --mc --conditions=112X_mcRun3_2021_realistic_v13 --customise=L1Trigger/Configuration/customiseReEmul.L1TReEmulMCFromRAW --customise=L1Trigger/L1TNtuples/customiseL1Ntuple.L1NtupleEMU --customise=L1Trigger/Configuration/customiseSettings.L1TSettingsToCaloParams_2018_v1_4 --filein=/store/mc/Run3Winter20DRPremixMiniAOD/TT_TuneCP5_14TeV-powheg-pythia8/GEN-SIM-RAW/110X_mcRun3_2021_realistic_v6-v2/20000/CFCAE998-5A17-FB48-A36F-A31EA28D2A72.root\\nimport FWCore.ParameterSet.Config as cms\\n\\nfrom Configuration.Eras.Era_Run3_cff import Run3\\n\\nprocess = cms.Process(\\'RAW2DIGI\\',Run3)\\n\\n# import of standard configurations\\nprocess.load(\\'Configuration.StandardSequences.Services_cff\\')\\nprocess.load(\\'SimGeneral.HepPDTESSource.pythiapdt_cfi\\')\\nprocess.load(\\'FWCore.MessageService.MessageLogger_cfi\\')\\nprocess.load(\\'Configuration.EventContent.EventContent_cff\\')\\nprocess.load(\\'SimGeneral.MixingModule.mixNoPU_cfi\\')\\nprocess.load(\\'Configuration.StandardSequences.GeometryRecoDB_cff\\')\\nprocess.load(\\'Configuration.StandardSequences.MagneticField_cff\\')\\nprocess.load(\\'Configuration.StandardSequences.RawToDigi_cff\\')\\nprocess.load(\\'Configuration.StandardSequences.EndOfProcess_cff\\')\\nprocess.load(\\'Configuration.StandardSequences.FrontierConditions_GlobalTag_cff\\')\\n\\nprocess.maxEvents = cms.untracked.PSet(\\n    input = cms.untracked.int32(10),\\n    output = cms.optional.untracked.allowed(cms.int32,cms.PSet)\\n)\\n\\n# Input source\\nprocess.source = cms.Source(\"PoolSource\",\\n    fileNames = cms.untracked.vstring(\\'/store/mc/Run3Winter20DRPremixMiniAOD/TT_TuneCP5_14TeV-powheg-pythia8/GEN-SIM-RAW/110X_mcRun3_2021_realistic_v6-v2/20000/CFCAE998-5A17-FB48-A36F-A31EA28D2A72.root\\'),\\n    secondaryFileNames = cms.untracked.vstring()\\n)\\n\\nprocess.options = cms.untracked.PSet(\\n    FailPath = cms.untracked.vstring(),\\n    IgnoreCompletely = cms.untracked.vstring(),\\n    Rethrow = cms.untracked.vstring(),\\n    SkipEvent = cms.untracked.vstring(),\\n    allowUnscheduled = cms.obsolete.untracked.bool,\\n    canDeleteEarly = cms.untracked.vstring(),\\n    emptyRunLumiMode = cms.obsolete.untracked.string,\\n    eventSetup = cms.untracked.PSet(\\n        forceNumberOfConcurrentIOVs = cms.untracked.PSet(\\n            allowAnyLabel_=cms.required.untracked.uint32\\n        ),\\n        numberOfConcurrentIOVs = cms.untracked.uint32(1)\\n    ),\\n    fileMode = cms.untracked.string(\\'FULLMERGE\\'),\\n    forceEventSetupCacheClearOnNewRun = cms.untracked.bool(False),\\n    makeTriggerResults = cms.obsolete.untracked.bool,\\n    numberOfConcurrentLuminosityBlocks = cms.untracked.uint32(1),\\n    numberOfConcurrentRuns = cms.untracked.uint32(1),\\n    numberOfStreams = cms.untracked.uint32(0),\\n    numberOfThreads = cms.untracked.uint32(1),\\n    printDependencies = cms.untracked.bool(False),\\n    sizeOfStackForThreadsInKB = cms.optional.untracked.uint32,\\n    throwIfIllegalParameter = cms.untracked.bool(True),\\n    wantSummary = cms.untracked.bool(False)\\n)\\n\\n# Production Info\\nprocess.configurationMetadata = cms.untracked.PSet(\\n    annotation = cms.untracked.string(\\'l1Ntuple nevts:2\\'),\\n    name = cms.untracked.string(\\'Applications\\'),\\n    version = cms.untracked.string(\\'$Revision: 1.19 $\\')\\n)\\n\\n# Output definition\\n\\n# Additional output definition\\n\\n# Other statements\\nfrom Configuration.AlCa.GlobalTag import GlobalTag\\nprocess.GlobalTag = GlobalTag(process.GlobalTag, \\'112X_mcRun3_2021_realistic_v13\\', \\'\\')\\n\\nprocess.GlobalTag.toGet = cms.VPSet(\\n        cms.PSet(record = cms.string(\"GEMeMapRcd\"),\\n                       tag = cms.string(\"GEMeMapDummy\"),\\n                       connect = cms.string(\"sqlite_file:L1Trigger/Configuration/test/GEMeMapDummy.db\")\\n                )\\n)\\nprocess.muonGEMDigis.useDBEMap = True\\n\\n# Path and EndPath definitions\\nprocess.raw2digi_step = cms.Path(process.RawToDigi)\\nprocess.endjob_step = cms.EndPath(process.endOfProcess)\\n\\n# Schedule definition\\nprocess.schedule = cms.Schedule(process.raw2digi_step,process.endjob_step)\\nfrom PhysicsTools.PatAlgos.tools.helpers import associatePatAlgosToolsTask\\nassociatePatAlgosToolsTask(process)\\n\\n# customisation of the process.\\n\\n# Automatic addition of the customisation function from L1Trigger.Configuration.customiseReEmul\\nfrom L1Trigger.Configuration.customiseReEmul import L1TReEmulMCFromRAW \\n\\n#call to customisation function L1TReEmulMCFromRAW imported from L1Trigger.Configuration.customiseReEmul\\nprocess = L1TReEmulMCFromRAW(process)\\n\\n# Automatic addition of the customisation function from L1Trigger.L1TNtuples.customiseL1Ntuple\\nfrom L1Trigger.L1TNtuples.customiseL1Ntuple import L1NtupleEMU \\n\\n#call to customisation function L1NtupleEMU imported from L1Trigger.L1TNtuples.customiseL1Ntuple\\nprocess = L1NtupleEMU(process)\\n\\n# Automatic addition of the customisation function from L1Trigger.Configuration.customiseSettings\\nfrom L1Trigger.Configuration.customiseSettings import L1TSettingsToCaloParams_2018_v1_4 \\n\\n#call to customisation function L1TSettingsToCaloParams_2018_v1_4 imported from L1Trigger.Configuration.customiseSettings\\nprocess = L1TSettingsToCaloParams_2018_v1_4(process)\\n\\n# End of customisation functions\\n\\n\\n# Customisation from command line\\n\\n# Add early deletion of temporary data products to reduce peak memory need\\nfrom Configuration.StandardSequences.earlyDeleteSettings_cff import customiseEarlyDelete\\nprocess = customiseEarlyDelete(process)\\n# End adding early deletion\\n',\n",
       " '# pylint: disable=g-bad-file-header\\n# Copyright 2016 The Bazel Authors. All rights reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#    http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"Wrapper script for executing the Microsoft Compiler.\"\"\"\\nimport os\\nimport sys\\nimport msvc_link\\nimport msvc_tools\\n\\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\\nsys.path.append(SCRIPT_DIR)\\n\\nGCCPATTERNS = [\\n    (\\'-I(.+)\\', [\\'/I$0\\']),\\n    (\\'-m(32|64)\\', [\\'$TARGET_ARCH\\']),\\n    (\\'-Xcompilation-mode=(dbg|fastbuild|opt)\\', [\\'$COMPILATION_MODE\\']),\\n    (\\'-msse\\', [\\'/arch:SSE\\']),\\n    (\\'-msse2\\', [\\'/arch:SSE2\\']),\\n    (\\'-D(.+)\\', [\\'/D$0\\']),\\n    (\\'-U(.+)\\', [\\'/U$0\\']),\\n    (\\'-E\\', [\\'/E\\']),\\n    (\\'-O0\\', [\\'/Od\\']),\\n    (\\'-Os\\', [\\'/O1\\']),\\n    (\\'-O2\\', [\\'/O2\\']),\\n    (\\'-g0\\', []),\\n    (\\'-g\\', [\\'$DEBUG_RT\\']),\\n    (\\'-fexceptions\\', [\\'/U_HAS_EXCEPTIONS\\', \\'/D_HAS_EXCEPTIONS=1\\', \\'/EHsc\\']),\\n    (\\'-fomit-frame-pointer\\', [\\'/Oy\\']),\\n    (\\'-fno-rtti\\', [\\'/GR-\\']),\\n    (\\'-frtti\\', [\\'/GR\\']),\\n    (\\'-fPIC\\', []),\\n\\n    # This is unneeded for Windows.\\n    ((\\'-include\\', \\'(.+)\\'), [\\'/FI$PATH0\\']),\\n    (\\'-w\\', [\\'/w\\']),\\n    (\\'-Wall\\', [\\'/Wall\\']),\\n    (\\'-Wsign-compare\\', [\\'/we4018\\']),\\n    (\\'-Wno-sign-compare\\', [\\'/wd4018\\']),\\n    (\\'-Wconversion\\', [\\'/we4244\\', \\'/we4267\\']),\\n    (\\'-Wno-conversion\\', [\\'/wd4244\\', \\'/wd4267\\']),\\n    (\\'-Wno-sign-conversion\\', []),\\n    (\\'-Wno-implicit-fallthrough\\', []),\\n    (\\'-Wno-implicit-function-declaration\\', [\\'/wd4013\\']),\\n    (\\'-Wimplicit-function-declaration\\', [\\'/we4013\\']),\\n    (\\'-Wcovered-switch-default\\', [\\'/we4062\\']),\\n    (\\'-Wno-covered-switch-default\\', [\\'/wd4062\\']),\\n    (\\'-Wno-error\\', []),\\n    (\\'-Wno-invalid-offsetof\\', []),\\n    (\\'-Wno-overloaded-virtual\\', []),\\n    (\\'-Wno-reorder\\', []),\\n    (\\'-Wno-string-plus-int\\', []),\\n    (\\'-Wl,S\\', []),  # Stripping is unnecessary since msvc uses pdb files.\\n    (\\'-Wl,-rpath(.+)\\', []),\\n    (\\'-B(.+)\\', []),\\n    (\\'-static\\', []),\\n    (\\'-shared\\', [\\'/DLL\\']),\\n    (\\'-std=(.+)\\', []),\\n]\\n\\n\\ndef _IsLink(args):\\n  \"\"\"Determines whether we need to link rather than compile.\\n\\n  A set of arguments is for linking if they contain -static, -shared, are adding\\n  adding library search paths through -L, or libraries via -l.\\n\\n  Args:\\n    args: List of arguments\\n\\n  Returns:\\n    Boolean whether this is a link operation or not.\\n  \"\"\"\\n  for arg in args:\\n    # Certain flags indicate we are linking.\\n    if (arg in [\\'-shared\\', \\'-static\\'] or arg[:2] in [\\'-l\\', \\'-L\\'] or\\n        arg[:3] == \\'-Wl\\'):\\n      return True\\n  return False\\n\\n\\nclass MsvcCompiler(msvc_tools.WindowsRunner):\\n  \"\"\"Driver for the Microsoft compiler.\"\"\"\\n\\n  def Run(self, argv):\\n    \"\"\"Runs the compiler using the passed clang/gcc style argument list.\\n\\n    Args:\\n      argv: List of arguments\\n\\n    Returns:\\n      The return code of the compilation.\\n\\n    Raises:\\n      ValueError: if target architecture isn\\'t specified\\n    \"\"\"\\n    parser = msvc_tools.ArgParser(self, argv, GCCPATTERNS)\\n\\n    # Select runtime option\\n    # Find the last runtime option passed\\n    rt = None\\n    rt_idx = -1\\n    for i, opt in enumerate(reversed(parser.options)):\\n      if opt in [\\'/MT\\', \\'/MTd\\', \\'/MD\\', \\'/MDd\\']:\\n        if opt[-1] == \\'d\\':\\n          parser.enforce_debug_rt = True\\n        rt = opt[:3]\\n        rt_idx = len(parser.options) - i - 1\\n        break\\n    rt = rt or \\'/MT\\'  # Default to static runtime\\n    # Add debug if necessary\\n    if parser.enforce_debug_rt:\\n      rt += \\'d\\'\\n    # Include runtime option\\n    if rt_idx >= 0:\\n      parser.options[rt_idx] = rt\\n    else:\\n      parser.options.append(rt)\\n\\n    compiler = \\'cl\\'\\n    if parser.is_cuda_compilation:\\n      compiler = \\'nvcc\\'\\n    return self.RunBinary(compiler, parser.options, parser)\\n\\n\\ndef main(argv):\\n  # If we are supposed to link create a static library.\\n  if _IsLink(argv[1:]):\\n    return msvc_link.main(argv)\\n  else:\\n    return MsvcCompiler().Run(argv[1:])\\n\\n\\nif __name__ == \\'__main__\\':\\n  sys.exit(main(sys.argv[1:]))  # need to skip the first argument\\n',\n",
       " '\"\"\"\\r\\nThe following program leverages the regression coefficients generated after training the model in task 2 as an input file\\r\\n\"\"\"\\r\\nfrom __future__ import print_function\\r\\nimport sys\\r\\nimport re\\r\\nfrom operator import add\\r\\nimport numpy as np \\r\\nfrom pyspark import SparkContext\\r\\n\\r\\nif __name__ == \"__main__\":\\r\\n\\r\\n    sc = SparkContext(appName=\"LogisticRegression_task3\")\\r\\n    \\r\\n    # Read the dataset \\r\\n    d_corpus = sc.textFile(sys.argv[1])\\r\\n    \\r\\n    # Each entry in validLines will be a line from the text file\\r\\n    validDocLines = d_corpus.filter(lambda x : \\'id\\' in x and \\'url=\\' in x)\\r\\n\\r\\n    # Now, we transform it into a set of (docID, text) pairs\\r\\n    keyAndText = validDocLines.map(lambda x : (x[x.index(\\'id=\"\\') + 4 : x.index(\\'\" url=\\')], x[x.index(\\'\">\\') + 2:][:-6])) \\r\\n\\r\\n    # leveraged the code from assignment 2\\r\\n    # remove all non letter characters\\r\\n    regex = re.compile(\\'[^a-zA-Z]\\')\\r\\n    keyAndWordsList = keyAndText.map(lambda x : (str(x[0]), regex.sub(\\' \\', x[1]).lower().split()))\\r\\n    \\r\\n    # Now get the top 20,000 words... first change (docID, [\"word1\", \"word2\", \"word3\", ...])\\r\\n    # to (\"word1\", 1) (\"word2\", 1)...\\r\\n    conslidatedWords = keyAndWordsList.flatMap(lambda x: x[1]).map(lambda x: (x,1))\\r\\n\\r\\n    # Now, count all of the words, giving us (\"word1\", 1433), (\"word2\", 3423423), etc.\\r\\n    allCounts = conslidatedWords.reduceByKey(add)\\r\\n\\r\\n    # Get the top 20,000 words in a local array in a sorted format based on frequency\\r\\n    topWordsinDict = allCounts.top(20000, key = lambda x : x[1])\\r\\n\\r\\n    # We\\'ll create a RDD that has a set of (word, dictNum) pairs\\r\\n    # start by creating an RDD that has the number 0 through 20000\\r\\n    # 20000 is the number of words that will be in our dictionary\\r\\n    top20000Words = sc.parallelize(range(20000))\\r\\n\\r\\n    # Now, we transform (0), (1), (2), ... to (\"MostCommonWord\", 1)\\r\\n    # (\"NextMostCommon\", 2), ...\\r\\n    # the number will be the spot in the dictionary used to tell us\\r\\n    # where the word is located\\r\\n    dictionary = top20000Words.map (lambda x : (topWordsinDict[x][0], x))\\r\\n    \\r\\n    \\r\\n    # The following function gets a list of dictionaryPos values,\\r\\n    # and then creates a TF vector\\r\\n    # corresponding to those values... for example,\\r\\n    # if we get [3, 4, 1, 1, 2] we would in the\\r\\n    # end have [0, 2/5, 1/5, 1/5, 1/5] because 0 appears zero times,\\r\\n    # 1 appears twice, 2 appears once, etc.\\r\\n\\r\\n    def buildArray(listOfIndices):\\r\\n        \\r\\n        returnVal = np.zeros(20000)\\r\\n        \\r\\n        for index in listOfIndices:\\r\\n            returnVal[index] = returnVal[index] + 1\\r\\n        \\r\\n        mysum = np.sum(returnVal)\\r\\n        \\r\\n        returnVal = np.divide(returnVal, mysum)\\r\\n        \\r\\n        return returnVal\\r\\n        \\r\\n    def getLabel(x):\\r\\n      if x[:2] == \\'AU\\':\\r\\n        return 1\\r\\n\\r\\n      else:\\r\\n        return 0\\r\\n    \\r\\n    \\r\\n    # Leverage the regression coefficients genereated by task2 (model training) to make the prediction\\r\\n    #filePathOutputTask2 = sys.argv[2]\\r\\n    \\r\\n    # Open the file containing regression coefficients and read it\\r\\n    \\r\\n    #task2Lines = filePathOutputTask2.map(lambda x: x.split(\"Final Regression Coeffients:\\\\n[\"))\\r\\n    filePathOutputTask2 =sc.textFile(sys.argv[2])\\r\\n    \\r\\n    #filePathOutputTask2.map(lambda x: )\\r\\n    #with open(filePath) as file:\\r\\n     #   allLines = file.read()\\r\\n     \\r\\n    # Extract out all of the lines present in the output of task 2\\r\\n    task2Lines = filePathOutputTask2.map(lambda x: x.split(\",\"))\\r\\n    \\r\\n    # Extract the line containing the regression coefficients and remove \\'[\\' and \\']\\' from the extremes\\r\\n    listOfLines = task2Lines.collect()[10]\\r\\n    listOfLines[0] = listOfLines[0][1:]\\r\\n    listOfLines[len(listOfLines)-1] = listOfLines[len(listOfLines)-1][:len(listOfLines[len(listOfLines)-1])-2]\\r\\n\\r\\n    # Convert the list of regression coefficients to numpy array to be used as an input for prediction in task 3\\r\\n    regressionCoefficients = np.array(listOfLines, dtype = np.float64 )\\r\\n        \\r\\n    # Split the file and extract the \\'Regression Coefficients\\'    \\r\\n    #listOfLines = allLines.split(\"Final Regression Coeffients:\\\\n[\")\\r\\n    #listOfLines[len(listOfLines)-1] = listOfLines[len(listOfLines)-1][:len(listOfLines[len(listOfLines)-1])-2]\\r\\n    #regressionCoefficients = np.array(listOfLines[1].split(\\',\\'), dtype = np.float64 )\\r\\n    \\r\\n    # Threshold for logistic regression\\r\\n    threshold = 0.3\\r\\n\\r\\n    # Prediction Function using logistic regression\\r\\n    def predictionLogisticRegresison(x):\\r\\n      value = 1/(1+np.exp(-(  np.dot( x, regressionCoefficients )  )  )) \\r\\n      # return value\\r\\n      if value >= threshold:\\r\\n        return 1\\r\\n      else:\\r\\n        return 0\\r\\n        \\r\\n    ###################################################### PREDICTION/ EVALUATION - TAKS 3 ########\\r\\n    # Read the dataset \\r\\n    testData = sc.textFile(sys.argv[3])\\r\\n\\r\\n    # Each entry in validLines will be a line from the text file\\r\\n    validDocLinesTest = testData.filter(lambda x : \\'id\\' in x and \\'url=\\' in x)\\r\\n\\r\\n    # Now, we transform it into a set of (docID, text) pairs\\r\\n    keyAndTextTest = validDocLinesTest.map(lambda x : (x[x.index(\\'id=\"\\') + 4 : x.index(\\'\" url=\\')], x[x.index(\\'\">\\') + 2:][:-6])) \\r\\n\\r\\n    # remove all non letter characters\\r\\n    keyAndWordsListTest = keyAndTextTest.map(lambda x : (str(x[0]), regex.sub(\\' \\', x[1]).lower().split()))\\r\\n\\r\\n    # Get a RDD that has, for each (docID, [\"word1\", \"word2\", \"word3\", ...]),\\r\\n    # (\"word1\", docID), (\"word2\", docId), ...\\r\\n    allWordsWithDocIDTest = keyAndWordsListTest.flatMap(lambda x: ((j, x[0]) for j in x[1]))\\r\\n\\r\\n    # Join and link them, to get a set of (\"word1\", (dictionaryPos, docID)) pairs\\r\\n    allDictionaryWordsTest = dictionary.join(allWordsWithDocIDTest)\\r\\n\\r\\n    # Drop the actual word itself to get a set of (docID, dictionaryPos) pairs\\r\\n    justDocAndPosTest = allDictionaryWordsTest.map(lambda x: (x[1][1],x[1][0]))\\r\\n\\r\\n    # Get a set of (docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\\r\\n    allDictionaryWordsInEachDocTest = justDocAndPosTest.groupByKey()\\r\\n\\r\\n    # The following line this gets us a set of\\r\\n    # (docID,  [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\\r\\n    # and converts the dictionary positions to a bag-of-words numpy array...\\r\\n    allDocsAsNumpyArraysTest = allDictionaryWordsInEachDocTest.map(lambda x: (x[0], buildArray(x[1])))\\r\\n\\r\\n    # Now, create a version of allDocsAsNumpyArrays where, in the array,\\r\\n    # every entry is either zero or one.\\r\\n    # A zero means that the word does not occur,\\r\\n    # and a one means that it does.\\r\\n    zeroOrOneTest = allDocsAsNumpyArraysTest.map(lambda x: (x[0],np.where(x[1] > 0, 1, 0)))\\r\\n    \\r\\n    # Create a RDD of testing data and derive features and labels ... x[0]-> label, x[1]-> features\\r\\n    yLabelAndXFeatures = zeroOrOneTest.map(lambda x: (getLabel(x[0]),x[1]))\\r\\n    \\r\\n    # Make the prediction using the function \\'predictionLogisticRegresison\\'\\r\\n    yLabelAndXFeaturesPrediction = yLabelAndXFeatures.map(lambda x: (x[0],x[1],predictionLogisticRegresison(x[1])))\\r\\n\\r\\n    # Function to calculate True Positives\\r\\n    def calculateTruePositives(x):\\r\\n      if (x[0] == 1 and x[2] == 1): # the article was Australian court case (x[0]) and the prediction was also Australian court case x[2]\\r\\n        return 1\\r\\n      else:\\r\\n        return 0\\r\\n\\r\\n    # Function to calculate False Positives\\r\\n    def calculateFalsePositives(x):\\r\\n      if (x[0] == 0 and x[2] == 1): # the article was not Australian court case (x[0]) but the prediction was Australian court case x[2]\\r\\n        return 1\\r\\n      else:\\r\\n        return 0\\r\\n\\r\\n    # Function to calculate False Negatives\\r\\n    def calculateFalseNegatives(x):\\r\\n      if (x[0] == 1 and x[2] == 0): # the article was Australian court case (x[0]) but the prediction was not Australian court case x[2]\\r\\n        return 1\\r\\n      else:\\r\\n        return 0\\r\\n    \\r\\n    # Function to calculate True Negatives\\r\\n    def calculateTrueNegatives(x):\\r\\n      if (x[0] == 0 and x[2] == 0): # the article was not Australian court case (x[0]) and the prediction was not Australian court case x[2]\\r\\n        return 1\\r\\n      else:\\r\\n        return 0\\r\\n\\r\\n    # Out of total positive labels predicted, how many correctly classified as positive, that is PPV\\r\\n    def precision(x):\\r\\n      # Number of true positives/ (Number of true positives + Number of false positives) \\r\\n      # return truePositive/(truePositive + falsePositive)\\r\\n      return x[1][0]/(float(x[1][0] + x[1][1]))\\r\\n\\r\\n    # Out of actual positive labels, how many correctly classified as positive, that is, TPR\\r\\n    def recall(x):\\r\\n      # Number of true positives/ (Number of true positives + Number of false Negatives) \\r\\n      # return truePositive/(truePositive + falseNegative)\\r\\n      return x[1][0]/(float(x[1][0] +  x[1][2]))\\r\\n      \\r\\n      \\r\\n    # Calculate \\'True Positives\\', \\'False Positives\\' and \\'False Negatives\\'\\r\\n    calcTP_FP_FN = yLabelAndXFeaturesPrediction.map(lambda x: (1, np.array([calculateTruePositives(x), calculateFalsePositives(x), calculateFalseNegatives(x),calculateTrueNegatives(x)]))).reduceByKey(np.add)\\r\\n    \\r\\n    print(\\'\\')\\r\\n    print (\\'#\\'*20)\\r\\n    print(\\'Number of True Positives:\\', calcTP_FP_FN.collect()[0][1][0])\\r\\n    print(\\'Number of False Positives:\\', calcTP_FP_FN.collect()[0][1][1])\\r\\n    print(\\'Number of False Negatives:\\', calcTP_FP_FN.collect()[0][1][2])\\r\\n    print(\\'Number of True Negatives:\\', calcTP_FP_FN.collect()[0][1][3])\\r\\n    print(\\'\\')\\r\\n    \\r\\n    \\r\\n    # if \\'Number of True Positives: 0 and \\'Number of False Positives: 0, then F1 score is N/A\\r\\n    if calcTP_FP_FN.collect()[0][1][0] == 0  and calcTP_FP_FN.collect()[0][1][1] == 0:\\r\\n        calculateF1score = \\'N/A\\'\\r\\n        print(\\'F1 score for classifier =\\',\\'N/A\\')\\r\\n        print (\\'#\\'*20)\\r\\n        print(\\'\\')\\r\\n    else:    \\r\\n        calculateF1score = calcTP_FP_FN.map(lambda x: (precision(x), recall(x))).map(lambda x: 2*x[0]*x[1] / (x[0] + x[1])).collect()[0]\\r\\n        print(\\'F1 score for classifier =\\',round(calculateF1score*100,2),\\'%\\')\\r\\n        print (\\'#\\'*20)\\r\\n        print(\\'\\')\\r\\n    \\r\\n    # List to store the results of task 3\\r\\n    ansForTask3 = []\\r\\n    \\r\\n    if calculateF1score != \\'N/A\\':\\r\\n        ansForTask3.append((\\'F1 score for classifier =\\',round(calculateF1score*100,2),\\'%\\'))\\r\\n    else:\\r\\n        ansForTask3.append((\\'F1 score for classifier =\\',\\'N/A\\'))\\r\\n    ansForTask3.append(\\'\\')\\r\\n    ansForTask3.append((\\'Number of True Positives\\', calcTP_FP_FN.collect()[0][1][0]))\\r\\n    ansForTask3.append((\\'Number of False Positives\\', calcTP_FP_FN.collect()[0][1][1]))\\r\\n    ansForTask3.append((\\'Number of False Negatives\\', calcTP_FP_FN.collect()[0][1][2]))\\r\\n    ansForTask3.append((\\'Number of True Negatives\\', calcTP_FP_FN.collect()[0][1][3]))\\r\\n    \\r\\n    # Save the results of task3 in a text file\\r\\n    sc.parallelize(ansForTask3).coalesce(1, shuffle = False).saveAsTextFile(sys.argv[4]) \\r\\n    \\r\\n    sc.stop()',\n",
       " '#!/usr/bin/env python\\n# This code is part of the Biopython distribution and governed by its\\n# license.  Please see the LICENSE file that should have been included\\n# as part of this package.\\n#\\n# Bio.Wise contains modules for running and processing the output of\\n# some of the models in the Wise2 package by Ewan Birney available from:\\n# ftp://ftp.ebi.ac.uk/pub/software/unix/wise2/\\n# http://www.ebi.ac.uk/Wise2/\\n#\\n# Bio.Wise.psw is for protein Smith-Waterman alignments\\n# Bio.Wise.dnal is for Smith-Waterman DNA alignments\\n\\nfrom __future__ import print_function\\n\\nimport os\\nimport sys\\nimport tempfile\\n\\nfrom Bio import SeqIO\\n\\n\\ndef _build_align_cmdline(cmdline, pair, output_filename, kbyte=None, force_type=None, quiet=False):\\n    \"\"\"Helper function to build a command line string (PRIVATE).\\n\\n    >>> os.environ[\"WISE_KBYTE\"]=\"300000\"\\n    >>> if os.isatty(sys.stderr.fileno()):\\n    ...    c = _build_align_cmdline([\"dnal\"], (\"seq1.fna\", \"seq2.fna\"),\\n    ...                             \"/tmp/output\", kbyte=100000)\\n    ...    assert c == \\'dnal -kbyte 100000 seq1.fna seq2.fna > /tmp/output\\', c\\n    ...    c = _build_align_cmdline([\"psw\"], (\"seq1.faa\", \"seq2.faa\"),\\n    ...                             \"/tmp/output_aa\")\\n    ...    assert c == \\'psw -kbyte 300000 seq1.faa seq2.faa > /tmp/output_aa\\', c\\n    ... else:\\n    ...    c = _build_align_cmdline([\"dnal\"], (\"seq1.fna\", \"seq2.fna\"),\\n    ...                             \"/tmp/output\", kbyte=100000)\\n    ...    assert c == \\'dnal -kbyte 100000 -quiet seq1.fna seq2.fna > /tmp/output\\', c\\n    ...    c = _build_align_cmdline([\"psw\"], (\"seq1.faa\", \"seq2.faa\"),\\n    ...                             \"/tmp/output_aa\")\\n    ...    assert c == \\'psw -kbyte 300000 -quiet seq1.faa seq2.faa > /tmp/output_aa\\', c\\n\\n    \"\"\"\\n    cmdline = cmdline[:]\\n\\n    ### XXX: force_type ignored\\n\\n    if kbyte is None:\\n        try:\\n            cmdline.extend((\"-kbyte\", os.environ[\"WISE_KBYTE\"]))\\n        except KeyError:\\n            pass\\n    else:\\n        cmdline.extend((\"-kbyte\", str(kbyte)))\\n\\n    if not os.isatty(sys.stderr.fileno()):\\n        cmdline.append(\"-quiet\")\\n\\n    cmdline.extend(pair)\\n    cmdline.extend((\">\", output_filename))\\n    if quiet:\\n        cmdline.extend((\"2>\", \"/dev/null\"))\\n    cmdline_str = \\' \\'.join(cmdline)\\n\\n    return cmdline_str\\n\\n\\ndef align(cmdline, pair, kbyte=None, force_type=None, dry_run=False, quiet=False, debug=False):\\n    \"\"\"\\n    Returns a filehandle\\n    \"\"\"\\n    if not pair or len(pair) != 2:\\n        raise ValueError(\"Expected pair of filename, not %s\" % repr(pair))\\n\\n    output_file = tempfile.NamedTemporaryFile(mode=\\'r\\')\\n    input_files = tempfile.NamedTemporaryFile(mode=\"w\"), tempfile.NamedTemporaryFile(mode=\"w\")\\n\\n    if dry_run:\\n        print(_build_align_cmdline(cmdline,\\n                                   pair,\\n                                   output_file.name,\\n                                   kbyte,\\n                                   force_type,\\n                                   quiet))\\n        return\\n\\n    for filename, input_file in zip(pair, input_files):\\n        # Pipe the file through Biopython\\'s Fasta parser/writer\\n        # to make sure it conforms to the Fasta standard (in particular,\\n        # Wise2 may choke on long lines in the Fasta file)\\n        records = SeqIO.parse(open(filename), \\'fasta\\')\\n        SeqIO.write(records, input_file, \\'fasta\\')\\n        input_file.flush()\\n\\n    input_file_names = [input_file.name for input_file in input_files]\\n\\n    cmdline_str = _build_align_cmdline(cmdline,\\n                                       input_file_names,\\n                                       output_file.name,\\n                                       kbyte,\\n                                       force_type,\\n                                       quiet)\\n\\n    if debug:\\n        sys.stderr.write(\"%s\\\\n\" % cmdline_str)\\n\\n    status = os.system(cmdline_str) >> 8\\n\\n    if status > 1:\\n        if kbyte != 0: # possible memory problem; could be None\\n            sys.stderr.write(\"INFO trying again with the linear model\\\\n\")\\n            return align(cmdline, pair, 0, force_type, dry_run, quiet, debug)\\n        else:\\n            raise OSError(\"%s returned %s\" % (\" \".join(cmdline), status))\\n\\n    return output_file\\n\\n\\ndef all_pairs(singles):\\n    \"\"\"\\n    Generate pairs list for all-against-all alignments\\n\\n    >>> all_pairs(range(4))\\n    [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\\n    \"\"\"\\n    pairs = []\\n\\n    singles = list(singles)\\n    while singles:\\n        suitor = singles.pop(0) # if sorted, stay sorted\\n        pairs.extend((suitor, single) for single in singles)\\n\\n    return pairs\\n\\n\\ndef main():\\n    pass\\n\\n\\ndef _test(*args, **keywds):\\n    import doctest\\n    doctest.testmod(sys.modules[__name__], *args, **keywds)\\n\\nif __name__ == \"__main__\":\\n    if __debug__:\\n        _test()\\n    main()\\n',\n",
       " 'import os\\nfrom pathlib import Path\\n\\nfrom yaml import load, SafeLoader\\n\\n\\ndef _service_path(folder: str, filename: str) -> str:\\n    current_directory = os.path.dirname(os.path.realpath(__file__))\\n    return os.path.normpath(os.path.join(current_directory, \"..\", folder, filename))\\n\\n\\ndef load_config() -> dict:\\n    config_file_path = Path(__file__).parent.parent / \"config/config.yml\"\\n    with open(config_file_path, \"r\") as f:\\n        data = f.read()\\n        config_dict = load(data, SafeLoader)\\n        return config_dict\\n\\n\\nclass Config:\\n    conf = load_config()\\n    if conf is None:\\n        conf = dict()\\n\\n    # Server config\\n    BASE_URL = conf.get(\"BASE_URL\", \"http://cv.local\")\\n    HTTP_PORT = conf.get(\"HTTP_PORT\", 8099)\\n    STATIC_PATH = conf.get(\"STATIC_PATH\", Path(__file__).parent.parent / \"templates/static\")\\n    TEMPLATES_PATH = conf.get(\"STATIC_PATH\", Path(__file__).parent.parent / \"templates\")\\n    LOG_LEVEL = conf.get(\"LOG_LEVEL\", \"info\")\\n    LOG_FORMAT = conf.get(\"LOG_FORMAT\", \"color\")\\n    WEB_SECURE_COOKIES = conf.get(\"WEB_SECURE_COOKIES\", False)\\n\\n    # Database config\\n    DEFAULT_PG_URL = conf.get(\"PDB_URL\", \"postgresql://api:hackme@127.0.0.1:5488/app_sharer\")\\n    PG_POOL_MIN_SIZE = conf.get(\"PG_POOL_MIN_SIZE\", 10)\\n    PG_POOL_MAX_SIZE = conf.get(\"PG_POOL_MAX_SIZE\", 10)\\n\\n\\nconfig = Config()\\n',\n",
       " 'from datetime import datetime\\n\\nfrom flaskblog import db\\n\\n\\nclass User(db.Model):\\n    id = db.Column(db.Integer, primary_key=True)\\n    username = db.Column(db.String(20), unique=True, nullable=False)\\n    email = db.Column(db.String(120), unique=True, nullable=False)\\n    image_file = db.Column(db.String(20), nullable=False, default=\\'default.jpg\\')\\n    password = db.Column(db.String(60), nullable=False)\\n    posts = db.relationship(\\'Post\\', backref=\\'author\\', lazy=True)\\n\\n    def __repr__(self):\\n        return f\"User(\\'{self.username}\\', \\'{self.email}\\', \\'{self.image_file}\\')\"\\n\\n\\nclass Post(db.Model):\\n    id = db.Column(db.Integer, primary_key=True)\\n    title = db.Column(db.String(100), nullable=False)\\n    date_posted = db.Column(db.DateTime, nullable=False, default=datetime.utcnow)\\n    content = db.Column(db.Text, nullable=False)\\n    user_id = db.Column(db.Integer, db.ForeignKey(\\'user.id\\'), nullable=False)\\n\\n    def __repr__(self):\\n        return f\"User(\\'{self.title}\\', \\'{self.date_posted}\\')\"\\n',\n",
       " 'from nose.plugins.attrib import attr\\nfrom test.integration.base import DBTIntegrationTest\\n\\n\\nclass TestMacros(DBTIntegrationTest):\\n\\n    def setUp(self):\\n        DBTIntegrationTest.setUp(self)\\n        self.run_sql_file(\"test/integration/016_macro_tests/seed.sql\")\\n\\n    @property\\n    def schema(self):\\n        return \"test_macros_016\"\\n\\n    @property\\n    def models(self):\\n        return \"test/integration/016_macro_tests/models\"\\n\\n    @property\\n    def project_config(self):\\n        return {\\n            \"models\": {\\n                \"vars\": {\\n                    \"test\": \"DUMMY\"\\n                }\\n            },\\n            \"macro-paths\": [\"test/integration/016_macro_tests/macros\"],\\n            \"repositories\": [\\n                \\'https://github.com/fishtown-analytics/dbt-integration-project\\'\\n            ]\\n        }\\n\\n    @attr(type=\\'postgres\\')\\n    def test_working_macros(self):\\n        self.run_dbt([\"deps\"])\\n        self.run_dbt([\"run\"])\\n\\n        self.assertTablesEqual(\"expected_dep_macro\", \"dep_macro\")\\n        self.assertTablesEqual(\"expected_local_macro\", \"local_macro\")\\n\\n\\nclass TestInvalidMacros(DBTIntegrationTest):\\n\\n    def setUp(self):\\n        DBTIntegrationTest.setUp(self)\\n\\n    @property\\n    def schema(self):\\n        return \"test_macros_016\"\\n\\n    @property\\n    def models(self):\\n        return \"test/integration/016_macro_tests/models\"\\n\\n    @property\\n    def project_config(self):\\n        return {\\n            \"macro-paths\": [\"test/integration/016_macro_tests/bad-macros\"]\\n        }\\n\\n    @attr(type=\\'postgres\\')\\n    def test_invalid_macro(self):\\n\\n        try:\\n            self.run_dbt([\"run\"], expect_pass=False)\\n            self.assertTrue(False,\\n                            \\'compiling bad macro should raise a runtime error\\')\\n\\n        except RuntimeError:\\n            pass\\n\\n\\nclass TestMisusedMacros(DBTIntegrationTest):\\n\\n    def setUp(self):\\n        DBTIntegrationTest.setUp(self)\\n\\n    @property\\n    def schema(self):\\n        return \"test_macros_016\"\\n\\n    @property\\n    def models(self):\\n        return \"test/integration/016_macro_tests/bad-models\"\\n\\n    @property\\n    def project_config(self):\\n        return {\\n            \"macro-paths\": [\"test/integration/016_macro_tests/macros\"],\\n            \"repositories\": [\\n                \\'https://github.com/fishtown-analytics/dbt-integration-project\\'\\n            ]\\n        }\\n\\n    # TODO: compilation no longer exists, so while the model calling this macro\\n    # fails, it does not raise a runtime exception. change this test to verify\\n    # that the model finished with ERROR state.\\n    #\\n    # @attr(type=\\'postgres\\')\\n    # def test_working_macros(self):\\n    #     self.run_dbt([\"deps\"])\\n\\n    #     try:\\n    #         self.run_dbt([\"run\"])\\n    #         self.assertTrue(False, \\'invoked a package macro from global scope\\')\\n    #     except RuntimeError:\\n    #         pass\\n',\n",
       " \"import abc\\nfrom typing import Any, Generic, TypeVar\\nfrom types import SimpleNamespace\\n\\nfrom amino import List, Lists, Nil, Maybe\\nfrom amino.util.string import ToStr\\n\\nA = TypeVar('A')\\n\\n\\ndef is_algebra(bases: List[type]) -> bool:\\n    return bases.exists(lambda a: hasattr(a, '__algebra_base__'))\\n\\n\\ndef find_algebra(name: str, bases: List[type]) -> Maybe[type]:\\n    return bases.find(lambda a: hasattr(a, '__algebra_variants__'))\\n\\n\\ndef setup_algebra(name: str, inst: type, bases: List[type]) -> None:\\n    if is_algebra(bases):\\n        inst.__algebra_variants__ = List()\\n    else:\\n        raise Exception(f'algebra subclass has no algebra superclass: {name}')\\n\\n\\ndef setup_variant(name: str, inst: type, bases: List[type], algebra: type) -> None:\\n    inst.__algebra_index__ = len(algebra.__algebra_variants__)\\n    algebra.__algebra_variants__.append(inst)\\n\\n\\ndef setup_algebraic_type(name: str, inst: type, bases: List[type]) -> None:\\n    return (\\n        find_algebra(name, bases)\\n        .cata_f(\\n            lambda a: setup_variant(name, inst, bases, a),\\n            lambda: setup_algebra(name, inst, bases)\\n        )\\n    )\\n\\n\\nclass AlgebraMeta(abc.ABCMeta):\\n\\n    def __new__(\\n            cls,\\n            name: str,\\n            bases: list,\\n            namespace: SimpleNamespace,\\n            algebra_base: bool=False,\\n            **kw: Any,\\n    ) -> None:\\n        inst = super().__new__(cls, name, bases, namespace, **kw)\\n        if not hasattr(inst, '__args__') or inst.__args__ is None:\\n            if algebra_base:\\n                inst.__algebra_base__ = None\\n            else:\\n                setup_algebraic_type(name, inst, Lists.wrap(bases))\\n        return inst\\n\\n\\nclass Algebra(Generic[A], ToStr, metaclass=AlgebraMeta, algebra_base=True):\\n    pass\\n\\n\\n__all__ = ('AlgebraMeta', 'Algebra')\\n\",\n",
       " 'i = 0  # This is an example of the while loop\\nwhile i < 5:\\n    print(i)\\n    i += 1\\ni = None  # 0\\n            # 1\\n            # 2\\n            # 3\\n            # 4\\n\\n\\n\\nfor i in range(5):  # This example uses the iterable object \\'range\\'\\n    print(i)  # 0\\n                # 1\\n                # 2\\n                # 3\\n                # 4\\n\\n\\n\\nfor i in [1, 2, 3, 4]:\\n    print(i)  # 1\\n                # 2\\n                # 3\\n                # 4\\n\\n\\n\\nfor c in \\'hello\\':\\n    print(c)  # h\\n                # e\\n                # l\\n                # l\\n                # o\\n\\n\\n\\nfor x in (\\'a\\',\"b\", \\'c\\', 4):\\n    print(x)  # a\\n                # b\\n                # c\\n                # 4\\n\\nfor x in [(1, 2), (3, 4), (5, 6)]:\\n    print(x)  # (1, 2)\\n            # (3, 4)\\n            # (5, 6)\\n\\n\\n\\nfor i in range(5):\\n    if i==3:\\n        continue\\n    print(i)  # 0\\n                # 1\\n                # 2\\n                # 4\\n\\n\\n\\n\\n\\n\\n\\nfor i in range(5):\\n    if i==3:\\n        break\\n    print(i)  # 0\\n                # 1\\n                # 2\\n\\n\\n\\nfor i in range(1, 5):\\n    print(i)\\n    if i % 7 == 0:\\n        print(\\'multiple of 7 found\\')\\n        break\\nelse:\\n    print(\\'no multiple of 7 found\\')  # 1\\n                                    # 2\\n                                    # 3\\n                                    # 4\\n                                    # no multiple of 7 found\\n\\n\\n\\nfor i in range(1, 8):\\n    print(i)\\n    if i % 7 == 0:\\n        print(\\'multiple of 7 found\\')\\n        break\\nelse:\\n    print(\\'no multiple of 7 found\\')  # 1\\n                                    # 2\\n                                    # 3\\n                                    # 4\\n                                    # 5\\n                                    # 6\\n                                    # 7\\n                                    # multiple of 7 found\\n\\n\\n\\nfor i in range(6):\\n    print(\\'------------------\\')\\n    \\n',\n",
       " '# -*- coding: utf-8 -*-\\n\"\"\"\\nTencent is pleased to support the open source community by making 蓝鲸智云PaaS平台社区版 (BlueKing PaaS Community\\nEdition) available.\\nCopyright (C) 2017-2021 THL A29 Limited, a Tencent company. All rights reserved.\\nLicensed under the MIT License (the \"License\"); you may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://opensource.org/licenses/MIT\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\"\"\"\\nimport os\\nimport uuid\\nfrom unittest import mock\\n\\nimport pytest\\nfrom kubernetes import client\\n\\nfrom backend.tests.conftest import TESTING_API_SERVER_URL\\nfrom backend.tests.testing_utils.mocks.collection import StubComponentCollection\\n\\n\\nclass FakeBcsKubeConfigurationService:\\n    \"\"\"Fake configuration service which return local apiserver as config\"\"\"\\n\\n    def __init__(self, *args, **kwargs):\\n        pass\\n\\n    def make_configuration(self):\\n        configuration = client.Configuration()\\n        configuration.api_key = {\"authorization\": f\\'Bearer {os.environ.get(\"TESTING_SERVER_API_KEY\")}\\'}\\n        configuration.verify_ssl = False\\n        configuration.host = TESTING_API_SERVER_URL\\n        return configuration\\n\\n\\n@pytest.fixture(autouse=True)\\ndef setup_fake_cluster_dependencies():\\n    # 替换所有 Comp 系统为测试专用的 Stub 系统；替换集群地址为测试用 API Server\\n    with mock.patch(\\n        \\'backend.container_service.core.ctx_models.ComponentCollection\\', new=StubComponentCollection\\n    ), mock.patch(\\n        \\'backend.resources.utils.kube_client.BcsKubeConfigurationService\\', new=FakeBcsKubeConfigurationService\\n    ):\\n        yield\\n',\n",
       " '#\\r\\n# This file is part of pyasn1-modules software.\\r\\n#\\r\\n# Created by Russ Housley.\\r\\n#\\r\\n# Copyright (c) 2019, Vigil Security, LLC\\r\\n# License: http://snmplabs.com/pyasn1/license.html\\r\\n#\\r\\n# Diffie-Hellman Proof-of-Possession Algorithms\\r\\n#\\r\\n# ASN.1 source from:\\r\\n# https://www.rfc-editor.org/rfc/rfc6955.txt\\r\\n#\\r\\n\\r\\nfrom pyasn1.type import namedtype\\r\\nfrom pyasn1.type import univ\\r\\n\\r\\nfrom pyasn1_modules import rfc3279\\r\\nfrom pyasn1_modules import rfc5280\\r\\nfrom pyasn1_modules import rfc5652\\r\\n\\r\\n\\r\\n# Imports from RFC 5652\\r\\n\\r\\nMessageDigest = rfc5652.MessageDigest\\r\\n\\r\\nIssuerAndSerialNumber = rfc5652.IssuerAndSerialNumber\\r\\n\\r\\n\\r\\n# Imports from RFC 5280\\r\\n\\r\\nid_pkix = rfc5280.id_pkix\\r\\n\\r\\n\\r\\n# Imports from RFC 3279\\r\\n\\r\\nDss_Sig_Value = rfc3279.Dss_Sig_Value\\r\\n\\r\\nDomainParameters = rfc3279.DomainParameters\\r\\n\\r\\n\\r\\n# Static DH Proof-of-Possession\\r\\n\\r\\nclass DhSigStatic(univ.Sequence):\\r\\n    componentType = namedtype.NamedTypes(\\r\\n        namedtype.OptionalNamedType(\\'issuerAndSerial\\', IssuerAndSerialNumber()),\\r\\n        namedtype.NamedType(\\'hashValue\\', MessageDigest())\\r\\n    )\\r\\n\\r\\n\\r\\n# Object Identifiers\\r\\n\\r\\nid_dh_sig_hmac_sha1 = id_pkix + (6, 3, )\\r\\n\\r\\nid_dhPop_static_sha1_hmac_sha1 = univ.ObjectIdentifier(id_dh_sig_hmac_sha1)\\r\\n\\r\\n\\r\\nid_alg_dh_pop = id_pkix + (6, 4, )\\r\\n\\r\\nid_alg_dhPop_sha1 = univ.ObjectIdentifier(id_alg_dh_pop)\\r\\n\\r\\nid_alg_dhPop_sha224 = id_pkix + (6, 5, )\\r\\n\\r\\nid_alg_dhPop_sha256 = id_pkix + (6, 6, )\\r\\n\\r\\nid_alg_dhPop_sha384 = id_pkix + (6, 7, )\\r\\n\\r\\nid_alg_dhPop_sha512 = id_pkix + (6, 8, )\\r\\n\\r\\n\\r\\nid_alg_dhPop_static_sha224_hmac_sha224 = id_pkix + (6, 15, )\\r\\n\\r\\nid_alg_dhPop_static_sha256_hmac_sha256 = id_pkix + (6, 16, )\\r\\n\\r\\nid_alg_dhPop_static_sha384_hmac_sha384 = id_pkix + (6, 17, )\\r\\n\\r\\nid_alg_dhPop_static_sha512_hmac_sha512 = id_pkix + (6, 18, )\\r\\n\\r\\n\\r\\nid_alg_ecdhPop_static_sha224_hmac_sha224 = id_pkix + (6, 25, )\\r\\n\\r\\nid_alg_ecdhPop_static_sha256_hmac_sha256 = id_pkix + (6, 26, )\\r\\n\\r\\nid_alg_ecdhPop_static_sha384_hmac_sha384 = id_pkix + (6, 27, )\\r\\n\\r\\nid_alg_ecdhPop_static_sha512_hmac_sha512 = id_pkix + (6, 28, )\\r\\n\\r\\n\\r\\n# Update the Algorithm Identifier map in rfc5280.py\\r\\n\\r\\n_algorithmIdentifierMapUpdate = {\\r\\n    id_alg_dh_pop: DomainParameters(),\\r\\n    id_alg_dhPop_sha224: DomainParameters(),\\r\\n    id_alg_dhPop_sha256: DomainParameters(),\\r\\n    id_alg_dhPop_sha384: DomainParameters(),\\r\\n    id_alg_dhPop_sha512: DomainParameters(),\\r\\n    id_dh_sig_hmac_sha1: univ.Null(\"\"),\\r\\n    id_alg_dhPop_static_sha224_hmac_sha224: univ.Null(\"\"),\\r\\n    id_alg_dhPop_static_sha256_hmac_sha256: univ.Null(\"\"),\\r\\n    id_alg_dhPop_static_sha384_hmac_sha384: univ.Null(\"\"),\\r\\n    id_alg_dhPop_static_sha512_hmac_sha512: univ.Null(\"\"),\\r\\n    id_alg_ecdhPop_static_sha224_hmac_sha224: univ.Null(\"\"),\\r\\n    id_alg_ecdhPop_static_sha256_hmac_sha256: univ.Null(\"\"),\\r\\n    id_alg_ecdhPop_static_sha384_hmac_sha384: univ.Null(\"\"),\\r\\n    id_alg_ecdhPop_static_sha512_hmac_sha512: univ.Null(\"\"),\\r\\n}\\r\\n\\r\\nrfc5280.algorithmIdentifierMap.update(_algorithmIdentifierMapUpdate)\\r\\n']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_python['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize train_dataset_python with the GPT2 tokenizer\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(examples[\"content\"], max_length=1024, padding='max_length', truncation=True)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 96.15 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 50.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# use tokenize_function to train_dataset_python\n",
    "train_dataset_python = train_dataset_python.map(tokenize_function, batched=True)\n",
    "\n",
    "# tokenize eval_dataset_python with the GPT2 tokenizer\n",
    "eval_dataset_python = Dataset.from_pandas(df_python_eval)\n",
    "eval_dataset_python = eval_dataset_python.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./gpt2-python\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset_python,\n",
    "    eval_dataset=eval_dataset_python\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content', 'lang', 'size', 'ext', 'max_stars_count', 'avg_line_length', 'max_line_length', 'alphanum_fraction', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 80\n",
       "})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"# -*- coding: utf-8 -*-\\n'''\\nReturn data to a mongodb server\\n\\nRequired python modules: pymongo\\n\\n\\nThis returner will send data from the minions to a MongoDB server. To\\nconfigure the settings for your MongoDB server, add the following lines\\nto the minion config files::\\n\\n    mongo.db: <database name>\\n    mongo.host: <server ip address>\\n    mongo.user: <MongoDB username>\\n    mongo.password: <MongoDB user password>\\n    mongo.port: 27017\\n\\nAlternative configuration values can be used by prefacing the configuration.\\nAny values not found in the alternative configuration will be pulled from\\nthe default location::\\n\\n    alternative.mongo.db: <database name>\\n    alternative.mongo.host: <server ip address>\\n    alternative.mongo.user: <MongoDB username>\\n    alternative.mongo.password: <MongoDB user password>\\n    alternative.mongo.port: 27017\\n\\n  To use the mongo returner, append '--return mongo' to the salt command. ex:\\n\\n    salt '*' test.ping --return mongo_return\\n\\n  To use the alternative configuration, append '--return_config alternative' to the salt command. ex:\\n\\n    salt '*' test.ping --return mongo_return --return_config alternative\\n'''\\nfrom __future__ import absolute_import\\n\\n# Import python libs\\nimport logging\\n\\n# import Salt libs\\nimport salt.utils\\nimport salt.returners\\nimport six\\n\\n# Import third party libs\\ntry:\\n    import pymongo\\n    HAS_PYMONGO = True\\nexcept ImportError:\\n    HAS_PYMONGO = False\\n\\n\\nlog = logging.getLogger(__name__)\\n\\n# Define the module's virtual name\\n# currently only used iby _get_options\\n__virtualname__ = 'mongo'\\n\\n\\ndef __virtual__():\\n    if not HAS_PYMONGO:\\n        return False\\n    return 'mongo_return'\\n\\n\\ndef _remove_dots(src):\\n    '''\\n    Remove dots from the given data structure\\n    '''\\n    output = {}\\n    for key, val in six.iteritems(src):\\n        if isinstance(val, dict):\\n            val = _remove_dots(val)\\n        output[key.replace('.', '-')] = val\\n    return output\\n\\n\\ndef _get_options(ret):\\n    '''\\n    Get the monogo_return options from salt.\\n    '''\\n    attrs = {'host': 'host',\\n             'port': 'port',\\n             'db': 'db',\\n             'username': 'username',\\n             'password': 'password'}\\n\\n    _options = salt.returners.get_returner_options(__virtualname__,\\n                                                   ret,\\n                                                   attrs,\\n                                                   __salt__=__salt__,\\n                                                   __opts__=__opts__)\\n    return _options\\n\\n\\ndef _get_conn(ret):\\n    '''\\n    Return a mongodb connection object\\n    '''\\n    _options = _get_options(ret)\\n\\n    host = _options.get('host')\\n    port = _options.get('port')\\n    db_ = _options.get('db')\\n    user = _options.get('user')\\n    password = _options.get('password')\\n\\n    conn = pymongo.Connection(host, port)\\n    mdb = conn[db_]\\n\\n    if user and password:\\n        mdb.authenticate(user, password)\\n    return conn, mdb\\n\\n\\ndef returner(ret):\\n    '''\\n    Return data to a mongodb server\\n    '''\\n    conn, mdb = _get_conn(ret)\\n    col = mdb[ret['id']]\\n\\n    if isinstance(ret['return'], dict):\\n        back = _remove_dots(ret['return'])\\n    else:\\n        back = ret['return']\\n\\n    log.debug(back)\\n    sdata = {ret['jid']: back, 'fun': ret['fun']}\\n    if 'out' in ret:\\n        sdata['out'] = ret['out']\\n    col.insert(sdata)\\n\\n\\ndef get_jid(jid):\\n    '''\\n    Return the return information associated with a jid\\n    '''\\n    conn, mdb = _get_conn(ret=None)\\n    ret = {}\\n    for collection in mdb.collection_names():\\n        rdata = mdb[collection].find_one({jid: {'$exists': 'true'}})\\n        if rdata:\\n            ret[collection] = rdata\\n    return ret\\n\\n\\ndef get_fun(fun):\\n    '''\\n    Return the most recent jobs that have executed the named function\\n    '''\\n    conn, mdb = _get_conn(ret=None)\\n    ret = {}\\n    for collection in mdb.collection_names():\\n        rdata = mdb[collection].find_one({'fun': fun})\\n        if rdata:\\n            ret[collection] = rdata\\n    return ret\\n\\n\\ndef prep_jid(nocache, passed_jid=None):  # pylint: disable=unused-argument\\n    '''\\n    Do any work necessary to prepare a JID, including sending a custom id\\n    '''\\n    return passed_jid if passed_jid is not None else salt.utils.gen_jid()\\n\",\n",
       " '# -*- coding: utf-8 -*-\\n# Copyright 2019 Cohesity Inc.\\n\\nclass Type21Enum(object):\\n\\n    \"\"\"Implementation of the \\'Type21\\' enum.\\n\\n    Specifies the type of the CloudDeploy target.\\n    \\'kAzure\\' indicates that Azure as a cloud deploy target type.\\n    \\'kAws\\' indicates that AWS as a cloud deploy target type.\\n    \\'kGcp\\' indicates that GCP as a cloud deploy target type.\\n\\n    Attributes:\\n        KAZURE: TODO: type description here.\\n        KAWS: TODO: type description here.\\n        KGCP: TODO: type description here.\\n\\n    \"\"\"\\n\\n    KAZURE = \\'kAzure\\'\\n\\n    KAWS = \\'kAws\\'\\n\\n    KGCP = \\'kGcp\\'\\n\\n',\n",
       " '#\\n# This file is part of pySMT.\\n#\\n#   Copyright 2014 Andrea Micheli and Marco Gario\\n#\\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\\n#   you may not use this file except in compliance with the License.\\n#   You may obtain a copy of the License at\\n#\\n#       http://www.apache.org/licenses/LICENSE-2.0\\n#\\n#   Unless required by applicable law or agreed to in writing, software\\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n#   See the License for the specific language governing permissions and\\n#   limitations under the License.\\n#\\nfrom pysmt.shortcuts import (And, Iff, Or, Symbol, Implies, Not,\\n                             Exists, ForAll,\\n                             Times, Plus, Minus, Equals, Real,\\n                             is_valid)\\nfrom pysmt.test import TestCase, skipIfNoSolverForLogic, main\\nfrom pysmt.rewritings import prenex_normal_form, nnf, conjunctive_partition, aig\\nfrom pysmt.rewritings import disjunctive_partition\\nfrom pysmt.rewritings import TimesDistributor\\nfrom pysmt.test.examples import get_example_formulae\\nfrom pysmt.exceptions import SolverReturnedUnknownResultError\\nfrom pysmt.logics import BOOL, QF_NRA, QF_LRA, QF_LIA\\nfrom pysmt.typing import REAL\\n\\n\\nclass TestRewritings(TestCase):\\n\\n    def test_prenex_basic(self):\\n        a,b,c = (Symbol(x) for x in \"abc\")\\n        f = Not(And(a, Exists([b], And(a, b)), ForAll([c], Or(a, c))))\\n        prenex = prenex_normal_form(f)\\n        # Two prenex normal forms are possible\\n        my_prenex_1 = Exists([c], ForAll([b], Not(And(a, And(a, b), Or(a, c)))))\\n        my_prenex_2 = ForAll([b], Exists([c], Not(And(a, And(a, b), Or(a, c)))))\\n        self.assertTrue(prenex == my_prenex_1 or prenex == my_prenex_2)\\n\\n    @skipIfNoSolverForLogic(BOOL)\\n    def test_prenex_simple_exists(self):\\n        a,b = (Symbol(x) for x in \"ab\")\\n        f = And(b, Exists([b], Implies(a, b)))\\n        prenex = prenex_normal_form(f)\\n        self.assertTrue(prenex.is_exists())\\n        self.assertValid(Iff(f, prenex), logic=BOOL)\\n\\n    @skipIfNoSolverForLogic(BOOL)\\n    def test_prenex_simple_forall(self):\\n        a,b = (Symbol(x) for x in \"ab\")\\n        f = Or(b, ForAll([b], Implies(a, b)))\\n        prenex = prenex_normal_form(f)\\n        self.assertTrue(prenex.is_forall())\\n        self.assertValid(Iff(f, prenex), logic=BOOL)\\n\\n    @skipIfNoSolverForLogic(BOOL)\\n    def test_prenex_negated_exists(self):\\n        a,b = (Symbol(x) for x in \"ab\")\\n        f = Implies(Exists([b], Implies(a, b)), b)\\n        prenex = prenex_normal_form(f)\\n        self.assertTrue(prenex.is_forall())\\n        self.assertValid(Iff(f, prenex), logic=BOOL)\\n\\n    @skipIfNoSolverForLogic(BOOL)\\n    def test_prenex_negated_forall(self):\\n        a,b = (Symbol(x) for x in \"ab\")\\n        f = Implies(ForAll([b], Implies(a, b)), b)\\n        prenex = prenex_normal_form(f)\\n        self.assertTrue(prenex.is_exists())\\n        self.assertValid(Iff(f, prenex), logic=BOOL)\\n\\n    def test_prenex_examples(self):\\n        for (f, _, _, logic) in get_example_formulae():\\n            if self.env.factory.has_solvers(logic=logic):\\n                prenex = prenex_normal_form(f)\\n                if ( prenex is not None):\\n                    try:\\n                        ok = is_valid(Iff(f, prenex), logic=logic)\\n                    except SolverReturnedUnknownResultError:\\n                        ok = not logic.quantifier_free\\n                    self.assertTrue(ok)\\n\\n    def test_nnf_examples(self):\\n        for (f, _, _, logic) in get_example_formulae():\\n            if self.env.factory.has_solvers(logic=logic):\\n                rf = nnf(f)\\n                try:\\n                    ok = is_valid(Iff(f, rf), logic=logic)\\n                except SolverReturnedUnknownResultError:\\n                    ok = not logic.quantifier_free\\n                self.assertTrue(ok)\\n\\n    def test_conj_partitioning(self):\\n        for (f, _, _, logic) in get_example_formulae():\\n            if self.env.factory.has_solvers(logic=logic):\\n                conjuncts = list(conjunctive_partition(f))\\n                try:\\n                    ok = is_valid(Iff(f, And(conjuncts)), logic=logic)\\n                except SolverReturnedUnknownResultError:\\n                    ok = not logic.quantifier_free\\n                self.assertTrue(ok)\\n\\n    def test_disj_partitioning(self):\\n        for (f, _, _, logic) in get_example_formulae():\\n            if self.env.factory.has_solvers(logic=logic):\\n                disjuncts = list(disjunctive_partition(f))\\n                try:\\n                    ok = is_valid(Iff(f, Or(disjuncts)), logic=logic)\\n                except SolverReturnedUnknownResultError:\\n                    ok = not logic.quantifier_free\\n                self.assertTrue(ok)\\n\\n    def test_aig_examples(self):\\n        for (f, _, _, logic) in get_example_formulae():\\n            if self.env.factory.has_solvers(logic=logic):\\n                f_aig = aig(f)\\n                try:\\n                    ok = is_valid(Iff(f, f_aig), logic=logic)\\n                except SolverReturnedUnknownResultError:\\n                    ok = not logic.quantifier_free\\n                self.assertTrue(ok, \"Was: %s\\\\n Got:%s\" % (f, f_aig))\\n\\n    @skipIfNoSolverForLogic(QF_NRA)\\n    def test_times_distributivity(self):\\n        r = Symbol(\"r\", REAL)\\n        s = Symbol(\"s\", REAL)\\n        td = TimesDistributor()\\n\\n        f = Times(Plus(r, Real(1)), Real(3))\\n        fp = td.walk(f)\\n        self.assertValid(Equals(f, fp), (f, fp))\\n\\n        f = Times(Plus(r, Real(1)), s)\\n        fp = td.walk(f)\\n        self.assertValid(Equals(f, fp), (f, fp))\\n\\n        f = Times(Plus(r, Real(1), s), Real(3))\\n        fp = td.walk(f)\\n        self.assertValid(Equals(f, fp), (f, fp))\\n\\n        f = Times(Minus(r, Real(1)), Real(3))\\n        fp = td.walk(f)\\n        self.assertValid(Equals(f, fp), (f, fp))\\n\\n        f = Times(Minus(r, Real(1)), s)\\n        fp = td.walk(f)\\n        self.assertValid(Equals(f, fp), (f, fp))\\n\\n        f = Times(Minus(Real(1), s), Real(3))\\n        fp = td.walk(f)\\n        self.assertValid(Equals(f, fp), (f, fp))\\n\\n        f = Times(Minus(r, Real(1)), Plus(r, s))\\n        fp = td.walk(f)\\n        self.assertValid(Equals(f, fp), (f, fp))\\n\\n        # (r + 1) * (s-1) = r*s + (-r) + s - 1\\n        f = Times(Plus(r, Real(1)), Minus(s, Real(1)))\\n        fp = td.walk(f).simplify()\\n        target = Plus(Times(r, s),\\n                      Times(r, Real(-1)),\\n                      s,\\n                      Real(-1))\\n        self.assertValid(Equals(fp, target), fp)\\n        self.assertTrue(fp.is_plus(), fp)\\n\\n    @skipIfNoSolverForLogic(QF_NRA)\\n    def test_times_distributivity_smtlib_nra(self):\\n        from pysmt.test.smtlib.parser_utils import formulas_from_smtlib_test_set\\n        test_set = formulas_from_smtlib_test_set(logics=[QF_LRA, QF_NRA])\\n        for (_, fname, f, _) in test_set:\\n            td = TimesDistributor()\\n            _ = td.walk(f)\\n            for (old, new) in td.memoization.items():\\n                if not old.is_times(): continue\\n                if old is new: continue # Nothing changed\\n                self.assertValid(Equals(old, new),\\n                                 (old, new), solver_name=\"z3\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n',\n",
       " 'import os\\nimport signal\\nimport subprocess\\nimport json\\nimport time\\nfrom datetime import datetime\\nimport threading\\nimport logging\\n\\nimport django\\n\\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"gpu_tasker.settings\")\\ndjango.setup()\\n\\nfrom base.utils import get_admin_config\\nfrom task.models import GPUTask\\nfrom task.utils import run_task\\nfrom gpu_info.models import GPUServer\\nfrom gpu_info.utils import GPUInfoUpdater\\n\\ntask_logger = logging.getLogger(\\'django.task\\')\\n\\n\\nif __name__ == \\'__main__\\':\\n    server_username, server_private_key_path = get_admin_config()\\n\\n    gpu_updater = GPUInfoUpdater(server_username, server_private_key_path)\\n    while True:\\n        task_logger.info(\\'Running processes: {:d}\\'.format(\\n            threading.active_count() - 1\\n        ))\\n        start_time = time.time()\\n        gpu_updater.update_gpu_info()\\n        for task in GPUTask.objects.filter(status=0):\\n            available_server = task.find_available_server()\\n            if available_server is not None:\\n                t = threading.Thread(target=run_task, args=(task, available_server))\\n                t.start()\\n                time.sleep(5)\\n        end_time = time.time()\\n\\n        # 确保至少间隔十秒更新一次\\n        duration = end_time - start_time\\n        if duration < 10:\\n            time.sleep(10 - duration)\\n',\n",
       " 'def raises(err, lamda):\\n    try:\\n        lamda()\\n        return False\\n    except err:\\n        return True\\n\\n\\ndef expand_tuples(L):\\n    \"\"\"\\n\\n    >>> expand_tuples([1, (2, 3)])\\n    [(1, 2), (1, 3)]\\n\\n    >>> expand_tuples([1, 2])\\n    [(1, 2)]\\n    \"\"\"\\n    if not L:\\n        return [()]\\n    elif not isinstance(L[0], tuple):\\n        rest = expand_tuples(L[1:])\\n        return [(L[0],) + t for t in rest]\\n    else:\\n        rest = expand_tuples(L[1:])\\n        return [(item,) + t for t in rest for item in L[0]]\\n\\n\\n# Taken from theano/theano/gof/sched.py\\n# Avoids licensing issues because this was written by Matthew Rocklin\\ndef _toposort(edges):\\n    \"\"\" Topological sort algorithm by Kahn [1] - O(nodes + vertices)\\n\\n    inputs:\\n        edges - a dict of the form {a: {b, c}} where b and c depend on a\\n    outputs:\\n        L - an ordered list of nodes that satisfy the dependencies of edges\\n\\n    >>> _toposort({1: (2, 3), 2: (3, )})\\n    [1, 2, 3]\\n\\n    Closely follows the wikipedia page [2]\\n\\n    [1] Kahn, Arthur B. (1962), \"Topological sorting of large networks\",\\n    Communications of the ACM\\n    [2] http://en.wikipedia.org/wiki/Toposort#Algorithms\\n    \"\"\"\\n    incoming_edges = reverse_dict(edges)\\n    incoming_edges = dict((k, set(val)) for k, val in incoming_edges.items())\\n    S = set((v for v in edges if v not in incoming_edges))\\n    L = []\\n\\n    while S:\\n        n = S.pop()\\n        L.append(n)\\n        for m in edges.get(n, ()):\\n            assert n in incoming_edges[m]\\n            incoming_edges[m].remove(n)\\n            if not incoming_edges[m]:\\n                S.add(m)\\n    if any(incoming_edges.get(v, None) for v in edges):\\n        raise ValueError(\"Input has cycles\")\\n    return L\\n\\n\\ndef reverse_dict(d):\\n    \"\"\"Reverses direction of dependence dict\\n\\n    >>> d = {\\'a\\': (1, 2), \\'b\\': (2, 3), \\'c\\':()}\\n    >>> reverse_dict(d)  # doctest: +SKIP\\n    {1: (\\'a\\',), 2: (\\'a\\', \\'b\\'), 3: (\\'b\\',)}\\n\\n    :note: dict order are not deterministic. As we iterate on the\\n        input dict, it make the output of this function depend on the\\n        dict order. So this function output order should be considered\\n        as undeterministic.\\n\\n    \"\"\"\\n    result = {}\\n    for key in d:\\n        for val in d[key]:\\n            result[val] = result.get(val, tuple()) + (key, )\\n    return result\\n\\n\\n# Taken from toolz\\n# Avoids licensing issues because this version was authored by Matthew Rocklin\\ndef groupby(func, seq):\\n    \"\"\" Group a collection by a key function\\n\\n    >>> names = [\\'Alice\\', \\'Bob\\', \\'Charlie\\', \\'Dan\\', \\'Edith\\', \\'Frank\\']\\n    >>> groupby(len, names)  # doctest: +SKIP\\n    {3: [\\'Bob\\', \\'Dan\\'], 5: [\\'Alice\\', \\'Edith\\', \\'Frank\\'], 7: [\\'Charlie\\']}\\n\\n    >>> iseven = lambda x: x % 2 == 0\\n    >>> groupby(iseven, [1, 2, 3, 4, 5, 6, 7, 8])  # doctest: +SKIP\\n    {False: [1, 3, 5, 7], True: [2, 4, 6, 8]}\\n\\n    See Also:\\n        ``countby``\\n    \"\"\"\\n\\n    d = dict()\\n    for item in seq:\\n        key = func(item)\\n        if key not in d:\\n            d[key] = list()\\n        d[key].append(item)\\n    return d\\n',\n",
       " 'import os #to run SLiM from python\\nimport numpy as np #for vectors etc\\nimport pyslim, tskit, msprime #to recapitate, mutate, and sample tree sequences, and compute statistics\\nimport csv #to save statistics\\n\\n# parameters for SLiM\\nK = 1e4 #carrying capacity\\nN0 = K #intial population size\\nd = 0.0 #decline rate of ancestral homozygote\\ns = 0.13 #selection coefficient\\nh = 0.5 #dominance coefficient\\nB = 2 #number of offspring per parent\\nL = 2e7 #number of sites on chromosome\\nL0 = round(L/2) #location of selected site (one of the center sites)\\nu = 1e-5 #mutation rate at selected site\\nm = 0 #migration rate\\nrbp = 2e-8 #per basepair recombination rate\\nk = 0 #initial number of beneficial alleles\\ndatadir = \"data/\" #location to put output files\\nnreps = 100 #number of replicates (number of runs where allele fixes and population recovers)\\nmaxt = 1000 #maximum number of generations (safestop that should never be reached)\\n\\n#parameters for msprime\\nNe = round(K * 4/7) #long-term effective population size\\nU = 6e-9 #per basepair mutation rate at neutral sites\\nnsamples = 100 #number of chromosomes to sample for stats\\nnwindows = 100 #number of windows across genome to compute stats for\\nR = 0.001 #recombination distance between neutral loci to calculate LD for\\n\\n#genome window calculation\\nwindows = np.linspace(0, L, nwindows+1) #delimit windows\\nmidpoints = windows[:-1]+(windows[1:]-windows[:-1])/2 #midpoint of each window\\ndistances = midpoints-L0 #distance of midpoint from selected site\\nrecombination = [(1-(1-2*rbp)**abs(i))/2*np.sign(i) for i in distances] #recombination rate at midpoint (signed for plotting)\\n\\n#empty vector for number of unique copies of beneficial allele remaining at fixation in each replicate\\nX = np.zeros(nreps)\\n\\n#for each replicate\\nfor i in range(nreps): \\n\\t\\n\\t# define SLiM script ...\\n\\tscript = \"\"\"\\n\\tinitialize() {\\n\\t\\t\\n\\t\\tdefineConstant(\"K\", %d); //carrying capacity (integer)\\n\\t\\tdefineConstant(\"N0\", %d); //initial pop size (integer)\\n\\t\\tdefineConstant(\"d\", %f); // wildtype decline rate [0,1]\\n\\t\\tdefineConstant(\"s\", %f); //beneficial selection coefficient ([0,1]; s>d for rescue to be possible) \\n\\t\\tdefineConstant(\"h\", %f); //beneficial dominance [0,1]\\n\\t\\tdefineConstant(\"B\", %d); //offspring per parent (positive integer; must be great than 1 for possible persistence) \\n\\t\\tdefineConstant(\"L\", %d - 1); //number of sites (positive integer)\\n\\t\\tdefineConstant(\"L0\", %d - 1); //site number of beneficial locus (positive integer, L0<L)\\n\\t\\tdefineConstant(\"u\", %.8f); //mutation rate at beneficial locus [0,1]\\n\\t\\tdefineConstant(\"m\", %.8f); //migration rate [0,1]\\n\\t\\tdefineConstant(\"rbp\", %.8f); //recombination rate per base pair [0,1]\\n\\t\\tdefineConstant(\"k\", %d); //initial number of mutants\\n\\t\\tdefineConstant(\"outfile_dynamics\", \"%sdynamics_%d.txt\"); //where to save dynamics\\n\\t\\tdefineConstant(\"outfile_tree\", \"%stree_%d.trees\"); //where to save tree\\n\\t\\tdefineConstant(\"simID\", getSeed()); //get the random seed to label temporary file\\n\\n\\t\\tinitializeSLiMModelType(\"nonWF\"); //non Wright Fisher model\\n\\t\\tinitializeTreeSeq(); //record the tree\\n\\t\\tinitializeMutationType(\"m1\", h, \"f\", s); //beneficial mutation characteristics\\n\\t\\tm1.mutationStackPolicy = \"f\"; //keep first mutation\\n\\t\\tinitializeMutationType(\"m2\", 0.5, \"f\", 0.0); //neutral mutations (heritability has no affect)\\n\\t\\tinitializeGenomicElementType(\"g1\", m1, 1.0); //define element g1 to have beneficial mutations\\n\\t\\tinitializeGenomicElementType(\"g2\", m2, 1.0); //define element g2 to have neutral mutations\\n\\t\\tinitializeGenomicElement(g1, L0, L0); //element g1 is just one site\\n\\t\\tinitializeGenomicElement(g2, 0, L0 - 1); // element g2 is everything to the left...\\n\\t\\tinitializeGenomicElement(g2, L0 + 1, L); // ...and everything to the right of LO\\n\\t\\tinitializeMutationRate(c(0,u,0), c(L0-1, L0, L)); //mutation rate per site \\n\\t\\tinitializeRecombinationRate(rbp); //recombination rate between sites\\n\\t\\twriteFile(outfile_dynamics, \"t n p\"); //start writing to the dynamics file     \\n\\t}\\n\\n\\treproduction() { //occurs immediately before early events\\n\\t\\tfor (i in 1:B) //B matings per parent\\n\\t\\t\\tsubpop.addCrossed(individual, subpop.sampleIndividuals(1)); //random mating, 1 offspring per pair\\n\\t}\\n\\n\\t//discrete generations, hard carrying capacity, census and update fitness\\n\\t1:%d early() {\\n\\n\\t\\t//initialize population\\n\\t\\tif (sim.generation == 1) {\\n\\t\\t\\tsim.addSubpop(\"p1\", N0); //initialize population of wildtypes\\n\\t\\t\\ttarget = sample(p1.genomes, k); //choose k chromosomes without replacement...\\n\\t\\t\\tfor (i in target)\\n\\t\\t\\t\\ti.addNewDrawnMutation(m1, L0); //... and give beneficial mutation\\n\\t\\t\\tsim.outputFull(\"/tmp/slim_\" + simID + \".txt\"); //output this initial state to use for future runs if needed\\n\\t\\t}\\n\\n\\t\\t//enforce discrete generations\\n\\t\\tinds = sim.subpopulations.individuals; //get info on all individuals\\n\\t\\tinds[inds.age > 0].fitnessScaling = 0.0; //parents all die at next instance of viability selection\\n\\n\\t\\t//hard carrying capacity by random culling\\n\\t\\toff = inds[inds.age == 0]; //offspring\\n\\t\\tN = length(off); //total number of offspring\\n\\t\\tindices = which(inds.age == 0); //indices of offspring\\n\\t\\tif (N > K) { //if more than K...\\n\\t\\t\\tinds[sample(indices, N-K)].fitnessScaling = 0.0; //...kill a random subset to reduce N to K\\n\\t\\t\\toff = inds[inds.fitnessScaling > 0]; //get surviving offspring\\n\\t\\t}\\t\\n\\n\\t\\t// migration\\n\\t\\tif (m>0 & N>0) { //if adapting from migration and some offspring made\\n\\t\\t\\tif (runif(1)<m) { //with probability m\\n\\t\\t\\t\\ttarget = sample(off.genomes, 1); //choose a chromosome to add a migrant allele to\\n\\t \\t\\t\\ttarget.addNewDrawnMutation(m1, L0); //add the migrant allele\\t\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\t\\n\\t\\t// census offspring\\n\\t\\tN = length(off); //population size\\n\\t\\tfreq = sum(asInteger(off.genomes.countOfMutationsOfType(m1)>0))/(2*N); //frequency of beneficial mutation\\n\\t\\tif ((u==0 & m==0 & freq == 0) | (N==0)) { //if fail to adapt\\n\\t        writeFile(\"data/prescue.csv\", \"0\", append=T); //record extinction\\n\\t        writeFile(outfile_dynamics, \"t n p\"); //erase and restart the output file\\n\\t\\t\\tcatn(\"all hope was lost in generation \" + sim.generation + \" - RESTARTING\"); //alert the user\\n\\t\\t\\tsim.readFromPopulationFile(\"/tmp/slim_\" + simID + \".txt\"); //reinitialize simulation\\n\\t\\t}\\n\\t\\telse {           \\n\\t\\t\\tcatn(sim.generation + \": \" + N + \", \" + freq); //print generation and population size and frequency\\n\\t\\t\\twriteFile(outfile_dynamics, sim.generation + \" \" + N + \" \" + freq, append=T);\\n\\t        if (freq == 1.0 & N == K) { //if mutation fixed and population recovered\\n\\t        \\twriteFile(\"data/prescue.csv\", \"1\", append=T); //record rescue\\n\\t\\t\\t\\tcatn(\"rescue complete in generation \" + sim.generation);\\n\\t            sim.simulationFinished(); //end simulation\\n\\t            sim.treeSeqOutput(outfile_tree); //save tree sequence\\n\\t        }\\n\\t\\t}\\n\\n\\t\\t//fitness scaling (viability selection occurs after early events)\\n\\t\\tp1.fitnessScaling = (1.0 - d)/B; //survival probability V = X(1-d)/B, where X is the fitness effect of the selected site (X=1 for wildtype, X=1+s*h for heterozygotes, X=1+s for mutant homozygotes)\\n\\t}\\n\\n\\t//backup: end simulation if runs too long and print warning to increase maxt\\n\\t%d late () {\\n\\t\\tcatn(\"times up, make maxt longer!\");\\n\\t\\tsim.simulationFinished();\\n\\t}\\n\\t\"\"\" %(K,N0,d,s,h,B,L,L0,u,m,rbp,k,datadir,i,datadir,i,maxt,maxt+1)\\n\\n\\t# and run it\\n\\tos.system(\"echo \\'\" + script + \"\\' | slim\") \\n\\n\\t# then load tree-sequences\\n\\tts = pyslim.load(\"%stree_%d.trees\" %(datadir, i)) \\n\\t\\n\\t# calculate the number of unique copies of beneficial allele\\n\\tX[i] = len(np.unique([i.genotypes for i in ts.variants()][0])) \\n\\t\\n\\t# recapitate the tree sequence and overlay neutral mutations\\n\\tts = msprime.mutate(ts.recapitate(rbp, Ne=Ne).simplify(), rate=U, keep=True)\\n\\t\\n\\t# take a random sample\\n\\toffspring_nodes = (np.where(ts.tables.nodes.time==0.)[0]).astype(np.int32) #chromosomes in offspring (to exclude the parental generation from the samples)\\n\\tsamples = np.random.choice(offspring_nodes, nsamples, replace=False) #random sample of chromosomes in offspring\\n\\tts = ts.simplify(samples) #simplify to sample only\\n\\tts.dump(\"%stree_%d_sample.trees\" %(datadir, i)) #save sample tree (in case need to calculate any more statistics)\\n\\t# ts = tskit.load(\"%stree_%d_sample.trees\" %(datadir,i)) #load dumped version if running new stats below\\n\\n\\t# calculate pairwise diversity\\n\\tsite_div = ts.diversity([ts.samples()], windows=windows) #compute average pairwise diversity in windows\\n\\tsite_div = [i[0] for i in site_div] #flatten site_div list\\n\\t\\n\\t# calculate Tajima\\'s D\\n\\ttajimasD = ts.Tajimas_D([ts.samples()], windows=windows) #compute average Tajima\\'s D in windows\\n\\ttajimasD = [i[0] for i in tajimasD] #flatten tajimasD list\\n\\t\\n\\t# save pairwise diversity and Tajima\\'s D to file, with associated recombination rates\\n\\tcsvData = zip(recombination, site_div, tajimasD)\\n\\twith open(\\'%sstats_%d.csv\\' %(datadir,i), \\'w\\') as csvFile:\\n\\t\\twriter = csv.writer(csvFile)\\n\\t\\twriter.writerows(csvData)\\n\\tcsvFile.close()\\n\\n\\t# calculate site frequency spectrum\\n\\tsfs = ts.allele_frequency_spectrum([ts.samples()], windows=windows, polarised=True) #unfolded SFS, averaged within windows\\n\\tnp.savetxt(\\'%ssfs_%d.txt\\' %(datadir,i), sfs)\\n\\n\\t# calculate linkage disequilibrium\\n\\tall_positions = [i.position for i in ts.mutations()] #positions of all mutations\\n\\tfreqs = [sum(i.genotypes)/nsamples for i in ts.variants()] #frequency of derived alleles at these positions\\n\\tseg = [i for i,j in enumerate(freqs) if 0<j and j<1] #indices of segregating mutations\\n\\tpositions = [all_positions[i] for i in seg] #positions of segregating mutations\\n\\tidxs = [np.argmin(np.abs(positions-i)) for i in midpoints] #find mutation indices nearest the window midpoints\\n\\tidx_positions = [positions[i] for i in idxs] #positions of mutations nearest midpoints\\n\\tdistance = np.log(1-R)/np.log(1-2*rbp) #convert the specified recombination rate between mutations to number of sites\\n\\tother_positions = idx_positions + distance #this is where we want the other mutation\\n\\tother_idxs = [np.argmin(np.abs(positions-i)) for i in other_positions] #mutation indices nearest the desired poition\\n\\tlds = [tskit.LdCalculator(ts).r2(seg[idxs[i]], seg[other_idxs[i]]) for i in range(nwindows)] #linkage disequilibrium between the two mutations\\n\\n\\t# save ld to file with associated recombination rates\\n\\tcsvData = zip(recombination, lds)\\n\\twith open(\\'%sld_%d.csv\\' %(datadir,i), \\'w\\') as csvFile:\\n\\t\\twriter = csv.writer(csvFile)\\n\\t\\twriter.writerows(csvData)\\n\\tcsvFile.close()\\n\\n# save number of unique copies of beneficial allele remaining in each replicate\\nnp.savetxt(\\'%snalleles.txt\\' %datadir, X, fmt=\\'%d\\')\\n',\n",
       " \"from collections import namedtuple\\n\\nfrom prettyparse import create_parser\\n\\nfrom precise.network_runner import Listener\\nfrom precise.params import inject_params\\nfrom precise.train_data import TrainData\\nfrom select import select\\nfrom sys import stdin\\nimport pygame\\nimport time\\nimport os\\nimport shutil\\nfrom collections import Counter\\n\\nusage = '''\\n    Retag false negatives as wakeword or not wakeword\\n    \\n    :model str\\n        Either Keras (.net) or TensorFlow (.pb) model to test\\n    \\n    :-t --use-train\\n        Evaluate training data instead of test data\\n    \\n    :-nf --no-filenames\\n        Don't print out the names of files that failed\\n    \\n    :-ap --append\\n        Append new tags file to old one\\n    ...\\n'''\\n\\nStats = namedtuple('Stats', 'false_pos false_neg true_pos true_neg')\\n\\ndef calc_stats(filenames, targets, predictions) -> Stats:\\n    stats = Stats([], [], [], [])\\n    for name, target, prediction in zip(filenames, targets, predictions):\\n        {\\n            (True, False): stats.false_pos,\\n            (True, True): stats.true_pos,\\n            (False, True): stats.false_neg,\\n            (False, False): stats.true_neg\\n        }[prediction[0] > 0.5, target[0] > 0.5].append(name)\\n    return stats\\n\\ndef main():\\n    args = TrainData.parse_args(create_parser(usage))\\n\\n    inject_params(args.model)\\n\\n    data = TrainData.from_both(args.tags_file, args.tags_folder, args.folder)\\n    train, test = data.load(args.use_train, not args.use_train)\\n    inputs, targets = train if args.use_train else test\\n\\n    filenames = sum(data.train_files if args.use_train else data.test_files, [])\\n    predictions = Listener.find_runner(args.model)(args.model).predict(inputs)\\n    stats = calc_stats(filenames, targets, predictions)\\n    false_negatives_array = stats.false_neg\\n    new_tags = open('tags.txt', 'w')\\n    \\n\\n    changed_tags_array = []\\n    for i in range(0, len(false_negatives_array)):\\n        pygame.mixer.init(frequency=8000, size=-16, channels=2, buffer=4096)\\n        pygame.mixer.music.load(false_negatives_array[i])\\n        pygame.mixer.music.play()\\n        print('False negative ', (i + 1), ' of ', (len(false_negatives_array)) + 1)\\n        user_input = input('Enter y if wakeword, enter n for not wakeword \\\\n')\\n        time.sleep(5)\\n        false_negatives_array[i] = false_negatives_array[i].lstrip('/Users/madmitrienko/wakewords/files/')\\n        false_negatives_array[i] = false_negatives_array[i].rstrip('.wav')\\n        if(user_input == 'y'):\\n            write_to_tags = '\\\\n' + false_negatives_array[i] + '\\twake-word'\\n            new_tags.write(write_to_tags)\\n\\n        elif(user_input == 'n'):\\n            write_to_tags = '\\\\n' + false_negatives_array[i] + '\\tnot-wake-word'          \\n            new_tags.write(write_to_tags)\\n    new_tags.close()\\n    tags.close()\\n    \\nif __name__ == '__main__':\\n    main()\\n\",\n",
       " '\"\"\"\\nPython Challenge Level 15.\\n\\nThe page title is \\'whom?\\' and the image is a calendar of January 1??6 with Monday\\nJan 26th circled. There are two comments in the page source, \\'he ain\\'t the\\nyoungest, he is the second\\' and \\'todo: buy flowers for tomorrow\\'.\\n\"\"\"\\nimport calendar\\nimport datetime\\n\\nGREGORIAN_CALENDAR_START = 1582\\n\\n\\ndef ends_in_6(year):\\n    \"\"\"\\n    Does the year end in \\'6\\'?\\n    \"\"\"\\n    return year % 10 == 6\\n\\n\\ndef jan_26th_is_monday(year):\\n    \"\"\"\\n    In this year, is January 26th a monday?\\n    \"\"\"\\n    return calendar.weekday(year, 1, 26) == 0\\n\\nmatching_years = []\\nfor year in range(GREGORIAN_CALENDAR_START, 2000):\\n    # Determine the years which could match the calendar conditions:\\n    if jan_26th_is_monday(year) and calendar.isleap(year) and ends_in_6(year):\\n        matching_years.append(year)\\n\\n# \\'he ain\\'t the youngest, he is the second\\' - take the second youngest year\\nyear = matching_years[-2]\\n\\n# \\'todo: buy flowers for tomorrow\\nprint(datetime.date(year, 1, 26 + 1))\\n# \\'1756-01-27\\', which is Mozart\\'s birthday\\n# http://www.pythonchallenge.com/pc/return/mozart.html is the next URL.\\n',\n",
       " '\"Rules for running Rollup under Bazel\"\\n\\nload(\"@build_bazel_rules_nodejs//:providers.bzl\", \"JSEcmaScriptModuleInfo\", \"NodeContextInfo\", \"NpmPackageInfo\", \"node_modules_aspect\", \"run_node\")\\nload(\"@build_bazel_rules_nodejs//internal/linker:link_node_modules.bzl\", \"module_mappings_aspect\")\\n\\n_DOC = \"\"\"Runs the Rollup.js CLI under Bazel.\\n\\nSee https://rollupjs.org/guide/en/#command-line-reference\\n\\nTypical example:\\n```python\\nload(\"@npm_bazel_rollup//:index.bzl\", \"rollup_bundle\")\\n\\nrollup_bundle(\\n    name = \"bundle\",\\n    srcs = [\"dependency.js\"],\\n    entry_point = \"input.js\",\\n    config_file = \"rollup.config.js\",\\n)\\n```\\n\\nNote that the command-line options set by Bazel override what appears in the rollup config file.\\nThis means that typically a single `rollup.config.js` can contain settings for your whole repo,\\nand multiple `rollup_bundle` rules can share the configuration.\\n\\nThus, setting options that Bazel controls will have no effect, e.g.\\n\\n```javascript\\nmodule.exports = {\\n    output: { file: \\'this_is_ignored.js\\' },\\n}\\n```\\n\\nYou must determine ahead of time whether Rollup needs to produce a directory output.\\nThis is the case if you have dynamic imports which cause code-splitting, or if you\\nprovide multiple entry points. Use the `output_dir` attribute to specify that you want a\\ndirectory output.\\nRollup\\'s CLI has the same behavior, forcing you to pick `--output.file` or `--output.dir`.\\n\\nTo get multiple output formats, wrap the rule with a macro or list comprehension, e.g.\\n\\n```python\\n[\\n    rollup_bundle(\\n        name = \"bundle.%s\" % format,\\n        entry_point = \"foo.js\",\\n        format = format,\\n    )\\n    for format in [\\n        \"cjs\",\\n        \"umd\",\\n    ]\\n]\\n```\\n\\nThis will produce one output per requested format.\\n\"\"\"\\n\\n_ROLLUP_ATTRS = {\\n    \"srcs\": attr.label_list(\\n        doc = \"\"\"Non-entry point JavaScript source files from the workspace.\\n\\nYou must not repeat file(s) passed to entry_point/entry_points.\\n\"\"\",\\n        # Don\\'t try to constrain the filenames, could be json, svg, whatever\\n        allow_files = True,\\n    ),\\n    \"args\": attr.string_list(\\n        doc = \"\"\"Command line arguments to pass to rollup. Can be used to override config file settings.\\n\\nThese argument passed on the command line before all arguments that are always added by the\\nrule such as `--output.dir` or `--output.file`, `--format`, `--config` and `--preserveSymlinks` and\\nalso those that are optionally added by the rule such as `--sourcemap`.\\n\\nSee rollup CLI docs https://rollupjs.org/guide/en/#command-line-flags for complete list of supported arguments.\"\"\",\\n        default = [],\\n    ),\\n    \"config_file\": attr.label(\\n        doc = \"\"\"A rollup.config.js file\\n\\nPassed to the --config \\nSee https://rollupjs.org/guide/en/#configuration-files\\n\\nIf not set, a default basic Rollup config is used.\\n\"\"\",\\n        allow_single_file = True,\\n        default = \"@npm_bazel_rollup//:rollup.config.js\",\\n    ),\\n    \"entry_point\": attr.label(\\n        doc = \"\"\"The bundle\\'s entry point (e.g. your main.js or app.js or index.js).\\n\\nThis is just a shortcut for the `entry_points` attribute with a single output chunk named the same as the rule.\\n\\nFor example, these are equivalent:\\n\\n```python\\nrollup_bundle(\\n    name = \"bundle\",\\n    entry_point = \"index.js\",\\n)\\n```\\n\\n```python\\nrollup_bundle(\\n    name = \"bundle\",\\n    entry_points = {\\n        \"index.js\": \"bundle\"\\n    }\\n)\\n```\\n\\nIf `rollup_bundle` is used on a `ts_library`, the `rollup_bundle` rule handles selecting the correct outputs from `ts_library`.\\nIn this case, `entry_point` can be specified as the `.ts` file and `rollup_bundle` will handle the mapping to the `.mjs` output file.\\n\\nFor example:\\n\\n```python\\nts_library(\\n    name = \"foo\",\\n    srcs = [\\n        \"foo.ts\",\\n        \"index.ts\",\\n    ],\\n)\\n\\nrollup_bundle(\\n    name = \"bundle\",\\n    deps = [ \"foo\" ],\\n    entry_point = \"index.ts\",\\n)\\n```\\n\"\"\",\\n        allow_single_file = True,\\n    ),\\n    \"entry_points\": attr.label_keyed_string_dict(\\n        doc = \"\"\"The bundle\\'s entry points (e.g. your main.js or app.js or index.js).\\n\\nPassed to the [`--input` option](https://github.com/rollup/rollup/blob/master/docs/999-big-list-of-options.md#input) in Rollup.\\n\\nKeys in this dictionary are labels pointing to .js entry point files.\\nValues are the name to be given to the corresponding output chunk.\\n\\nEither this attribute or `entry_point` must be specified, but not both.\\n\"\"\",\\n        allow_files = True,\\n    ),\\n    \"format\": attr.string(\\n        doc = \"\"\"\"Specifies the format of the generated bundle. One of the following:\\n\\n- `amd`: Asynchronous Module Definition, used with module loaders like RequireJS\\n- `cjs`: CommonJS, suitable for Node and other bundlers\\n- `esm`: Keep the bundle as an ES module file, suitable for other bundlers and inclusion as a `<script type=module>` tag in modern browsers\\n- `iife`: A self-executing function, suitable for inclusion as a `<script>` tag. (If you want to create a bundle for your application, you probably want to use this.)\\n- `umd`: Universal Module Definition, works as amd, cjs and iife all in one\\n- `system`: Native format of the SystemJS loader\\n\"\"\",\\n        values = [\"amd\", \"cjs\", \"esm\", \"iife\", \"umd\", \"system\"],\\n        default = \"esm\",\\n    ),\\n    \"node_context_data\": attr.label(\\n        default = \"@build_bazel_rules_nodejs//internal:node_context_data\",\\n        providers = [NodeContextInfo],\\n        doc = \"Internal use only\",\\n    ),\\n    \"output_dir\": attr.bool(\\n        doc = \"\"\"Whether to produce a directory output.\\n\\nWe will use the [`--output.dir` option](https://github.com/rollup/rollup/blob/master/docs/999-big-list-of-options.md#outputdir) in rollup\\nrather than `--output.file`.\\n\\nIf the program produces multiple chunks, you must specify this attribute.\\nOtherwise, the outputs are assumed to be a single file.\\n\"\"\",\\n    ),\\n    \"rollup_bin\": attr.label(\\n        doc = \"Target that executes the rollup binary\",\\n        executable = True,\\n        cfg = \"host\",\\n        default = \"@npm//rollup/bin:rollup\",\\n    ),\\n    \"rollup_worker_bin\": attr.label(\\n        doc = \"Internal use only\",\\n        executable = True,\\n        cfg = \"host\",\\n        # NB: will be substituted with \"@npm//@bazel/rollup/bin:rollup-worker\" when the pkg_npm target is built\\n        default = \"@npm//@bazel/rollup/bin:rollup-worker\",\\n    ),\\n    \"silent\": attr.bool(\\n        doc = \"\"\"Whether to execute the rollup binary with the --silent flag, defaults to False.\\n\\nUsing --silent can cause rollup to [ignore errors/warnings](https://github.com/rollup/rollup/blob/master/docs/999-big-list-of-options.md#onwarn) \\nwhich are only surfaced via logging.  Since bazel expects printing nothing on success, setting silent to True\\nis a more Bazel-idiomatic experience, however could cause rollup to drop important warnings.\\n\"\"\",\\n    ),\\n    \"sourcemap\": attr.string(\\n        doc = \"\"\"Whether to produce sourcemaps.\\n\\nPassed to the [`--sourcemap` option](https://github.com/rollup/rollup/blob/master/docs/999-big-list-of-options.md#outputsourcemap\") in Rollup\\n\"\"\",\\n        default = \"inline\",\\n        values = [\"inline\", \"hidden\", \"true\", \"false\"],\\n    ),\\n    \"supports_workers\": attr.bool(\\n        doc = \"\"\"Experimental! Use only with caution.\\n\\nAllows you to enable the Bazel Worker strategy for this library.\\nWhen enabled, this rule invokes the \"rollup_worker_bin\"\\nworker aware binary rather than \"rollup_bin\".\"\"\",\\n        default = False,\\n    ),\\n    \"deps\": attr.label_list(\\n        aspects = [module_mappings_aspect, node_modules_aspect],\\n        doc = \"\"\"Other libraries that are required by the code, or by the rollup.config.js\"\"\",\\n    ),\\n}\\n\\ndef _desugar_entry_point_names(name, entry_point, entry_points):\\n    \"\"\"Users can specify entry_point (sugar) or entry_points (long form).\\n\\n    This function allows our code to treat it like they always used the long form.\\n\\n    It also performs validation:\\n    - exactly one of these attributes should be specified\\n    \"\"\"\\n    if entry_point and entry_points:\\n        fail(\"Cannot specify both entry_point and entry_points\")\\n    if not entry_point and not entry_points:\\n        fail(\"One of entry_point or entry_points must be specified\")\\n    if entry_point:\\n        return [name]\\n    return entry_points.values()\\n\\ndef _desugar_entry_points(name, entry_point, entry_points, inputs):\\n    \"\"\"Like above, but used by the implementation function, where the types differ.\\n\\n    It also performs validation:\\n    - attr.label_keyed_string_dict doesn\\'t accept allow_single_file\\n      so we have to do validation now to be sure each key is a label resulting in one file\\n\\n    It converts from dict[target: string] to dict[file: string]\\n    \"\"\"\\n    names = _desugar_entry_point_names(name, entry_point.label if entry_point else None, entry_points)\\n\\n    if entry_point:\\n        return {_resolve_js_input(entry_point.files.to_list()[0], inputs): names[0]}\\n\\n    result = {}\\n    for ep in entry_points.items():\\n        entry_point = ep[0]\\n        name = ep[1]\\n        f = entry_point.files.to_list()\\n        if len(f) != 1:\\n            fail(\"keys in rollup_bundle#entry_points must provide one file, but %s has %s\" % (entry_point.label, len(f)))\\n        result[_resolve_js_input(f[0], inputs)] = name\\n    return result\\n\\ndef _resolve_js_input(f, inputs):\\n    if f.extension == \"js\" or f.extension == \"mjs\":\\n        return f\\n\\n    # look for corresponding js file in inputs\\n    no_ext = _no_ext(f)\\n    for i in inputs:\\n        if i.extension == \"js\" or i.extension == \"mjs\":\\n            if _no_ext(i) == no_ext:\\n                return i\\n    fail(\"Could not find corresponding javascript entry point for %s. Add the %s.js to your deps.\" % (f.path, no_ext))\\n\\ndef _rollup_outs(sourcemap, name, entry_point, entry_points, output_dir):\\n    \"\"\"Supply some labelled outputs in the common case of a single entry point\"\"\"\\n    result = {}\\n    entry_point_outs = _desugar_entry_point_names(name, entry_point, entry_points)\\n    if output_dir:\\n        # We can\\'t declare a directory output here, because RBE will be confused, like\\n        # com.google.devtools.build.lib.remote.ExecutionStatusException:\\n        # INTERNAL: failed to upload outputs: failed to construct CAS files:\\n        # failed to calculate file hash:\\n        # read /b/f/w/bazel-out/k8-fastbuild/bin/packages/rollup/test/multiple_entry_points/chunks: is a directory\\n        #result[\"chunks\"] = output_dir\\n        return {}\\n    else:\\n        if len(entry_point_outs) > 1:\\n            fail(\"Multiple entry points require that output_dir be set\")\\n        out = entry_point_outs[0]\\n        result[out] = out + \".js\"\\n        if sourcemap == \"true\":\\n            result[out + \"_map\"] = \"%s.map\" % result[out]\\n    return result\\n\\ndef _no_ext(f):\\n    return f.short_path[:-len(f.extension) - 1]\\n\\ndef _filter_js(files):\\n    return [f for f in files if f.extension == \"js\" or f.extension == \"mjs\"]\\n\\ndef _rollup_bundle(ctx):\\n    \"Generate a rollup config file and run rollup\"\\n\\n    # rollup_bundle supports deps with JS providers. For each dep,\\n    # JSEcmaScriptModuleInfo is used if found, then JSModuleInfo and finally\\n    # the DefaultInfo files are used if the former providers are not found.\\n    deps_depsets = []\\n    for dep in ctx.attr.deps:\\n        if JSEcmaScriptModuleInfo in dep:\\n            deps_depsets.append(dep[JSEcmaScriptModuleInfo].sources)\\n        elif hasattr(dep, \"files\"):\\n            deps_depsets.append(dep.files)\\n\\n        # Also include files from npm deps as inputs.\\n        # These deps are identified by the NpmPackageInfo provider.\\n        if NpmPackageInfo in dep:\\n            deps_depsets.append(dep[NpmPackageInfo].sources)\\n    deps_inputs = depset(transitive = deps_depsets).to_list()\\n\\n    inputs = _filter_js(ctx.files.entry_point) + _filter_js(ctx.files.entry_points) + ctx.files.srcs + deps_inputs\\n    outputs = [getattr(ctx.outputs, o) for o in dir(ctx.outputs)]\\n\\n    # See CLI documentation at https://rollupjs.org/guide/en/#command-line-reference\\n    args = ctx.actions.args()\\n\\n    if ctx.attr.supports_workers:\\n        # Set to use a multiline param-file for worker mode\\n        args.use_param_file(\"@%s\", use_always = True)\\n        args.set_param_file_format(\"multiline\")\\n\\n    # Add user specified arguments *before* rule supplied arguments\\n    args.add_all(ctx.attr.args)\\n\\n    # List entry point argument first to save some argv space\\n    # Rollup doc says\\n    # When provided as the first options, it is equivalent to not prefix them with --input\\n    entry_points = _desugar_entry_points(ctx.label.name, ctx.attr.entry_point, ctx.attr.entry_points, inputs).items()\\n\\n    # If user requests an output_dir, then use output.dir rather than output.file\\n    if ctx.attr.output_dir:\\n        outputs.append(ctx.actions.declare_directory(ctx.label.name))\\n        for entry_point in entry_points:\\n            args.add_joined([entry_point[1], entry_point[0]], join_with = \"=\")\\n        args.add_all([\"--output.dir\", outputs[0].path])\\n    else:\\n        args.add(entry_points[0][0])\\n        args.add_all([\"--output.file\", outputs[0].path])\\n\\n    args.add_all([\"--format\", ctx.attr.format])\\n\\n    if ctx.attr.silent:\\n        # Run the rollup binary with the --silent flag\\n        args.add(\"--silent\")\\n\\n    stamp = ctx.attr.node_context_data[NodeContextInfo].stamp\\n\\n    config = ctx.actions.declare_file(\"_%s.rollup_config.js\" % ctx.label.name)\\n    ctx.actions.expand_template(\\n        template = ctx.file.config_file,\\n        output = config,\\n        substitutions = {\\n            \"bazel_stamp_file\": \"\\\\\"%s\\\\\"\" % ctx.version_file.path if stamp else \"undefined\",\\n        },\\n    )\\n\\n    args.add_all([\"--config\", config.path])\\n    inputs.append(config)\\n\\n    if stamp:\\n        inputs.append(ctx.version_file)\\n\\n    # Prevent rollup\\'s module resolver from hopping outside Bazel\\'s sandbox\\n    # When set to false, symbolic links are followed when resolving a file.\\n    # When set to true, instead of being followed, symbolic links are treated as if the file is\\n    # where the link is.\\n    args.add(\"--preserveSymlinks\")\\n\\n    if (ctx.attr.sourcemap and ctx.attr.sourcemap != \"false\"):\\n        args.add_all([\"--sourcemap\", ctx.attr.sourcemap])\\n\\n    executable = \"rollup_bin\"\\n    execution_requirements = {}\\n\\n    if ctx.attr.supports_workers:\\n        executable = \"rollup_worker_bin\"\\n        execution_requirements[\"supports-workers\"] = str(int(ctx.attr.supports_workers))\\n\\n    run_node(\\n        ctx,\\n        progress_message = \"Bundling JavaScript %s [rollup]\" % outputs[0].short_path,\\n        executable = executable,\\n        inputs = inputs,\\n        outputs = outputs,\\n        arguments = [args],\\n        mnemonic = \"Rollup\",\\n        execution_requirements = execution_requirements,\\n        env = {\"COMPILATION_MODE\": ctx.var[\"COMPILATION_MODE\"]},\\n    )\\n\\n    return [\\n        DefaultInfo(files = depset(outputs)),\\n    ]\\n\\nrollup_bundle = rule(\\n    doc = _DOC,\\n    implementation = _rollup_bundle,\\n    attrs = _ROLLUP_ATTRS,\\n    outputs = _rollup_outs,\\n)\\n',\n",
       " 'import os\\nimport sys\\nimport pprint\\nfrom math import sin, cos, sqrt, atan2, radians\\nfrom deprecated import deprecated\\n\\n\\n# classe di supporto per controllare la quantità\\n# di output stampato a video\\nclass Verbosity:\\n    def __init__(self, quiet, verbose, more_verbose):\\n        self.quiet = quiet\\n        self.verbose = verbose or more_verbose  # se ho output more_verbose voglio che si stampi anche il verbose\\n        self.more_verbose = more_verbose\\n\\n\\n# Inutilizzato.\\n# Creato in origine per calcolare la distanza in 2D, ora si usa\\n# un metodo più preciso che tiene conto della curvatura terrestre.\\n@deprecated(reason=\"Utilizzare il metodo distance(), che tiene conto della curvatura terrestre\")\\ndef distance_in_2d(sens_one, sens_two):\\n    x_0 = sens_one.longitudine\\n    y_0 = sens_one.latitudine\\n    x_1 = sens_two.longitudine\\n    y_1 = sens_two.latitudine\\n    return sqrt((y_0 - y_1) ** 2 + (x_0 - x_1) ** 2)\\n\\n\\ndef find_sensor_by_id(sensor):\\n    for sen in get_global_sensors():\\n        if sen.id == sensor:\\n            return sen\\n    return None\\n\\n\\n# Prende in input due tuple di coordinate e restituisce la loro distanza sulla superficie terrestre\\ndef distance_by_coord(node_one, node_two):\\n    # Approssimazione del raggio della Terra in Km\\n    raggio_terra = 6373.0\\n\\n    lat1 = radians(node_one[0])\\n    lon1 = radians(node_one[1])\\n    lat2 = radians(node_two[0])\\n    lon2 = radians(node_two[1])\\n\\n    diff_lon = lon2 - lon1\\n    diff_lat = lat2 - lat1\\n\\n    a = sin(diff_lat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(diff_lon / 2) ** 2\\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\\n\\n    distanza = raggio_terra * c * 1000\\n    return distanza\\n\\n\\n# Prende in input due sensori e restituisce\\n# la loro distanza sulla superficie terrestre\\ndef distance(sens_one, sens_two):\\n    return distance_by_coord((sens_one.latitudine, sens_one.longitudine),\\n                             (sens_two.latitudine, sens_two.longitudine))\\n\\n\\ndef print_scenario(a_dict, order_by):\\n    print(\"\\\\n\\\\n\\\\n\\\\n\\\\n---------------------------------------------------\\\\n\\\\n\\\\n\\\\n\\\\n\")\\n    print(\"SCENARIO - ORDINATO PER: \" + order_by)\\n    for temp_sens in a_dict.keys():\\n        print(\"\\\\nSensore \" + str(temp_sens.id) + \":\")\\n        temp_val = a_dict[temp_sens]\\n        temp_sens_list = temp_val[\"senders\"]\\n        temp_tot_cap = temp_val[\"tot_capacita\"]\\n        temp_rapp_cap_costo = temp_val[\"rapp_cap_costo\"]\\n        temp_rapp_numsensori_costo = temp_val[\"rapp_numsensori_costo\"]\\n        print(\"Senders: \", end=\\'\\')\\n        for temp_sender in temp_sens_list:\\n            print(str(temp_sender.id) + \" \", end=\\'\\')\\n        print(\"\\\\nTot_capacità: \" + str(temp_tot_cap))\\n        print(\"Rapporto capacità/costo: \" + str(temp_rapp_cap_costo))\\n        print(\"Rapporto numsensori/costo: \" + str(temp_rapp_numsensori_costo))\\n    print(\"\\\\n\\\\n\")\\n\\n\\ndef print_greedy_result(result):\\n    if get_verbosity().verbose:\\n        print(\"\\\\n\\\\n\\\\n\")\\n        print(\"Dispositivi installati dalla greedy:\\\\n\")\\n        pp = pprint.PrettyPrinter(indent=3)\\n        pp.pprint(result)\\n    elif not get_verbosity().quiet:  # Se ho verbosity \"normale\" stampo solo i primi 3\\n        print(\"\\\\n\\\\n\\\\n\")\\n        print(\"Dispositivi installati dalla greedy (parziale):\\\\n\")\\n        pp = pprint.PrettyPrinter(indent=3)\\n        pp.pprint(dict(list(result.items())[:3]))\\n        print(\"\\\\t\\\\t.\\\\n\\\\t\\\\t.\\\\n\\\\t\\\\t.\\\\n\")\\n\\n\\ndef print_mst_result(mst):\\n    if get_verbosity().verbose:\\n        print(\"\\\\n\\\\n\\\\nArchi selezionati per il MST:\\\\n\")\\n        for edge in mst:\\n            print(f\"{edge[\\'node_one\\']} - {edge[\\'node_two\\']} - Costo {edge[\\'costo\\']}\")\\n    elif not get_verbosity().quiet:  # Se ho verbosity \"normale\" stampo solo i primi 3\\n        print(\"\\\\n\\\\n\\\\nArchi selezionati per il MST (parziale):\\\\n\")\\n        for edge in mst[:3]:\\n            print(f\"{edge[\\'node_one\\']} - {edge[\\'node_two\\']} - Costo {edge[\\'costo\\']}\")\\n        print(\"\\\\t.\\\\n\\\\t.\\\\n\\\\t.\\\\n\")\\n\\n\\ndef prepara_cartelle_e_file(num_sensori, order_by, pack_by, num_iter, no_display):\\n    if not os.path.isdir(\"./solutions\"):\\n        os.mkdir(\"./solutions\")\\n\\n    intestazione_csv = \"seed,numsensori,order_by,pack_by,num_iter_ls,\" + \\\\\\n                       \"greedy_cost,mst_cost,first_tot,first_ls_tot,second_ls_tot,\" + \\\\\\n                       \"num_gw_class_1,fattore_riduzione\"\\n\\n    # Se viene passata l\\'opzione --no-display si aggiunge solamente il risultato\\n    # dell\\'esecuzione al file .csv (per analisi e creazione di grafici)\\n    if no_display:\\n        text_output_path_grafici = f\"./solutions/graph_data.csv\"\\n\\n        if not os.path.isfile(text_output_path_grafici):\\n            with open(text_output_path_grafici, \\'w\\') as f:\\n                original_stdout = sys.stdout\\n                sys.stdout = f\\n                print(intestazione_csv)\\n                sys.stdout = original_stdout\\n\\n        return None, None, None, text_output_path_grafici\\n\\n    saving_path = f\"./solutions/{num_sensori}/{get_seed()}/{order_by}+{pack_by}+{num_iter}/\"\\n    saving_path_ls = saving_path + \"localsearch/\"\\n    text_output_path = saving_path + \"output.txt\"\\n    text_output_path_grafici = f\"./solutions/graph_data.csv\"\\n\\n    if not os.path.isdir(f\"./solutions/{num_sensori}\"):\\n        os.mkdir(f\"./solutions/{num_sensori}\")\\n\\n    if not os.path.isdir(f\"./solutions/{num_sensori}/{get_seed()}\"):\\n        os.mkdir(f\"./solutions/{num_sensori}/{get_seed()}\")\\n\\n    if not os.path.isdir(saving_path):\\n        os.mkdir(saving_path)\\n\\n    if not os.path.isdir(saving_path_ls):\\n        os.mkdir(saving_path_ls)\\n\\n    if os.path.isfile(text_output_path):\\n        os.remove(text_output_path)\\n\\n    if not os.path.isfile(text_output_path_grafici):\\n        with open(text_output_path_grafici, \\'w\\') as f:\\n            original_stdout = sys.stdout\\n            sys.stdout = f\\n            print(intestazione_csv)\\n            sys.stdout = original_stdout\\n\\n    return saving_path, saving_path_ls, text_output_path, text_output_path_grafici\\n\\n\\nverbosity = Verbosity(False, False, False)\\n\\n\\ndef get_verbosity():\\n    return verbosity\\n\\n\\ndef set_verbosity(quiet=False, verbose=False, more_verbose=False):\\n    global verbosity\\n    verbosity = Verbosity(quiet, verbose, more_verbose)\\n\\n\\nrandom_seed = 12345  # Per la riproducibilità degli esempi\\n# Il seed originale è 1625\\n\\n\\ndef get_seed():\\n    return random_seed\\n\\n\\ndef set_seed(new_seed):\\n    global random_seed\\n    random_seed = new_seed\\n\\n\\ngateway_classes = []\\n\\n\\ndef get_gateways_classes():\\n    return gateway_classes\\n\\n\\ndef set_gateways_classes(new_gateway_classes):\\n    global gateway_classes\\n    gateway_classes = new_gateway_classes\\n\\n\\nsensors = []\\n\\n\\ndef get_global_sensors():\\n    return sensors\\n\\n\\ndef set_global_sensors(new_sensors):\\n    global sensors\\n    sensors = new_sensors\\n',\n",
       " '\"\"\"\\nA simple IO staging mechanism\\n\\n\\n\\n\"\"\"\\n\\n#-----------------------------------------------------------------------------\\n# Copyright (c) 2013, yt Development Team.\\n#\\n# Distributed under the terms of the Modified BSD License.\\n#\\n# The full license is in the file COPYING.txt, distributed with this software.\\n#-----------------------------------------------------------------------------\\n\\nimport np\\nfrom yt.utilities.logger import ytLogger as mylog\\nfrom .parallel_analysis_interface import \\\\\\n    ProcessorPool, parallel_objects\\nfrom yt.utilities.io_handler import BaseIOHandler\\nfrom contextlib import contextmanager\\nimport time\\n\\ntry:\\n    from .parallel_analysis_interface import MPI\\nexcept ImportError:\\n    pass\\n\\nYT_TAG_MESSAGE = 317 # Cell 317 knows where to go\\n\\nclass IOCommunicator(BaseIOHandler):\\n    def __init__(self, ds, wg, pool):\\n        mylog.info(\"Initializing IOCommunicator\")\\n        self.ds = ds\\n        self.wg = wg # We don\\'t need to use this!\\n        self.pool = pool\\n        self.comm = pool.comm\\n        # We read our grids here\\n        self.grids = []\\n        storage = {}\\n        grids = ds.index.grids.tolist()\\n        grids.sort(key=lambda a:a.filename)\\n        for sto, g in parallel_objects(grids, storage = storage):\\n            sto.result = self.comm.rank\\n            sto.result_id = g.id\\n            self.grids.append(g)\\n        self._id_offset = ds.index.grids[0]._id_offset\\n        mylog.info(\"Reading from disk ...\")\\n        self.initialize_data()\\n        mylog.info(\"Broadcasting ...\")\\n        self.comm.comm.bcast(storage, root = wg.ranks[0])\\n        mylog.info(\"Done.\")\\n        self.hooks = []\\n\\n    def initialize_data(self):\\n        ds = self.ds\\n        fields = [f for f in ds.field_list\\n                  if not ds.field_info[f].particle_type]\\n        pfields = [f for f in ds.field_list\\n                   if ds.field_info[f].particle_type]\\n        # Preload is only defined for Enzo ...\\n        if ds.index.io._dataset_type == \"enzo_packed_3d\":\\n            self.queue = ds.index.io.queue\\n            ds.index.io.preload(self.grids, fields)\\n            for g in self.grids:\\n                for f in fields:\\n                    if f not in self.queue[g.id]:\\n                        d = np.zeros(g.ActiveDimensions, dtype=\\'float64\\')\\n                        self.queue[g.id][f] = d\\n                for f in pfields:\\n                    self.queue[g.id][f] = self._read(g, f)\\n        else:\\n            self.queue = {}\\n            for g in self.grids:\\n                for f in fields + pfields:\\n                    self.queue[g.id][f] = ds.index.io._read(g, f)\\n\\n    def _read(self, g, f):\\n        fi = self.ds.field_info[f]\\n        if fi.particle_type and g.NumberOfParticles == 0:\\n            # because this gets upcast to float\\n            return np.array([],dtype=\\'float64\\')\\n        try:\\n            temp = self.ds.index.io._read_data_set(g, f)\\n        except:# self.ds.index.io._read_exception as exc:\\n            if fi.not_in_all:\\n                temp = np.zeros(g.ActiveDimensions, dtype=\\'float64\\')\\n            else:\\n                raise\\n        return temp\\n\\n    def wait(self):\\n        status = MPI.Status()\\n        while 1:\\n            if self.comm.comm.Iprobe(MPI.ANY_SOURCE,\\n                                YT_TAG_MESSAGE,\\n                                status = status):\\n                msg = self.comm.comm.recv(\\n                        source = status.source, tag = YT_TAG_MESSAGE)\\n                if msg[\\'op\\'] == \"end\":\\n                    mylog.debug(\"Shutting down IO.\")\\n                    break\\n                self._send_data(msg, status.source)\\n                status = MPI.Status()\\n            else:\\n                time.sleep(1e-2)\\n\\n    def _send_data(self, msg, dest):\\n        grid_id = msg[\\'grid_id\\']\\n        field = msg[\\'field\\']\\n        ts = self.queue[grid_id][field].astype(\"float64\")\\n        mylog.debug(\"Opening send to %s (%s)\", dest, ts.shape)\\n        self.hooks.append(self.comm.comm.Isend([ts, MPI.DOUBLE], dest = dest))\\n\\nclass IOHandlerRemote(BaseIOHandler):\\n    _dataset_type = \"remote\"\\n\\n    def __init__(self, ds, wg, pool):\\n        self.ds = ds\\n        self.wg = wg # probably won\\'t need\\n        self.pool = pool\\n        self.comm = pool.comm\\n        self.proc_map = self.comm.comm.bcast(None,\\n                root = pool[\\'io\\'].ranks[0])\\n        super(IOHandlerRemote, self).__init__()\\n\\n    def _read_data_set(self, grid, field):\\n        dest = self.proc_map[grid.id]\\n        msg = dict(grid_id = grid.id, field = field, op=\"read\")\\n        mylog.debug(\"Requesting %s for %s from %s\", field, grid, dest)\\n        if self.ds.field_info[field].particle_type:\\n            data = np.empty(grid.NumberOfParticles, \\'float64\\')\\n        else:\\n            data = np.empty(grid.ActiveDimensions, \\'float64\\')\\n        hook = self.comm.comm.Irecv([data, MPI.DOUBLE], source = dest)\\n        self.comm.comm.send(msg, dest = dest, tag = YT_TAG_MESSAGE)\\n        mylog.debug(\"Waiting for data.\")\\n        MPI.Request.Wait(hook)\\n        return data\\n\\n    def _read_data_slice(self, grid, field, axis, coord):\\n        sl = [slice(None), slice(None), slice(None)]\\n        sl[axis] = slice(coord, coord + 1)\\n        #sl = tuple(reversed(sl))\\n        return self._read_data_set(grid,field)[sl]\\n\\n    def terminate(self):\\n        msg = dict(op=\\'end\\')\\n        if self.wg.comm.rank == 0:\\n            for rank in self.pool[\\'io\\'].ranks:\\n                mylog.debug(\"Sending termination message to %s\", rank)\\n                self.comm.comm.send(msg, dest=rank, tag=YT_TAG_MESSAGE)\\n\\n@contextmanager\\ndef remote_io(ds, wg, pool):\\n    original_io = ds.index.io\\n    ds.index.io = IOHandlerRemote(ds, wg, pool)\\n    yield\\n    ds.index.io.terminate()\\n    ds.index.io = original_io\\n\\ndef io_nodes(fn, n_io, n_work, func, *args, **kwargs):\\n    from yt.mods import load\\n    pool, wg = ProcessorPool.from_sizes([(n_io, \"io\"), (n_work, \"work\")])\\n    rv = None\\n    if wg.name == \"work\":\\n        ds = load(fn)\\n        with remote_io(ds, wg, pool):\\n            rv = func(ds, *args, **kwargs)\\n    elif wg.name == \"io\":\\n        ds = load(fn)\\n        io = IOCommunicator(ds, wg, pool)\\n        io.wait()\\n    # We should broadcast the result\\n    rv = pool.comm.mpi_bcast(rv, root=pool[\\'work\\'].ranks[0])\\n    pool.free_all()\\n    mylog.debug(\"Return value: %s\", rv)\\n    return rv\\n\\n# Here is an example of how to use this functionality.\\nif __name__ == \"__main__\":\\n    def gq(ds):\\n        dd = ds.all_data()\\n        return dd.quantities[\"TotalQuantity\"](\"CellMassMsun\")\\n    q = io_nodes(\"DD0087/DD0087\", 8, 24, gq)\\n    mylog.info(q)\\n\\n\\n',\n",
       " '# -*- coding: utf-8 -*-\\n\\nfrom setuptools import setup, find_packages\\n\\n\\nwith open(\\'README.md\\') as f:\\n    readme = f.read()\\n\\nwith open(\\'LICENSE\\') as f:\\n    license = f.read()\\n\\nsetup(\\n    name=\\'xcodeproject\\',\\n    version=\\'0.9\\',\\n    description=\\'Xcode project file inspection utilities\\',\\n    long_description=readme,\\n    author=\\'Marc Liyanage\\',\\n    author_email=\\'reg.python-xcodeproject@entropy.ch\\',\\n    url=\\'https://github.com/liyanage/python-xcodeproject\\',\\n    license=license,\\n    packages=find_packages(exclude=(\\'tests\\', \\'docs\\')),\\n    entry_points = {\\n        \"console_scripts\": [\\n            \"xcodeproject-util=xcodeproject.tool:XcodeprojectTool.main\",\\n        ],\\n    }\\n)\\n',\n",
       " '# Licensed to the Apache Software Foundation (ASF) under one\\n# or more contributor license agreements.  See the NOTICE file\\n# distributed with this work for additional information\\n# regarding copyright ownership.  The ASF licenses this file\\n# to you under the Apache License, Version 2.0 (the\\n# \"License\"); you may not use this file except in compliance\\n# with the License.  You may obtain a copy of the License at\\n#\\n#   http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing,\\n# software distributed under the License is distributed on an\\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\n# KIND, either express or implied.  See the License for the\\n# specific language governing permissions and limitations\\n# under the License.\\nimport datetime\\nimport json\\nimport random\\n\\nimport pandas as pd\\nfrom sqlalchemy import Date, Float, String\\n\\nfrom superset import db\\nfrom superset.models.dashboard import Dashboard\\nfrom superset.models.slice import Slice\\nfrom superset.utils import core as utils\\n\\nfrom .helpers import (\\n    config,\\n    get_example_data,\\n    get_slice_json,\\n    merge_slice,\\n    TBL,\\n    update_slice_ids,\\n)\\n\\n\\ndef load_unicode_test_data(only_metadata: bool = False, force: bool = False) -> None:\\n    \"\"\"Loading unicode test dataset from a csv file in the repo\"\"\"\\n    tbl_name = \"unicode_test\"\\n    database = utils.get_example_database()\\n    table_exists = database.has_table_by_name(tbl_name)\\n\\n    if not only_metadata and (not table_exists or force):\\n        data = get_example_data(\\n            \"unicode_utf8_unixnl_test.csv\", make_bytes=False\\n        )\\n        df = pd.read_csv(data, encoding=\"utf-8\")\\n        # generate date/numeric data\\n        df[\"dttm\"] = datetime.datetime.now().date()\\n        df[\"value\"] = [random.randint(1, 100) for _ in range(len(df))]\\n        df.to_sql(  # pylint: disable=no-member\\n            tbl_name,\\n            database.get_sqla_engine(),\\n            if_exists=\"replace\",\\n            chunksize=500,\\n            dtype={\\n                \"phrase\": String(500),\\n                \"short_phrase\": String(10),\\n                \"with_missing\": String(100),\\n                \"dttm\": Date(),\\n                \"value\": Float(),\\n            },\\n            index=False,\\n        )\\n        print(\"Done loading table!\")\\n        print(\"-\" * 80)\\n\\n    print(\"Creating table [unicode_test] reference\")\\n    obj = db.session.query(TBL).filter_by(table_name=tbl_name).first()\\n    if not obj:\\n        obj = TBL(table_name=tbl_name)\\n    obj.main_dttm_col = \"dttm\"\\n    obj.database = database\\n    db.session.merge(obj)\\n    db.session.commit()\\n    obj.fetch_metadata()\\n    tbl = obj\\n\\n    slice_data = {\\n        \"granularity_sqla\": \"dttm\",\\n        \"groupby\": [],\\n        \"metric\": {\\n            \"aggregate\": \"SUM\",\\n            \"column\": {\"column_name\": \"value\"},\\n            \"expressionType\": \"SIMPLE\",\\n            \"label\": \"Value\",\\n        },\\n        \"row_limit\": config[\"ROW_LIMIT\"],\\n        \"since\": \"100 years ago\",\\n        \"until\": \"now\",\\n        \"viz_type\": \"word_cloud\",\\n        \"size_from\": \"10\",\\n        \"series\": \"short_phrase\",\\n        \"size_to\": \"70\",\\n        \"rotation\": \"square\",\\n        \"limit\": \"100\",\\n    }\\n\\n    print(\"Creating a slice\")\\n    slc = Slice(\\n        slice_name=\"Unicode Cloud\",\\n        viz_type=\"word_cloud\",\\n        datasource_type=\"table\",\\n        datasource_id=tbl.id,\\n        params=get_slice_json(slice_data),\\n    )\\n    merge_slice(slc)\\n\\n    print(\"Creating a dashboard\")\\n    dash = db.session.query(Dashboard).filter_by(slug=\"unicode-test\").first()\\n\\n    if not dash:\\n        dash = Dashboard()\\n    js = \"\"\"\\\\\\n{\\n    \"CHART-Hkx6154FEm\": {\\n        \"children\": [],\\n        \"id\": \"CHART-Hkx6154FEm\",\\n        \"meta\": {\\n            \"chartId\": 2225,\\n            \"height\": 30,\\n            \"sliceName\": \"slice 1\",\\n            \"width\": 4\\n        },\\n        \"type\": \"CHART\"\\n    },\\n    \"GRID_ID\": {\\n        \"children\": [\\n            \"ROW-SyT19EFEQ\"\\n        ],\\n        \"id\": \"GRID_ID\",\\n        \"type\": \"GRID\"\\n    },\\n    \"ROOT_ID\": {\\n        \"children\": [\\n            \"GRID_ID\"\\n        ],\\n        \"id\": \"ROOT_ID\",\\n        \"type\": \"ROOT\"\\n    },\\n    \"ROW-SyT19EFEQ\": {\\n        \"children\": [\\n            \"CHART-Hkx6154FEm\"\\n        ],\\n        \"id\": \"ROW-SyT19EFEQ\",\\n        \"meta\": {\\n            \"background\": \"BACKGROUND_TRANSPARENT\"\\n        },\\n        \"type\": \"ROW\"\\n    },\\n    \"DASHBOARD_VERSION_KEY\": \"v2\"\\n}\\n    \"\"\"\\n    dash.dashboard_title = \"Unicode Test\"\\n    pos = json.loads(js)\\n    update_slice_ids(pos, [slc])\\n    dash.position_json = json.dumps(pos, indent=4)\\n    dash.slug = \"unicode-test\"\\n    dash.slices = [slc]\\n    db.session.merge(dash)\\n    db.session.commit()\\n',\n",
       " 'import os\\nimport time\\nimport datetime\\nimport collections\\nimport socket\\nfrom calendar import timegm\\nfrom future.utils import iteritems\\n\\nimport json\\ntry:\\n    import cPickle as pickle\\nexcept ImportError:\\n    import pickle\\n\\ntry:\\n    from threading import get_ident\\nexcept ImportError:\\n    from thread import get_ident\\n\\nfrom pandaharvester.harvesterconfig import harvester_config\\nfrom pandaharvester.harvestercore import core_utils\\nfrom pandaharvester.harvestercore.plugin_factory import PluginFactory\\nfrom pandaharvester.harvestercore.db_proxy_pool import DBProxyPool as DBProxy\\nfrom pandaharvester.harvestercore.db_interface import DBInterface\\n\\n# attribute list\\n_attribute_list = [\\'id\\', \\'item\\', \\'score\\']\\n\\n# fifo object spec\\nFifoObject = collections.namedtuple(\\'FifoObject\\', _attribute_list, verbose=False, rename=False)\\n\\n# logger\\n_logger = core_utils.setup_logger(\\'fifos\\')\\n\\n# base class of fifo message queue\\nclass FIFOBase(object):\\n    # constructor\\n    def __init__(self, **kwarg):\\n        for tmpKey, tmpVal in iteritems(kwarg):\\n            setattr(self, tmpKey, tmpVal)\\n        self.hostname = socket.gethostname()\\n        self.os_pid = os.getpid()\\n        self.dbProxy = DBProxy()\\n        self.dbInterface = DBInterface()\\n\\n    # get process identifier\\n    def get_pid(self):\\n        thread_id = get_ident()\\n        if thread_id is None:\\n            thread_id = 0\\n        return \\'{0}_{1}-{2}\\'.format(self.hostname, self.os_pid, format(get_ident(), \\'x\\'))\\n\\n    # make logger\\n    def make_logger(self, base_log, token=None, method_name=None, send_dialog=True):\\n        if send_dialog and hasattr(self, \\'dbInterface\\'):\\n            hook = self.dbInterface\\n        else:\\n            hook = None\\n        return core_utils.make_logger(base_log, token=token, method_name=method_name, hook=hook)\\n\\n    # intialize fifo from harvester configuration\\n    def _initialize_fifo(self, force_enable=False):\\n        self.fifoName = \\'{0}_fifo\\'.format(self.titleName)\\n        self.config = getattr(harvester_config, self.titleName)\\n        if force_enable:\\n            self.enabled = True\\n        elif hasattr(self.config, \\'fifoEnable\\') and self.config.fifoEnable:\\n            self.enabled = True\\n        else:\\n            self.enabled = False\\n            return\\n        pluginConf = vars(self.config).copy()\\n        pluginConf.update( {\\'titleName\\': self.titleName} )\\n        if hasattr(self.config, \\'fifoModule\\') and hasattr(self.config, \\'fifoClass\\'):\\n            pluginConf.update( {\\'module\\': self.config.fifoModule,\\n                                \\'name\\': self.config.fifoClass,} )\\n        else:\\n            if not hasattr(harvester_config, \\'fifo\\'):\\n                return\\n            pluginConf.update( {\\'module\\': harvester_config.fifo.fifoModule,\\n                                \\'name\\': harvester_config.fifo.fifoClass,} )\\n        pluginFactory = PluginFactory()\\n        self.fifo = pluginFactory.get_plugin(pluginConf)\\n\\n    # encode\\n    def encode(self, item):\\n        item_serialized = pickle.dumps(item, -1)\\n        return item_serialized\\n\\n    # decode\\n    def decode(self, item_serialized):\\n        item = pickle.loads(item_serialized)\\n        return item\\n\\n    # size of queue\\n    def size(self):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'size\\')\\n        retVal = self.fifo.size()\\n        mainLog.debug(\\'size={0}\\'.format(retVal))\\n        return retVal\\n\\n    # enqueue\\n    def put(self, item, score=None, encode_item=True):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'put\\')\\n        if encode_item:\\n            item_serialized = self.encode(item)\\n        else:\\n            item_serialized = item\\n        if score is None:\\n            score = time.time()\\n        retVal = self.fifo.put(item_serialized, score)\\n        mainLog.debug(\\'score={0}\\'.format(score))\\n        return retVal\\n\\n    # enqueue by id, which is unique\\n    def putbyid(self, id, item, score=None, encode_item=True):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'putbyid\\')\\n        if encode_item:\\n            item_serialized = self.encode(item)\\n        else:\\n            item_serialized = item\\n        if score is None:\\n            score = time.time()\\n        retVal = self.fifo.putbyid(id, item_serialized, score)\\n        mainLog.debug(\\'id={0} score={1}\\'.format(id, score))\\n        return retVal\\n\\n    # dequeue to get the first fifo object\\n    def get(self, timeout=None, protective=False, decode_item=True):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'get\\')\\n        object_tuple = self.fifo.get(timeout, protective)\\n        if object_tuple is None:\\n            retVal = None\\n        else:\\n            id, item_serialized, score = object_tuple\\n            if item_serialized is not None and decode_item:\\n                item = self.decode(item_serialized)\\n            else:\\n                item = item_serialized\\n            retVal = FifoObject(id, item, score)\\n        mainLog.debug(\\'called. protective={0} decode_item={1}\\'.format(protective, decode_item))\\n        return retVal\\n\\n    # dequeue to get the last fifo object\\n    def getlast(self, timeout=None, protective=False, decode_item=True):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'getlast\\')\\n        object_tuple = self.fifo.getlast(timeout, protective)\\n        if object_tuple is None:\\n            retVal = None\\n        else:\\n            id, item_serialized, score = object_tuple\\n            if item_serialized is not None and decode_item:\\n                item = self.decode(item_serialized)\\n            else:\\n                item = item_serialized\\n            retVal = FifoObject(id, item, score)\\n        mainLog.debug(\\'called. protective={0} decode_item={1}\\'.format(protective, decode_item))\\n        return retVal\\n\\n    # dequeue list of objects with some conditions\\n    def getmany(self, mode=\\'first\\', minscore=None, maxscore=None, count=None,\\n                    protective=False, temporary=False, decode_item=True):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'getmany\\')\\n        object_tuple_list = self.fifo.getmany(mode, minscore, maxscore, count, protective, temporary)\\n        if not object_tuple_list:\\n            mainLog.debug(\\'empty list\\')\\n        ret_list = []\\n        for object_tuple in object_tuple_list:\\n            id, item_serialized, score = object_tuple\\n            if item_serialized is not None and decode_item:\\n                item = self.decode(item_serialized)\\n            else:\\n                item = item_serialized\\n            val_tuple = FifoObject(id, item, score)\\n            ret_list.append(val_tuple)\\n        mainLog.debug(\\'mode={0} minscore={1} maxscore={2} count={3} protective={4} temporary={5} decode_item={6}\\'.format(\\n                        mode, minscore, maxscore, count, protective, temporary, decode_item))\\n        return ret_list\\n\\n    # get tuple of the first object and its score without dequeuing\\n    # If item is large un unnecessary to show int peek, set skip_item=True\\n    def peek(self, skip_item=False):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'peek\\')\\n        object_tuple = self.fifo.peek(skip_item=skip_item)\\n        if object_tuple is None:\\n            retVal = None\\n            mainLog.debug(\\'fifo empty\\')\\n        else:\\n            id, item_serialized, score = object_tuple\\n            if item_serialized is None and score is None:\\n                retVal = FifoObject(None, None, None)\\n            else:\\n                if score is None:\\n                    score = time.time()\\n                retVal = FifoObject(id, item_serialized, score)\\n            mainLog.debug(\\'score={0}\\'.format(score))\\n        return retVal\\n\\n    # get tuple of the last object and its score without dequeuing\\n    def peeklast(self, skip_item=False):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'peeklast\\')\\n        object_tuple = self.fifo.peeklast(skip_item=skip_item)\\n        if object_tuple is None:\\n            retVal = None\\n            mainLog.debug(\\'fifo empty\\')\\n        else:\\n            id, item_serialized, score = object_tuple\\n            if item_serialized is None and score is None:\\n                retVal = FifoObject(None, None, None)\\n            else:\\n                if score is None:\\n                    score = time.time()\\n                retVal = FifoObject(id, item_serialized, score)\\n            mainLog.debug(\\'score={0}\\'.format(score))\\n        return retVal\\n\\n    # get tuple of the object by id without dequeuing\\n    def peekbyid(self, id, temporary=False, skip_item=False):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'peekbyid\\')\\n        object_tuple = self.fifo.peekbyid(id, temporary, skip_item=skip_item)\\n        if object_tuple is None:\\n            retVal = None\\n            mainLog.debug(\\'fifo empty\\')\\n        else:\\n            id_gotten, item_serialized, score = object_tuple\\n            if item_serialized is None and score is None:\\n                retVal = FifoObject(None, None, None)\\n            else:\\n                if score is None:\\n                    score = time.time()\\n                retVal = FifoObject(id, item_serialized, score)\\n            mainLog.debug(\\'id={0} score={1} temporary={2}\\'.format(id, score, temporary))\\n        return retVal\\n\\n    # get list of object tuples without dequeuing\\n    def peekmany(self, mode=\\'first\\', minscore=None, maxscore=None, count=None, skip_item=False):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'peekmany\\')\\n        object_tuple_list = self.fifo.peekmany(mode, minscore, maxscore, count, skip_item)\\n        if not object_tuple_list:\\n            mainLog.debug(\\'empty list\\')\\n        ret_list = []\\n        for object_tuple in object_tuple_list:\\n            id_gotten, item_serialized, score = object_tuple\\n            if item_serialized is None and score is None:\\n                val_tuple = FifoObject(None, None, None)\\n            else:\\n                if score is None:\\n                    score = time.time()\\n                val_tuple = FifoObject(id, item_serialized, score)\\n            ret_list.append(val_tuple)\\n        mainLog.debug(\\'mode={0} minscore={1} maxscore={2} count={3}\\'.format(mode, minscore, maxscore, count))\\n        return ret_list\\n\\n    # delete objects by list of ids from temporary space, return the number of objects successfully deleted\\n    def delete(self, ids):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'release\\')\\n        retVal = self.fifo.delete(ids)\\n        mainLog.debug(\\'released {0} objects in {1}\\'.format(retVal, ids))\\n        return retVal\\n\\n    # restore objects by list of ids from temporary space to fifo; ids=None to restore all objects\\n    def restore(self, ids=None):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'restore\\')\\n        retVal = self.fifo.restore(ids)\\n        if ids is None:\\n            mainLog.debug(\\'restored all objects\\')\\n        else:\\n            mainLog.debug(\\'restored objects in {0}\\'.format(ids))\\n        return retVal\\n\\n    # update a object by its id with some conditions\\n    def update(self, id, item=None, score=None, temporary=None, cond_score=\\'gt\\'):\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'update\\')\\n        retVal = self.fifo.update(id, item, score, temporary, cond_score)\\n        update_report_list = []\\n        if item is not None:\\n            update_report_list.append(\\'item={0}\\'.format(item))\\n        if score is not None:\\n            update_report_list.append(\\'score={0}\\'.format(score))\\n        if temporary is not None:\\n            update_report_list.append(\\'temporary={0}\\'.format(temporary))\\n        update_report = \\' \\'.join(update_report_list)\\n        mainLog.debug(\\'update id={0} cond_score={1}: return={2}, {3}\\'.format(id, cond_score, retVal, update_report))\\n        return retVal\\n\\n\\n# Special fifo base for non havester-agent\\nclass SpecialFIFOBase(FIFOBase):\\n    # constructor\\n    def __init__(self, **kwarg):\\n        FIFOBase.__init__(self, **kwarg)\\n        self.fifoName = \\'{0}_fifo\\'.format(self.titleName)\\n        pluginConf = {}\\n        pluginConf.update( {\\'titleName\\': self.titleName} )\\n        pluginConf.update( {\\'module\\': harvester_config.fifo.fifoModule,\\n                            \\'name\\': harvester_config.fifo.fifoClass,} )\\n        pluginFactory = PluginFactory()\\n        self.fifo = pluginFactory.get_plugin(pluginConf)\\n\\n\\n# Benchmark fifo\\nclass BenchmarkFIFO(SpecialFIFOBase):\\n    titleName = \\'benchmark\\'\\n\\n\\n# monitor fifo\\nclass MonitorFIFO(FIFOBase):\\n    titleName = \\'monitor\\'\\n\\n    # constructor\\n    def __init__(self, **kwarg):\\n        FIFOBase.__init__(self, **kwarg)\\n        self._initialize_fifo()\\n\\n    def populate(self, seconds_ago=0, clear_fifo=False):\\n        \"\"\"\\n        Populate monitor fifo with all active worker chunks and timeNow as score from DB\\n        with modificationTime earlier than seconds_ago seconds ago\\n        object in fifo = [(queueName_1, [[worker_1_1], [worker_1_2], ...]), (queueName_2, ...)]\\n        \"\"\"\\n        if clear_fifo:\\n            self.fifo.clear()\\n        try:\\n            fifoMaxWorkersToPopulate = self.config.fifoMaxWorkersToPopulate\\n        except AttributeError:\\n            fifoMaxWorkersToPopulate = 2**32\\n        try:\\n            fifoMaxWorkersPerChunk = self.config.fifoMaxWorkersPerChunk\\n        except AttributeError:\\n            fifoMaxWorkersPerChunk = 500\\n        workspec_iterator = self.dbProxy.get_active_workers(fifoMaxWorkersToPopulate, seconds_ago)\\n        last_queueName = None\\n        workspec_chunk = []\\n        timeNow_timestamp = time.time()\\n        score = timeNow_timestamp\\n        for workspec in workspec_iterator:\\n            workspec.set_work_params({\\'lastCheckAt\\': timeNow_timestamp})\\n            if last_queueName is None:\\n                try:\\n                    score = timegm(workspec.modificationTime.utctimetuple())\\n                except Exception:\\n                    pass\\n                workspec_chunk = [[workspec]]\\n                last_queueName = workspec.computingSite\\n            elif workspec.computingSite == last_queueName \\\\\\n                and len(workspec_chunk) < fifoMaxWorkersPerChunk:\\n                workspec_chunk.append([workspec])\\n            else:\\n                self.put((last_queueName, workspec_chunk), score)\\n                try:\\n                    score = timegm(workspec.modificationTime.utctimetuple())\\n                except Exception:\\n                    pass\\n                workspec_chunk = [[workspec]]\\n                last_queueName = workspec.computingSite\\n        if len(workspec_chunk) > 0:\\n            self.put((last_queueName, workspec_chunk), score)\\n\\n    def to_check_workers(self, check_interval=harvester_config.monitor.checkInterval):\\n        \"\"\"\\n        Justify whether to check any worker by the modificationTime of the first worker in fifo\\n        retVal True if OK to dequeue to check;\\n        retVal False otherwise.\\n        Return retVal, overhead_time\\n        \"\"\"\\n        mainLog = self.make_logger(_logger, \\'id={0}-{1}\\'.format(self.fifoName, self.get_pid()), method_name=\\'to_check_worker\\')\\n        retVal = False\\n        overhead_time = None\\n        timeNow_timestamp = time.time()\\n        peeked_tuple = self.peek(skip_item=True)\\n        if peeked_tuple is not None:\\n            score = peeked_tuple.score\\n            overhead_time = timeNow_timestamp - score\\n            if overhead_time > 0:\\n                retVal = True\\n                if score < 0:\\n                    mainLog.debug(\\'True. Preempting\\')\\n                    overhead_time = None\\n                else:\\n                    mainLog.debug(\\'True\\')\\n                    mainLog.info(\\'Overhead time is {0} sec\\'.format(overhead_time))\\n            else:\\n                mainLog.debug(\\'False. Workers too young to check\\')\\n                mainLog.debug(\\'Overhead time is {0} sec\\'.format(overhead_time))\\n        else:\\n            mainLog.debug(\\'False. Got nothing in FIFO\\')\\n        return retVal, overhead_time\\n\\n\\nclass MonitorEventFIFO(SpecialFIFOBase):\\n    titleName = \\'monitorEvent\\'\\n\\n    # constructor\\n    def __init__(self, **kwarg):\\n        self.config = getattr(harvester_config, \\'monitor\\')\\n        self.enabled = False\\n        if hasattr(self.config, \\'fifoEnable\\') and self.config.fifoEnable \\\\\\n            and getattr(self.config, \\'eventBasedEnable\\', False):\\n            self.enabled = True\\n        SpecialFIFOBase.__init__(self, **kwarg)\\n',\n",
       " '#! /usr/bin/env python\\n# for fgas ppruns only\\nppruns=[ \"2009-03-28_W-S-I+\" ,\"2009-09-19_W-J-V\" ,\"2009-04-29_W-S-Z+\" ,\"2009-04-29_W-J-B\" ,\"2010-03-12_W-J-B\" ,\"2010-03-12_W-S-Z+\" ,\"2010-03-12_W-C-RC\" ,\"2010-11-04_W-J-B\" ,\"2010-11-04_W-S-Z+\" ,\"2012-07-23_W-C-RC\" ,\"2013-06-10_W-S-Z+\" ,\"2007-02-13_W-S-I+\" ,\"2007-02-13_W-J-V\" ,\"2010-03-12_W-J-V\" ,\"2010-03-12_W-S-I+\" ,\"2010-12-05_W-J-V\" ,\"2015-12-15_W-J-B\" ,\"2015-12-15_W-C-RC\" ,\"2015-12-15_W-S-Z+\"]\\nppruns_MACS1115=[ \"2009-04-29_W-S-Z+\" ,\"2009-04-29_W-J-B\" ,\"2010-03-12_W-J-B\" ,\"2010-03-12_W-S-Z+\" ,\"2010-03-12_W-C-RC\"]\\nppruns_preH=[ \"2010-03-12_W-S-I+\" ,\"2010-12-05_W-J-V\" ,\"2007-02-13_W-J-V\" ,\"2007-02-13_W-S-I+\" ,\"2009-03-28_W-S-I+\" ,\"2010-03-12_W-J-V\"]\\nppruns_10_3= [\\'2009-03-28_W-S-I+\\', \\'2009-09-19_W-J-V\\', \\'2009-04-29_W-S-Z+\\', \\'2009-04-29_W-J-B\\', \\'2010-03-12_W-J-B\\', \\'2010-03-12_W-S-Z+\\', \\'2010-03-12_W-C-RC\\', \\'2010-11-04_W-J-B\\', \\'2010-11-04_W-S-Z+\\', \\'2012-07-23_W-C-RC\\', \\'2013-06-10_W-S-Z+\\', \\'2010-03-12_W-J-V\\', \\'2010-03-12_W-S-I+\\', \\'2010-12-05_W-J-V\\', \\'2015-12-15_W-J-B\\', \\'2015-12-15_W-C-RC\\', \\'2015-12-15_W-S-Z+\\']\\nppruns_10_2= [\\'2007-02-13_W-S-I+\\', \\'2007-02-13_W-J-V\\']\\nppruns_postH= [\\'2009-09-19_W-J-V\\', \\'2009-04-29_W-S-Z+\\', \\'2009-04-29_W-J-B\\', \\'2010-03-12_W-J-B\\', \\'2010-03-12_W-S-Z+\\', \\'2010-03-12_W-C-RC\\', \\'2010-11-04_W-J-B\\', \\'2010-11-04_W-S-Z+\\', \\'2012-07-23_W-C-RC\\', \\'2013-06-10_W-S-Z+\\', \\'2015-12-15_W-J-B\\', \\'2015-12-15_W-C-RC\\', \\'2015-12-15_W-S-Z+\\']\\nppruns_nonMACS1115= [\\'2009-03-28_W-S-I+\\', \\'2009-09-19_W-J-V\\', \\'2010-11-04_W-J-B\\', \\'2010-11-04_W-S-Z+\\', \\'2012-07-23_W-C-RC\\', \\'2013-06-10_W-S-Z+\\', \\'2007-02-13_W-S-I+\\', \\'2007-02-13_W-J-V\\', \\'2010-03-12_W-J-V\\', \\'2010-03-12_W-S-I+\\', \\'2010-12-05_W-J-V\\', \\'2015-12-15_W-J-B\\', \\'2015-12-15_W-C-RC\\', \\'2015-12-15_W-S-Z+\\']\\nppruns_dec= [\\'2009-03-28_W-S-I+\\', \\'2009-09-19_W-J-V\\', \\'2010-11-04_W-J-B\\', \\'2010-11-04_W-S-Z+\\', \\'2012-07-23_W-C-RC\\', \\'2013-06-10_W-S-Z+\\', \\'2010-03-12_W-J-V\\', \\'2010-03-12_W-S-I+\\', \\'2010-12-05_W-J-V\\', \\'2015-12-15_W-J-B\\', \\'2015-12-15_W-C-RC\\', \\'2015-12-15_W-S-Z+\\']\\n\\nic_cldata={\\'MACS0429-02\\':{},\\'MACS1226+21\\':{},\\'RXJ2129\\':{},\\'MACS1115+01\\':{},\"MACS0416-24\":{},\\'MACS0159-08\\':{}, \\'Zw2089\\':{}, \\'Zw2701\\':{}, \\'A2204\\':{}}\\nic_cldata[\\'MACS1226+21\\'][\\'FILTERs\\']=[\"W-J-B\",\"W-J-V\",\"W-C-RC\",\"W-C-IC\",\"W-S-Z+\"]\\nic_cldata[\\'MACS1226+21\\'][\\'PPRUNs\\']=[\"W-C-IC_2010-02-12\", \"W-C-IC_2011-01-06\",\"W-C-RC_2010-02-12\", \"W-J-B_2010-02-12\", \"W-J-V_2010-02-12\", \"W-S-Z+_2011-01-06\"]\\nic_cldata[\\'MACS1226+21\\'][\\'FILTERs_matching_PPRUNs\\']=[\"W-C-IC\", \"W-C-IC\",\"W-C-RC\", \"W-J-B\", \"W-J-V\", \"W-S-Z+\"] \\nic_cldata[\\'MACS0429-02\\'][\\'FILTERs\\']=[\"W-J-B\",\"W-S-Z+\"]\\nic_cldata[\\'MACS0429-02\\'][\\'FILTERs_matching_PPRUNs\\']=[\"W-J-B\",\"W-S-Z+\"]\\nic_cldata[\\'MACS0429-02\\'][\\'PPRUNs\\']=[\"W-J-B_2015-12-15\",\"W-S-Z+_2015-12-15\"]\\nic_cldata[\\'RXJ2129\\'][\\'FILTERs\\']=[\"W-J-B\",\"W-C-RC\",\"W-S-Z+\"]\\nic_cldata[\\'RXJ2129\\'][\\'FILTERs_matching_PPRUNs\\']=[\"W-J-B\",\"W-C-RC\",\"W-S-Z+\"]\\nic_cldata[\\'RXJ2129\\'][\\'PPRUNs\\']=[\"W-J-B_2010-11-04\",\"W-C-RC_2012-07-23\",\"W-S-Z+_2010-11-04\"]\\nic_cldata[\\'MACS1115+01\\'][\\'FILTERs\\']=[\"W-J-B\",\"W-C-RC\",\"W-S-Z+\"] \\nic_cldata[\\'MACS1115+01\\'][\\'FILTERs_matching_PPRUNs\\']=[\"W-S-Z+\",\"W-J-B\",\"W-J-B\",\"W-S-Z+\",\"W-C-RC\"]\\nic_cldata[\\'MACS1115+01\\'][\\'PPRUNs\\']=[\"W-S-Z+_2009-04-29\",\"W-J-B_2009-04-29\",\"W-J-B_2010-03-12\",\"W-S-Z+_2010-03-12\",\"W-C-RC_2010-03-12\"]\\nic_cldata[\\'MACS0416-24\\'][\\'FILTERs\\']=[\"W-J-B\",\"W-C-RC\",\"W-S-Z+\"] \\nic_cldata[\\'MACS0416-24\\'][\\'FILTERs_matching_PPRUNs\\']=[\"W-J-B\",\"W-C-RC\",\"W-S-Z+\"] \\nic_cldata[\\'MACS0416-24\\'][\\'PPRUNs\\']=[\"W-J-B_2010-11-04\",\"W-C-RC_2010-11-04\",\"W-S-Z+_2010-11-04\"] \\nic_cldata[\\'MACS0159-08\\'][\\'FILTERs\\']=[\"W-J-B\"]\\nic_cldata[\\'MACS0159-08\\'][\\'FILTERs_matching_PPRUNs\\']=[\"W-J-B\"]\\nic_cldata[\\'MACS0159-08\\'][\\'PPRUNs\\']=[\"W-J-B_2015-12-15\"]\\nic_cldata[\\'Zw2089\\'][\\'FILTERs\\']=[\\'W-J-B\\',\\'W-J-V\\',\\'W-C-RC\\',\\'W-S-I+\\',\\'W-S-Z+\\']\\nic_cldata[\\'Zw2089\\'][\\'FILTERs_matching_PPRUNs\\']=[\\'W-C-RC\\',\\'W-J-B\\',\\'W-J-V\\',\\'W-S-I+\\',\\'W-S-I+\\',\\'W-S-Z+\\']\\nic_cldata[\\'Zw2089\\'][\\'PPRUNs\\']=[\\'W-C-RC_2015-12-15\\',\\'W-J-B_2015-12-15\\',\\'W-J-V_2010-12-05\\',\\'W-S-I+_2007-02-13\\',\\'W-S-I+_2009-03-28\\',\\'W-S-Z+_2015-12-15\\']\\nic_cldata[\\'Zw2701\\'][\\'FILTERs\\']=[\\'W-J-B\\',\\'W-J-V\\',\\'W-C-RC\\',\\'W-S-I+\\']\\nic_cldata[\\'Zw2701\\'][\\'FILTERs_matching_PPRUNs\\']=[\\'W-C-RC\\',\\'W-J-B\\',\\'W-J-V\\',\\'W-J-V\\',\\'W-S-I+\\']\\nic_cldata[\\'Zw2701\\'][\\'PPRUNs\\']=[\\'W-C-RC_2015-12-15\\',\\'W-J-B_2015-12-15\\',\\'W-J-V_2010-03-12\\',\\'W-J-V_2010-12-05\\',\\'W-S-I+_2010-03-12\\']\\nic_cldata[\\'A2204\\'][\\'FILTERs\\']=[\\'W-J-V\\',\\'W-S-I+\\']\\nic_cldata[\\'A2204\\'][\\'FILTERs_matching_PPRUNs\\']=[\\'W-J-V\\',\\'W-S-I+\\']\\nic_cldata[\\'A2204\\'][\\'PPRUNs\\']=[\\'W-J-V_2009-09-19\\',\\'W-S-I+_2009-03-28\\']\\nra_cluster={}\\ndec_cluster={}\\nra_cluster[\\'MACS0429-02\\']=67.40041667 ; dec_cluster[\\'MACS0429-02\\']=-2.88555556\\nra_cluster[\\'RXJ2129\\']=322.41625000 ; dec_cluster[\\'RXJ2129\\']=0.08888889\\nra_cluster[\\'MACS1226+21\\']=186.71268; dec_cluster[\\'MACS1226+21\\']=21.831938\\nra_cluster[\\'A2204\\']=248.19666667; dec_cluster[\\'A2204\\']=5.57555556\\nra_cluster[\\'MACS0159-08\\']=29.9579;dec_cluster[\\'MACS0159-08\\']=-8.83028\\n#MACSJ0429.6-0253\\t67.4\\t-2.88375\\n#RXJ2129.6+0005\\t322.408\\t0.094\\nra_cluster[\\'Zw2089\\']=135.158;dec_cluster[\\'Z2089\\']=20.916\\nra_cluster[\\'Zw2701\\']=148.198;dec_cluster[\\'Z2701\\']=51.891\\nra_cluster[\\'MACS1115+01\\']=168.972;dec_cluster[\\'MACS1115+01\\']=1.49639\\nra_cluster[\\'MACS0416-24\\']=64.0413;dec_cluster[\\'MACS0416-24\\']=6-24.0662\\n\\n\\nclusters =[\\'MACS0429-02\\',\\'MACS1226+21\\',\\'RXJ2129\\',\\'MACS1115+01\\',\"MACS0416-24\",\\'MACS0159-08\\', \\'Zw2089\\', \\'Zw2701\\', \\'A2204\\']\\nfgas_clusters =[\\'MACS0429-02\\',\\'RXJ2129\\',\\'MACS1115+01\\',\\'MACS0159-08\\', \\'Zw2089\\', \\'Zw2701\\', \\'A2204\\']\\nactive_fgas_clusters =[\\'Zw2089\\',\\'MACS0429-02\\',\\'RXJ2129\\', \\'MACS1115+01\\', \\'A2204\\']\\nclusters_refcats={}\\nclusters_refcats[\\'MACS0429-02\\']=\\'PANSTARRS\\'\\nclusters_refcats[\\'RXJ2129\\']=\\'SDSS\\'\\nclusters_refcats[\\'MACS1226+21\\']=\\'SDSS\\'\\nclusters_refcats[\\'A2204\\']=\\'PANSTARRS\\'\\nclusters_refcats[\\'MACS0159-08\\']=\\'SDSS\\'\\nclusters_refcats[\\'Zw2089\\']=\\'SDSS\\'\\nclusters_refcats[\\'Zw2701\\']=\\'SDSS\\'\\nclusters_refcats[\\'MACS1115+01\\']=\\'SDSS\\'\\nclusters_refcats[\\'MACS0416-24\\']=\\'PANSTARRS\\'\\n\\nstuff_todo=[\\'Coadding (1 day)\\',\\n\\'By-hand masking (1 day)\\',\\n\\'Coadd Masking (1/3 day)\\',\\n\\'Photometric Measurement and Calibration\\',\\n\\'Photo-Zs\\', \\'cc masses\\', \\'p(z) masses\\']\\n\\nfor cl in active_fgas_clusters:\\n\\tfor item in stuff_todo:\\n\\t\\tprint \":\".join([item,cl])\\n\\nmask_todo=[\\'coadd init\\',\\'standard by-hand masking\\',\\'backmasking\\',\\'autosuppression\\',\\'asteroid\\',\\'star (ALL BANDS)\\',\\'edgemask (ALL BANDS)\\',\\'check background-sub errors (ALL BANDS)\\',\\'coadd final\\']\\nfor cl in active_fgas_clusters:\\n\\tfor item in  mask_todo:\\n\\t\\tprint \":\".join([item,cl])\\n',\n",
       " 'import argparse\\nimport tools.find_mxnet\\nimport mxnet as mx\\nimport os\\nimport sys\\nfrom train.train_net import train_net\\n\\ndef parse_args():\\n    parser = argparse.ArgumentParser(description=\\'Train a Single-shot detection network\\')\\n    parser.add_argument(\\'--dataset\\', dest=\\'dataset\\', help=\\'which dataset to use\\',\\n                        default=\\'pascal\\', type=str)\\n    parser.add_argument(\\'--image-set\\', dest=\\'image_set\\', help=\\'train set, can be trainval or train\\',\\n                        default=\\'trainval\\', type=str)\\n    parser.add_argument(\\'--year\\', dest=\\'year\\', help=\\'can be 2007, 2012\\',\\n                        default=\\'2007,2012\\', type=str)\\n    parser.add_argument(\\'--val-image-set\\', dest=\\'val_image_set\\', help=\\'validation set, can be val or test\\',\\n                        default=\\'test\\', type=str)\\n    parser.add_argument(\\'--val-year\\', dest=\\'val_year\\', help=\\'can be 2007, 2010, 2012\\',\\n                        default=\\'2007\\', type=str)\\n    parser.add_argument(\\'--devkit-path\\', dest=\\'devkit_path\\', help=\\'VOCdevkit path\\',\\n                        default=os.path.join(os.getcwd(), \\'data\\', \\'VOCdevkit\\'), type=str)\\n    parser.add_argument(\\'--network\\', dest=\\'network\\', type=str, default=\\'vgg16_reduced\\',\\n                        choices=[\\'vgg16_reduced\\'], help=\\'which network to use\\')\\n    parser.add_argument(\\'--batch-size\\', dest=\\'batch_size\\', type=int, default=32,\\n                        help=\\'training batch size\\')\\n    parser.add_argument(\\'--resume\\', dest=\\'resume\\', type=int, default=-1,\\n                        help=\\'resume training from epoch n\\')\\n    parser.add_argument(\\'--finetune\\', dest=\\'finetune\\', type=int, default=-1,\\n                        help=\\'finetune from epoch n, rename the model before doing this\\')\\n    parser.add_argument(\\'--pretrained\\', dest=\\'pretrained\\', help=\\'pretrained model prefix\\',\\n                        default=os.path.join(os.getcwd(), \\'model\\', \\'vgg16_reduced\\'), type=str)\\n    parser.add_argument(\\'--epoch\\', dest=\\'epoch\\', help=\\'epoch of pretrained model\\',\\n                        default=1, type=int)\\n    parser.add_argument(\\'--prefix\\', dest=\\'prefix\\', help=\\'new model prefix\\',\\n                        default=os.path.join(os.getcwd(), \\'model\\', \\'ssd\\'), type=str)\\n    parser.add_argument(\\'--gpus\\', dest=\\'gpus\\', help=\\'GPU devices to train with\\',\\n                        default=\\'0\\', type=str)\\n    parser.add_argument(\\'--begin-epoch\\', dest=\\'begin_epoch\\', help=\\'begin epoch of training\\',\\n                        default=0, type=int)\\n    parser.add_argument(\\'--end-epoch\\', dest=\\'end_epoch\\', help=\\'end epoch of training\\',\\n                        default=100, type=int)\\n    parser.add_argument(\\'--frequent\\', dest=\\'frequent\\', help=\\'frequency of logging\\',\\n                        default=20, type=int)\\n    parser.add_argument(\\'--data-shape\\', dest=\\'data_shape\\', type=int, default=300,\\n                        help=\\'set image shape\\')\\n    parser.add_argument(\\'--lr\\', dest=\\'learning_rate\\', type=float, default=0.001,\\n                        help=\\'learning rate\\')\\n    parser.add_argument(\\'--momentum\\', dest=\\'momentum\\', type=float, default=0.9,\\n                        help=\\'momentum\\')\\n    parser.add_argument(\\'--wd\\', dest=\\'weight_decay\\', type=float, default=0.0001,\\n                        help=\\'weight decay\\')\\n    parser.add_argument(\\'--mean-r\\', dest=\\'mean_r\\', type=float, default=123,\\n                        help=\\'red mean value\\')\\n    parser.add_argument(\\'--mean-g\\', dest=\\'mean_g\\', type=float, default=117,\\n                        help=\\'green mean value\\')\\n    parser.add_argument(\\'--mean-b\\', dest=\\'mean_b\\', type=float, default=104,\\n                        help=\\'blue mean value\\')\\n    parser.add_argument(\\'--lr-epoch\\', dest=\\'lr_refactor_epoch\\', type=int, default=50,\\n                        help=\\'refactor learning rate every N epoch\\')\\n    parser.add_argument(\\'--lr-ratio\\', dest=\\'lr_refactor_ratio\\', type=float, default=0.9,\\n                        help=\\'ratio to refactor learning rate\\')\\n    parser.add_argument(\\'--log\\', dest=\\'log_file\\', type=str, default=\"train.log\",\\n                        help=\\'save training log to file\\')\\n    parser.add_argument(\\'--monitor\\', dest=\\'monitor\\', type=int, default=0,\\n                        help=\\'log network parameters every N iters if larger than 0\\')\\n    args = parser.parse_args()\\n    return args\\n\\nif __name__ == \\'__main__\\':\\n    args = parse_args()\\n    ctx = [mx.gpu(int(i)) for i in args.gpus.split(\\',\\')]\\n    ctx = mx.cpu() if not ctx else ctx\\n    train_net(args.network, args.dataset, args.image_set, args.year,\\n              args.devkit_path, args.batch_size,\\n              args.data_shape, [args.mean_r, args.mean_g, args.mean_b],\\n              args.resume, args.finetune, args.pretrained,\\n              args.epoch, args.prefix, ctx, args.begin_epoch, args.end_epoch,\\n              args.frequent, args.learning_rate, args.momentum, args.weight_decay,\\n              args.val_image_set, args.val_year, args.lr_refactor_epoch,\\n              args.lr_refactor_ratio, args.monitor, args.log_file)\\n',\n",
       " '\"\"\"Generic SSH class providing method to connect switches, routers and other devices using SSHv2.\\r\\n\\r\\ncNetSSH uses paramiko, see API docs http://docs.paramiko.org/en/latest/ \\r\\n\"\"\"\\r\\n\\r\\n__author__    = \"dumplab\"\\r\\n__copyright__ = \"2015 dumplab\"\\r\\n__license__   = \"MIT\"\\r\\n__version__   = \"0.5\"\\r\\n__status__    = \"Developement\"\\r\\n\\r\\nimport paramiko,re,sys,time\\r\\n\\r\\nclass cNetSSH(object):\\r\\n\\t\"\"\"SSH connection object\"\"\"\\r\\n\\t\\r\\n        def __init__(self):\\r\\n\\t\\t\"\"\"Set default attribute values only\\r\\n\\t\\t\\r\\n\\t\\tNo arguments\\r\\n\\t\\t\"\"\"\\r\\n\\t\\tself.__host      = \"\"\\r\\n\\t\\tself.__user      = \"\"\\r\\n\\t\\tself.__pass      = \"\"\\r\\n\\t\\tself.__conn      = None # paramiko connection\\r\\n\\t\\tself.__shell     = None # when using a channel\\r\\n\\t\\tself.__connected = False\\r\\n\\t\\tself.__input     = \"\"\\r\\n\\t\\tself.__output    = \"\"\\r\\n\\t\\tself.__timeout   = 1.0  # right now we use a timeout of 2 seconds to connect\\r\\n\\t\\tself.__outEnd    = \"\"   # will be considered as the prompt and end of an output see recv is discovered during the login\\r\\n\\t\\tself.__debug     = False\\r\\n\\r\\n\\tdef connect(self,hostName,userName=\"\",userPass=\"\",newShell=True,paging=False):\\r\\n\\t\\t\"\"\"connect a device using provided credentials and start an interactive shell\\r\\n\\t\\t\\r\\n\\t\\tKeyword arguments:\\r\\n\\t\\thostName = hostname (default \"\")\\r\\n\\t\\tuserName = username to authenticate (default \"\")\\r\\n\\t\\tuserPass = password to use for authenticating and for unlocking a private key (default \"\")\\r\\n\\t\\tnewShell = shall we start a shell, opens a new Channel (default True)\\r\\n\\t\\tpaging   = enable or disable paging/more (Default False)\\r\\n\\t\\treturns the configuration as a string\\r\\n\\t\\t\"\"\"\\r\\n\\t\\tself.__host = hostName\\r\\n\\t\\tself.__user = userName\\r\\n\\t\\tself.__pass = userPass\\r\\n\\r\\n\\t\\ttry:\\r\\n\\t\\t\\tself.__conn = paramiko.SSHClient()\\r\\n\\t\\t\\t# add untrusted hosts\\r\\n\\t\\t\\tself.__conn.set_missing_host_key_policy(paramiko.AutoAddPolicy())\\r\\n\\t\\t\\t# connect to host\\r\\n\\t\\t\\tself.__conn.connect(self.__host,username=self.__user,password=self.__pass,look_for_keys=False,timeout=1.0)\\r\\n\\t\\t\\t# set connected flag\\r\\n\\t\\t\\tself.__connected = True\\r\\n\\t\\t\\t# debug\\r\\n\\t\\t\\tif self.__debug:\\r\\n\\t\\t\\t\\tprint(\"cNetSSH::connect - connected to device \" + self.__outEnd)\\r\\n\\t\\t\\t# Start an interactive shell session on the SSH server. A new Channel is opened and connected to a pseudo-terminal using the requested terminal type and size.\\r\\n\\t\\t\\tif newShell==True:\\r\\n\\t\\t\\t\\tself.__shell = self.__conn.invoke_shell()\\r\\n\\t\\t\\t\\ttime.sleep(0.3)\\r\\n\\t\\t\\t\\t# Save the initial router prompt\\r\\n\\t\\t\\t\\tself.__output = self.__shell.recv(32000)\\r\\n\\t\\t\\t\\tself.__output = self.__output.splitlines(True)\\r\\n\\t\\t\\t\\tself.__outEnd = self.__output[len(self.__output)-1]\\r\\n\\t\\t\\t\\tif self.__debug:\\r\\n\\t\\t\\t\\t\\tprint(\"cNetSSH::connect - I\\'ll consider this string as the prompt in further requests: \" + self.__outEnd)\\r\\n\\t\\t\\t\\tif paging==False:\\r\\n\\t\\t\\t\\t\\tself.__disablePaging()\\r\\n\\t\\texcept paramiko.AuthenticationException:\\r\\n\\t\\t\\tprint(\"Authentication failed when connecting to \" + self.__host)\\r\\n\\t\\t\\tsys.exit(1)\\r\\n\\t\\texcept:\\r\\n\\t\\t\\tprint(\"Could not connect to host \" + self.__host)\\r\\n\\r\\n\\r\\n\\tdef enable(self,password=\"\"):\\r\\n\\t\\t\"\"\"enable - enter enable mode. please use this method as it stores the new prompt to spped up futher command processing\\r\\n\\t\\t\\r\\n\\t\\tKeyword arguments:\\r\\n\\t\\tpassword = enable password (default \"\")\\r\\n\\t\\t\"\"\"\\r\\n\\t\\tif self.__connected:\\r\\n\\t\\t\\tself.__input = \"enable\"\\r\\n\\t\\t\\tnumBytes     = self.__shell.send(self.__input + \"\\\\n\" + password + \"\\\\n\")\\r\\n\\t\\t\\tif numBytes > 0:\\r\\n\\t\\t\\t\\ttime.sleep(0.3)\\r\\n\\t\\t\\t\\t# Save the router prompt\\r\\n\\t\\t\\t\\tself.__output = self.__shell.recv(32000)\\r\\n\\t\\t\\t\\tself.__output = self.__output.splitlines(True)\\r\\n\\t\\t\\t\\tself.__outEnd = self.__output[len(self.__output)-1]\\r\\n\\t\\t\\t\\tif self.__debug:\\r\\n\\t\\t\\t\\t\\tprint(\"cNetSSH::enable - change expected prompt to \" + self.__outEnd)\\r\\n\\t\\t\\t\\treturn True\\r\\n\\t\\telse:\\r\\n\\t\\t\\tprint(\"Not connected to \" + self.__host + \". Connect first.\")\\r\\n\\r\\n\\tdef send(self,command):\\r\\n\\t\\tif self.__connected:\\r\\n\\t\\t\\tself.__input = command\\r\\n\\t\\t\\tnumBytes     = self.__shell.send(command + \"\\\\n\")\\r\\n\\t\\t\\tif numBytes > 0:\\r\\n\\t\\t\\t\\tself.__output = \"\"\\r\\n\\t\\t\\t\\tmyTempBuffer  = \"\"\\r\\n\\t\\t\\t\\tmax_try       = 500\\r\\n\\t\\t\\t\\tx             = 0\\r\\n\\t\\t\\t\\tsTime         = time.time()\\r\\n\\t\\t\\t\\tbailedOut     = False\\r\\n\\t\\t\\t\\twhile x < max_try:\\r\\n\\t\\t\\t\\t\\tif self.__shell.recv_ready():\\r\\n\\t\\t\\t\\t\\t\\tif self.__debug:\\r\\n\\t\\t\\t\\t\\t\\t\\tprint(\"cNetSSH::send - recv_ready() on cycle=\" + str(x))\\r\\n\\t\\t\\t\\t\\t\\twhile True:\\r\\n\\t\\t\\t\\t\\t\\t\\t# note recv returns if there is > 0 < 1024\\r\\n\\t\\t\\t\\t\\t\\t\\tmyTempBuffer = self.__shell.recv(1024)\\r\\n\\t\\t\\t\\t\\t\\t\\tself.__output += myTempBuffer\\r\\n\\t\\t\\t\\t\\t\\t\\t#print(\"cNetSSH: recv() returned ... len=\" + str(len(myTempBuffer)))\\r\\n\\t\\t\\t\\t\\t\\t\\tif len(myTempBuffer)==0 or self.__shell.recv_ready()==False:\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tbreak\\r\\n\\t\\t\\t\\t\\telse:\\r\\n\\t\\t\\t\\t\\t\\ttime.sleep(0.00005)\\r\\n\\t\\t\\t\\t\\tx += 1\\r\\n\\t\\t\\t\\t\\t# bail out if we\\'ve found the prompt again\\r\\n\\t\\t\\t\\t\\tif re.search(self.__outEnd,self.__output):\\r\\n\\t\\t\\t\\t\\t\\tbailedOut = True\\r\\n\\t\\t\\t\\t\\t\\tbreak\\r\\n\\t\\t\\t\\tif self.__debug:\\r\\n\\t\\t\\t\\t\\teTime = time.time()-sTime\\r\\n\\t\\t\\t\\t\\tprint(\"cNetSSH::send - received \" + str(len(self.__output)) + \" bytes in \" + str(x) + \" cycles and \" + str(eTime) + \"s. BailedOut: \" + str(bailedOut))\\r\\n\\t\\t\\t\\treturn self.__sanitizeOutput(self.__output)\\r\\n\\t\\telse:\\r\\n\\t\\t\\tprint(\"Not connected to \" + self.__host + \". Connect first.\")\\r\\n\\r\\n\\tdef disconnect(self):\\r\\n\\t\\t\"\"\"disconnect\\r\\n\\t\\t\\r\\n\\t\\treturns the configuration as a string\\r\\n\\t\\t\"\"\"\\r\\n\\t\\tself.__conn = None\\r\\n\\r\\n\\tdef __sanitizeOutput(self,output):\\r\\n\\t\\t\"\"\"sanitizeOutput - remove the sent command from output and the prompt\\r\\n\\t\\t\\r\\n\\t\\treturns the configuration as a string\\r\\n\\t\\t\"\"\"\\r\\n\\t\\ttempOut = output.splitlines(True)\\r\\n\\t\\tnewOut  = \"\"\\r\\n\\t\\tfor line in tempOut:\\r\\n\\t\\t\\tif not re.search(\"^\" + self.__outEnd,line) and not re.search(self.__input,line):\\r\\n\\t\\t\\t\\tnewOut += line\\r\\n\\t\\treturn newOut\\r\\n\\r\\n\\tdef __disablePaging(self):\\r\\n\\t\\tif self.__connected:\\r\\n\\t\\t\\tself.__shell.send(\"terminal length 0\\\\n\")\\r\\n\\t\\t\\ttime.sleep(0.25)\\r\\n\\t\\t\\tif self.__shell.recv_ready():\\r\\n\\t\\t\\t\\tself.__output = self.__shell.recv(200)\\r\\n\\t\\telse:\\r\\n\\t\\t\\tprint(\"Not connected to \" + self.__host + \". Connect first.\")\\r\\n\\r\\n\\tdef configure(self):\\r\\n\\t\\t\"\"\"enter configuration mode using configure terminal\\r\\n\\t\\t\\r\\n\\t\\treturns True on success, False on problems\\r\\n\\t\\t\"\"\"\\r\\n\\t\\tif self.__connected:\\r\\n\\t\\t\\tself.__shell.send(\"\\\\nconfigure terminal\\\\n\")\\r\\n\\t\\t\\ttime.sleep(0.25)\\r\\n\\t\\t\\tif self.__shell.recv_ready():\\r\\n\\t\\t\\t\\tself.__output = self.__shell.recv(200)\\r\\n\\t\\t\\t\\t# we expect (config)#\\r\\n\\t\\t\\t\\tif re.search(\"config\\\\)#\",self.__output):\\r\\n\\t\\t\\t\\t\\treturn True\\r\\n\\t\\t\\t\\treturn False\\r\\n\\t\\telse:\\r\\n\\t\\t\\tprint(\"Not connected to \" + self.__host + \". Connect first.\")\\r\\n\\t\\t\\treturn False\\r\\n\\r\\n\\tdef noconfigure(self):\\r\\n\\t\\t\"\"\"leave configuration mode using configure terminal\\r\\n\\t\\t\\r\\n\\t\\treturns True on success, False on problems\\r\\n\\t\\t\"\"\"\\r\\n\\t\\tif self.__connected:\\r\\n\\t\\t\\tself.__shell.send(\"\\\\nend\\\\n\")\\r\\n\\t\\t\\ttime.sleep(0.25)\\r\\n\\t\\t\\tif self.__shell.recv_ready():\\r\\n\\t\\t\\t\\tself.__output = self.__shell.recv(200)\\r\\n\\t\\t\\t\\t# we expect (config)#\\r\\n\\t\\t\\t\\tif re.search(\"#\",self.__output):\\r\\n\\t\\t\\t\\t\\treturn True\\r\\n\\t\\t\\t\\treturn False\\r\\n\\t\\telse:\\r\\n\\t\\t\\tprint(\"Not connected to \" + self.__host + \". Connect first.\")\\r\\n\\t\\t\\treturn False\\r\\n\\r\\nif __name__ == \\'__main__\\':\\r\\n\\tprint(\"This class should only be imported and not run directly!\")\\r\\n',\n",
       " 'from skimage.draw import polygon\\nfrom scipy.spatial import distance\\nimport numpy as np\\nimport cv2\\n\\n\\nclass PicLabeler:\\n    def __init__(self, model, config):\\n        self.model = model\\n        self.slots = config\\n\\n    def run(self, image):\\n\\n        self.image = image\\n        self.image = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)\\n\\n        self.pts2 = np.float32([[0, 60], [0, 0], [40, 0], [40, 60]])\\n        slots = []  # list of preprocessed slot images\\n        ids = []  # list of slot ids\\n\\n        for index, space in enumerate(self.slots):\\n            slot, _ = self.process_slot(space)\\n            ids.append(index + 1)\\n            slots.append(slot)\\n\\n        return self.predict(slots, ids)\\n\\n    def preprocess_coords(self, xs, ys):\\n        distances = []\\n        # calculate all side lengths of the quadrilateral\\n        for i in range(4):\\n            distances.append(\\n                distance.euclidean(\\n                    np.float32([xs[i], ys[i]]),\\n                    np.float32([xs[(i + 1) % 4], ys[(i + 1) % 4]])))\\n        # which one is the longest?\\n        starting_point = np.argmax(np.array(distances))\\n        # rearrange coordinates cyclically, so that longest side goes first\\n        new_xs = xs[starting_point:] + xs[:starting_point]\\n        new_ys = ys[starting_point:] + ys[:starting_point]\\n        return new_xs, new_ys\\n\\n    def predict(self, slots, ids):\\n        answer = {}\\n        if not slots:\\n            print(\"answer empty\")\\n            return answer\\n        # batch_size = 16\\n        # Verbosity mode: 1 = progress bar\\n        pred = self.model.predict(np.array(slots), 16, 1)\\n\\n        # construct a JSON entity with results\\n        pred = pred.ravel().tolist()\\n        for i, one_id in enumerate(ids):\\n            answer[one_id] = \\'Occupied\\' if pred[i] else \\'Empty\\'\\n        return answer\\n\\n    def process_slot(self, space):\\n        xs = []\\n        ys = []\\n        for point in space:\\n            xs.append(point[0])\\n            ys.append(point[1])\\n        # ensure contour is a quadrilateral. This assertion failed once.\\n        assert len(xs) == 4\\n        assert len(ys) == 4\\n        # preprocess and save coordinates\\n        xs, ys = self.preprocess_coords(xs, ys)\\n        xs = np.float32(xs)\\n        ys = np.float32(ys)\\n        coords = np.vstack((xs, ys)).T\\n        # get a matrix for perspective transformation\\n        M = cv2.getPerspectiveTransform(coords, self.pts2)\\n\\n        # transform a quadrilateral into a solid rectangle\\n        dst = cv2.warpPerspective(self.image, M, (40, 60))\\n        # apply the perspective transformation matrix to a slot\\n        # and return as 40x60x1 NumPy array\\n        return np.reshape(dst, (40, 60, 1)), coords\\n',\n",
       " \"# -*- coding: utf-8 -*-\\n# Generated by Django 1.9.1 on 2016-11-14 19:51\\nfrom __future__ import unicode_literals\\n\\nfrom django.db import migrations, models\\n\\n\\nclass Migration(migrations.Migration):\\n\\n    dependencies = [\\n        ('annotation', '0031_auto_20161111_1943'),\\n    ]\\n\\n    operations = [\\n        migrations.AddField(\\n            model_name='masterobservation',\\n            name='observation_time',\\n            field=models.PositiveIntegerField(blank=True, null=True),\\n        ),\\n        migrations.AddField(\\n            model_name='observation',\\n            name='observation_time',\\n            field=models.PositiveIntegerField(blank=True, null=True),\\n        ),\\n    ]\\n\",\n",
       " '# -*- coding: UTF-8 -*-\\n#addonHandler.py\\n#A part of NonVisual Desktop Access (NVDA)\\n#Copyright (C) 2012-2014 Rui Batista, NV Access Limited, Noelia Ruiz Martínez\\n#This file is covered by the GNU General Public License.\\n#See the file COPYING for more details.\\n\\nimport sys\\nimport os.path\\nimport gettext\\nimport glob\\nimport tempfile\\nimport cPickle\\nimport inspect\\nimport itertools\\nimport collections\\nimport pkgutil\\nimport shutil\\nfrom cStringIO import StringIO\\nimport zipfile\\n\\nfrom configobj import ConfigObj, ConfigObjError\\nfrom validate import Validator\\n\\nimport config\\nimport globalVars\\nimport languageHandler\\nfrom logHandler import log\\nimport winKernel\\n\\nMANIFEST_FILENAME = \"manifest.ini\"\\nstateFilename=\"addonsState.pickle\"\\nBUNDLE_EXTENSION = \"nvda-addon\"\\nBUNDLE_MIMETYPE = \"application/x-nvda-addon\"\\nNVDA_ADDON_PROG_ID = \"NVDA.Addon.1\"\\nADDON_PENDINGINSTALL_SUFFIX=\".pendingInstall\"\\nDELETEDIR_SUFFIX=\".delete\"\\n\\nstate={}\\n\\ndef loadState():\\n\\tglobal state\\n\\tstatePath=os.path.join(globalVars.appArgs.configPath,stateFilename)\\n\\ttry:\\n\\t\\tstate = cPickle.load(file(statePath, \"r\"))\\n\\texcept:\\n\\t\\t# Defaults.\\n\\t\\tstate = {\\n\\t\\t\\t\"pendingRemovesSet\":set(),\\n\\t\\t\\t\"pendingInstallsSet\":set(),\\n\\t\\t}\\n\\ndef saveState():\\n\\tstatePath=os.path.join(globalVars.appArgs.configPath,stateFilename)\\n\\ttry:\\n\\t\\tcPickle.dump(state, file(statePath, \"wb\"))\\n\\texcept:\\n\\t\\tlog.debugWarning(\"Error saving state\", exc_info=True)\\n\\ndef getRunningAddons():\\n\\t\"\"\" Returns currently loaded addons.\\n\\t\"\"\"\\n\\treturn (addon for addon in getAvailableAddons() if addon.isRunning)\\n\\ndef completePendingAddonRemoves():\\n\\t\"\"\"Removes any addons that could not be removed on the last run of NVDA\"\"\"\\n\\tuser_addons = os.path.abspath(os.path.join(globalVars.appArgs.configPath, \"addons\"))\\n\\tpendingRemovesSet=state[\\'pendingRemovesSet\\']\\n\\tfor addonName in list(pendingRemovesSet):\\n\\t\\taddonPath=os.path.join(user_addons,addonName)\\n\\t\\tif os.path.isdir(addonPath):\\n\\t\\t\\taddon=Addon(addonPath)\\n\\t\\t\\ttry:\\n\\t\\t\\t\\taddon.completeRemove()\\n\\t\\t\\texcept RuntimeError:\\n\\t\\t\\t\\tlog.exception(\"Failed to remove %s add-on\"%addonName)\\n\\t\\t\\t\\tcontinue\\n\\t\\tpendingRemovesSet.discard(addonName)\\n\\ndef completePendingAddonInstalls():\\n\\tuser_addons = os.path.abspath(os.path.join(globalVars.appArgs.configPath, \"addons\"))\\n\\tpendingInstallsSet=state[\\'pendingInstallsSet\\']\\n\\tfor addonName in pendingInstallsSet:\\n\\t\\tnewPath=os.path.join(user_addons,addonName)\\n\\t\\toldPath=newPath+ADDON_PENDINGINSTALL_SUFFIX\\n\\t\\ttry:\\n\\t\\t\\tos.rename(oldPath,newPath)\\n\\t\\texcept:\\n\\t\\t\\tlog.error(\"Failed to complete addon installation for %s\"%addonName,exc_info=True)\\n\\tpendingInstallsSet.clear()\\n\\ndef removeFailedDeletions():\\n\\tuser_addons = os.path.abspath(os.path.join(globalVars.appArgs.configPath, \"addons\"))\\n\\tfor p in os.listdir(user_addons):\\n\\t\\tif p.endswith(DELETEDIR_SUFFIX):\\n\\t\\t\\tpath=os.path.join(user_addons,p)\\n\\t\\t\\tshutil.rmtree(path,ignore_errors=True)\\n\\t\\t\\tif os.path.exists(path):\\n\\t\\t\\t\\tlog.error(\"Failed to delete path %s, try removing manually\"%path)\\n\\ndef initialize():\\n\\t\"\"\" Initializes the add-ons subsystem. \"\"\"\\n\\tloadState()\\n\\tremoveFailedDeletions()\\n\\tcompletePendingAddonRemoves()\\n\\tcompletePendingAddonInstalls()\\n\\tsaveState()\\n\\tgetAvailableAddons(refresh=True)\\n\\ndef terminate():\\n\\t\"\"\" Terminates the add-ons subsystem. \"\"\"\\n\\tpass\\n\\ndef _getDefaultAddonPaths():\\n\\t\"\"\" Returns paths where addons can be found.\\n\\tFor now, only <userConfig\\\\addons is supported.\\n\\t@rtype: list(string)\\n\\t\"\"\"\\n\\taddon_paths = []\\n\\tuser_addons = os.path.abspath(os.path.join(globalVars.appArgs.configPath, \"addons\"))\\n\\tif os.path.isdir(user_addons):\\n\\t\\taddon_paths.append(user_addons)\\n\\treturn addon_paths\\n\\ndef _getAvailableAddonsFromPath(path):\\n\\t\"\"\" Gets available add-ons from path.\\n\\tAn addon is only considered available if the manifest file is loaded with no errors.\\n\\t@param path: path from where to find addon directories.\\n\\t@type path: string\\n\\t@rtype generator of Addon instances\\n\\t\"\"\"\\n\\tlog.debug(\"Listing add-ons from %s\", path)\\n\\tfor p in os.listdir(path):\\n\\t\\tif p.endswith(DELETEDIR_SUFFIX): continue\\n\\t\\taddon_path = os.path.join(path, p)\\n\\t\\tif os.path.isdir(addon_path) and addon_path not in (\\'.\\', \\'..\\'):\\n\\t\\t\\tlog.debug(\"Loading add-on from %s\", addon_path)\\n\\t\\t\\ttry:\\n\\t\\t\\t\\ta = Addon(addon_path)\\n\\t\\t\\t\\tlog.debug(\"Found add-on %s\", a.manifest[\\'name\\'])\\n\\t\\t\\t\\tyield a\\n\\t\\t\\texcept:\\n\\t\\t\\t\\tlog.error(\"Error loading Addon from path: %s\", addon_path, exc_info=True)\\n\\n_availableAddons = collections.OrderedDict()\\ndef getAvailableAddons(refresh=False):\\n\\t\"\"\" Gets all available addons on the system.\\n\\t@rtype generator of Addon instances.\\n\\t\"\"\"\\n\\tif refresh:\\n\\t\\t_availableAddons.clear()\\n\\t\\tgenerators = [_getAvailableAddonsFromPath(path) for path in _getDefaultAddonPaths()]\\n\\t\\tfor addon in itertools.chain(*generators):\\n\\t\\t\\t_availableAddons[addon.path] = addon\\n\\treturn _availableAddons.itervalues()\\n\\ndef installAddonBundle(bundle):\\n\\t\"\"\"Extracts an Addon bundle in to a unique subdirectory of the user addons directory, marking the addon as needing install completion on NVDA restart.\"\"\"\\n\\taddonPath = os.path.join(globalVars.appArgs.configPath, \"addons\",bundle.manifest[\\'name\\']+ADDON_PENDINGINSTALL_SUFFIX)\\n\\tbundle.extract(addonPath)\\n\\taddon=Addon(addonPath)\\n\\t# #2715: The add-on must be added to _availableAddons here so that\\n\\t# translations can be used in installTasks module.\\n\\t_availableAddons[addon.path]=addon\\n\\ttry:\\n\\t\\taddon.runInstallTask(\"onInstall\")\\n\\texcept:\\n\\t\\tlog.error(\"task \\'onInstall\\' on addon \\'%s\\' failed\"%addon.name,exc_info=True)\\n\\t\\tdel _availableAddons[addon.path]\\n\\t\\taddon.completeRemove(runUninstallTask=False)\\n\\t\\traise AddonError(\"Installation failed\")\\n\\tstate[\\'pendingInstallsSet\\'].add(bundle.manifest[\\'name\\'])\\n\\tsaveState()\\n\\treturn addon\\n\\nclass AddonError(Exception):\\n\\t\"\"\" Represents an exception coming from the addon subsystem. \"\"\"\\n\\n\\nclass Addon(object):\\n\\t\"\"\" Represents an Add-on available on the file system.\"\"\"\\n\\tdef __init__(self, path):\\n\\t\\t\"\"\" Constructs an L[Addon} from.\\n\\t\\t@param path: the base directory for the addon data.\\n\\t\\t@type path: string\\n\\t\\t\"\"\"\\n\\t\\tself.path = os.path.abspath(path)\\n\\t\\tself._extendedPackages = set()\\n\\t\\tself._isLoaded = False\\n\\t\\tmanifest_path = os.path.join(path, MANIFEST_FILENAME)\\n\\t\\twith open(manifest_path) as f:\\n\\t\\t\\ttranslatedInput = None\\n\\t\\t\\tfor translatedPath in _translatedManifestPaths():\\n\\t\\t\\t\\tp = os.path.join(self.path, translatedPath)\\n\\t\\t\\t\\tif os.path.exists(p):\\n\\t\\t\\t\\t\\tlog.debug(\"Using manifest translation from %s\", p)\\n\\t\\t\\t\\t\\ttranslatedInput = open(p, \\'r\\')\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\tself.manifest = AddonManifest(f, translatedInput)\\n\\n\\t@property\\n\\tdef isPendingInstall(self):\\n\\t\\t\"\"\"True if this addon has not yet been fully installed.\"\"\"\\n\\t\\treturn self.path.endswith(ADDON_PENDINGINSTALL_SUFFIX)\\n\\n\\t@property\\n\\tdef isPendingRemove(self):\\n\\t\\t\"\"\"True if this addon is marked for removal.\"\"\"\\n\\t\\treturn not self.isPendingInstall and self.name in state[\\'pendingRemovesSet\\']\\n\\n\\tdef requestRemove(self):\\n\\t\\t\"\"\"Markes this addon for removal on NVDA restart.\"\"\"\\n\\t\\tif self.isPendingInstall:\\n\\t\\t\\tself.completeRemove()\\n\\t\\t\\tstate[\\'pendingInstallsSet\\'].discard(self.name)\\n\\t\\t\\t#Force availableAddons to be updated\\n\\t\\t\\tgetAvailableAddons(refresh=True)\\n\\t\\telse:\\n\\t\\t\\tstate[\\'pendingRemovesSet\\'].add(self.name)\\n\\t\\tsaveState()\\n\\n\\tdef completeRemove(self,runUninstallTask=True):\\n\\t\\tif runUninstallTask:\\n\\t\\t\\ttry:\\n\\t\\t\\t\\t# #2715: The add-on must be added to _availableAddons here so that\\n\\t\\t\\t\\t# translations can be used in installTasks module.\\n\\t\\t\\t\\t_availableAddons[self.path] = self\\n\\t\\t\\t\\tself.runInstallTask(\"onUninstall\")\\n\\t\\t\\texcept:\\n\\t\\t\\t\\tlog.error(\"task \\'onUninstall\\' on addon \\'%s\\' failed\"%self.name,exc_info=True)\\n\\t\\t\\tfinally:\\n\\t\\t\\t\\tdel _availableAddons[self.path]\\n\\t\\ttempPath=tempfile.mktemp(suffix=DELETEDIR_SUFFIX,dir=os.path.dirname(self.path))\\n\\t\\ttry:\\n\\t\\t\\tos.rename(self.path,tempPath)\\n\\t\\texcept (WindowsError,IOError):\\n\\t\\t\\traise RuntimeError(\"Cannot rename add-on path for deletion\")\\n\\t\\tshutil.rmtree(tempPath,ignore_errors=True)\\n\\t\\tif os.path.exists(tempPath):\\n\\t\\t\\tlog.error(\"Error removing addon directory %s, deferring until next NVDA restart\"%self.path)\\n\\n\\t@property\\n\\tdef name(self):\\n\\t\\treturn self.manifest[\\'name\\']\\n\\n\\tdef addToPackagePath(self, package):\\n\\t\\t\"\"\" Adds this L{Addon} extensions to the specific package path if those exist.\\n\\t\\t@param package: the python module representing the package.\\n\\t\\t@type package: python module.\\n\\t\\t\"\"\"\\n\\t\\textension_path = os.path.join(self.path, package.__name__)\\n\\t\\tif not os.path.isdir(extension_path):\\n\\t\\t\\t# This addon does not have extension points for this package\\n\\t\\t\\treturn\\n\\t\\t# Python 2.x doesn\\'t properly handle unicode import paths, so convert them before adding.\\n\\t\\tconverted_path = self._getPathForInclusionInPackage(package)\\n\\t\\tpackage.__path__.insert(0, converted_path)\\n\\t\\tself._extendedPackages.add(package)\\n\\t\\tlog.debug(\"Addon %s added to %s package path\", self.manifest[\\'name\\'], package.__name__)\\n\\n\\t@property\\n\\tdef isRunning(self):\\n\\t\\treturn not self.isPendingInstall\\n\\n\\tdef _getPathForInclusionInPackage(self, package):\\n\\t\\textension_path = os.path.join(self.path, package.__name__)\\n\\t\\treturn extension_path.encode(\"mbcs\")\\n\\n\\tdef loadModule(self, name):\\n\\t\\t\"\"\" loads a python module from the addon directory\\n\\t\\t@param name: the module name\\n\\t\\t@type name: string\\n\\t\\t@returns the python module with C[name}\\n\\t\\t@rtype python module\\n\\t\\t\"\"\"\\n\\t\\tlog.debug(\"Importing module %s from plugin %s\", name, self.name)\\n\\t\\timporter = pkgutil.ImpImporter(self.path)\\n\\t\\tloader = importer.find_module(name)\\n\\t\\tif not loader:\\n\\t\\t\\treturn None\\n\\t\\t# Create a qualified full name to avoid modules with the same name on sys.modules.\\n\\t\\tfullname = \"addons.%s.%s\" % (self.name, name)\\n\\t\\ttry:\\n\\t\\t\\treturn loader.load_module(fullname)\\n\\t\\texcept ImportError:\\n\\t\\t\\t# in this case return None, any other error throw to be handled elsewhere\\n\\t\\t\\treturn None\\n\\n\\tdef getTranslationsInstance(self, domain=\\'nvda\\'):\\n\\t\\t\"\"\" Gets the gettext translation instance for this addon.\\n\\t\\t<addon-path<\\\\locale will be used to find .mo files, if exists.\\n\\t\\tIf a translation file is not found the default fallback null translation is returned.\\n\\t\\t@param domain: the tranlation domain to retrieve. The \\'nvda\\' default should be used in most cases.\\n\\t\\t@returns: the gettext translation class.\\n\\t\\t\"\"\"\\n\\t\\tlocaledir = os.path.join(self.path, \"locale\")\\n\\t\\treturn gettext.translation(domain, localedir=localedir, languages=[languageHandler.getLanguage()], fallback=True)\\n\\n\\tdef runInstallTask(self,taskName,*args,**kwargs):\\n\\t\\t\"\"\"\\n\\t\\tExecutes the function having the given taskName with the given args and kwargs in the addon\\'s installTasks module if it exists.\\n\\t\\t\"\"\"\\n\\t\\tif not hasattr(self,\\'_installTasksModule\\'):\\n\\t\\t\\tself._installTasksModule=self.loadModule(\\'installTasks\\')\\n\\t\\tif self._installTasksModule:\\n\\t\\t\\tfunc=getattr(self._installTasksModule,taskName,None)\\n\\t\\t\\tif func:\\n\\t\\t\\t\\tfunc(*args,**kwargs)\\n\\n\\tdef getDocFilePath(self, fileName=None):\\n\\t\\t\"\"\"Get the path to a documentation file for this add-on.\\n\\t\\tThe file should be located in C{doc\\\\lang\\\\file} inside the add-on,\\n\\t\\twhere C{lang} is the language code and C{file} is the requested file name.\\n\\t\\tFailing that, the language without country is tried.\\n\\t\\tEnglish is tried as a last resort.\\n\\t\\tAn add-on can specify a default documentation file name\\n\\t\\tvia the docFileName parameter in its manifest.\\n\\t\\t@param fileName: The requested file name or C{None} for the add-on\\'s default.\\n\\t\\t@type fileName: basestring\\n\\t\\t@return: The path to the requested file or C{None} if it wasn\\'t found.\\n\\t\\t@rtype: basestring\\n\\t\\t\"\"\"\\n\\t\\tif not fileName:\\n\\t\\t\\tfileName = self.manifest[\"docFileName\"]\\n\\t\\t\\tif not fileName:\\n\\t\\t\\t\\treturn None\\n\\t\\tdocRoot = os.path.join(self.path, \"doc\")\\n\\t\\tlang = languageHandler.getLanguage()\\n\\t\\tlangs = [lang]\\n\\t\\tif \"_\" in lang:\\n\\t\\t\\tlang = lang.split(\"_\", 1)[0]\\n\\t\\t\\tlangs.append(lang)\\n\\t\\tif lang != \"en\":\\n\\t\\t\\tlangs.append(\"en\")\\n\\t\\tfor lang in langs:\\n\\t\\t\\tdocFile = os.path.join(docRoot, lang, fileName)\\n\\t\\t\\tif os.path.isfile(docFile):\\n\\t\\t\\t\\treturn docFile\\n\\t\\treturn None\\n\\ndef getCodeAddon(obj=None, frameDist=1):\\n\\t\"\"\" Returns the L{Addon} where C{obj} is defined. If obj is None the caller code frame is assumed to allow simple retrieval of \"current calling addon\".\\n\\t@param obj: python object or None for default behaviour.\\n\\t@param frameDist: howmany frames is the caller code. Only change this for functions in this module.\\n\\t@return: L{Addon} instance or None if no code does not belong to a add-on package.\\n\\t@rtype: C{Addon}\\n\\t\"\"\"\\n\\tglobal _availableAddons\\n\\tif obj is None:\\n\\t\\tobj = sys._getframe(frameDist)\\n\\tfileName  = inspect.getfile(obj)\\n\\tdir= unicode(os.path.abspath(os.path.dirname(fileName)), \"mbcs\")\\n\\t# if fileName is not a subdir of one of the addon paths\\n\\t# It does not belong to an addon.\\n\\tfor p in _getDefaultAddonPaths():\\n\\t\\tif dir.startswith(p):\\n\\t\\t\\tbreak\\n\\telse:\\n\\t\\traise AddonError(\"Code does not belong to an addon package.\")\\n\\tcurdir = dir\\n\\twhile curdir not in _getDefaultAddonPaths():\\n\\t\\tif curdir in _availableAddons.keys():\\n\\t\\t\\treturn _availableAddons[curdir]\\n\\t\\tcurdir = os.path.abspath(os.path.join(curdir, \"..\"))\\n\\t# Not found!\\n\\traise AddonError(\"Code does not belong to an addon\")\\n\\ndef initTranslation():\\n\\taddon = getCodeAddon(frameDist=2)\\n\\ttranslations = addon.getTranslationsInstance()\\n\\t# Point _ to the translation object in the globals namespace of the caller frame\\n\\t# FIXME: shall we retrieve the caller module object explicitly?\\n\\ttry:\\n\\t\\tcallerFrame = inspect.currentframe().f_back\\n\\t\\tcallerFrame.f_globals[\\'_\\'] = translations.ugettext\\n\\t\\t# Install our pgettext function.\\n\\t\\tcallerFrame.f_globals[\\'pgettext\\'] = languageHandler.makePgettext(translations)\\n\\tfinally:\\n\\t\\tdel callerFrame # Avoid reference problems with frames (per python docs)\\n\\ndef _translatedManifestPaths(lang=None, forBundle=False):\\n\\tif lang is None:\\n\\t\\tlang = languageHandler.getLanguage() # can\\'t rely on default keyword arguments here.\\n\\tlangs=[lang]\\n\\tif \\'_\\' in lang:\\n\\t\\tlangs.append(lang.split(\\'_\\')[0])\\n\\t\\tif lang!=\\'en\\' and not lang.startswith(\\'en_\\'):\\n\\t\\t\\tlangs.append(\\'en\\')\\n\\tsep = \"/\" if forBundle else os.path.sep\\n\\treturn [sep.join((\"locale\", lang, MANIFEST_FILENAME)) for lang in langs]\\n\\n\\nclass AddonBundle(object):\\n\\t\"\"\" Represents the contents of an NVDA addon suitable for distribution.\\n\\tThe bundle is compressed using the zip file format. Manifest information\\n\\tis available without the need for extraction.\"\"\"\\n\\tdef __init__(self, bundlePath):\\n\\t\\t\"\"\" Constructs an L{AddonBundle} from a filename.\\n\\t\\t@param bundlePath: The path for the bundle file.\\n\\t\\t\"\"\"\\n\\t\\tself._path = bundlePath if isinstance(bundlePath, unicode) else unicode(bundlePath, \"mbcs\")\\n\\t\\t# Read manifest:\\n\\t\\ttranslatedInput=None\\n\\t\\twith zipfile.ZipFile(self._path, \\'r\\') as z:\\n\\t\\t\\tfor translationPath in _translatedManifestPaths(forBundle=True):\\n\\t\\t\\t\\ttry:\\n\\t\\t\\t\\t\\ttranslatedInput = z.open(translationPath, \\'r\\')\\n\\t\\t\\t\\t\\tbreak\\n\\t\\t\\t\\texcept KeyError:\\n\\t\\t\\t\\t\\tpass\\n\\t\\t\\tself._manifest = AddonManifest(z.open(MANIFEST_FILENAME), translatedInput=translatedInput)\\n\\n\\tdef extract(self, addonPath):\\n\\t\\t\"\"\" Extracts the bundle content to the specified path.\\n\\t\\tThe addon will be extracted to L{addonPath}\\n\\t\\t@param addonPath: Path where to extract contents.\\n\\t\\t@type addonPath: string\\n\\t\\t\"\"\"\\n\\t\\twith zipfile.ZipFile(self._path, \\'r\\') as z:\\n\\t\\t\\tfor info in z.infolist():\\n\\t\\t\\t\\tif isinstance(info.filename, str):\\n\\t\\t\\t\\t\\t# #2505: Handle non-Unicode file names.\\n\\t\\t\\t\\t\\t# Most archivers seem to use the local OEM code page, even though the spec says only cp437.\\n\\t\\t\\t\\t\\t# HACK: Overriding info.filename is a bit ugly, but it avoids a lot of code duplication.\\n\\t\\t\\t\\t\\tinfo.filename = info.filename.decode(\"cp%d\" % winKernel.kernel32.GetOEMCP())\\n\\t\\t\\t\\tz.extract(info, addonPath)\\n\\n\\t@property\\n\\tdef manifest(self):\\n\\t\\t\"\"\" Gets the manifest for the represented Addon.\\n\\t\\t@rtype: AddonManifest\\n\\t\\t\"\"\"\\n\\t\\treturn self._manifest\\n\\n\\tdef __repr__(self):\\n\\t\\treturn \"<AddonBundle at %s>\" % self._path\\n\\ndef createAddonBundleFromPath(path, destDir=None):\\n\\t\"\"\" Creates a bundle from a directory that contains a a addon manifest file.\"\"\"\\n\\tbasedir = os.path.abspath(path)\\n\\t# If  caller did not provide a destination directory name\\n\\t# Put the bundle at the same level of the addon\\'s top directory,\\n\\t# That is, basedir/..\\n\\tif destDir is None:\\n\\t\\tdestDir = os.path.dirname(basedir)\\n\\tmanifest_path = os.path.join(basedir, MANIFEST_FILENAME)\\n\\tif not os.path.isfile(manifest_path):\\n\\t\\traise AddonError(\"Can\\'t find %s manifest file.\" % manifest_path)\\n\\twith open(manifest_path) as f:\\n\\t\\tmanifest = AddonManifest(f)\\n\\tif manifest.errors is not None:\\n\\t\\t_report_manifest_errors(manifest)\\n\\t\\traise AddonError(\"Manifest file as errors.\")\\n\\tbundleFilename = \"%s-%s.%s\" % (manifest[\\'name\\'], manifest[\\'version\\'], BUNDLE_EXTENSION)\\n\\tbundleDestination = os.path.join(destDir, bundleFilename)\\n\\twith zipfile.ZipFile(bundleDestination, \\'w\\') as z:\\n\\t\\t# FIXME: the include/exclude feature may or may not be useful. Also python files can be pre-compiled.\\n\\t\\tfor dir, dirnames, filenames in os.walk(basedir):\\n\\t\\t\\trelativePath = os.path.relpath(dir, basedir)\\n\\t\\t\\tfor filename in filenames:\\n\\t\\t\\t\\tpathInBundle = os.path.join(relativePath, filename)\\n\\t\\t\\t\\tabsPath = os.path.join(dir, filename)\\n\\t\\t\\t\\tz.write(absPath, pathInBundle)\\n\\treturn AddonBundle(bundleDestination)\\n\\n\\ndef _report_manifest_errors(manifest):\\n\\tlog.warning(\"Error loading manifest:\\\\n%s\", manifest.errors)\\n\\nclass AddonManifest(ConfigObj):\\n\\t\"\"\" Add-on manifest file. It contains metadata about an NVDA add-on package. \"\"\"\\n\\tconfigspec = ConfigObj(StringIO(\\n\\t\"\"\"\\n# NVDA Add-on Manifest configuration specification\\n# Add-on unique name\\nname = string()\\n# short  summary (label) of the add-on to show to users.\\nsummary = string()\\n# Long description with further information and instructions\\ndescription = string(default=None)\\n# Name of the author or entity that created the add-on\\nauthor = string()\\n# Version of the add-on. Should preferably in some standard format such as x.y.z\\nversion = string()\\n# URL for more information about the add-on. New versions and such.\\nurl= string(default=None)\\n# Name of default documentation file for the add-on.\\ndocFileName = string(default=None)\\n\\n\"\"\"))\\n\\n\\tdef __init__(self, input, translatedInput=None):\\n\\t\\t\"\"\" Constructs an L{AddonManifest} instance from manifest string data\\n\\t\\t@param input: data to read the manifest informatinon\\n\\t\\t@type input: a fie-like object.\\n\\t\\t@param translatedInput: translated manifest input\\n\\t\\t@type translatedInput: file-like object\\n\\t\\t\"\"\"\\n\\t\\tsuper(AddonManifest, self).__init__(input, configspec=self.configspec, encoding=\\'utf-8\\', default_encoding=\\'utf-8\\')\\n\\t\\tself._errors = []\\n\\t\\tval = Validator()\\n\\t\\tresult = self.validate(val, copy=True, preserve_errors=True)\\n\\t\\tif result != True:\\n\\t\\t\\tself._errors = result\\n\\t\\tself._translatedConfig = None\\n\\t\\tif translatedInput is not None:\\n\\t\\t\\tself._translatedConfig = ConfigObj(translatedInput, encoding=\\'utf-8\\', default_encoding=\\'utf-8\\')\\n\\t\\t\\tfor k in (\\'summary\\',\\'description\\'):\\n\\t\\t\\t\\tval=self._translatedConfig.get(k)\\n\\t\\t\\t\\tif val:\\n\\t\\t\\t\\t\\tself[k]=val\\n\\n\\t@property\\n\\tdef errors(self):\\n\\t\\treturn self._errors\\n',\n",
       " 'from .entry_storage import EntryOperations\\nfrom .functions import batch_list\\n',\n",
       " 'import gc\\nimport hashlib\\nimport itertools\\nimport logging\\nimport math\\nimport sys\\nimport traceback\\nfrom logging import warning, debug\\n\\nimport numpy as np\\nfrom pulp import LpProblem, LpMinimize, LpVariable, LpInteger, CPLEX, LpStatus\\n\\nfrom src.dbms.utils import sql_get_all_attributes, sql_table_column_data_type\\nfrom src.paql.constraints import *\\nfrom src.paql.expression_trees.expression_trees import ArithmeticExpression\\nfrom src.paql.expression_trees.syntax_tree import Expression\\nfrom src.paql.objectives import *\\nfrom src.utils.utils import op_to_opstr\\n\\n\\n\\nclass NotPackageQueryException(Exception):\\n    pass\\n\\n\\nclass PaQLParserError(Exception):\\n    pass\\n\\n\\n\\n\\n\\n\\nclass PackageQuery(object):\\n    allowed_dbms_data_types = {\\n        \"integer\",\\n        \"bigint\",\\n        \"double precision\",\\n        # \"numeric\",\\n        # \"numeric(15,2)\"\\n    }\\n\\n    @property\\n    def table_name(self):\\n        assert len(self.rel_namespace.values()) == 1\\n        return self.rel_namespace.itervalues().next()\\n\\n\\n    @table_name.setter\\n    def table_name(self, table_name):\\n        assert len(self.rel_namespace.values()) == 1\\n        if self.table_name is not None and self.rel_namespace is not None:\\n            for rel, relname in self.rel_namespace.iteritems():\\n                if relname.lower() == self.table_name.lower():\\n                    self.rel_namespace[rel] = table_name\\n\\n        self._paql_query_str_stale = True\\n\\n\\n    @property\\n    def bc_query(self):\\n        bc_query = \"SELECT * FROM {}\".format(\\n            \\',\\'.join([\\n                rel_name + \" \" + rel_alias for rel_alias, rel_name in self.rel_namespace.iteritems()\\n            ]))\\n        where_clause_str = self.where_expr.get_str()\\n        if where_clause_str:\\n            bc_query += \" WHERE {}\".format(where_clause_str)\\n        if self.limit is not None and self.limit[\"TYPE\"] ==\"INPUT\":\\n            bc_query += \" LIMIT {}\".format(self.limit[\"LIMIT\"])\\n        return bc_query\\n\\n\\n    def __init__(self, d):\\n        assert isinstance(d, dict)\\n\\n        self._paql_query_str = None\\n        self._paql_query_str_stale = True\\n\\n        # self.package_rel_names = d[\"package rel names\"]\\n        self.rel_namespace = d[\"namespace\"]\\n        self.rel_repeats = d[\"repeats\"]\\n        self.where_expr = d[\"where expr\"]\\n        self.such_that_expr = d[\"such that expr\"]\\n\\n        if d[\"objective expr\"] is not None:\\n            self.objective = PackageQueryObjective(\\n                sqlquery_expr=d[\"objective expr\"].get_sql_arithmetic_expression(),\\n                sense=d[\"objective sense\"])\\n        else:\\n            self.objective = None\\n\\n        self.limit = d[\"limit\"]\\n\\n        # NOTE: For now, assuming that the query is single-table.\\n        # TODO: We need to take into account REPEAT! It\\'s not implemented yet!\\n        # rel_names = self.rel_namespace.values()\\n        assert len(self.rel_namespace.values()) == 1, \"Not a single-table package query!\"\\n        # self.table_name = self.bc_query.lower().split(\"from\")[1].split(\"where\")[0].split()[0].strip()\\n        # self.table_name = rel_names[0]\\n\\n\\n    def __str__(self):\\n        raise NotImplementedError\\n\\n\\n    def md5(self):\\n        return hashlib.md5(str(self)).hexdigest()\\n\\n\\n    @classmethod\\n    def get_json_from_paql(cls, paql_str):\\n        from subprocess import Popen, PIPE\\n\\n        p = Popen([\"PaQL_Parser\"], stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)\\n        json_str, err = p.communicate(input=paql_str)\\n        p.wait()\\n\\n        if err != \"\":\\n            raise PaQLParserError(err)\\n\\n        return json_str\\n\\n\\n    @classmethod\\n    def from_paql(cls, paql_str):\\n        \"\"\"\\n        Returns a new PackageQuery object from a PaQL query string. This is the method that you would call more often.\\n        :param paql_str: A string containing a PaQL query\\n        :rtype : PackageQuery\\n        \"\"\"\\n        json_str = PackageQuery.get_json_from_paql(paql_str)\\n\\n        try:\\n            package_query = cls.from_json(json_str)\\n        except ValueError as e:\\n            traceback.print_exc(file=sys.stdout)\\n            raise PaQLParserError(e)\\n        else:\\n            package_query._paql_query_str = paql_str\\n            package_query._paql_query_str_stale = False\\n            return package_query\\n        finally:\\n            gc.collect()\\n\\n\\n    @classmethod\\n    def from_json(cls, json_str):\\n        \"\"\"\\n        Returns a new PackageQuery object from a JSON string. This method is usually called by from_PaQL() to\\n        transform the paql parser output (which is a JSON) into a PackageQuery object.\\n        This is the main entry point from the direct output of the paql parser.\\n        :param json_str: A string containing a JSON structure for a parsed PaQL query\\n        \"\"\"\\n        import json\\n\\n        q = json.loads(json_str)\\n\\n        # The namespace of relations defined by the query. A dictionary alias -> relation-name.\\n        # This way, all references to relations can be just made based on the alias names, and we can avoid confusion\\n        # when nested queries contain the same relation names, etc.\\n        rel_namespace = { }\\n\\n        # The mapping from relation aliases into their corresponding REPEAT values.\\n        rel_repeats = { }\\n\\n        # The list of relation aliases which form the PACKAGE.\\n        package_rel_names = []\\n\\n        # TODO: Ideally, if the query is not a package query we may want to just execute it as it is...\\n        # TODO: If it doesn\\'t contain the PACKAGE clause, we should make sure it does not contain SUCH THAT either.\\n\\n        # Check if it\\'s package query and store reference to relation names\\n        for select_item in q[\"SELECT\"]:\\n            assert type(select_item) == dict\\n\\n            if select_item[\"NODE_TYPE\"] == \"*\":\\n                raise NotPackageQueryException()\\n\\n            elif select_item[\"NODE_TYPE\"] == \"COL_REF\":\\n                raise NotPackageQueryException()\\n\\n            elif select_item[\"NODE_TYPE\"] == \"PACKAGE\":\\n                package_rel_names.extend(r[\"REL_NAME\"] for r in select_item[\"PACKAGE_RELS\"])\\n\\n            else:\\n                raise Exception(\"Problem in SELECT clause, NODE_TYPE non recognized: \" + select_item[\"NODE_TYPE\"])\\n\\n        # Store relation names and aliases, and repeat constraint for each of them\\n        # These are stored in a dictionary rel_namespace(key=rel_alias, val=rel_names)\\n        for from_ in q[\"FROM\"]:\\n            assert type(from_) == dict\\n            rel_name = from_[\"REL_NAME\"]\\n            rel_alias = from_.get(\"REL_ALIAS\", rel_name)\\n            repeat = from_.get(\"REPEAT\", -1)\\n            rel_namespace[rel_alias] = rel_name\\n            rel_repeats[rel_alias] = repeat\\n\\n        # Make sure that all relation aliases referred in PACKAGE(...) are in the FROM clause as well\\n        assert all(p_rel_name in rel_namespace for p_rel_name in package_rel_names)\\n\\n        # Stricter (for now): Make sure that they are exactly the same relation references\\n        assert set(package_rel_names) == set(rel_namespace.iterkeys())\\n\\n        # Create WHERE clause expression tree\\n        where_clause = Expression(q[\"WHERE\"])\\n\\n        # Create SUCH THAT clause expression tree\\n        such_that_clause = Expression(q[\"SUCH-THAT\"])\\n\\n        # Create objective clause expression tree\\n        if q[\"OBJECTIVE\"] is not None:\\n            objective_expr = Expression(q[\"OBJECTIVE\"][\"EXPR\"])\\n\\n            if q[\"OBJECTIVE\"][\"TYPE\"] == \"MAXIMIZE\":\\n                # objective = { \"type\": \"maximize\", \"expr\": objective_expr }\\n                objective_sense = ObjectiveSenseMAX()\\n\\n            elif q[\"OBJECTIVE\"][\"TYPE\"] == \"MINIMIZE\":\\n                # objective = { \"type\": \"minimize\", \"expr\": objective_expr }\\n                objective_sense = ObjectiveSenseMIN()\\n\\n            else:\\n                raise Exception(\"Unsupported objective type: `{}\\'\".format(q[\"OBJECTIVE\"][\"TYPE\"]))\\n\\n        else:\\n            objective_expr = objective_sense = None\\n\\n        query_dict = {\\n            # \"package rel names\": package_rel_names,\\n            \"where expr\": where_clause,\\n            \"such that expr\": such_that_clause,\\n            \"objective expr\": objective_expr,\\n            \"objective sense\": objective_sense,\\n            \"namespace\": rel_namespace,\\n            \"repeats\": rel_repeats,\\n            \"limit\": q[\"LIMIT\"],\\n        }\\n\\n        if such_that_clause.is_conjunctive() and where_clause.is_conjunctive():\\n            return ConjunctivePackageQuery(query_dict)\\n        else:\\n            return cls(query_dict)\\n\\n\\n    @staticmethod\\n    def from_uncoalesced_constraints(table_name, unc_bcs, unc_gcs, objective):\\n        \"\"\"\\n        This method creates a new PackageQuery from sets of uncoalesced constraints and an objective.\\n        \"\"\"\\n        bc_query = \"SELECT * FROM {} {}\".format(table_name, \"WHERE true\" if len(unc_bcs) > 0 else \"\")\\n        for attr, op, n in unc_bcs:\\n            bc_query += \" AND {a} {o} {b}\".format(a=attr, o=op_to_opstr(op), b=n)\\n\\n        gc_queries = []\\n        gc_ranges = []\\n        for (aggr, attr), op, n in unc_gcs:\\n            gc_query = \"SELECT {aggr}({attr}) FROM memory_representations\".format(aggr=aggr, attr=attr)\\n            if op == operator.le:\\n                # gc_range = (-sys.maxint, n)\\n                gc_range = (-float(\"inf\"), n)\\n            elif op == operator.ge:\\n                # gc_range = (n, sys.maxint)\\n                gc_range = (n, float(\"inf\"))\\n            elif op == operator.eq:\\n                gc_range = (n, n)\\n            else:\\n                raise Exception(\"Operator \\'{}\\' not supported yet.\".format(op))\\n            gc_queries.append(gc_query)\\n            gc_ranges.append(gc_range)\\n\\n        return PackageQuery({\\n            \"bc\": bc_query,\\n            \"gc\": map(lambda x: (x[0], x[1][0], x[1][1]), zip(gc_queries, gc_ranges)),\\n            \"objective\": objective,\\n        })\\n\\n\\n    def get_objective_attributes(self):\\n        attrs = set()\\n        if self.objective is not None:\\n            for attr in self.objective.get_attributes():\\n                if attr != \"*\":\\n                    attrs.add(attr)\\n        return attrs\\n\\n\\n    def get_bcs_attributes(self):\\n        return set(attr for attr in self.coalesced_bcs) - {\"*\"}\\n\\n\\n    def get_gcs_attributes(self):\\n        gcs_attrs = set()\\n        for gc in self.coalesced_gcs:\\n            assert isinstance(gc, CGlobalConstraint)\\n\\n            gcs_attrs.update(gc.get_attributes())\\n\\n        return gcs_attrs\\n\\n\\n    def get_attributes(self):\\n        # FIXME: If this is a relaxed query, you should return all attributes including those of the original query.\\n        return self.get_bcs_attributes() | self.get_gcs_attributes() | self.get_objective_attributes()\\n\\n\\n    def get_data_attributes(self, db):\\n        all_data_attributes = sql_get_all_attributes(db, self.table_name)\\n\\n        # Only pick the data attributes of the allowed data type\\n        data_attributes = set()\\n        for data_attr in all_data_attributes:\\n            attribute_type = sql_table_column_data_type(db, self.table_name, data_attr)\\n            if attribute_type in self.allowed_dbms_data_types:\\n                data_attributes.add(data_attr)\\n        return sorted(data_attributes)\\n\\n\\n\\n    def get_paql_str(self, redo=False, recompute_gcs=True, coalesced=False):\\n        raise NotImplementedError\\n\\n\\n    def abs_ugc_errors(self, gc_scores, attrs=None):\\n        \"\"\"\\n        Returns absolute errors for each (uncoalesced) global constraint.\\n        \"\"\"\\n        if attrs is None:\\n            use_attrs = self.get_attributes()\\n        else:\\n            use_attrs = set(attrs)\\n\\n        return {\\n            (aggr, attr): max(0, c - gc_scores[aggr, attr] if op == operator.ge else gc_scores[aggr, attr] - c)\\n            for (aggr, attr), op, c in self.uncoalesced_gcs if attr == \"*\" or attr in use_attrs\\n        }\\n\\n\\n    def error_mape(self, u_gc_scores, u_bc_scores):\\n        errorsum = .0\\n        n_gcs = 0\\n        n_bcs = 0\\n\\n        for i, ((aggr, attr), op, c) in enumerate(self.uncoalesced_gcs):\\n            score = u_gc_scores[i]\\n            if not op(score, c):\\n                errorsum += abs((c - score) / c)\\n            n_gcs += 1\\n\\n        for bscores in u_bc_scores:\\n            for i, (attr, op, c) in enumerate(self.uncoalesced_bcs):\\n                score = bscores[i]\\n                if not op(score, c):\\n                    errorsum += abs((c - score) / c)\\n                n_bcs += 1\\n\\n        if n_gcs + n_bcs > 0:\\n            return errorsum / (n_gcs + n_bcs)\\n\\n        else:\\n            assert errorsum == 0\\n            return 0\\n\\n\\n    def generate_data_for_selectivity(self, selectivity, n_tuples):\\n        \"\"\"\\n        NOTE: This is currently unused. Not even sure if I completed it. But give a look at it again because\\n        there were some interesting ideas.\\n        \"\"\"\\n        def generate_valid_and_invalid_subsets(n_vars, n_subsets, n_valid):\\n            # TODO: Read again this function. There\\'s some interesting logic\\n            n_subsets = int(math.ceil(n_subsets))\\n            n_valid = int(math.ceil(n_valid))\\n\\n            assert n_valid <= n_subsets == 2**n_vars\\n\\n            valid = []\\n            invalid = []\\n\\n            # This must be always valid (it is the sum of no tuples)\\n            # valid.append( (0,)*n_vars )\\n            valid.append(0)\\n\\n            # Generate half of vars valid and half invalid\\n            for i in range(n_vars):\\n                if len(valid) < n_valid/2.:\\n                    # valid.append(tuple( bit for bit in (\\'{:0%dbms}\\' % n_tuples).format(2**i) ))\\n                    valid.append(2**i)\\n                elif len(invalid) < (n_subsets - n_valid)/2.:\\n                    # invalid.append(tuple( bit for bit in (\\'{:0%dbms}\\' % n_tuples).format(2**i) ))\\n                    invalid.append(2**i)\\n                else:\\n                    valid.append(2**i)\\n\\n            # Generate more invalid (up to n_subsets-n_valid) by combining invalid + invalid\\n            while len(invalid) < n_subsets-n_valid:\\n                found = False\\n                for i in range(len(invalid)):\\n                    for j in range(len(invalid)):\\n                        new_invalid = invalid[i] | invalid[j]\\n                        if new_invalid not in invalid:\\n                            invalid.append(new_invalid)\\n                            found = True\\n                            break\\n                    if found:\\n                        break\\n                if not found:\\n                    break\\n\\n            # If more invalid are needed, generate them by combining invalid + valid\\n            while len(invalid) < n_subsets-n_valid:\\n                found = False\\n                for i in range(len(invalid)):\\n                    for j in range(len(valid)):\\n                        new_invalid = invalid[i] | valid[j]\\n                        if new_invalid not in invalid:\\n                            invalid.append(new_invalid)\\n                            found = True\\n                            break\\n                    if found:\\n                        break\\n                if not found:\\n                    raise Exception\\n\\n            # All the remaining ones are valid\\n            valid = set(range(n_subsets)) - set(invalid)\\n\\n            assert len(valid) == n_valid\\n            assert len(valid) + len(invalid) == n_subsets\\n\\n            if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\\n                debug(\"n invalid = {}\".format(n_subsets - n_valid))\\n                debug(\"{}\".format(valid))\\n                debug(\"{}\".format(invalid))\\n                debug(\"{}\".format([ tuple( bit for bit in (\\'{:0%dbms}\\' % n_tuples).format(i) ) for i in valid ]))\\n                debug(\"{}\".format([ tuple( bit for bit in (\\'{:0%dbms}\\' % n_tuples).format(i) ) for i in invalid ]))\\n\\n            return valid, invalid\\n\\n\\n        def generate_set_of_problems(base_prob, vars, total_n_constraints, n_valid_constraints, a, b):\\n            problems = []\\n\\n            for valid in itertools.combinations(range(total_n_constraints), int(math.ceil(n_valid_constraints))):\\n                valid = set(valid)\\n\\n                invalid = set(range(total_n_constraints)) - valid\\n                assert set(valid) | invalid == set(range(total_n_constraints))\\n\\n                # The empty package must always be valid. TODO: Really?\\n                # valid = [0] + list(valid)\\n\\n                prob = base_prob.copy()\\n\\n                # valid = generate_valid_and_invalid_subsets(n_tuples, total_n_constraints, n_valid_constraints)[0]\\n                # valid = np.random.choice(range(total_n_constraints), size=n_valid_constraints, replace=False)\\n                if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\\n                    debug(\"VALID: {}\".format(valid))\\n                    debug(\"INVALID: {}\".format(sorted(set(range(total_n_constraints)) - set(valid))))\\n\\n                # Add valid constraints to the problem\\n                n_valid_added = 0\\n                for i in valid:\\n                    package_bitmap = [ int(bit) for bit in (\\'{:0%dbms}\\' % n_tuples).format(i) ]\\n                    assert len(package_bitmap) == len(vars)\\n\\n                    # Add a VALID constraint for this combination of tuples\\n                    prob += np.dot(vars, package_bitmap) >= a\\n                    prob += np.dot(vars, package_bitmap) <= b\\n                    n_valid_added += 1\\n                assert n_valid_added == len(valid)\\n\\n                # Add invalid constraints to the problem\\n                n_invalid_added = 0\\n\\n                if float(a) > -float(\"inf\") and float(b) < float(\"inf\"):\\n                    # In this case, we produce 2**(len(invalid)) new sub-problems, each for a different set of ways\\n                    # to break the constraints a <= sum() <= b\\n                    pairs_of_invalid_constraints = []\\n                    for i in invalid:\\n                        package_bitmap = [ int(bit) for bit in (\\'{:0%dbms}\\' % n_tuples).format(i) ]\\n                        pairs_of_invalid_constraints.append((\\n                            (package_bitmap, operator.le, a-1),\\n                            (package_bitmap, operator.ge, b+1),\\n                        ))\\n\\n                    orig_prob = prob.copy()\\n                    for set_of_invalid in itertools.product(*pairs_of_invalid_constraints):\\n                        new_prob = orig_prob.copy()\\n                        for invalid_bitmap, op, c in set_of_invalid:\\n                            new_prob += op(np.dot(vars, invalid_bitmap), c)\\n                        problems.append(new_prob)\\n\\n                else:\\n                    # In this case, we only generate one sub-problem by adding all invalid constraints\\n                    for i in invalid:\\n                        package_bitmap = [ int(bit) for bit in (\\'{:0%dbms}\\' % n_tuples).format(i) ]\\n                        assert len(package_bitmap) == len(vars)\\n\\n                        # Add an INVALID (out of range) constraint for this combination of tuples\\n                        if float(a) > -float(\"inf\") and float(b) < float(\"inf\"):\\n                            raise Exception(\"Should never happen!\")\\n                            # prob += np.dot(vars, package_bitmap) <= a-1\\n                        elif float(a) > -float(\"inf\"):\\n                            prob += np.dot(vars, package_bitmap) <= a-1\\n                        elif float(b) < float(\"inf\"):\\n                            prob += np.dot(vars, package_bitmap) >= b+1\\n                        else:\\n                            raise Exception\\n                    assert n_invalid_added == len(invalid)\\n\\n                    problems.append(prob)\\n\\n            return problems\\n\\n\\n        assert 0 <= selectivity <= 1\\n        assert n_tuples >= 0\\n\\n        table_name_start = self.bc_query.lower().find(\"from \")\\n        table_name_end = self.bc_query[table_name_start+5:].lower().find(\" \")\\n        table_name = self.bc_query[table_name_start+5:table_name_start+5+table_name_end]\\n\\n        attribute_names = []\\n        ranges = []\\n        for j in range(len(self.gc_queries)):\\n            if \\'sum(\\' in self.gc_queries[j].lower():\\n                attr_start = self.gc_queries[j].lower().find(\\'sum(\\')\\n                attr_end = self.gc_queries[j][attr_start+4:].lower().find(\\')\\')\\n                attribute_names.append(self.gc_queries[j][attr_start+4:attr_start+4+attr_end])\\n                ranges.append(self.gc_ranges[j])\\n        debug(\"{} {}\".format(attribute_names, ranges))\\n        assert len(attribute_names) == len(ranges)\\n\\n        # Generate the data via CPLEX\\n        data_columns = []\\n\\n        # Generate one column at a time. Each column is generated with a CPLEX problem\\n        for j in range(len(attribute_names)):\\n            a, b = ranges[j]\\n\\n            total_n_constraints = 2**n_tuples\\n            n_valid_constraints = (1-selectivity) * total_n_constraints\\n\\n            # Check satisfiability of requirements\\n            if n_valid_constraints == 0 and a <= 0 <= b:\\n                warning(\"Since a<=0<=b there is always at least one valid package, i.e. the empty package, \"\\n                        \"therefore selectivity=1 (where no package is valid) is impossible.\")\\n                return None\\n            if n_valid_constraints == total_n_constraints and not a <= 0 <= b:\\n                warning(\"Since not a<=0<=b, the empty package may never be a valid package, \"\\n                        \"therefore selectivity=0 (where all packages are valid) is impossible.\")\\n                return None\\n\\n            # Create the base problem\\n            base_prob = LpProblem(\"package-builder\", LpMinimize)\\n            base_prob += 0 # no objective\\n\\n            # Add constraints to the problem\\n            vars = [\\n                LpVariable(\"{}_{}\".format(attribute_names[j], i), -float(\"inf\"), float(\"inf\"), LpInteger)\\n                for i in range(n_tuples)\\n            ]\\n\\n            # Generate all possible combination of problem constraints\\n            # One of them will be feasible and will give us the dataset\\n            problems = generate_set_of_problems(base_prob, vars, total_n_constraints, n_valid_constraints, a, b)\\n\\n            # Now try to find one feasible problem\\n            for prob in problems:\\n                # Solve the problem\\n                debug(\"{}\".format(prob))\\n                solver = CPLEX(msg=True, timeLimit=None)\\n                solver.solve(prob)\\n\\n                # Check the problem status\\n                if LpStatus[prob.status]==\\'Infeasible\\':\\n                    debug(\"@@@@@@@@@@@@@@@@@ INFEASIBLE: CONTINUE\")\\n                    continue\\n\\n                elif LpStatus[prob.status]==\\'Undefined\\':\\n                    raise Exception(\"Problem is undefined.\")\\n\\n                elif LpStatus[prob.status]==\\'Optimal\\':\\n                    debug(\"################## OPTIMAL\")\\n                    prob.roundSolution()\\n                    sol = [ v.varValue for v in prob.tuple_variables() if type(v.varValue) is float ]\\n                    data_columns.append(sol)\\n                    break\\n\\n                else:\\n                    raise Exception(\"LP status: {}\".format(LpStatus[prob.status]))\\n\\n            else:\\n                raise Exception(\"Could not find feasible combination of constraints \"\\n                                \"for selectivity {} and {} tuples.\".format(selectivity, n_tuples))\\n\\n        tuples = np.array(data_columns).transpose()\\n\\n        return table_name, attribute_names, tuples\\n\\n\\n\\nclass ConjunctivePackageQuery(PackageQuery):\\n    # TODO: later on, move the two staticmethods from_... outside. Make them just functions.\\n    # TODO: IMPORTANT! All base and gc queries MUST be instance of some class SQL_Query instead of just strings\\n\\n    def __init__(self, query_dict):\\n        super(ConjunctivePackageQuery, self).__init__(query_dict)\\n\\n        # Store the base and global constraints as coalesced and un-coalesced constraints\\n        gc_constraint_trees = []\\n        gc_ranges = []\\n        gcs = self.such_that_expr.get_ANDed_gc_list()\\n        for sqlquery_expr, gc_range_a, gc_range_b in gcs:\\n            if isinstance(sqlquery_expr, SQLQueryExpression):\\n                # Note: Technically, you\\'ll get an expression tree of \"constraint trees\" (query plans). So you\\n                # should actually try to combine them into one single constraint tree. Right now I\\'m simplifying\\n                # by assuming that the expression tree is always a simple leaf (so directly a constraint tree).\\n                operator_tree_expr = sqlquery_expr.traverse_leaf_func(leaf_func=\"get_constraint_tree\")\\n                assert isinstance(operator_tree_expr, ArithmeticExpression)\\n            else:\\n                raise Exception\\n            gc_constraint_trees.append(operator_tree_expr)\\n            gc_ranges.append((np.float64(gc_range_a), np.float64(gc_range_b)))\\n        self.coalesced_gcs = get_coalesced_global_constraints(gc_constraint_trees, gc_ranges)\\n        self.uncoalesced_gcs = get_uncoalesced_global_constraints(self.coalesced_gcs)\\n        self.coalesced_bcs = get_coalesced_base_constraints(self.bc_query)\\n        self.uncoalesced_bcs = get_uncoalesced_base_constraints(self.coalesced_bcs)\\n\\n\\n    def __str__(self):\\n        return (\\n            \"/-------------------------------------------- PaQL Query ---------------------------------------------\\\\\\\\\\\\n\"\\n            \"|  PaQL query:\\\\n\"\\n            \"|     \" + str(self._paql_query_str) + \"\\\\n\"\\n            \"|  Base SQL query:\\\\n\"\\n            \"|     \" + str(self.bc_query) + \"\\\\n\"\\n            \"|  Global SQL queries:\\\\n\"\\n            \"|     \" + (\"|     \".join([ str(q) + \"\\\\n\" for q in self.gc_queries ]) if self.gc_queries else \"None\\\\n\") + \"\"\\n            \"|  Glogal constraint ranges:\\\\n\"\\n            \"|     \" + (\"|     \".join([ str(q) + \"\\\\n\" for q in self.gc_ranges ]) if self.gc_ranges else \"None\\\\n\") + \"\"\\n            \"|  Optimization objective:\\\\n\"\\n            \"|     \" + (str(self.objective) if self.objective else \"None\") + \"\\\\n\"\\n            \"\\\\-----------------------------------------------------------------------------------------------------/\"\\n        )\\n\\n\\n    def get_paql_str(self, redo=False, recompute_gcs=True, coalesced=False):\\n        if redo or self._paql_query_str is None or self._paql_query_str_stale:\\n\\n            if recompute_gcs:\\n                self.coalesced_gcs = get_coalesced_global_constraints(self.gc_queries, self.gc_ranges)\\n                self.uncoalesced_gcs = get_uncoalesced_global_constraints(self.coalesced_gcs)\\n                self.coalesced_bcs = get_coalesced_base_constraints(self.bc_query)\\n                self.uncoalesced_bcs = get_uncoalesced_base_constraints(self.coalesced_bcs)\\n\\n            if self.rel_namespace is None:\\n                # raise Exception(\"rel_namespace is None\")\\n                # return \"\"\\n                self.rel_namespace = { \"R\": self.table_name }\\n\\n            bcs_str = []\\n            gcs_str = []\\n            obj_str = None\\n\\n            if not coalesced:\\n                if len(self.uncoalesced_bcs) > 0:\\n                    for attr, op, n in self.uncoalesced_bcs:\\n                        bcs_str.append(\"{} {} {}\".format(attr, op_to_opstr(op), n))\\n\\n                if len(self.uncoalesced_gcs) > 0:\\n                    for (aggr, attr), op, n in self.uncoalesced_gcs:\\n                        gcs_str.append(\"{}({}) {} {}\".format(aggr, attr, op_to_opstr(op), n))\\n\\n            else:\\n                if len(self.coalesced_bcs) > 0:\\n                    for attr, (lb, ub) in self.coalesced_bcs.iteritems():\\n                        if float(lb) == -float(\"inf\") and float(ub) == float(\"inf\"):\\n                            continue\\n                        elif float(ub) == float(\"inf\"):\\n                            bcs_str.append(\"{} {} {}\".format(attr, op_to_opstr(operator.ge), lb))\\n                        elif float(lb) == -float(\"inf\"):\\n                            bcs_str.append(\"{} {} {}\".format(attr, op_to_opstr(operator.le), ub))\\n                        elif lb == ub:\\n                            bcs_str.append(\"{} {} {}\".format(attr, op_to_opstr(operator.eq), ub))\\n                        else:\\n                            bcs_str.append(\"{} BETWEEN {} AND {}\".format(attr, lb, ub))\\n\\n                if len(self.coalesced_gcs) > 0:\\n                    for (aggr, attr), (lb, ub) in self.coalesced_gcs.iteritems():\\n                        if aggr.lower() == \"count\":\\n                            lb, ub = int(lb), int(ub)\\n\\n                        uaggr = aggr.upper()\\n\\n                        if float(lb) == -float(\"inf\") and float(ub) == float(\"inf\"):\\n                            continue\\n                        elif float(ub) == float(\"inf\"):\\n                            gcs_str.append(\"{}({}) {} {}\".format(uaggr, attr, op_to_opstr(operator.ge), lb))\\n                        elif float(lb) == -float(\"inf\"):\\n                            gcs_str.append(\"{}({}) {} {}\".format(uaggr, attr, op_to_opstr(operator.le), ub))\\n                        elif lb == ub:\\n                            gcs_str.append(\"{}({}) {} {}\".format(uaggr, attr, op_to_opstr(operator.eq), ub))\\n                        else:\\n                            gcs_str.append(\"{}({}) BETWEEN {} AND {}\".format(uaggr, attr, lb, ub))\\n\\n            if self.objective is not None:\\n                if self.objective[\"type\"] == \"maximize\":\\n                    obj_str = \"MAXIMIZE \"\\n                elif self.objective[\"type\"] == \"minimize\":\\n                    obj_str = \"MINIMIZE \"\\n                else:\\n                    raise\\n                obj_str += self.objective[\"func\"].get_str()\\n\\n            self._paql_query_str = \\\\\\n                \"SELECT \\\\n\\\\tPACKAGE({pack}) \\\\n\" \\\\\\n                \"FROM \\\\n\\\\t{tables} {bcs}{gcs}{obj};\".format(\\n                pack=\", \".join(self.rel_namespace.keys()),\\n                tables=\", \".join(\"{} {}\".format(name, alias) for alias, name in self.rel_namespace.iteritems()),\\n                bcs=\"\\\\nWHERE \\\\n\\\\t{} \".format(\" AND\\\\n\\\\t\".join(bcs_str)) if bcs_str else \"\",\\n                gcs=\"\\\\nSUCH THAT \\\\n\\\\t{} \".format(\" AND\\\\n\\\\t\".join(gcs_str)) if gcs_str else \"\",\\n                obj=\"\\\\n{}\".format(obj_str) if obj_str is not None else \"\")\\n\\n            self._paql_query_str_stale = False\\n\\n        return self._paql_query_str\\n',\n",
       " '\"\"\"Creates a brownian tree\"\"\"\\nimport random\\nimport os\\nimport numpy as np\\nfrom PIL import Image\\n\\nrandom.seed()\\n\\nclass MyTuple():\\n    \"\"\"Custom tuple with operator overloads\"\"\"\\n    def __init__(self, x, y):\\n        self.x_val = x\\n        self.y_val = y\\n\\n    def __add__(self, rhs):\\n        return MyTuple(self.x_val + rhs.x_val, self.y_val + rhs.y_val)\\n\\n    def __sub__(self, rhs):\\n        return MyTuple(self.x_val - rhs.x_val, self.y_val - rhs.y_val)\\n\\n    def __lt__(self, rhs):\\n        return self.x_val < rhs.x_val and self.y_val < rhs.y_val\\n\\n    def __gt__(self, rhs):\\n        return self.x_val > rhs.x_val and self.y_val > rhs.y_val\\n\\n    def set_vals(self, x, y):\\n        \"\"\"sets both values\"\"\"\\n        self.x_val = x\\n        self.y_val = y\\n\\n    def get_vals(self):\\n        \"\"\"casts to a normal tuple\"\"\"\\n        return (self.x_val, self.y_val)\\n\\n\\nclass Grid():\\n    \"\"\"Stores the data about the grid and tree\"\"\"\\n    def __init__(self, size):\\n        \"\"\"grid constructor\"\"\"\\n        self.size = size\\n        self.data = np.zeros(self.size.get_vals(), dtype=np.uint8)\\n        self.max_steps = 0\\n\\n    def get_val(self, location):\\n        \"\"\"returns the cell value at that location\"\"\"\\n        return self.data[location.x_val, location.y_val]\\n\\n    def is_occupied(self, location):\\n        \"\"\"returns True if the location is occupied\"\"\"\\n        return self.data[location.x_val, location.y_val]\\n\\n    def set_val(self, location, steps):\\n        \"\"\"sets the value at that location\"\"\"\\n        if self.max_steps < steps:\\n            self.max_steps = steps\\n        self.data[location.x_val, location.y_val] = 255\\n\\n    def is_outside_bounds(self, location):\\n        \"\"\"Checks if the location supplied is inside the grid\"\"\"\\n        lower = location > MyTuple(-1, -1)\\n        upper = location < self.size\\n        return upper and lower\\n\\n    def normalise(self):\\n        max = self.max_steps\\n        def normalise(data):\\n            return 255 - (data / (self.max_steps / 255))\\n        np.vectorize(normalise)(self.data)\\n\\nBOUNDS = MyTuple(100, 75)\\n# BOUNDS = MyTuple(160, 120)\\n\\nLEFT = MyTuple(-1, 0)\\nUP = MyTuple(0, 1)\\nRIGHT = MyTuple(1, 0)\\nDOWN = MyTuple(0, -1)\\nNONE = MyTuple(0, 0)\\nDIRECTIONS = [UP, DOWN, LEFT, RIGHT]\\n\\n\\nclass Particle():\\n    \"\"\"Stores data about individual particles\"\"\"\\n    def __init__(self, direction, location):\\n        \"\"\"particle constructor\"\"\"\\n        self.direction = direction\\n        self.location = location\\n        self.is_free = True\\n\\n    def move(self, new_direction):\\n        \"\"\"moves particle in a direction\"\"\"\\n        self.direction = new_direction\\n        self.location += new_direction\\n\\n    def fix_location(self):\\n        \"\"\"sets the location of said particle\"\"\"\\n        self.direction = NONE\\n        self.is_free = False\\n\\n\\ndef random_xy_loc(bounds):\\n    \"\"\"Creates a random xy tuple within bounds\"\"\"\\n    return MyTuple(random.randint(0, bounds.x_val - 1), random.randint(0, bounds.y_val - 1))\\n\\n\\ndef get_next_direction():\\n    return random.choice(DIRECTIONS)\\n\\n\\ndef main():\\n    \"\"\"main init\"\"\"\\n    data_grid = Grid(BOUNDS)\\n\\n    max_particles = int(BOUNDS.x_val * BOUNDS.y_val / 3)\\n\\n    for y in range(BOUNDS.y_val):\\n        for x in range(BOUNDS.x_val):\\n            if x == 0 or x == (BOUNDS.x_val - 1) or y == 0 or y == (BOUNDS.y_val - 1):\\n                data_grid.set_val(MyTuple(x, y), 1)\\n\\n    data_grid.set_val(MyTuple(int(BOUNDS.x_val / 2), int(BOUNDS.y_val / 2)), 1)\\n\\n    is_occupied = data_grid.is_occupied\\n    is_outside_bounds = data_grid.is_outside_bounds\\n\\n    for i in range(max_particles):\\n        count = -1\\n        location = random_xy_loc(BOUNDS)\\n        while is_occupied(location):\\n            location = random_xy_loc(BOUNDS)\\n        new_particle = Particle(direction=NONE, location=location)\\n        while new_particle.is_free:\\n            count += 1\\n            direction = get_next_direction()\\n            new_location = new_particle.location + direction\\n            if not is_outside_bounds(new_location):\\n                continue\\n            if not is_occupied(new_location):\\n                new_particle.move(direction)\\n            else:\\n                new_particle.fix_location()\\n        data_grid.set_val(new_particle.location, count)\\n        print(f\"Finished {i + 1}, loops: {count}\")\\n    output_name = f\"time_data_{max_particles}_{BOUNDS.get_vals()}.bmp\"\\n    full_path = os.path.join(os.path.dirname(__file__), output_name)\\n    Image.fromarray(data_grid.data).save(full_path)\\n\\n\\nif __name__ == \"__main__\":\\n    main()\\n',\n",
       " '\"\"\" Provides named dictionary \"\"\"\\n#-------------------------------------------------------------------------------\\nimport collections\\n\\n#-------------------------------------------------------------------------------\\nclass NamedDict(collections.OrderedDict):\\n  \"\"\" A named dictionary is an ordered dictionary with a name \"\"\"\\n  _name = None\\n  \\n#-------------------------------------------------------------------------------\\n  def __init__(self, name, *args, **kwds):\\n    self.name = name\\n    super().__init__(*args, **kwds)\\n\\n#-------------------------------------------------------------------------------\\n  @property\\n  def name(self):\\n    return self._name\\n\\n  @name.setter\\n  def name(self, name):\\n    assert isinstance(name, str), \\\\\\n        \"Name must be a string, not {}\".format(type(name))\\n    self._name = name\\n\\n#-------------------------------------------------------------------------------\\n  def __repr__(self):\\n    return \"{}: {}\".format(self.name, super().__repr__())\\n\\n#-------------------------------------------------------------------------------\\n',\n",
       " '# Copyright 2019 Google LLC\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     https://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nimport csv\\nimport io\\nimport urllib.request\\n\\noutput_columns = [\\n    \\'Date\\', \\'GeoId\\', \\'CumulativeCount_MedicalTest_ConditionCOVID_19\\',\\n    \\'CumulativeCount_MedicalTest_ConditionCOVID_19_Positive\\',\\n    \\'CumulativeCount_MedicalTest_ConditionCOVID_19_Negative\\',\\n    \\'Count_MedicalTest_ConditionCOVID_19_Pending\\',\\n    \\'CumulativeCount_MedicalConditionIncident_COVID_19_PatientRecovered\\',\\n    \\'CumulativeCount_MedicalConditionIncident_COVID_19_PatientDeceased\\',\\n    \\'Count_MedicalConditionIncident_COVID_19_PatientHospitalized\\',\\n    \\'CumulativeCount_MedicalConditionIncident_COVID_19_PatientHospitalized\\',\\n    \\'Count_MedicalConditionIncident_COVID_19_PatientInICU\\',\\n    \\'CumulativeCount_MedicalConditionIncident_COVID_19_PatientInICU\\',\\n    \\'Count_MedicalConditionIncident_COVID_19_PatientOnVentilator\\',\\n    \\'CumulativeCount_MedicalConditionIncident_COVID_19_PatientOnVentilator\\'\\n]\\nwith open(\\'COVIDTracking_States.csv\\', \\'w\\', newline=\\'\\') as f_out:\\n    writer = csv.DictWriter(f_out,\\n                            fieldnames=output_columns,\\n                            lineterminator=\\'\\\\n\\')\\n    with urllib.request.urlopen(\\n            \\'https://covidtracking.com/api/v1/states/daily.csv\\') as response:\\n        reader = csv.DictReader(io.TextIOWrapper(response))\\n\\n        writer.writeheader()\\n        for row_dict in reader:\\n            processed_dict = {\\n                \\'Date\\':\\n                    \\'%s-%s-%s\\' % (row_dict[\\'date\\'][:4], row_dict[\\'date\\'][4:6],\\n                                  row_dict[\\'date\\'][6:]),\\n                \\'GeoId\\':\\n                    \\'dcid:geoId/%s\\' % row_dict[\\'fips\\'],\\n                \\'CumulativeCount_MedicalTest_ConditionCOVID_19\\':\\n                    row_dict[\\'totalTestResults\\'],\\n                \\'CumulativeCount_MedicalTest_ConditionCOVID_19_Positive\\':\\n                    row_dict[\\'positive\\'],\\n                \\'CumulativeCount_MedicalTest_ConditionCOVID_19_Negative\\':\\n                    row_dict[\\'negative\\'],\\n                \\'Count_MedicalTest_ConditionCOVID_19_Pending\\':\\n                    row_dict[\\'pending\\'],\\n                (\\'CumulativeCount_MedicalConditionIncident\\'\\n                 \\'_COVID_19_PatientRecovered\\'):\\n                    row_dict[\\'recovered\\'],\\n                (\\'CumulativeCount_MedicalConditionIncident\\'\\n                 \\'_COVID_19_PatientDeceased\\'):\\n                    row_dict[\\'death\\'],\\n                \\'Count_MedicalConditionIncident_COVID_19_PatientHospitalized\\':\\n                    row_dict[\\'hospitalizedCurrently\\'],\\n                (\\'CumulativeCount_MedicalConditionIncident\\'\\n                 \\'_COVID_19_PatientHospitalized\\'):\\n                    row_dict[\\'hospitalizedCumulative\\'],\\n                \\'Count_MedicalConditionIncident_COVID_19_PatientInICU\\':\\n                    row_dict[\\'inIcuCurrently\\'],\\n                (\\'CumulativeCount_MedicalConditionIncident\\'\\n                 \\'_COVID_19_PatientInICU\\'):\\n                    row_dict[\\'inIcuCumulative\\'],\\n                \\'Count_MedicalConditionIncident_COVID_19_PatientOnVentilator\\':\\n                    row_dict[\\'onVentilatorCurrently\\'],\\n                (\\'CumulativeCount_MedicalConditionIncident\\'\\n                 \\'_COVID_19_PatientOnVentilator\\'):\\n                    row_dict[\\'onVentilatorCumulative\\'],\\n            }\\n\\n            writer.writerow(processed_dict)\\n\\n# Automate Template MCF generation since there are many Statitical Variables.\\nTEMPLATE_MCF_TEMPLATE = \"\"\"\\nNode: E:COVIDTracking_States->E{index}\\ntypeOf: dcs:StatVarObservation\\nvariableMeasured: dcs:{stat_var}\\nmeasurementMethod: dcs:CovidTrackingProject\\nobservationAbout: C:COVIDTracking_States->GeoId\\nobservationDate: C:COVIDTracking_States->Date\\nvalue: C:COVIDTracking_States->{stat_var}\\n\"\"\"\\n\\nstat_vars = output_columns[2:]\\nwith open(\\'COVIDTracking_States.tmcf\\', \\'w\\', newline=\\'\\') as f_out:\\n    for i in range(len(stat_vars)):\\n        f_out.write(\\n            TEMPLATE_MCF_TEMPLATE.format_map({\\n                \\'index\\': i,\\n                \\'stat_var\\': output_columns[2:][i]\\n            }))\\n',\n",
       " \"from django.urls import path,include\\nfrom . import views\\nfrom django.contrib.auth import views as auth_views\\n# from rest_framework import routers\\nfrom django.conf import settings\\nfrom django.conf.urls.static import static\\n\\nurlpatterns = [\\n    path('index/',views.index, name='index'),\\n    path('register', views.register, name='register'),\\n    path('',auth_views.LoginView.as_view(), name='login'),\\n    path('account/', include('django.contrib.auth.urls')),\\n    path('all-hoods/', views.hoods, name='hood'),\\n    path('new-hood/', views.create_hood, name='new-hood'),\\n    path('profile/<username>', views.profile, name='profile'),\\n    path('profile/<username>/edit/', views.edit_profile, name='edit-profile'),\\n    path('join_hood/<id>', views.join_hood, name='join-hood'),\\n    path('leave_hood/<id>', views.leave_hood, name='leave-hood'),\\n    path('single_hood/<hood_id>', views.single_hood, name='single-hood'),\\n    path('<hood_id>/new-post', views.create_post, name='post'),\\n    path('<hood_id>/members', views.hood_members, name='members'),\\n    path('search/', views.search_business, name='search'),\\n]\\nif settings.DEBUG:\\n    urlpatterns+= static(settings.MEDIA_URL, document_root = settings.MEDIA_ROOT)\",\n",
       " 'from PIL import Image\\nimport numpy as np\\n# from streamlit.logger import update_formatter\\nimport torch\\nfrom matplotlib import cm\\n\\n\\n\\ndef min_max_norm(array):\\n    lim = [array.min(), array.max()]\\n    array = array - lim[0] \\n    array.mul_(1 / (1.e-10+ (lim[1] - lim[0])))\\n    # array = torch.clamp(array, min=0, max=1)\\n    return array\\n\\ndef torch_to_rgba(img):\\n    img = min_max_norm(img)\\n    rgba_im = img.permute(1, 2, 0).cpu()\\n    if rgba_im.shape[2] == 3:\\n        rgba_im = torch.cat((rgba_im, torch.ones(*rgba_im.shape[:2], 1)), dim=2)\\n    assert rgba_im.shape[2] == 4\\n    return rgba_im\\n\\n\\ndef numpy_to_image(img, size):\\n    \"\"\"\\n    takes a [0..1] normalized rgba input and returns resized image as [0...255] rgba image\\n    \"\"\"\\n    resized = Image.fromarray((img*255.).astype(np.uint8)).resize((size, size))\\n    return resized\\n\\ndef upscale_pytorch(img:np.array, size):\\n    torch_img = torch.from_numpy(img).unsqueeze(0).permute(0,3,1,2)\\n    print(torch_img)\\n    upsampler = torch.nn.Upsample(size=size)    \\n    return upsampler(torch_img)[0].permute(1,2,0).cpu().numpy()\\n\\n\\ndef heatmap_helper(image:torch.Tensor, heatmap: torch.Tensor, size=None, alpha=.6):\\n    if not size:\\n        size = image.shape[1]\\n\\n    img = numpy_to_image(min_max_norm(heatmap).numpy(), size)\\n    return np.asarray(img)\\n\\ndef heatmap(image:torch.Tensor, heatmap: torch.Tensor, size=None, alpha=.6):\\n    if not size:\\n        size = image.shape[1]\\n    # print(heatmap)\\n    # print(min_max_norm(heatmap))\\n\\n    img = torch_to_rgba(image).numpy() # [0...1] rgba numpy \"image\"\\n    hm = cm.jet(min_max_norm(heatmap).numpy()) # [0...1] rgba numpy \"image\"\\n\\n    img = np.array(numpy_to_image(img,size))\\n    hm = np.array(numpy_to_image(hm, size))\\n    # hm = upscale_pytorch(hm, size)\\n    # print (hm) \\n\\n    return Image.fromarray((alpha * hm + (1-alpha)*img).astype(np.uint8))\\n    # return Image.fromarray(hm)',\n",
       " '\"\"\"Proto related build rules for fhir.\\n\"\"\"\\n\\nload(\"@rules_proto//proto:defs.bzl\", \"proto_library\")\\nload(\"@rules_cc//cc:defs.bzl\", \"cc_proto_library\")\\nload(\"@io_bazel_rules_go//proto:def.bzl\", \"go_proto_library\")\\nload(\"@com_google_protobuf//:protobuf.bzl\", \"py_proto_library\")\\n\\nWELL_KNOWN_PROTOS = [\"descriptor_proto\", \"any_proto\"]\\nGO_WELL_KNOWN_PROTOS = {\\n    \"descriptor_proto\": \"@org_golang_google_protobuf//types/descriptorpb:go_default_library\",\\n    \"any_proto\": \"@org_golang_google_protobuf//types/known/anypb:go_default_library\",\\n}\\n\\ndef fhir_proto_library(proto_library_prefix, srcs = [], proto_deps = [], **kwargs):\\n    \"\"\"Generates proto_library target, as well as {py,cc,java,go}_proto_library targets.\\n\\n    Args:\\n      proto_library_prefix: Name prefix to be added to various proto libraries.\\n      srcs: Srcs for the proto library.\\n      proto_deps: Deps by the proto_library.\\n      **kwargs: varargs. Passed through to proto rules.\\n    \"\"\"\\n    py_deps = []\\n    cc_deps = []\\n    go_deps = []\\n    has_well_known_dep = False\\n    for x in proto_deps:\\n        tokens = x.split(\":\")\\n        if len(tokens) == 2 and tokens[1] in WELL_KNOWN_PROTOS:\\n            go_deps.append(GO_WELL_KNOWN_PROTOS[tokens[1]])\\n            if not has_well_known_dep:\\n                py_deps.append(tokens[0] + \":protobuf_python\")\\n                cc_deps.append(tokens[0] + \":cc_wkt_protos\")\\n                has_well_known_dep = True\\n        elif x.endswith(\"_proto\"):\\n            py_deps.append(x[:-6] + \"_py_pb2\")\\n            cc_deps.append(x[:-6] + \"_cc_proto\")\\n            go_deps.append(x[:-6] + \"_go_proto\")\\n\\n    proto_library(\\n        name = proto_library_prefix + \"_proto\",\\n        srcs = srcs,\\n        deps = proto_deps,\\n        **kwargs\\n    )\\n\\n    py_proto_library(\\n        name = proto_library_prefix + \"_py_pb2\",\\n        srcs = srcs,\\n        deps = py_deps,\\n        default_runtime = \"@com_google_protobuf//:protobuf_python\",\\n        protoc = \"@com_google_protobuf//:protoc\",\\n        **kwargs\\n    )\\n\\n    cc_proto_library(\\n        name = proto_library_prefix + \"_cc_proto\",\\n        deps = [proto_library_prefix + \"_proto\"],\\n    )\\n\\n    native.java_proto_library(\\n        name = proto_library_prefix + \"_java_proto\",\\n        deps = [\\n            \":\" + proto_library_prefix + \"_proto\",\\n        ],\\n        **kwargs\\n    )\\n\\n    importpath_prefix = \"github.com/google/fhir/go/\"\\n    if native.package_name().startswith(\"go/\"):\\n        importpath_prefix = \"github.com/google/fhir/\"\\n\\n    go_proto_library(\\n        name = proto_library_prefix + \"_go_proto\",\\n        deps = go_deps,\\n        proto = \":\" + proto_library_prefix + \"_proto\",\\n        importpath = importpath_prefix + native.package_name() + \"/\" + proto_library_prefix + \"_go_proto\",\\n        **kwargs\\n    )\\n\\ndef _fhir_individual_resource_rules(resource_files, deps):\\n    for resource_file in resource_files:\\n        fhir_proto_library(\\n            srcs = [resource_file],\\n            proto_deps = deps,\\n            proto_library_prefix = resource_file[:-6],\\n        )\\n\\ndef fhir_resource_rules(resource_files, deps):\\n    resource_files.remove(\"bundle_and_contained_resource.proto\")\\n\\n    _fhir_individual_resource_rules(resource_files, deps)\\n\\n    resources_as_dep = [\":\" + file[:-6] + \"_proto\" for file in resource_files]\\n\\n    fhir_proto_library(\\n        srcs = [\"bundle_and_contained_resource.proto\"],\\n        proto_deps = deps + resources_as_dep,\\n        proto_library_prefix = \"bundle_and_contained_resource\",\\n    )\\n\\ndef fhir_profile_rules(resource_files, deps):\\n    _fhir_individual_resource_rules(resource_files, deps)\\n',\n",
       " \"import pymysql as pymysql\\na = None\\nuser_data = {}\\ndef mysql_db(name,age):\\n\\tconn = pymysql.connect(host='127.0.0.1', user='root', password='mysql', database='zhangqiang_db', charset='utf8')\\n\\tcursor = conn.cursor()\\n\\tsql = '''\\n\\tinsert into USER1(name, age) value(%s, %s);\\n\\t'''\\n\\t# name = name\\n\\t# age = age\\n\\tcursor.execute(sql, [name, age])\\n\\tconn.commit()\\n\\tcursor.close()\\n\\tconn.close()\\n\\n\\ndef serach_db(name):\\n\\tconn = pymysql.connect(host='127.0.0.1', user='root', password='mysql', database='zhangqiang_db', charset='utf8')\\n\\tcursor = conn.cursor()\\n\\tsql = '''\\n\\tselect name from USER1 where name=%s;\\n\\t'''\\n\\t# name = name\\n\\tcursor.execute(sql, [name])\\n\\ta = cursor.fetchone()\\n\\tcursor.close()\\n\\tconn.close()\\n\\tprint(a)\\n\\treturn a\\n\\ndef serach_db_passwd(name):\\n\\tconn = pymysql.connect(host='127.0.0.1', user='root', password='mysql', database='zhangqiang_db', charset='utf8')\\n\\tcursor = conn.cursor()\\n\\tsql = '''\\n\\tselect age from USER1 where age=%s;\\n\\t'''\\n\\t# name = name\\n\\tcursor.execute(sql, [name])\\n\\tpasswd = cursor.fetchone()\\n\\tcursor.close()\\n\\tconn.close()\\n\\tprint(passwd)\\n\\treturn passwd\\n\\ndef new_user():\\n\\tname = input('new_name\\\\t\\\\n')\\n\\twhile 1:\\n\\n\\t\\tif name in user_data:\\n\\t\\t\\tname = input('The user name already exists')\\n\\t\\t\\tcontinue\\n\\t\\telse:\\n\\t\\t\\tbreak\\n\\tpasswd = int(input('password\\\\n'))\\n\\tmysql_db(name, passwd)\\n\\t# user_data[name] = passwd\\n\\tprint('successful')\\n\\n\\ndef old_user():\\n\\n\\tname = input('name')\\n\\twhile 1:\\n\\t\\tusername = serach_db(name)\\n\\t\\t# print(serach_db(name))\\n\\t\\tusername = username[0]\\n\\t\\t# print(a)\\n\\n\\t\\t# if name not in user_data:\\n\\t\\tif username == name:\\n\\t\\t\\tprint('name ok')\\n\\t\\t\\tpassword = input('password\\\\n\\t')\\n\\n\\t\\t\\tpasswd = serach_db_passwd(password)\\n\\t\\t\\tpasswd = str(passwd[0])\\n\\t\\t\\tif passwd == password:\\n\\n\\t\\t\\t\\tprint('登录成功')\\n\\n\\n\\t\\t\\tname = input('name is error,please enter again')\\n\\t\\t\\tcontinue\\n\\t\\telse:\\n\\t\\t\\tbreak\\n\\tpwd= user_data.get(name)\\n\\tif passwd == pwd:\\n\\t\\tprint('successful')\\n\\t\\t\\n\\telse:\\n\\t\\tprint('password is error')\\ndef showmenu():\\n\\tname = ''\\n\\tprint('---new_user:N/n--')\\n\\tprint('---login:E/e------')\\n\\tprint('---quit:Q/q------')\\n\\tprint('---input code----')\\n\\tchoose=input('input code\\\\n')\\t\\n\\twhile 1 :\\n\\t\\tif choose not in 'NEQneq':\\n\\t\\t\\tchoose = input('code is error,try again')\\n\\t\\telif choose == 'q' or choose == 'Q':\\n\\t\\t\\tbreak\\n\\t\\telif  choose == 'N' or choose =='n':\\n\\t\\t\\tnew_user()\\n\\t\\t\\tshowmenu()\\n\\t\\telif  choose =='e' or choose == 'E':\\n\\t\\t\\told_user()\\nshowmenu()\\n# if serach_db('laowang') == ('laowang',):\\n# \\tprint('1')\\n# else:\\n# \\tprint('2')\\n#\\n\\n\\n\\t\\t\\n\",\n",
       " 'import time\\nimport logging\\nimport httplib as http_client\\nimport requests\\nfrom requests.adapters import HTTPAdapter\\nfrom requests.packages.urllib3.poolmanager import PoolManager\\nfrom requests.packages.urllib3.util.retry import Retry\\nimport ssl\\nimport base64\\ntry:\\n    from cStringIO import StringIO\\nexcept BaseException:\\n    from StringIO import StringIO\\nimport zipfile\\n\\n\\nclass MyAdapter(HTTPAdapter):\\n    def init_poolmanager(self, connections, maxsize, block=False):\\n        self.poolmanager = PoolManager(num_pools=connections,\\n                                       maxsize=maxsize,\\n                                       block=block,\\n                                       ssl_version=getattr(ssl, \\'PROTOCOL_TLSv1_2\\', ssl.PROTOCOL_TLSv1))\\n\\n# Retry logic if the API fails to responde\\n\\n\\ndef requests_retry_session(\\n        retries=5,\\n        backoff_factor=0.5,\\n        status_forcelist=(500, 502, 504, 495, 496, 525, 526),\\n        session=None,\\n):\\n    session = session or requests.Session()\\n    retry = Retry(\\n        total=retries,\\n        read=retries,\\n        connect=retries,\\n        backoff_factor=backoff_factor,\\n        status_forcelist=status_forcelist,\\n    )\\n    adapter = MyAdapter(max_retries=retry)\\n    session.mount(\\'http://\\', adapter)\\n    session.mount(\\'https://\\', adapter)\\n    return session\\n\\n# 1and1 Object Classes\\n\\n\\nclass OneAndOneService(object):\\n\\n    # Init Function\\n    def __init__(\\n            self,\\n            api_token,\\n            api_url=\\'https://cloudpanel-api.1and1.com/v1\\',\\n            enable_logs=False):\\n        if api_url == \\'\\' or api_url == \\'default\\':\\n            api_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.api_token = api_token\\n        self.base_url = api_url\\n        self.header = {\\'X-TOKEN\\': self.api_token}\\n        self.success_codes = (200, 201, 202)\\n        if enable_logs:\\n            http_client.HTTPConnection.debuglevel = 1\\n            logging.basicConfig()\\n            logging.getLogger().setLevel(logging.DEBUG)\\n            requests_log = logging.getLogger(\"requests.packages.urllib3\")\\n            requests_log.setLevel(logging.ERROR)\\n            requests_log.propagate = True\\n\\n    def __repr__(self):\\n        return \\'OneAndOneService: api_token=%s, base_url=%s\\' % (self.api_token,\\n                                                                self.base_url)\\n\\n    # Server Functions\\n\\n    # \\'GET\\' methods\\n\\n    def list_servers(self, page=None, per_page=None, sort=None, q=None,\\n                     fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/servers\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def fixed_server_flavors(self):\\n\\n        # Perform Request\\n        url = \\'%s/servers/fixed_instance_sizes\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_fixed_server(self, fixed_server_id=None):\\n\\n        # Error Handling\\n        if(fixed_server_id is None):\\n            raise ValueError(\\'fixed_server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/servers/fixed_instance_sizes/%s\\' %\\n               (self.base_url, fixed_server_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_server(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s\\' % (self.base_url, server_id)\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_server_hardware(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/hardware\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_server_hdds(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/hardware/hdds\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_server_hdd(self, server_id=None, hdd_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(hdd_id is None):\\n            raise ValueError(\\'hdd_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/hardware/hdds/%s\\' %\\n               (self.base_url, server_id, hdd_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_server_image(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/image\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_server_ips(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/ips\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_server_ip(self, server_id=None, ip_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/ips/%s\\' % (self.base_url, server_id, ip_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_ip_firewall_policy(self, server_id=None, ip_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/ips/%s/firewall_policy\\' %\\n               (self.base_url, server_id, ip_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_ip_load_balancers(self, server_id=None, ip_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/ips/%s/load_balancers\\' %\\n               (self.base_url, server_id, ip_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_server_status(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/status\\' % (self.base_url, server_id)\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_server_dvd(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/dvd\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_server_private_networks(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/private_networks\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def private_network_info(self, server_id=None, private_network_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/private_networks/%s\\' %\\n               (self.base_url, server_id, private_network_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_server_snapshots(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/%s/snapshots\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_baremetal_models(\\n            self,\\n            page=None,\\n            per_page=None,\\n            sort=None,\\n            q=None,\\n            fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/servers/baremetal_models\\' % self.base_url\\n\\n        r = requests.get(url, headers=self.header, params=parameters)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def get_baremetal_model(self, model_id=None):\\n\\n        # Error Handling\\n        if (model_id is None):\\n            raise ValueError(\\'model_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/servers/baremetal_models/%s\\' % (self.base_url, model_id)\\n\\n        r = requests.get(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # \\'PUT\\' methods\\n\\n    def modify_server(self, server_id=None, name=None, description=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(name is None):\\n            raise ValueError(\\'name is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description\\n        }\\n\\n        url = \\'%s/servers/%s\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def modify_server_hardware(\\n            self,\\n            server_id=None,\\n            fixed_instance_size_id=None,\\n            vcore=None,\\n            cores_per_processor=None,\\n            ram=None,\\n            test=False):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Use \\'test\\' flag to skip this block when running unit test\\n        if(test == False):\\n\\n            # Prevent hot decreasing of server hardware, allow cold decreasing.\\n            server_specs = self.get_server_hardware(server_id=server_id)\\n\\n            server_status = self.get_server_status(server_id=server_id)\\n\\n            if(server_status[\\'state\\'] == \\'POWERED_ON\\'):\\n                if(vcore is not None):\\n                    if(server_specs[\\'vcore\\'] > vcore):\\n                        raise ValueError((\\'Cannot perform a hot decrease of \\'\\n                                          \\'server CPU.  The new value must be \\'\\n                                          \\'greater than current value.\\'))\\n                if(ram is not None):\\n                    if(server_specs[\\'ram\\'] > ram):\\n                        raise ValueError((\\'Cannot perform a hot decrease of \\'\\n                                          \\'server RAM.  The new value must be \\'\\n                                          \\'greater than current value.\\'))\\n\\n        # Perform Request\\n        data = {\\n            \\'fixed_instance_size_id\\': fixed_instance_size_id,\\n            \\'vcore\\': vcore,\\n            \\'cores_per_processor\\': cores_per_processor,\\n            \\'ram\\': ram\\n        }\\n\\n        url = \\'%s/servers/%s/hardware\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def modify_hdd(self, server_id=None, hdd_id=None, size=None, test=False):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(hdd_id is None):\\n            raise ValueError(\\'hdd_id is a required parameter\\')\\n\\n        # Use \\'test\\' flag to skip this block when running unit test\\n        if(test == False):\\n\\n            # Make sure size argument is valid.  HDD size can\\'t be decreased.\\n            old_hdd = self.get_server_hdd(server_id=server_id, hdd_id=hdd_id)\\n\\n            if(size is not None):\\n                if(old_hdd[\\'size\\'] > size):\\n                    raise ValueError(\\'HDD size can never be decreased. \\'\\n                                     \\'Must be greater than or equal to the \\'\\n                                     \\'current HDD size.\\')\\n\\n        # Perform Request\\n        data = {\\'size\\': size}\\n\\n        url = (\\'%s/servers/%s/hardware/hdds/%s\\' %\\n               (self.base_url, server_id, hdd_id))\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_firewall_policy(\\n            self,\\n            server_id=None,\\n            ip_id=None,\\n            firewall_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\'id\\': firewall_id}\\n\\n        url = (\\'%s/servers/%s/ips/%s/firewall_policy\\' %\\n               (self.base_url, server_id, ip_id))\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def modify_server_status(\\n            self,\\n            server_id=None,\\n            action=None,\\n            method=\\'SOFTWARE\\',\\n            recovery_mode=False,\\n            recovery_image_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(action is None):\\n            raise ValueError(\\'action is a required parameter\\')\\n\\n        # Make sure user is passing in correct arguments\\n        if(action != \\'POWER_ON\\' and action != \\'POWER_OFF\\' and\\n                action != \\'REBOOT\\'):\\n            raise ValueError((\\'action must be set to \"POWER_ON\",\\'\\n                              \\'\"POWER_OFF\", or \"REBOOT\".\\'))\\n\\n        if method != \\'HARDWARE\\' and method != \\'SOFTWARE\\':\\n            raise ValueError((\\'method must be set to either \\'\\n                              \\'\"HARDWARE\" or \"SOFTWARE\".\\'))\\n        if recovery_mode and recovery_image_id is None:\\n            raise ValueError(\\n                (\\'If you want to reboot in recovery mode you must specify an image id recovery_image_id\\'))\\n\\n        # Perform Request\\n        if recovery_mode:\\n            data = {\\n                \\'action\\': action,\\n                \\'method\\': method,\\n                \\'recovery_mode\\': True,\\n                \\'recovery_image_id\\': recovery_image_id\\n            }\\n        else:\\n            data = {\\n                \\'action\\': action,\\n                \\'method\\': method,\\n            }\\n\\n        url = \\'%s/servers/%s/status/action\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def stop_server(self, server_id=None, method=\\'SOFTWARE\\'):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Make sure user is passing in correct arguments\\n        if(method != \\'HARDWARE\\' and method != \\'SOFTWARE\\'):\\n            raise ValueError((\\'method must be set to either \\'\\n                              \\'\"HARDWARE\" or \"SOFTWARE\".\\'))\\n\\n        # Perform Request\\n        data = {\\n            \\'action\\': \\'POWER_OFF\\',\\n            \\'method\\': method\\n        }\\n\\n        url = \\'%s/servers/%s/status/action\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def start_server(self, server_id=None, method=\\'SOFTWARE\\'):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Make sure user is passing in correct arguments\\n        if(method != \\'HARDWARE\\' and method != \\'SOFTWARE\\'):\\n            raise ValueError((\\'method must be set to either \\'\\n                              \\'\"HARDWARE\" or \"SOFTWARE\".\\'))\\n\\n        # Perform Request\\n        data = {\\n            \\'action\\': \\'POWER_ON\\',\\n            \\'method\\': method\\n        }\\n\\n        url = \\'%s/servers/%s/status/action\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def load_dvd(self, server_id=None, dvd_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(dvd_id is None):\\n            raise ValueError(\\'dvd_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\'id\\': dvd_id}\\n\\n        url = \\'%s/servers/%s/dvd\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def restore_snapshot(self, server_id=None, snapshot_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(snapshot_id is None):\\n            raise ValueError(\\'snapshot_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/servers/%s/snapshots/%s\\' %\\n               (self.base_url, server_id, snapshot_id))\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def reinstall_image(self, server_id=None, image_id=None, password=None,\\n                        firewall_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(image_id is None):\\n            raise ValueError(\\'image_id is a required parameter\\')\\n\\n        # Create firewall object, if necessary\\n        firewall_policy = {\\'id\\': firewall_id}\\n\\n        # Perform Request\\n        data = {\\n            \\'id\\': image_id,\\n            \\'password\\': password,\\n            \\'firewall_policy\\': firewall_policy\\n        }\\n\\n        url = \\'%s/servers/%s/image\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' methods\\n\\n    def delete_server(self, server_id=None, keep_ips=None, keep_hdds=True):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n        parameters = {\\'keep_ips\\': keep_ips, \\'keep_hdds\\': keep_hdds}\\n\\n        url = \\'%s/servers/%s\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().delete(\\n                url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_hdd(self, server_id=None, hdd_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(hdd_id is None):\\n            raise ValueError(\\'hdd_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/servers/%s/hardware/hdds/%s\\' %\\n               (self.base_url, server_id, hdd_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_ip(self, server_id=None, ip_id=None, keep_ip=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n        parameters = {\\'keep_ip\\': keep_ip}\\n\\n        url = \\'%s/servers/%s/ips/%s\\' % (self.base_url, server_id, ip_id)\\n\\n        try:\\n            r = requests_retry_session().delete(\\n                url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_load_balancer(self, server_id=None, ip_id=None,\\n                             load_balancer_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/servers/%s/ips/%s/load_balancers/%s\\' %\\n               (self.base_url, server_id, ip_id, load_balancer_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_private_network(self, server_id=None, private_network_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/servers/%s/private_networks/%s\\' %\\n               (self.base_url, server_id, private_network_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def eject_dvd(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/servers/%s/dvd\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def delete_snapshot(self, server_id=None, snapshot_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(snapshot_id is None):\\n            raise ValueError(\\'snapshot_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/servers/%s/snapshots/%s\\' %\\n               (self.base_url, server_id, snapshot_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' methods\\n\\n    def add_new_ip(self, server_id=None, ip_type=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(ip_type is not None) and (ip_type != \\'IPV4\\'):\\n            raise ValueError((\"ip_type.  Only type \\'IPV4\\' is currently \"\\n                              \"supported.\"))\\n\\n        # Perform Request\\n        data = {\\'type\\': ip_type}\\n\\n        url = \\'%s/servers/%s/ips\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_load_balancer(self, server_id=None, ip_id=None,\\n                          load_balancer_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\'load_balancer_id\\': load_balancer_id}\\n\\n        url = (\\'%s/servers/%s/ips/%s/load_balancers\\' %\\n               (self.base_url, server_id, ip_id))\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def assign_private_network(self, server_id=None, private_network_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\'id\\': private_network_id}\\n\\n        url = \\'%s/servers/%s/private_networks\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def create_snapshot(self, server_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/servers/%s/snapshots\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def clone_server(self, server_id=None, name=None, datacenter_id=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(name is None):\\n            raise ValueError(\\'name is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'datacenter_id\\': datacenter_id\\n        }\\n\\n        url = \\'%s/servers/%s/clone\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def create_server(self, server=None, hdds=None):\\n\\n        # Error Handling\\n        if(server is None):\\n            raise ValueError((\\'server is a required parameter. Make \\'\\n                              \\'sure you pass a Server object.\\'))\\n\\n        # Unpack hdds\\n        if hdds:\\n            hdd = []\\n\\n            for value in hdds:\\n                hdd.append(value.specs)\\n\\n            # Add hdds to server object\\n            server.specs[\\'hardware\\'][\\'hdds\\'] = hdd\\n\\n        # Clean dictionary\\n        keys = [k for k, v in server.specs[\\'hardware\\'].items() if\\n                v is None]\\n        for x in keys:\\n            del server.specs[\\'hardware\\'][x]\\n\\n        # Build URL and perform request\\n        url = \\'%s/servers\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=server.specs)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            # Assign new server_id back to calling Server object\\n            response = r.json()\\n\\n            server.specs.update(server_id=response[\\'id\\'])\\n            server.specs.update(api_token=self.header)\\n            server.first_password = response[\\'first_password\\']\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_hdd(self, server_id=None, hdds=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(hdds is None):\\n            raise ValueError((\\'hdds is a required parameter.  Make \\'\\n                              \\'sure you pass a list with at least \\'\\n                              \\'one Hdd object.\\'))\\n\\n        # Unpack hdds\\n        hdd = []\\n\\n        for value in hdds:\\n            hdd.append(value.specs)\\n\\n        # Perform Request\\n        data = {\\'hdds\\': hdd}\\n\\n        url = \\'%s/servers/%s/hardware/hdds\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Image Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_images(self, page=None, per_page=None, sort=None, q=None,\\n                    fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/images\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_image(self, image_id=None):\\n\\n        # Error Handling\\n        if(image_id is None):\\n            raise ValueError(\\'image_id is a required parameter\\')\\n\\n        # Perform Request\\n\\n        url = \\'%s/images/%s\\' % (self.base_url, image_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_image(self, image=None):\\n\\n        # Error Handling\\n        if(image.server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(image.name is None):\\n            raise ValueError(\\'name is a required parameter\\')\\n        if(image.frequency is None):\\n            raise ValueError(\\'frequency is a required parameter\\')\\n        if(image.num_images is None):\\n            raise ValueError(\\'num_images is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'server_id\\': image.server_id,\\n            \\'name\\': image.name,\\n            \\'frequency\\': image.frequency,\\n            \\'num_images\\': image.num_images,\\n            \\'description\\': image.description,\\n            \\'source\\': image.source,\\n            \\'url\\': image.url,\\n            \\'os_id\\': image.os_id,\\n            \\'type\\': image.type\\n        }\\n\\n        url = \\'%s/images\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            # Assign new image_id back to calling Image object\\n            response = r.json()\\n\\n            image.specs.update(image_id=response[\\'id\\'])\\n            image.specs.update(api_token=self.header)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_image(self, image_id=None):\\n\\n        # Error Handling\\n        if(image_id is None):\\n            raise ValueError(\\'image_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/images/%s\\' % (self.base_url, image_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_image(self, image_id=None, name=None, description=None,\\n                     frequency=None):\\n\\n        # Error Handling\\n        if(image_id is None):\\n            raise ValueError(\\'image_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'frequency\\': frequency,\\n            \\'description\\': description\\n        }\\n\\n        url = \\'%s/images/%s\\' % (self.base_url, image_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Shared Storage Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_shared_storages(self, page=None, per_page=None, sort=None,\\n                             q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/shared_storages\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_shared_storage(self, shared_storage_id=None):\\n\\n        # Error Handling\\n        if(shared_storage_id is None):\\n            raise ValueError(\\'shared_storage_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/shared_storages/%s\\' % (self.base_url, shared_storage_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_servers_attached_storage(self, shared_storage_id=None):\\n\\n        # Error Handling\\n        if(shared_storage_id is None):\\n            raise ValueError(\\'shared_storage_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/shared_storages/%s/servers\\' %\\n               (self.base_url, shared_storage_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_shared_storage_server(\\n            self,\\n            shared_storage_id=None,\\n            server_id=None):\\n\\n        # Error Handling\\n        if(shared_storage_id is None):\\n            raise ValueError(\\'shared_storage_id parameter is required\\')\\n        if(server_id is None):\\n            raise ValueError(\\'server_id parameter is required\\')\\n\\n        # Perform Request\\n        url = (\\'%s/shared_storages/%s/servers/%s\\' %\\n               (self.base_url, shared_storage_id, server_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_credentials(self):\\n\\n        # Perform Request\\n        url = \\'%s/shared_storages/access\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_shared_storage(self, shared_storage=None):\\n\\n        # Error Handling\\n        if(shared_storage.name is None):\\n            raise ValueError(\\'name is a required parameter\\')\\n        if(shared_storage.size is None):\\n            raise ValueError(\\'size is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': shared_storage.name,\\n            \\'description\\': shared_storage.description,\\n            \\'size\\': shared_storage.size,\\n            \\'datacenter_id\\': shared_storage.datacenter_id\\n        }\\n\\n        url = \\'%s/shared_storages\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            # Assign new shared_storage_id back to calling SharedStorage object\\n            response = r.json()\\n\\n            shared_storage.specs.update(shared_storage_id=response[\\'id\\'])\\n            shared_storage.specs.update(api_token=self.header)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def attach_server_shared_storage(self, shared_storage_id=None,\\n                                     server_ids=None):\\n\\n        # Error Handling\\n        if(shared_storage_id is None):\\n            raise ValueError(\\'shared_storage_id is a required parameter\\')\\n        if(server_ids is None):\\n            raise ValueError((\\'server_ids is a required parameter.  \\'\\n                              \\'Must attach at least one server\\'))\\n\\n        # Unpack servers\\n        servers = []\\n\\n        for value in server_ids:\\n            servers.append({\\'id\\': value.server_id, \\'rights\\': value.rights})\\n\\n        # Perform Request\\n        data = {\\'servers\\': servers}\\n\\n        url = (\\'%s/shared_storages/%s/servers\\' %\\n               (self.base_url, shared_storage_id))\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_shared_storage(self, shared_storage_id=None, name=None,\\n                              description=None, size=None):\\n\\n        # Error Handling\\n        if(shared_storage_id is None):\\n            raise ValueError(\\'shared_storage_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'size\\': size\\n        }\\n\\n        url = \\'%s/shared_storages/%s\\' % (self.base_url, shared_storage_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def change_password(self, password=None):\\n\\n        # Error Handlong\\n        if(password is None):\\n            raise ValueError((\\'password is a required parameter. \\'\\n                              \\'password must contain at least 8 characters.\\'))\\n\\n        # Perform Request\\n        data = {\\'password\\': password}\\n\\n        url = \\'%s/shared_storages/access\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_shared_storage(self, shared_storage_id=None):\\n\\n        # Error Handling\\n        if(shared_storage_id is None):\\n            raise ValueError(\\'shared_storage_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/shared_storages/%s\\' % (self.base_url, shared_storage_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def detach_server_shared_storage(self, shared_storage_id=None,\\n                                     server_id=None):\\n\\n        # Error Handling\\n        if(shared_storage_id is None):\\n            raise ValueError(\\'shared_storage_id is a required parameter\\')\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/shared_storages/%s/servers/%s\\' %\\n               (self.base_url, shared_storage_id, server_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Firewall Policy Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_firewall_policies(self, page=None, per_page=None, sort=None,\\n                               q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/firewall_policies\\' % self.base_url\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_firewall(self, firewall_id=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/firewall_policies/%s\\' % (self.base_url, firewall_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_firewall_servers(self, firewall_id=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/firewall_policies/%s/server_ips\\' %\\n               (self.base_url, firewall_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_firewall_server(self, firewall_id=None, server_ip_id=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n        if(server_ip_id is None):\\n            raise ValueError(\\'server_ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/firewall_policies/%s/server_ips/%s\\' %\\n               (self.base_url, firewall_id, server_ip_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_firewall_policy_rules(self, firewall_id=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/firewall_policies/%s/rules\\' % (self.base_url, firewall_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_firewall_policy_rule(self, firewall_id=None, rule_id=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n        if(rule_id is None):\\n            raise ValueError(\\'rule_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/firewall_policies/%s/rules/%s\\' %\\n               (self.base_url, firewall_id, rule_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_firewall(self, firewall_id=None, name=None, description=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description\\n        }\\n\\n        url = \\'%s/firewall_policies/%s\\' % (self.base_url, firewall_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_firewall_policy(self, firewall_policy=None,\\n                               firewall_policy_rules=None):\\n\\n        # Error Handling\\n        if(firewall_policy.specs[\\'name\\'] is None):\\n            raise ValueError((\\'Policy name is required.  Make sure your \\'\\n                              \\'FirewallPolicy object was initialized with \\'\\n                              \\'a name parameter\\'))\\n        if(firewall_policy_rules is None):\\n            raise ValueError((\\'firewall_policy_rules is required.  Make sure \\'\\n                              \\'you pass a list with at least one \\'\\n                              \\'FirewallPolicyRule object.\\'))\\n\\n        # Unpack Rules\\n        rules = []\\n\\n        for value in firewall_policy_rules:\\n            rules.append(value.rule_set)\\n\\n        # Attach rules and Perform Request\\n        firewall_policy.specs[\\'rules\\'] = rules\\n\\n        url = \\'%s/firewall_policies\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(\\n                url, headers=self.header, json=firewall_policy.specs)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            # Assign new firewall_id back to calling FirewallPolicy object\\n            response = r.json()\\n\\n            firewall_policy.specs.update(firewall_id=response[\\'id\\'])\\n            firewall_policy.specs.update(api_token=self.header)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_firewall_policy_rule(self, firewall_id=None,\\n                                 firewall_policy_rules=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n        if(firewall_policy_rules is None):\\n            raise ValueError((\\'firewall_policy_rules is required.  Make \\'\\n                              \\'sure you pass a list with at least one \\'\\n                              \\'FirewallPolicyRule object\\'))\\n\\n        # Unpack rules\\n        rules = []\\n\\n        for value in firewall_policy_rules:\\n            rules.append(value.rule_set)\\n\\n        # Perform Request\\n        data = {\\'rules\\': rules}\\n\\n        url = \\'%s/firewall_policies/%s/rules\\' % (self.base_url, firewall_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def attach_server_firewall_policy(self, firewall_id=None, server_ips=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n        if(server_ips is None):\\n            raise ValueError((\\'server_ips is required. Make sure you pass \\'\\n                              \\'a list with at least one AttachServer object\\'))\\n\\n        # Unpack servers\\n        servers = []\\n\\n        for value in server_ips:\\n            servers.append(value.server_ip_id)\\n\\n        # Perform Request\\n        data = {\\'server_ips\\': servers}\\n\\n        url = (\\'%s/firewall_policies/%s/server_ips\\' %\\n               (self.base_url, firewall_id))\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_firewall(self, firewall_id=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/firewall_policies/%s\\' % (self.base_url, firewall_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_firewall_rule(self, firewall_id=None, rule_id=None):\\n\\n        # Error Handling\\n        if(firewall_id is None):\\n            raise ValueError(\\'firewall_id is a required parameter\\')\\n        if(rule_id is None):\\n            raise ValueError(\\'rule_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/firewall_policies/%s/rules/%s\\' %\\n               (self.base_url, firewall_id, rule_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Load Balancer Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_load_balancers(self, page=None, per_page=None, sort=None, q=None,\\n                            fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/load_balancers\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_load_balancer(self, load_balancer_id=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/load_balancers/%s\\' % (self.base_url, load_balancer_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_load_balancer_servers(self, load_balancer_id=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/load_balancers/%s/server_ips\\' %\\n               (self.base_url, load_balancer_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_load_balancer_server(self, load_balancer_id=None,\\n                                 server_ip_id=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n        if(server_ip_id is None):\\n            raise ValueError(\\'server_ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/load_balancers/%s/server_ips/%s\\' %\\n               (self.base_url, load_balancer_id, server_ip_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def load_balancer_rules(self, load_balancer_id=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/load_balancers/%s/rules\\' % (self.base_url, load_balancer_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_load_balancer_rule(self, load_balancer_id=None, rule_id=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n        if(rule_id is None):\\n            raise ValueError(\\'rule_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/load_balancers/%s/rules/%s\\' %\\n               (self.base_url, load_balancer_id, rule_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_load_balancer(\\n            self,\\n            load_balancer_id=None,\\n            name=None,\\n            description=None,\\n            health_check_test=None,\\n            health_check_interval=None,\\n            health_check_path=None,\\n            health_check_parse=None,\\n            persistence=None,\\n            persistence_time=None,\\n            method=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n\\n        if(method is not None and method != \\'ROUND_ROBIN\\' and\\n           method != \\'LEAST_CONNECTIONS\\'):\\n            raise ValueError((\\'method must be set to either \"ROUND_ROBIN\" \\'\\n                              \\'or \"LEAST_CONNECTIONS\".\\'))\\n\\n        if(health_check_test is not None and health_check_test != \\'TCP\\'):\\n            raise ValueError((\\'health_check_test must be set to \"TCP\". \\'\\n                              \\'\"HTTP\" is not currently supported.\\'))\\n\\n        if(health_check_interval is not None and health_check_interval < 5 and health_check_interval > 300):\\n            raise ValueError((\\'health_check_interval must be an integer \\'\\n                              \\'between 5 and 300.\\'))\\n\\n        if(persistence_time is not None and persistence_time < 30 and persistence_time > 1200):\\n            raise ValueError((\\'persistence_time must be an integer \\'\\n                              \\'between 30 and 1200.\\'))\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'health_check_test\\': health_check_test,\\n            \\'health_check_interval\\': health_check_interval,\\n            \\'health_check_path\\': health_check_path,\\n            \\'health_check_parse\\': health_check_parse,\\n            \\'persistence\\': persistence,\\n            \\'persistence_time\\': persistence_time,\\n            \\'method\\': method\\n        }\\n\\n        url = \\'%s/load_balancers/%s\\' % (self.base_url, load_balancer_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_load_balancer(self, load_balancer=None,\\n                             load_balancer_rules=None):\\n\\n        # Error Handling\\n        if(load_balancer is None):\\n            raise ValueError((\\'load_balancer parameter is required.  Must \\'\\n                              \\'pass a LoadBalancer object.\\'))\\n        if(load_balancer_rules is None):\\n            raise ValueError((\\'load_balancer_rules parameter is required. \\'\\n                              \\'Must pass a list with at least one \\'\\n                              \\'LoadBalancerRule object.\\'))\\n        if(load_balancer.specs[\\'method\\'] is not None and\\n                load_balancer.specs[\\'method\\'] != \\'ROUND_ROBIN\\' and\\n                load_balancer.specs[\\'method\\'] != \\'LEAST_CONNECTIONS\\'):\\n            raise ValueError((\\'method must be set to either \"ROUND_ROBIN\" \\'\\n                              \\'or \"LEAST_CONNECTIONS\".\\'))\\n\\n        # Unpack rules\\n        rules = []\\n\\n        for value in load_balancer_rules:\\n            rules.append(value.rule_set)\\n\\n        # Perform Request\\n        load_balancer.specs[\\'rules\\'] = rules\\n\\n        url = \\'%s/load_balancers\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(\\n                url, headers=self.header, json=load_balancer.specs)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            # Assign new load_balancer_id back to calling LoadBalancer object\\n            response = r.json()\\n\\n            load_balancer.specs.update(load_balancer_id=response[\\'id\\'])\\n            load_balancer.specs.update(api_token=self.header)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def attach_load_balancer_server(self, load_balancer_id=None,\\n                                    server_ips=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter.\\')\\n        if(server_ips is None):\\n            raise ValueError((\\'server_ips is a required parameter. Must \\'\\n                              \\'pass a list with at least one AttachServer \\'\\n                              \\'object\\'))\\n\\n        # Unpack servers\\n        servers = []\\n\\n        for value in server_ips:\\n            servers.append(value.server_ip_id)\\n\\n        # Perform Request\\n        data = {\\'server_ips\\': servers}\\n\\n        url = (\\'%s/load_balancers/%s/server_ips\\' %\\n               (self.base_url, load_balancer_id))\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_load_balancer_rule(self, load_balancer_id=None,\\n                               load_balancer_rules=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter.\\')\\n        if(load_balancer_rules is None):\\n            raise ValueError((\\'load_balancer_rules is a required \\'\\n                              \\'parameter. Must pass a list with at least one \\'\\n                              \\'LoadBalancerRule object\\'))\\n\\n        # Unpack rules\\n        rules = []\\n\\n        for value in load_balancer_rules:\\n            rules.append(value.rule_set)\\n\\n        # Perform Request\\n        data = {\\'rules\\': rules}\\n\\n        url = (\\'%s/load_balancers/%s/rules\\' %\\n               (self.base_url, load_balancer_id))\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_load_balancer(self, load_balancer_id=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/load_balancers/%s\\' % (self.base_url, load_balancer_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_load_balancer_server(self, load_balancer_id=None,\\n                                    server_ip_id=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter.\\')\\n        if(server_ip_id is None):\\n            raise ValueError(\\'server_ip_id is a required parameter.\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/load_balancers/%s/server_ips/%s\\' %\\n               (self.base_url, load_balancer_id, server_ip_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_load_balancer_rule(self, load_balancer_id=None, rule_id=None):\\n\\n        # Error Handling\\n        if(load_balancer_id is None):\\n            raise ValueError(\\'load_balancer_id is a required parameter.\\')\\n        if(rule_id is None):\\n            raise ValueError(\\'rule_id is a required parameter.\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/load_balancers/%s/rules/%s\\' %\\n               (self.base_url, load_balancer_id, rule_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Public IP Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_public_ips(self, page=None, per_page=None, sort=None, q=None,\\n                        fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/public_ips\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_public_ip(self, ip_id=None):\\n\\n        # Error Handling\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/public_ips/%s\\' % (self.base_url, ip_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_public_ip(self, reverse_dns=None, ip_type=None,\\n                         datacenter_id=None):\\n\\n        # Error Handling\\n        if(ip_type != \\'IPV4\\' and ip_type is not None):\\n            raise ValueError(\\'ip_type must be set to \"IPV4\".\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'reverse_dns\\': reverse_dns,\\n            \\'type\\': ip_type,\\n            \\'datacenter_id\\': datacenter_id\\n        }\\n\\n        url = \\'%s/public_ips\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_public_ip(self, ip_id=None, reverse_dns=None):\\n\\n        # Error Handling\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\'reverse_dns\\': reverse_dns}\\n\\n        url = \\'%s/public_ips/%s\\' % (self.base_url, ip_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_public_ip(self, ip_id=None):\\n\\n        # Error Handling\\n        if(ip_id is None):\\n            raise ValueError(\\'ip_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/public_ips/%s\\' % (self.base_url, ip_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Private Network Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_private_networks(self, page=None, per_page=None, sort=None,\\n                              q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/private_networks\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_private_network(self, private_network_id):\\n\\n        # Error Handling\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/private_networks/%s\\' % (self.base_url, private_network_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_private_network_servers(self, private_network_id=None):\\n\\n        # Error Handling\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/private_networks/%s/servers\\' %\\n               (self.base_url, private_network_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_private_network_server(self, private_network_id=None,\\n                                   server_id=None):\\n\\n        # Error Handling\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/private_networks/%s/servers/%s\\' %\\n               (self.base_url, private_network_id, server_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_private_network(self, private_network=None):\\n\\n        # Error Handling\\n        if(private_network.name is None):\\n            raise ValueError(\\'name is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': private_network.name,\\n            \\'description\\': private_network.description,\\n            \\'network_address\\': private_network.network_address,\\n            \\'subnet_mask\\': private_network.subnet_mask,\\n            \\'datacenter_id\\': private_network.datacenter_id\\n        }\\n\\n        url = \\'%s/private_networks\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            # Assign new private_network_id back to calling PrivateNetwork\\n            # object\\n            response = r.json()\\n\\n            private_network.specs.update(private_network_id=response[\\'id\\'])\\n            private_network.specs.update(api_token=self.header)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def attach_private_network_servers(self, private_network_id=None,\\n                                       server_ids=None):\\n\\n        # Error Handling\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n        if(server_ids is None):\\n            raise ValueError((\\'server_ids is a required parameter.  Make \\'\\n                              \\'sure you pass a list with at least one \\'\\n                              \\'server_id string\\'))\\n\\n        # Unpack servers\\n        servers = []\\n\\n        for value in server_ids:\\n            servers.append(value.server_id)\\n\\n        # Perform Request\\n        data = {\\'servers\\': servers}\\n\\n        url = (\\'%s/private_networks/%s/servers\\' %\\n               (self.base_url, private_network_id))\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_private_network(\\n            self,\\n            private_network_id=None,\\n            name=None,\\n            description=None,\\n            network_address=None,\\n            subnet_mask=None):\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'network_address\\': network_address,\\n            \\'subnet_mask\\': subnet_mask\\n        }\\n\\n        url = \\'%s/private_networks/%s\\' % (self.base_url, private_network_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_private_network(self, private_network_id=None):\\n\\n        # Error Handling\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/private_networks/%s\\' % (self.base_url, private_network_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_private_network_server(self, private_network_id=None,\\n                                      server_id=None):\\n\\n        # Error Handling\\n        if(private_network_id is None):\\n            raise ValueError(\\'private_network_id is a required parameter\\')\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/private_networks/%s/servers/%s\\' %\\n               (self.base_url, private_network_id, server_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Monitoring Center Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_server_usages(self, page=None, per_page=None, sort=None,\\n                           q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/monitoring_center\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_usage(self, server_id=None, period=\\'LAST_24H\\',\\n                  start_date=None, end_date=None):\\n\\n        # Error Handling\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n        if(period == \\'CUSTOM\\'):\\n            if(start_date is None):\\n                raise ValueError((\\'start_date parameter is required when \\'\\n                                  \\'using CUSTOM period\\'))\\n            if(end_date is None):\\n                raise ValueError((\\'end_date parameter is required when \\'\\n                                  \\'using CUSTOM period\\'))\\n\\n        # Perform Request\\n        parameters = {\\n            \\'period\\': period,\\n            \\'start_date\\': start_date,\\n            \\'end_date\\': end_date\\n        }\\n\\n        url = \\'%s/monitoring_center/%s\\' % (self.base_url, server_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Monitoring Policy Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_monitoring_policies(self, page=None, per_page=None,\\n                                 sort=None, q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/monitoring_policies\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_monitoring_policy(self, monitoring_policy_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s\\' %\\n               (self.base_url, monitoring_policy_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_monitoring_policy_ports(self, monitoring_policy_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/ports\\' %\\n               (self.base_url, monitoring_policy_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_monitoring_policy_port(self, monitoring_policy_id=None,\\n                                   port_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(port_id is None):\\n            raise ValueError(\\'port_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/ports/%s\\' %\\n               (self.base_url, monitoring_policy_id, port_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_monitoring_policy_processes(self, monitoring_policy_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/processes\\' %\\n               (self.base_url, monitoring_policy_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_monitoring_policy_process(self, monitoring_policy_id=None,\\n                                      process_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(process_id is None):\\n            raise ValueError(\\'process_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/processes/%s\\' %\\n               (self.base_url, monitoring_policy_id, process_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def list_monitoring_policy_servers(self, monitoring_policy_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/servers\\' %\\n               (self.base_url, monitoring_policy_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_monitoring_policy_server(self, monitoring_policy_id=None,\\n                                     server_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/servers/%s\\' %\\n               (self.base_url, monitoring_policy_id, server_id))\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_monitoring_policy(self, monitoring_policy_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/monitoring_policies/%s\\' %\\n               (self.base_url, monitoring_policy_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def delete_monitoring_policy_port(self, monitoring_policy_id=None,\\n                                      port_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(port_id is None):\\n            raise ValueError(\\'port_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/monitoring_policies/%s/ports/%s\\' %\\n               (self.base_url, monitoring_policy_id, port_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def delete_monitoring_policy_process(self, monitoring_policy_id=None,\\n                                         process_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(process_id is None):\\n            raise ValueError(\\'process_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/monitoring_policies/%s/processes/%s\\' %\\n               (self.base_url, monitoring_policy_id, process_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def detach_monitoring_policy_server(self, monitoring_policy_id=None,\\n                                        server_id=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/monitoring_policies/%s/servers/%s\\' %\\n               (self.base_url, monitoring_policy_id, server_id))\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_monitoring_policy(self, monitoring_policy=None,\\n                                 thresholds=None, ports=None, processes=None):\\n\\n        # Error Handling\\n        if(monitoring_policy is None):\\n            raise ValueError((\\'monitoring_policy is a required parameter. \\'\\n                              \\'Make sure you pass a MonitoringPolicy object.\\'))\\n        if(thresholds is None):\\n            raise ValueError((\\'thresholds is a required parameter.  Make \\'\\n                              \\'sure you pass a list with all 5 Threshold \\'\\n                              \\'objects(cpu, ram, disk, transfer, \\'\\n                              \\'internal_ping).\\'))\\n        if(ports is None):\\n            raise ValueError(\\n                (\\'ports is a required parameter.  Make sure \\'\\n                 \\'you pass a list with at least one Port object.\\'))\\n        if(processes is None):\\n            raise ValueError((\\'processes is a required parameter.  Make \\'\\n                              \\'sure you pass a list with at least one \\'\\n                              \\'Process object.\\'))\\n\\n        # Unpack Thresholds\\n        new_thresholds = {}\\n\\n        for value in thresholds:\\n            new_thresholds[value.entity] = {\\n                \\'warning\\': {\\n                    \\'value\\': value.warning_value,\\n                    \\'alert\\': value.warning_alert\\n                },\\n                \\'critical\\': {\\n                    \\'value\\': value.critical_value,\\n                    \\'alert\\': value.critical_alert\\n                }\\n            }\\n\\n        # Unpack Ports\\n        new_ports = []\\n\\n        for value in ports:\\n            new_ports.append(value.specs)\\n\\n        # Unpack Processes\\n        new_processes = []\\n\\n        for value in processes:\\n            new_processes.append(value.process_set)\\n\\n        # Add Ports, Processes, and Thresholds to Monitoring Policy object\\n        monitoring_policy.specs[\\'thresholds\\'] = new_thresholds\\n        monitoring_policy.specs[\\'ports\\'] = new_ports\\n        monitoring_policy.specs[\\'processes\\'] = new_processes\\n\\n        # Perform Request\\n        url = \\'%s/monitoring_policies\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header,\\n                                              json=monitoring_policy.specs)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            # Assign new monitoring_policy_id back to calling MonitoringPolicy\\n            # object\\n            response = r.json()\\n\\n            monitoring_policy.specs.update(monitoring_policy_id=response[\\'id\\'])\\n            monitoring_policy.specs.update(api_token=self.header)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_port(self, monitoring_policy_id=None, ports=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(ports is None):\\n            raise ValueError((\\'ports is a required parameter. Make sure you \\'\\n                              \\'send in a list with at least one Port object\\'))\\n\\n        # Unpack ports\\n        new_ports = []\\n\\n        for value in ports:\\n            new_ports.append(value.specs)\\n\\n        # Perform Request\\n        data = {\\'ports\\': new_ports}\\n\\n        url = (\\'%s/monitoring_policies/%s/ports\\' %\\n               (self.base_url, monitoring_policy_id))\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_process(self, monitoring_policy_id=None, processes=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(processes is None):\\n            raise ValueError((\\'processes is a required parameter. Make \\'\\n                              \\'sure you send in a list with at least one \\'\\n                              \\'Process object\\'))\\n\\n        # Unpack processes\\n        new_processes = []\\n\\n        for value in processes:\\n            new_processes.append(value.process_set)\\n\\n        # Perform Request\\n        data = {\\'processes\\': new_processes}\\n\\n        url = (\\'%s/monitoring_policies/%s/processes\\' %\\n               (self.base_url, monitoring_policy_id))\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def attach_monitoring_policy_server(self, monitoring_policy_id=None,\\n                                        servers=None):\\n\\n        # Error Handling\\n        if(monitoring_policy_id is None):\\n            raise ValueError(\\'monitoring_policy_id is a required parameter\\')\\n        if(servers is None):\\n            raise ValueError((\\'servers is a required parameter. Make sure \\'\\n                              \\'you send in a list with at least one \\'\\n                              \\'AttachServer object\\'))\\n\\n        # Unpack servers\\n        add_servers = []\\n\\n        for value in servers:\\n            add_servers.append(value.server_id)\\n\\n        # Perform Request\\n        data = {\\'servers\\': add_servers}\\n\\n        url = (\\'%s/monitoring_policies/%s/servers\\' %\\n               (self.base_url, monitoring_policy_id))\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_monitoring_policy(\\n            self,\\n            monitoring_policy_id=None,\\n            monitoring_policy=None,\\n            thresholds=None,\\n            test=False):\\n\\n        try:\\n            # Error Handling\\n            if(monitoring_policy_id is None):\\n                raise ValueError(\\n                    \\'monitoring_policy_id is a required parameter\\')\\n\\n            # Use flag to skip this live API call when running unit test\\n            if(test == False):\\n                # Make request for existing monitoring policy object\\n                json = self.get_monitoring_policy(\\n                    monitoring_policy_id=monitoring_policy_id)\\n\\n                # Update policy specs with new values, if necessary.\\n                if(monitoring_policy):\\n                    if(json[\\'name\\'] != monitoring_policy.specs[\\'name\\']):\\n                        if(monitoring_policy.specs[\\'name\\'] is not None):\\n                            json[\\'name\\'] = monitoring_policy.specs[\\'name\\']\\n\\n                    if(json[\\'description\\'] !=\\n                            monitoring_policy.specs[\\'description\\']):\\n                        if(monitoring_policy.specs[\\'description\\'] is not None):\\n                            json[\\'description\\'] = monitoring_policy.specs[\\'description\\']\\n\\n                    if(json[\\'email\\'] != monitoring_policy.specs[\\'email\\']):\\n                        if(monitoring_policy.specs[\\'email\\'] is not None):\\n                            json[\\'email\\'] = monitoring_policy.specs[\\'email\\']\\n\\n                # Unpack thresholds\\n                if(thresholds):\\n                    new_thresholds = {}\\n\\n                    for value in thresholds:\\n                        new_thresholds[value.entity] = {\\n                            \\'warning\\': {\\n                                \\'value\\': value.warning_value,\\n                                \\'alert\\': value.warning_alert\\n                            },\\n                            \\'critical\\': {\\n                                \\'value\\': value.critical_value,\\n                                \\'alert\\': value.critical_alert\\n                            }\\n                        }\\n\\n                    # Compare all threshold values and update, if necessary.\\n                    threshold_entities = [\\'cpu\\', \\'ram\\', \\'disk\\', \\'transfer\\',\\n                                          \\'internal_ping\\']\\n\\n                    for value in threshold_entities:\\n\\n                        if(value in new_thresholds.keys()):\\n                            if(json[\\'thresholds\\'][value][\\'warning\\'][\\'value\\'] !=\\n                                    new_thresholds[value][\\'warning\\'][\\'value\\']):\\n                                json[\\'thresholds\\'][value][\\'warning\\'][\\'value\\'] = new_thresholds[value][\\'warning\\'][\\'value\\']\\n\\n                            if(json[\\'thresholds\\'][value][\\'warning\\'][\\'alert\\'] !=\\n                                    new_thresholds[value][\\'warning\\'][\\'alert\\']):\\n                                json[\\'thresholds\\'][value][\\'warning\\'][\\'alert\\'] = new_thresholds[value][\\'warning\\'][\\'alert\\']\\n\\n                            if(json[\\'thresholds\\'][value][\\'critical\\'][\\'value\\'] !=\\n                                    new_thresholds[value][\\'critical\\'][\\'value\\']):\\n                                json[\\'thresholds\\'][value][\\'critical\\'][\\'value\\'] = new_thresholds[value][\\'critical\\'][\\'value\\']\\n\\n                            if(json[\\'thresholds\\'][value][\\'critical\\'][\\'alert\\'] !=\\n                                    new_thresholds[value][\\'critical\\'][\\'alert\\']):\\n                                json[\\'thresholds\\'][value][\\'critical\\'][\\'alert\\'] = new_thresholds[value][\\'critical\\'][\\'alert\\']\\n\\n                # Perform Request\\n                data = {\\n                    \\'name\\': json[\\'name\\'],\\n                    \\'description\\': json[\\'description\\'],\\n                    \\'email\\': json[\\'email\\'],\\n                    \\'thresholds\\': json[\\'thresholds\\']\\n                }\\n\\n                url = (\\'%s/monitoring_policies/%s\\' %\\n                       (self.base_url, monitoring_policy_id))\\n\\n                r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            else:\\n                # Mock Request for Unit Testing\\n                r = requests_retry_session().put(\\n                    self.base_url + \\'/monitoring_policies/%s\\' %\\n                    (monitoring_policy_id), headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def modify_port(self, monitoring_policy_id=None, port_id=None, port=None,\\n                    test=False):\\n\\n        try:\\n            # Error Handling\\n            if(monitoring_policy_id is None):\\n                raise ValueError(\\n                    \\'monitoring_policy_id is a required parameter\\')\\n            if(port_id is None):\\n                raise ValueError(\\'port_id is a required parameter\\')\\n\\n            # Use flag to skip this live API call when running unit test\\n            if(test == False):\\n                # Make request for existing port object\\n                json = self.get_monitoring_policy_port(\\n                    monitoring_policy_id=monitoring_policy_id, port_id=port_id)\\n                del json[\\'id\\']\\n\\n                # Update port object with new values, if necessary.\\n                if(json[\\'alert_if\\'] != port.specs[\\'alert_if\\']):\\n                    if(port.specs[\\'alert_if\\'] is not None):\\n                        json[\\'alert_if\\'] = port.specs[\\'alert_if\\']\\n\\n                if(json[\\'email_notification\\'] != port.specs[\\'email_notification\\']):\\n                    if(port.specs[\\'email_notification\\'] is not None):\\n                        json[\\'email_notification\\'] = port.specs[\\'email_notification\\']\\n\\n                # Perform Request\\n                data = {\\'ports\\': json}\\n\\n                url = (\\'%s/monitoring_policies/%s/ports/%s\\' %\\n                       (self.base_url, monitoring_policy_id, port_id))\\n\\n                r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            else:\\n                # Mock Request for Unit Testing\\n                r = requests_retry_session().put(\\n                    self.base_url + \\'/monitoring_policies/%s/ports/%s\\' %\\n                    (monitoring_policy_id, port_id), headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def modify_process(self, monitoring_policy_id=None, process_id=None,\\n                       process=None, test=False):\\n\\n        try:\\n            # Error Handling\\n            if(monitoring_policy_id is None):\\n                raise ValueError(\\n                    \\'monitoring_policy_id is a required parameter\\')\\n            if(process_id is None):\\n                raise ValueError(\\'process_id is a required parameter\\')\\n\\n            # Use flag to skip this live API call when running unit test\\n            if(test == False):\\n                # Make request for existing process object\\n                json = self.get_monitoring_policy_process(\\n                    monitoring_policy_id=monitoring_policy_id,\\n                    process_id=process_id)\\n                del json[\\'id\\']\\n\\n                # Update process object with new values, if necessary.\\n                if(json[\\'alert_if\\'] != process.process_set[\\'alert_if\\']):\\n                    if(process.process_set[\\'alert_if\\'] is not None):\\n                        json[\\'alert_if\\'] = process.process_set[\\'alert_if\\']\\n\\n                if(json[\\'email_notification\\'] !=\\n                        process.process_set[\\'email_notification\\']):\\n                    if(process.process_set[\\'email_notification\\'] is not None):\\n                        json[\\'email_notification\\'] = process.process_set[\\'email_notification\\']\\n\\n                # Perform Request\\n                data = {\\'processes\\': json}\\n\\n                url = (\\'%s/monitoring_policies/%s/processes/%s\\' %\\n                       (self.base_url, monitoring_policy_id, process_id))\\n\\n                r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            else:\\n                # Mock Request for Unit Testing\\n                r = requests_retry_session().put(\\n                    self.base_url + \\'/monitoring_policies/%s/processes/%s\\' %\\n                    (monitoring_policy_id, process_id), headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Log Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_logs(\\n            self,\\n            page=None,\\n            per_page=None,\\n            sort=None,\\n            q=None,\\n            fields=None,\\n            period=\\'LAST_24H\\',\\n            start_date=None,\\n            end_date=None):\\n\\n        # Error Handling\\n        if(period == \\'CUSTOM\\'):\\n            if(start_date is None):\\n                raise ValueError((\\'start_date parameter is required when \\'\\n                                  \\'using CUSTOM period\\'))\\n            if(end_date is None):\\n                raise ValueError((\\'end_date parameter is required when \\'\\n                                  \\'using CUSTOM period\\'))\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields,\\n            \\'period\\': period,\\n            \\'start_date\\': start_date,\\n            \\'end_date\\': end_date\\n        }\\n\\n        url = \\'%s/logs\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_log(self, log_id=None):\\n\\n        # Error Handling\\n        if(log_id is None):\\n            raise ValueError(\\'log_id parameter is required\\')\\n\\n        # Perform Request\\n        url = \\'%s/logs/%s\\' % (self.base_url, log_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # User Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_users(self, page=None, per_page=None, sort=None, q=None,\\n                   fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/users\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_user(self, user_id=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/users/%s\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def api_info(self, user_id=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/users/%s/api\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def show_api_key(self, user_id=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/users/%s/api/key\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def show_user_permissions(self):\\n\\n        # Perform Request\\n        url = \\'%s/users/current_user_permissions\\' % (self.base_url)\\n\\n        r = requests.get(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def ips_api_access_allowed(self, user_id=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/users/%s/api/ips\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_user(self, name=None, password=None, email=None,\\n                    description=None):\\n\\n        # Error Handling\\n        if(name is None):\\n            raise ValueError(\\'name is a required parameter\\')\\n        if(password is None):\\n            raise ValueError(\\'password is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'password\\': password,\\n            \\'email\\': email,\\n            \\'description\\': description\\n        }\\n\\n        url = \\'%s/users\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_user_ip(self, user_id=None, user_ips=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n        if(user_ips is None):\\n            raise ValueError((\\'user_ips is a required parameter. Make \\'\\n                              \\'sure you pass a list with at least \\'\\n                              \\'one IP string.\\'))\\n\\n        # Unpack IPs\\n        ips = []\\n\\n        for value in user_ips:\\n            ips.append(value)\\n\\n        # Perform Request\\n        data = {\\'ips\\': ips}\\n\\n        url = \\'%s/users/%s/api/ips\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_user(self, user_id=None, description=None, email=None,\\n                    password=None, state=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n        if(password is not None) and (len(password) < 8):\\n            raise ValueError(\\'password must be at least 8 characters long\\')\\n        if(state is not None) and (state != \\'ACTIVE\\') and (state != \\'DISABLE\\'):\\n            raise ValueError(\\'state should be set to \"ACTIVE\" or \"DISABLE\".\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'description\\': description,\\n            \\'email\\': email,\\n            \\'password\\': password,\\n            \\'state\\': state\\n        }\\n\\n        url = \\'%s/users/%s\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def modify_user_api(self, user_id=None, active=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n        if(active is not None) and (active != True) and (active != False):\\n            raise ValueError(\\'active parameter only accepts a boolean value\\')\\n\\n        # Perform Request\\n        data = {\\'active\\': active}\\n\\n        url = \\'%s/users/%s/api\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def change_api_key(self, user_id=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/users/%s/api/key\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_user(self, user_id=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/users/%s\\' % (self.base_url, user_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_user_ip(self, user_id=None, ip=None):\\n\\n        # Error Handling\\n        if(user_id is None):\\n            raise ValueError(\\'user_id is a required parameter\\')\\n        if(ip is None):\\n            raise ValueError(\\'ip is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/users/%s/api/ips/%s\\' % (self.base_url, user_id, ip)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Usage Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_usages(\\n            self,\\n            page=None,\\n            per_page=None,\\n            sort=None,\\n            q=None,\\n            fields=None,\\n            period=\\'LAST_24H\\',\\n            start_date=None,\\n            end_date=None):\\n\\n        # Error Handling\\n        if(period == \\'CUSTOM\\'):\\n            if(start_date is None):\\n                raise ValueError((\\'start_date parameter is required when \\'\\n                                  \\'using CUSTOM period\\'))\\n            if(end_date is None):\\n                raise ValueError((\\'end_date parameter is required when \\'\\n                                  \\'using CUSTOM period\\'))\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields,\\n            \\'period\\': period,\\n            \\'start_date\\': start_date,\\n            \\'end_date\\': end_date\\n        }\\n\\n        url = \\'%s/usages\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_pricing(self):\\n\\n        # Perform Request\\n        url = \\'%s/pricing\\' % (self.base_url)\\n\\n        r = requests.get(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # Recovery images\\n\\n    # \\'GET\\' Methods\\n\\n    def list_recovery_images(self, page=None, per_page=None, sort=None,\\n                             q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/recovery_appliances\\' % self.base_url\\n\\n        r = requests.get(url, headers=self.header, params=parameters)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def get_recovery_image(self, image_id=None):\\n\\n        # Error Handling\\n        if(image_id is None):\\n            raise ValueError(\\'appliance_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/recovery_appliances/%s\\' % (self.base_url, image_id)\\n\\n        r = requests.get(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # Server Appliance Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_appliances(self, page=None, per_page=None, sort=None,\\n                        q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/server_appliances\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_appliance(self, appliance_id=None):\\n\\n        # Error Handling\\n        if(appliance_id is None):\\n            raise ValueError(\\'appliance_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/server_appliances/%s\\' % (self.base_url, appliance_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # DVD Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_dvds(self, page=None, per_page=None, sort=None,\\n                  q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/dvd_isos\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_dvd(self, iso_id=None):\\n\\n        # Error Handling\\n        if(iso_id is None):\\n            raise ValueError(\\'iso_id parameter is required\\')\\n\\n        # Perform Request\\n        url = \\'%s/dvd_isos/%s\\' % (self.base_url, iso_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Datacenter Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_datacenters(self, page=None, per_page=None, sort=None,\\n                         q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/datacenters\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_datacenter(self, datacenter_id=None):\\n\\n        # Error Handling\\n        if(datacenter_id is None):\\n            raise ValueError(\\'datacenter_id parameter is required\\')\\n\\n        # Perform Request\\n        url = \\'%s/datacenters/%s\\' % (self.base_url, datacenter_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Pricing Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def pricing(self):\\n\\n        # Perform Request\\n        url = \\'%s/pricing\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Ping Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def ping(self):\\n\\n        # Perform Request\\n        url = \\'%s/ping\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Ping Auth Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def ping_auth(self):\\n\\n        # Perform Request\\n        url = \\'%s/ping_auth\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # VPN Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_vpns(self, page=None, per_page=None, sort=None, q=None,\\n                  fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/vpns\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_vpn(self, vpn_id=None):\\n\\n        # Error Handling\\n        if(vpn_id is None):\\n            raise ValueError(\\'vpn_id is a required parameter\\')\\n\\n        # Perform Request\\n\\n        url = \\'%s/vpns/%s\\' % (self.base_url, vpn_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def download_config(self, vpn_id=None, file_path=None):\\n        # Error Handling\\n        if(vpn_id is None):\\n            raise ValueError(\\'vpn_id is a required parameter\\')\\n\\n        # Perform Request\\n\\n        url = \\'%s/vpns/%s/configuration_file\\' % (self.base_url, vpn_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n                body = r.json()\\n                filestring = base64.b64decode(body[\"config_zip_file\"])\\n                zipPath = file_path + \\'.zip\\'\\n                with open(zipPath, \\'wb\\') as zipFile:\\n                    zipFile.write(filestring)\\n\\n                    return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_vpn(self, vpn=None):\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': vpn.name,\\n            \\'description\\': vpn.description,\\n            \\'datacenter_id\\': vpn.datacenter_id\\n        }\\n\\n        url = \\'%s/vpns\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            # Assign new image_id back to calling Image object\\n            response = r.json()\\n\\n            vpn.specs.update(vpn_id=response[\\'id\\'])\\n            vpn.specs.update(api_token=self.header)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_vpn(self, vpn_id=None):\\n\\n        # Error Handling\\n        if(vpn_id is None):\\n            raise ValueError(\\'vpn_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/vpns/%s\\' % (self.base_url, vpn_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_vpn(self, vpn_id=None, name=None, description=None):\\n\\n        # Error Handling\\n        if(vpn_id is None):\\n            raise ValueError(\\'vpn_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description\\n        }\\n\\n        url = \\'%s/vpns/%s\\' % (self.base_url, vpn_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Role Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_roles(self, page=None, per_page=None, sort=None, q=None,\\n                   fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/roles\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_role(self, role_id=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n\\n        url = \\'%s/roles/%s\\' % (self.base_url, role_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def permissions(self, role_id=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n\\n        url = \\'%s/roles/%s/permissions\\' % (self.base_url, role_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def current_user_permissions(self):\\n\\n        # Perform Request\\n\\n        url = \\'%s/users/current_user_permissions\\' % (self.base_url)\\n\\n        r = requests_retry_session().get(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def role_users(self, role_id=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n\\n        url = \\'%s/roles/%s/users\\' % (self.base_url, role_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def get_role_user(self, role_id=None, user_id=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/roles/%s/users/%s\\' % (self.base_url, role_id, user_id)\\n\\n        try:\\n            r = requests_retry_session().get(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'POST\\' Methods\\n\\n    def create_role(self, name=None):\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name\\n        }\\n\\n        url = \\'%s/roles\\' % self.base_url\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def add_users(self, role_id=None, users=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'users\\': users\\n        }\\n\\n        url = \\'%s/roles/%s/users\\' % (self.base_url, role_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def clone_role(self, role_id=None, name=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name\\n        }\\n\\n        url = \\'%s/roles/%s/clone\\' % (self.base_url, role_id)\\n\\n        try:\\n            r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_role(self, role_id=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/roles/%s\\' % (self.base_url, role_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def remove_user(self, role_id=None, user_id=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/roles/%s/users/%s\\' % (self.base_url, role_id, user_id)\\n\\n        try:\\n            r = requests_retry_session().delete(url, headers=self.header)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_role(self, role_id=None, name=None, description=None,\\n                    state=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'state\\': state\\n        }\\n\\n        url = \\'%s/roles/%s\\' % (self.base_url, role_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    def modify_permissions(\\n            self,\\n            role_id=None,\\n            servers=None,\\n            images=None,\\n            shared_storages=None,\\n            firewalls=None,\\n            load_balancers=None,\\n            ips=None,\\n            private_networks=None,\\n            vpns=None,\\n            monitoring_centers=None,\\n            monitoring_policies=None,\\n            backups=None,\\n            logs=None,\\n            users=None,\\n            roles=None,\\n            usages=None,\\n            interactive_invoices=None):\\n\\n        # Error Handling\\n        if(role_id is None):\\n            raise ValueError(\\'role_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'servers\\': servers,\\n            \\'images\\': images,\\n            \\'sharedstorages\\': shared_storages,\\n            \\'firewalls\\': firewalls,\\n            \\'loadbalancers\\': load_balancers,\\n            \\'ips\\': ips,\\n            \\'privatenetwork\\': private_networks,\\n            \\'vpn\\': vpns,\\n            \\'monitoringcenter\\': monitoring_centers,\\n            \\'monitoringpolicies\\': monitoring_policies,\\n            \\'backups\\': backups,\\n            \\'logs\\': logs,\\n            \\'users\\': users,\\n            \\'roles\\': roles,\\n            \\'usages\\': usages,\\n            \\'interactiveinvoice\\': interactive_invoices\\n        }\\n\\n        url = \\'%s/roles/%s/permissions\\' % (self.base_url, role_id)\\n\\n        try:\\n            r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n            # Handle Potential Response Errors\\n            if r.status_code not in self.success_codes:\\n                error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                                 (r.status_code, r.text))\\n                raise Exception(error_message)\\n\\n            return r.json()\\n        except http_client.HTTPException:\\n            if r is not None:\\n                error_message = (\\n                    \\'Error Code: %s. Error Message: %s. Response Headers :%s\\' %\\n                    (r.status_code, r.text, r.headers))\\n                raise Exception(error_message)\\n            else:\\n                raise\\n\\n    # Block Storage Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_block_storages(self, page=None, per_page=None, sort=None,\\n                            q=None, fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/block_storages\\' % self.base_url\\n\\n        r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def get_block_storage(self, block_storage_id=None):\\n\\n        # Error Handling\\n        if(block_storage_id is None):\\n            raise ValueError(\\'block_storage_id is a required parameter\\')\\n\\n        # Perform Request\\n        url = \\'%s/block_storages/%s\\' % (self.base_url, block_storage_id)\\n\\n        r = requests_retry_session().get(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # \\'POST\\' Methods\\n\\n    def create_block_storage(self, block_storage=None):\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': block_storage.name,\\n            \\'description\\': block_storage.description,\\n            \\'size\\': block_storage.size,\\n            \\'server\\': block_storage.server_id,\\n            \\'datacenter_id\\': block_storage.datacenter_id,\\n            \\'execution_group\\': block_storage.execution_group\\n        }\\n\\n        url = \\'%s/block_storages\\' % self.base_url\\n\\n        r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        # Assign new block_storage_id back to calling BlockStorage object\\n        response = r.json()\\n\\n        block_storage.specs.update(block_storage_id=response[\\'id\\'])\\n        block_storage.specs.update(api_token=self.header)\\n\\n        return r.json()\\n\\n    def attach_block_storage(self, block_storage_id=None,\\n                             server_id=None):\\n\\n        # Error Handling\\n        if(block_storage_id is None):\\n            raise ValueError(\\'block_storage_id is a required parameter\\')\\n        if(server_id is None):\\n            raise ValueError(\\'server_id is a required parameter.\\')\\n\\n        # Perform Request\\n        data = {\\'server\\': server_id}\\n\\n        url = (\\'%s/block_storages/%s/server\\' %\\n               (self.base_url, block_storage_id))\\n\\n        r = requests_retry_session().post(url, headers=self.header, json=data)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_block_storage(self, block_storage_id=None, name=None,\\n                             description=None, size=None):\\n\\n        # Error Handling\\n        if(block_storage_id is None):\\n            raise ValueError(\\'block_storage_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'size\\': size\\n        }\\n\\n        url = \\'%s/block_storages/%s\\' % (self.base_url, block_storage_id)\\n\\n        r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_block_storage(self, block_storage_id=None):\\n\\n        # Error Handling\\n        if(block_storage_id is None):\\n            raise ValueError(\\'block_storage_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/block_storages/%s\\' % (self.base_url, block_storage_id)\\n\\n        r = requests_retry_session().delete(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def detach_block_storage(self, block_storage_id=None):\\n\\n        # Error Handling\\n        if(block_storage_id is None):\\n            raise ValueError(\\'block_storage_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = (\\'%s/block_storages/%s/server\\' %\\n               (self.base_url, block_storage_id))\\n\\n        r = requests_retry_session().delete(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # Ssh Key Functions\\n\\n    # \\'GET\\' Methods\\n\\n    def list_ssh_keys(self, page=None, per_page=None, sort=None, q=None,\\n                      fields=None):\\n\\n        # Perform Request\\n        parameters = {\\n            \\'page\\': page,\\n            \\'per_page\\': per_page,\\n            \\'sort\\': sort,\\n            \\'q\\': q,\\n            \\'fields\\': fields\\n        }\\n\\n        url = \\'%s/ssh_keys\\' % self.base_url\\n\\n        r = requests_retry_session().get(url, headers=self.header, params=parameters)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def get_ssh_key(self, ssh_key_id=None):\\n\\n        # Error Handling\\n        if(ssh_key_id is None):\\n            raise ValueError(\\'ssh_key_id is a required parameter\\')\\n\\n        # Perform Request\\n\\n        url = \\'%s/ssh_keys/%s\\' % (self.base_url, ssh_key_id)\\n\\n        r = requests_retry_session().get(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # \\'POST\\' Methods\\n\\n    def create_ssh_key(self, ssh_key=None):\\n\\n        # Perform Request\\n        url = \\'%s/ssh_keys\\' % self.base_url\\n\\n        r = requests_retry_session().post(url, headers=self.header, json=ssh_key.specs)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        # Assign new ssh_key_id back to calling SshKey object\\n        response = r.json()\\n\\n        ssh_key.specs.update(ssh_key_id=response[\\'id\\'])\\n        ssh_key.specs.update(api_token=self.header)\\n\\n        return r.json()\\n\\n    # \\'DELETE\\' Methods\\n\\n    def delete_ssh_key(self, ssh_key_id=None):\\n\\n        # Error Handling\\n        if(ssh_key_id is None):\\n            raise ValueError(\\'ssh_key_id is a required parameter\\')\\n\\n        # Perform Request\\n        self.header[\\'content-type\\'] = \\'application/json\\'\\n\\n        url = \\'%s/ssh_keys/%s\\' % (self.base_url, ssh_key_id)\\n\\n        r = requests_retry_session().delete(url, headers=self.header)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    # \\'PUT\\' Methods\\n\\n    def modify_ssh_key(self, ssh_key_id=None, name=None, description=None):\\n\\n        # Error Handling\\n        if(ssh_key_id is None):\\n            raise ValueError(\\'ssh_key_id is a required parameter\\')\\n\\n        # Perform Request\\n        data = {\\n            \\'name\\': name,\\n            \\'description\\': description\\n        }\\n\\n        url = \\'%s/ssh_keys/%s\\' % (self.base_url, ssh_key_id)\\n\\n        r = requests_retry_session().put(url, headers=self.header, json=data)\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n\\n# Utility Classes\\n\\nclass Server(object):\\n\\n    # Init Function\\n    def __init__(\\n            self,\\n            name=None,\\n            description=None,\\n            fixed_instance_size_id=None,\\n            vcore=None,\\n            cores_per_processor=None,\\n            ram=None,\\n            appliance_id=None,\\n            password=None,\\n            power_on=None,\\n            firewall_policy_id=None,\\n            ip_id=None,\\n            load_balancer_id=None,\\n            monitoring_policy_id=None,\\n            datacenter_id=None,\\n            rsa_key=None,\\n            private_network_id=None,\\n            server_type=None,\\n            public_key=None,\\n            baremetal_model_id=None,\\n            ipv6_range=None,\\n            hostname=None,\\n            execution_group=None):\\n\\n        self.first_password = None\\n        self.first_ip = None\\n\\n        self.specs = {\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'hardware\\': {\\n                \\'fixed_instance_size_id\\': fixed_instance_size_id,\\n                \\'vcore\\': vcore,\\n                \\'cores_per_processor\\': cores_per_processor,\\n                \\'ram\\': ram,\\n                \\'baremetal_model_id\\': baremetal_model_id\\n            },\\n            \\'appliance_id\\': appliance_id,\\n            \\'password\\': password,\\n            \\'power_on\\': power_on,\\n            \\'firewall_policy_id\\': firewall_policy_id,\\n            \\'ip_id\\': ip_id,\\n            \\'load_balancer_id\\': load_balancer_id,\\n            \\'monitoring_policy_id\\': monitoring_policy_id,\\n            \\'datacenter_id\\': datacenter_id,\\n            \\'rsa_key\\': rsa_key,\\n            \\'private_network_id\\': private_network_id,\\n            \\'server_type\\': server_type,\\n            \\'public_key\\': public_key,\\n            \\'ipv6_range\\': ipv6_range,\\n            \\'hostname\\': hostname,\\n            \\'execution_group\\': execution_group\\n        }\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\n            \\'ACTIVE\\',\\n            \\'ENABLED\\',\\n            \\'POWERED_ON\\',\\n            \\'POWERED_OFF\\',\\n            \\'ON RECOVERY\\')\\n\\n    def __repr__(self):\\n        return (\\n            \\'Server: name=%s, description=%s, fixed_instance_size_id=%s, \\'\\n            \\'vcore=%s, cores_per_processor=%s, ram=%s, baremetal_model_id=%s, appliance_id=%s, \\'\\n            \\'password=%s, power_on=%s, firewall_policy_id=%s, ip_id=%s, \\'\\n            \\'load_balancer_id=%s, monitoring_policy_id=%s, \\'\\n            \\'rsa_key=%s, datacenter_id=%s, first_password=%s, \\'\\n            \\'first_ip=%s, public_key=%s, server_type=%s, ipv6_range=%s, execution_group=%s, hostname=%s\\' %\\n            (self.specs[\\'name\\'],\\n             self.specs[\\'description\\'],\\n             self.specs[\\'hardware\\'][\\'fixed_instance_size_id\\'],\\n             self.specs[\\'hardware\\'][\\'vcore\\'],\\n             self.specs[\\'hardware\\'][\\'cores_per_processor\\'],\\n             self.specs[\\'hardware\\'][\\'ram\\'],\\n             self.specs[\\'hardware\\'][\\'baremetal_model_id\\'],\\n             self.specs[\\'appliance_id\\'],\\n             self.specs[\\'password\\'],\\n             self.specs[\\'power_on\\'],\\n             self.specs[\\'firewall_policy_id\\'],\\n             self.specs[\\'ip_id\\'],\\n             self.specs[\\'load_balancer_id\\'],\\n             self.specs[\\'monitoring_policy_id\\'],\\n             self.specs[\\'rsa_key\\'],\\n             self.specs[\\'datacenter_id\\'],\\n             self.first_password,\\n             self.first_ip,\\n             self.specs[\\'server_type\\'],\\n             self.specs[\\'ipv6_range\\'],\\n             self.specs[\\'execution_group\\'],\\n             self.specs[\\'hostname\\'],\\n             ))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def hardware(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/hardware\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def hdds(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/hardware/hdds\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def image(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/image\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def ips(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/ips\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def status(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/status\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def dvd(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/dvd\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def private_networks(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/private_networks\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def snapshots(self):\\n\\n        # Perform Request\\n        url = (\\'%s/servers/%s/snapshots\\' %\\n               (self.base_url, self.specs[\\'server_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=15):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial server status\\n        url = \\'%s/servers/%s\\' % (self.base_url, self.specs[\\'server_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial server state and percent values\\n        server_state = response[\\'status\\'][\\'state\\']\\n        percent = response[\\'status\\'][\\'percent\\']\\n\\n        # Keep polling the server\\'s status until good\\n        while (server_state not in self.good_states) or (percent is not None):\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update server state and percent values\\n            server_state = response[\\'status\\'][\\'state\\']\\n            percent = response[\\'status\\'][\\'percent\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n            # Parse for first IP address\\n            if len(response[\\'ips\\']) == 1:\\n                self.first_ip = response[\\'ips\\'][0]\\n\\n        return {\\'duration\\': duration}\\n\\n    def wait_deleted(self, timeout=25, interval=15):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial server status\\n        url = \\'%s/servers/%s\\' % (self.base_url, self.specs[\\'server_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Keep polling the server\\'s status until got 404\\n        while r.status_code != 404 :\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n        return {\\'duration\\': duration}\\n\\n\\nclass Hdd(object):\\n\\n    # Init Function\\n    def __init__(self, size=None, is_main=None):\\n        self.specs = {\\n            \\'size\\': size,\\n            \\'is_main\\': is_main\\n        }\\n\\n    def __repr__(self):\\n        return (\\'HDD: size=%s, is_main=%s\\' %\\n                (self.specs[\\'size\\'], self.specs[\\'is_main\\']))\\n\\n\\nclass AttachServer(object):\\n\\n    # Init Function\\n    def __init__(self, server_id=None, rights=None, server_ip_id=None):\\n        self.server_id = server_id\\n        self.rights = rights\\n        self.server_ip_id = server_ip_id\\n\\n    def __repr__(self):\\n        return (\\'AttachServer: server_id=%s, rights=%s, server_ip_id=%s\\' %\\n                (self.server_id, self.rights, self.server_ip_id))\\n\\n\\nclass Image(object):\\n\\n    # Init Function\\n    def __init__(\\n            self,\\n            server_id=None,\\n            name=None,\\n            description=None,\\n            frequency=None,\\n            num_images=None,\\n            source=\\'server\\',\\n            url=None,\\n            os_id=None,\\n            isotype=None,\\n            type=None):\\n\\n        self.server_id = server_id\\n        self.name = name\\n        self.description = description\\n        self.frequency = frequency\\n        self.num_images = num_images\\n        self.source = source\\n        self.url = url\\n        self.os_id = os_id\\n        self.type = isotype\\n\\n        self.specs = {}\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\n            \\'Image: server_id=%s, name=%s, description=%s, \\'\\n            \\'frequency=%s, num_images=%s, source=%s, url=%s\\'\\n            \\'os_id=%s, type=%s\\' %\\n            (self.server_id,\\n             self.name,\\n             self.description,\\n             self.frequency,\\n             self.num_images,\\n             self.source,\\n             self.url,\\n             self.os_id,\\n             self.type))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/images/%s\\' %\\n               (self.base_url, self.specs[\\'image_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=15):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial image status\\n        url = \\'%s/images/%s\\' % (self.base_url, self.specs[\\'image_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial server state and percent values\\n        image_state = response[\\'state\\']\\n\\n        # Keep polling the server\\'s status until good\\n        while image_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update server state and percent values\\n            image_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n\\n\\nclass SharedStorage(object):\\n\\n    # Init Function\\n    def __init__(self, name=None, description=None, size=None,\\n                 datacenter_id=None):\\n\\n        self.name = name\\n        self.description = description\\n        self.size = size\\n        self.datacenter_id = datacenter_id\\n\\n        self.specs = {}\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\'Shared Storage: name=%s, description=%s, size=%s\\' %\\n                (self.name, self.description, self.size, self.datacenter_id))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/shared_storages/%s\\' %\\n               (self.base_url, self.specs[\\'shared_storage_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def servers(self):\\n\\n        # Perform Request\\n        url = (\\'%s/shared_storages/%s/servers\\' %\\n               (self.base_url, self.specs[\\'shared_storage_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=5):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial image status\\n        url = \\'%s/shared_storages/%s\\' % (self.base_url,\\n                                         self.specs[\\'shared_storage_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial server state and percent values\\n        shared_storage_state = response[\\'state\\']\\n\\n        # Keep polling the server\\'s status until good\\n        while shared_storage_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update server state and percent values\\n            shared_storage_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n\\n\\nclass FirewallPolicyRule(object):\\n\\n    # Init Function\\n    def __init__(self, protocol=None, port_from=None, port_to=None,\\n                 source=None, action=None, description=None, port=None):\\n\\n        self.rule_set = {\\n            \\'protocol\\': protocol,\\n            \\'port_from\\': port_from,\\n            \\'port_to\\': port_to,\\n            \\'source\\': source,\\n            \\'action\\': action,\\n            \\'description\\': description,\\n            \\'port\\': port\\n        }\\n\\n    def __repr__(self):\\n        return (\\'FirewallPolicyRule: protocol=%s, port_from=%s, \\'\\n                \\'port_to=%s, source=%s, action=%s, description=%s, port=%s\\' %\\n                (self.rule_set[\\'protocol\\'], self.rule_set[\\'port_from\\'],\\n                    self.rule_set[\\'port_to\\'], self.rule_set[\\'source\\'], self.rule_set[\\'action\\'], self.rule_set[\\'description\\'], self.rule_set[\\'port\\']))\\n\\n\\nclass FirewallPolicy(object):\\n\\n    # Init Function\\n    def __init__(self, name=None, description=None):\\n        self.specs = {\\n            \\'name\\': name,\\n            \\'description\\': description\\n        }\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\'FirewallPolicy: name=%s, description=%s\\' %\\n                (self.specs[\\'name\\'], self.specs[\\'description\\']))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/firewall_policies/%s\\' %\\n               (self.base_url, self.specs[\\'firewall_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def ips(self):\\n\\n        # Perform Request\\n        url = (\\'%s/firewall_policies/%s/server_ips\\' %\\n               (self.base_url, self.specs[\\'firewall_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def rules(self):\\n\\n        # Perform Request\\n        url = (\\'%s/firewall_policies/%s/rules\\' %\\n               (self.base_url, self.specs[\\'firewall_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=5):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial image status\\n        url = \\'%s/firewall_policies/%s\\' % (self.base_url,\\n                                           self.specs[\\'firewall_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial server state and percent values\\n        firewall_state = response[\\'state\\']\\n\\n        # Keep polling the server\\'s status until good\\n        while firewall_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update server state and percent values\\n            firewall_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n\\n\\nclass LoadBalancerRule(object):\\n\\n    # Init Function\\n    def __init__(self, protocol=None, port_balancer=None, port_server=None,\\n                 source=None):\\n\\n        self.rule_set = {\\n            \\'protocol\\': protocol,\\n            \\'port_balancer\\': port_balancer,\\n            \\'port_server\\': port_server,\\n            \\'source\\': source\\n        }\\n\\n    def __repr__(self):\\n        return (\\n            \\'LoadBalancerRule: protocol=%s, port_balancer=%s, \\'\\n            \\'port_server=%s, source=%s\\' %\\n            (self.rule_set[\\'protocol\\'],\\n             self.rule_set[\\'port_balancer\\'],\\n             self.rule_set[\\'port_server\\'],\\n             self.rule_set[\\'source\\']))\\n\\n\\nclass LoadBalancer(object):\\n\\n    # Init Function\\n    def __init__(self, health_check_path=None, health_check_parse=None,\\n                 name=None, description=None, health_check_test=None,\\n                 health_check_interval=None, persistence=None,\\n                 persistence_time=None, method=None, datacenter_id=None):\\n\\n        self.specs = {\\n            \\'health_check_path\\': health_check_path,\\n            \\'health_check_parse\\': health_check_parse,\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'health_check_test\\': health_check_test,\\n            \\'health_check_interval\\': health_check_interval,\\n            \\'persistence\\': persistence,\\n            \\'persistence_time\\': persistence_time,\\n            \\'method\\': method,\\n            \\'datacenter_id\\': datacenter_id\\n        }\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\'LoadBalancer: health_check_path=%s, health_check_parse=%s, \\'\\n                \\'name=%s, description=%s, health_check_test=%s, \\'\\n                \\'health_check_interval=%s, persistence=%s, \\'\\n                \\'persistence_time=%s, method=%s, datacenter_id=%s\\' %\\n                (self.specs[\\'health_check_path\\'],\\n                    self.specs[\\'health_check_parse\\'], self.specs[\\'name\\'],\\n                    self.specs[\\'description\\'], self.specs[\\'health_check_test\\'],\\n                    self.specs[\\'health_check_interval\\'],\\n                    self.specs[\\'persistence\\'], self.specs[\\'persistence_time\\'],\\n                    self.specs[\\'method\\'], self.datacenter_id))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/load_balancers/%s\\' %\\n               (self.base_url, self.specs[\\'load_balancer_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def ips(self):\\n\\n        # Perform Request\\n        url = (\\'%s/load_balancers/%s/server_ips\\' %\\n               (self.base_url, self.specs[\\'load_balancer_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def rules(self):\\n\\n        # Perform Request\\n        url = (\\'%s/load_balancers/%s/rules\\' %\\n               (self.base_url, self.specs[\\'load_balancer_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=5):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial image status\\n        url = \\'%s/load_balancers/%s\\' % (self.base_url,\\n                                        self.specs[\\'load_balancer_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial server state and percent values\\n        load_balancer_state = response[\\'state\\']\\n\\n        # Keep polling the server\\'s status until good\\n        while load_balancer_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update server state and percent values\\n            load_balancer_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n\\n\\nclass PrivateNetwork(object):\\n\\n    # Init Function\\n    def __init__(self, name=None, description=None, network_address=None,\\n                 subnet_mask=None, datacenter_id=None):\\n\\n        self.name = name\\n        self.description = description\\n        self.network_address = network_address\\n        self.subnet_mask = subnet_mask\\n        self.datacenter_id = datacenter_id\\n\\n        self.specs = {}\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\n            \\'Private Network: name=%s, description=%s, network_address=%s, \\'\\n            \\'subnet_mask=%s\\' %\\n            (self.name, self.description, self.network_address, self.subnet_mask))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/private_networks/%s\\' %\\n               (self.base_url, self.specs[\\'private_network_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def servers(self):\\n\\n        # Perform Request\\n        url = (\\'%s/private_networks/%s/servers\\' %\\n               (self.base_url, self.specs[\\'private_network_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=5):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial image status\\n        url = \\'%s/private_networks/%s\\' % (self.base_url,\\n                                          self.specs[\\'private_network_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial server state and percent values\\n        private_network_state = response[\\'state\\']\\n\\n        # Keep polling the server\\'s status until good\\n        while private_network_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update server state and percent values\\n            private_network_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n\\n\\nclass MonitoringPolicy(object):\\n\\n    # Init Function\\n    def __init__(self, name=None, description=None, email=None, agent=None):\\n        self.specs = {\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'email\\': email,\\n            \\'agent\\': agent\\n        }\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\'MonitoringPolicy: name=%s, description=%s, email=%s, \\'\\n                \\'agent=%s\\' %\\n                (self.specs[\\'name\\'], self.specs[\\'description\\'],\\n                    self.specs[\\'email\\'], self.specs[\\'agent\\']))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s\\' %\\n               (self.base_url, self.specs[\\'monitoring_policy_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def ports(self):\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/ports\\' %\\n               (self.base_url, self.specs[\\'monitoring_policy_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def processes(self):\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/processes\\' %\\n               (self.base_url, self.specs[\\'monitoring_policy_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def servers(self):\\n\\n        # Perform Request\\n        url = (\\'%s/monitoring_policies/%s/servers\\' %\\n               (self.base_url, self.specs[\\'monitoring_policy_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=5):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial image status\\n        url = \\'%s/monitoring_policies/%s\\' % (self.base_url,\\n                                             self.specs[\\'monitoring_policy_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial server state and percent values\\n        mp_state = response[\\'state\\']\\n\\n        # Keep polling the server\\'s status until good\\n        while mp_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update server state and percent values\\n            mp_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n\\n\\nclass Threshold(object):\\n\\n    # Init Function\\n    def __init__(self, entity=None, warning_value=None, warning_alert=None,\\n                 critical_value=None, critical_alert=None):\\n\\n        self.entity = entity\\n        self.warning_value = warning_value\\n        self.warning_alert = warning_alert\\n        self.critical_value = critical_value\\n        self.critical_alert = critical_alert\\n\\n    def __repr__(self):\\n        return (\\n            \\'Threshold: entity=%s, warning_value=%s, warning_alert=%s, \\'\\n            \\'critical_value=%s, critical_alert=%s\\' %\\n            (self.entity,\\n             self.warning_value,\\n             self.warning_alert,\\n             self.critical_value,\\n             self.critical_alert))\\n\\n\\nclass Port(object):\\n\\n    # Init Function\\n    def __init__(self, protocol=None, port=None, alert_if=None,\\n                 email_notification=None):\\n\\n        self.specs = {\\n            \\'protocol\\': protocol,\\n            \\'port\\': port,\\n            \\'alert_if\\': alert_if,\\n            \\'email_notification\\': email_notification\\n        }\\n\\n    def __repr__(self):\\n        return (\\n            \\'Port: protocol=%s, port=%s, alert_if=%s, \\'\\n            \\'email_notification=%s\\' %\\n            (self.specs[\\'protocol\\'],\\n             self.specs[\\'port\\'],\\n             self.specs[\\'alert_if\\'],\\n             self.specs[\\'email_notification\\']))\\n\\n\\nclass Process(object):\\n\\n    # Init Function\\n    def __init__(self, process=None, alert_if=None, email_notification=None):\\n        self.process_set = {\\n            \\'process\\': process,\\n            \\'alert_if\\': alert_if,\\n            \\'email_notification\\': email_notification\\n        }\\n\\n    def __repr__(self):\\n        return (\\'Process: process=%s, alert_if=%s, email_notification=%s\\' %\\n                (self.process_set[\\'process\\'], self.process_set[\\'alert_if\\'],\\n                    self.process_set[\\'email_notification\\']))\\n\\n\\nclass Vpn(object):\\n\\n    # Init Function\\n    def __init__(self, name=None, description=None, datacenter_id=None):\\n\\n        self.name = name\\n        self.description = description\\n        self.datacenter_id = datacenter_id\\n\\n        self.specs = {}\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\'Vpn: name=%s, description=%s, datacenter_id=%s\\' %\\n                (self.name, self.description, self.datacenter_id))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/vpns/%s\\' %\\n               (self.base_url, self.specs[\\'vpn_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=15):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial image status\\n        url = \\'%s/vpns/%s\\' % (self.base_url, self.specs[\\'vpn_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial server state and percent values\\n        vpn_state = response[\\'state\\']\\n\\n        # Keep polling the server\\'s status until good\\n        while vpn_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check server status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update server state and percent values\\n            vpn_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n\\n\\nclass BlockStorage(object):\\n\\n    # Init Function\\n    def __init__(self, name=None, description=None, size=None,\\n                 datacenter_id=None, server_id=None, execution_group=None):\\n\\n        self.name = name\\n        self.description = description\\n        self.size = size\\n        self.datacenter_id = datacenter_id\\n        self.server_id = server_id\\n        self.execution_group = execution_group\\n\\n        self.specs = {}\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\'Block Storage: name=%s, description=%s, size=%s, execution_group=%s, server_id=%s\\' % (\\n            self.name, self.description, self.size, self.datacenter_id, self.execution_group, self.server_id))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/block_storages/%s\\' %\\n               (self.base_url, self.specs[\\'block_storage_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def server(self):\\n\\n        # Perform Request\\n        url = (\\'%s/block_storages/%s/server\\' %\\n               (self.base_url, self.specs[\\'block_storage_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=5):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial block storage status\\n        url = \\'%s/block_storages/%s\\' % (self.base_url,\\n                                        self.specs[\\'block_storage_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial block storage state and percent values\\n        block_storage_state = response[\\'state\\']\\n\\n        # Keep polling the block storage\\'s status until good\\n        while block_storage_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check block storage status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update block storage state and percent values\\n            block_storage_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n\\n\\nclass SshKey(object):\\n\\n    # Init Function\\n    def __init__(self, name=None, description=None,\\n                 state=None, servers=None, md5=None,\\n                 public_key=None, creation_date=None):\\n\\n        self.specs = {\\n            \\'name\\': name,\\n            \\'description\\': description,\\n            \\'state\\': state,\\n            \\'servers\\': servers,\\n            \\'md5\\': md5,\\n            \\'public_key\\': public_key,\\n            \\'creation_date\\': creation_date\\n        }\\n\\n        self.base_url = \\'https://cloudpanel-api.1and1.com/v1\\'\\n        self.success_codes = (200, 201, 202)\\n        self.good_states = (\\'ACTIVE\\', \\'ENABLED\\', \\'POWERED_ON\\', \\'POWERED_OFF\\')\\n\\n    def __repr__(self):\\n        return (\\'SshKey: name=%s, description=%s, \\'\\n                \\'state=%s, servers=%s, md5=%s, \\'\\n                \\'public_key=%s, creation_date=%s, \\' %\\n                (self.specs[\\'name\\'], self.specs[\\'description\\'],\\n                 self.specs[\\'state\\'], self.specs[\\'servers\\'],\\n                 self.specs[\\'md5\\'], self.specs[\\'public_key\\'],\\n                 self.specs[\\'creation_date\\']))\\n\\n    def get(self):\\n\\n        # Perform Request\\n        url = (\\'%s/ssh_keys/%s\\' %\\n               (self.base_url, self.specs[\\'ssh_key_id\\']))\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n\\n        # Handle Potential Response Errors\\n        if r.status_code not in self.success_codes:\\n            error_message = (\\'Error Code: %s. Error Message: %s.\\' %\\n                             (r.status_code, r.text))\\n            raise Exception(error_message)\\n\\n        return r.json()\\n\\n    def wait_for(self, timeout=25, interval=5):\\n\\n        # Capture start time\\n        start = time.time()\\n        duration = 0\\n\\n        # Check initial ssh_key status\\n        url = \\'%s/ssh_keys/%s\\' % (self.base_url,\\n                                  self.specs[\\'ssh_key_id\\'])\\n\\n        r = requests_retry_session().get(url, headers=self.specs[\\'api_token\\'])\\n        response = r.json()\\n\\n        # Store initial ssh_key state and percent values\\n        ssh_key_state = response[\\'state\\']\\n\\n        # Keep polling the ssh_key\\'s status until good\\n        while ssh_key_state not in self.good_states:\\n\\n            # Wait 15 seconds before polling again\\n            time.sleep(interval)\\n\\n            # Check ssh_key status again\\n            r = requests_retry_session().get(\\n                url, headers=self.specs[\\'api_token\\'])\\n            response = r.json()\\n\\n            # Update ssh_key state and percent values\\n            ssh_key_state = response[\\'state\\']\\n\\n            # Check for timeout\\n            seconds = (time.time() - start)\\n            duration = seconds / 60\\n            if duration > timeout:\\n                print(\\'The operation timed out after %s minutes.\\' % timeout)\\n                return\\n\\n        return {\\'duration\\': duration}\\n',\n",
       " '\"\"\"\\nTests for DragonflyBackend class.\\n\"\"\"\\n\\nfrom tuun.backend import DragonflyBackend\\n\\n\\ndef test_initialize():\\n    \"\"\"Test initialize DragonflyBackend.\"\"\"\\n    domain_config = {\\'name\\': \\'real\\', \\'min_max\\': [[-5, 5]]}\\n    opt_config = {\\'name\\': \\'real\\'}\\n    dragonfly_config = {\\'acq_str\\': \\'ucb-ei\\', \\'n_init_rs\\': 0}\\n    db = DragonflyBackend(domain_config, opt_config, dragonfly_config)\\n    assert getattr(db, \\'domain_config\\', None)\\n\\ndef test_suggest_to_minimize():\\n    \"\"\"Test DragonflyBackend suggest_to_minimize on a dataset.\"\"\"\\n    domain_config = {\\'name\\': \\'real\\', \\'min_max\\': [[0.0, 2.0]]}\\n    opt_config = {\\'name\\': \\'real\\'}\\n    dragonfly_config = {\\'acq_str\\': \\'ucb-ei\\', \\'n_init_rs\\': 0}\\n    db = DragonflyBackend(domain_config, opt_config, dragonfly_config)\\n\\n    data = {\\n        \\'x\\': [[0.5], [1.0], [1.5]],\\n        \\'y\\': [6.0, 1.0, 4.0],\\n    }\\n\\n    suggestion = db.suggest_to_minimize(data)\\n    assert 0.75 < suggestion[0] < 1.25\\n',\n",
       " '# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nimport sys\\nsys.path.append(\\'../\\')\\n\\nfrom auto_scan_test import AutoScanTest, IgnoreReasons\\nfrom program_config import TensorConfig, ProgramConfig, OpConfig, CxxConfig, TargetType, PrecisionType, DataLayoutType, Place\\nimport unittest\\n\\nimport hypothesis\\nfrom hypothesis import given, settings, seed, example, assume\\nimport hypothesis.strategies as st\\nimport argparse\\nimport numpy as np\\nfrom functools import partial\\nimport copy\\n\\n\\nclass TestInverseOp(AutoScanTest):\\n    def __init__(self, *args, **kwargs):\\n        AutoScanTest.__init__(self, *args, **kwargs)\\n        self.enable_testing_on_place(\\n            TargetType.Host,\\n            PrecisionType.FP32,\\n            DataLayoutType.NCHW,\\n            thread=[1, 2])\\n\\n    def is_program_valid(self,\\n                         program_config: ProgramConfig,\\n                         predictor_config: CxxConfig) -> bool:\\n        return True\\n\\n    def sample_program_configs(self, draw):\\n        in_shape = draw(\\n            st.lists(\\n                st.integers(\\n                    min_value=1, max_value=32), min_size=1, max_size=2))\\n\\n        def generate_input(*args, **kwargs):\\n            last_dim = np.random.randint(\\n                low=1, high=16, size=[1]).astype(np.int32)\\n            input_dim = copy.deepcopy(in_shape)\\n            input_dim.append(last_dim[0])  #last 2 dim must be equal\\n            input_dim.append(last_dim[0])\\n            return np.random.random(input_dim).astype(np.float32)\\n\\n        build_ops = OpConfig(\\n            type=\"inverse\",\\n            inputs={\"Input\": [\"input_data\"], },\\n            outputs={\"Output\": [\"output_data\"], },\\n            attrs={})\\n\\n        program_config = ProgramConfig(\\n            ops=[build_ops],\\n            weights={},\\n            inputs={\\n                \"input_data\": TensorConfig(data_gen=partial(generate_input)),\\n            },\\n            outputs=[\"output_data\"])\\n        return program_config\\n\\n    def sample_predictor_configs(self):\\n        return self.get_predictor_configs(), [\"inverse\"], (5e-5, 5e-5)\\n\\n    def add_ignore_pass_case(self):\\n        pass\\n\\n    def test(self, *args, **kwargs):\\n        self.run_and_statis(quant=False, max_examples=25)\\n\\n\\nif __name__ == \"__main__\":\\n    unittest.main(argv=[\\'\\'])\\n',\n",
       " 'import re\\n\\nimport sqlparse\\n\\nfrom django.db.backends.base.introspection import (\\n    BaseDatabaseIntrospection, FieldInfo, TableInfo,\\n)\\nfrom django.db.models.indexes import Index\\n\\nfield_size_re = re.compile(r\\'^\\\\s*(?:var)?char\\\\s*\\\\(\\\\s*(\\\\d+)\\\\s*\\\\)\\\\s*$\\')\\n\\n\\ndef get_field_size(name):\\n    \"\"\" Extract the size number from a \"varchar(11)\" type name \"\"\"\\n    m = field_size_re.search(name)\\n    return int(m.group(1)) if m else None\\n\\n\\n# This light wrapper \"fakes\" a dictionary interface, because some SQLite data\\n# types include variables in them -- e.g. \"varchar(30)\" -- and can\\'t be matched\\n# as a simple dictionary lookup.\\nclass FlexibleFieldLookupDict:\\n    # Maps SQL types to Django Field types. Some of the SQL types have multiple\\n    # entries here because SQLite allows for anything and doesn\\'t normalize the\\n    # field type; it uses whatever was given.\\n    base_data_types_reverse = {\\n        \\'bool\\': \\'BooleanField\\',\\n        \\'boolean\\': \\'BooleanField\\',\\n        \\'smallint\\': \\'SmallIntegerField\\',\\n        \\'smallint unsigned\\': \\'PositiveSmallIntegerField\\',\\n        \\'smallinteger\\': \\'SmallIntegerField\\',\\n        \\'int\\': \\'IntegerField\\',\\n        \\'integer\\': \\'IntegerField\\',\\n        \\'bigint\\': \\'BigIntegerField\\',\\n        \\'integer unsigned\\': \\'PositiveIntegerField\\',\\n        \\'decimal\\': \\'DecimalField\\',\\n        \\'real\\': \\'FloatField\\',\\n        \\'text\\': \\'TextField\\',\\n        \\'char\\': \\'CharField\\',\\n        \\'blob\\': \\'BinaryField\\',\\n        \\'date\\': \\'DateField\\',\\n        \\'datetime\\': \\'DateTimeField\\',\\n        \\'time\\': \\'TimeField\\',\\n    }\\n\\n    def __getitem__(self, key):\\n        key = key.lower()\\n        try:\\n            return self.base_data_types_reverse[key]\\n        except KeyError:\\n            size = get_field_size(key)\\n            if size is not None:\\n                return (\\'CharField\\', {\\'max_length\\': size})\\n            raise KeyError\\n\\n\\nclass DatabaseIntrospection(BaseDatabaseIntrospection):\\n    data_types_reverse = FlexibleFieldLookupDict()\\n\\n    def get_table_list(self, cursor):\\n        \"\"\"Return a list of table and view names in the current database.\"\"\"\\n        # Skip the sqlite_sequence system table used for autoincrement key\\n        # generation.\\n        cursor.execute(\"\"\"\\n            SELECT name, type FROM sqlite_master\\n            WHERE type in (\\'table\\', \\'view\\') AND NOT name=\\'sqlite_sequence\\'\\n            ORDER BY name\"\"\")\\n        return [TableInfo(row[0], row[1][0]) for row in cursor.fetchall()]\\n\\n    def get_table_description(self, cursor, table_name):\\n        \"\"\"\\n        Return a description of the table with the DB-API cursor.description\\n        interface.\\n        \"\"\"\\n        return [\\n            FieldInfo(\\n                info[\\'name\\'],\\n                info[\\'type\\'],\\n                None,\\n                info[\\'size\\'],\\n                None,\\n                None,\\n                info[\\'null_ok\\'],\\n                info[\\'default\\'],\\n            ) for info in self._table_info(cursor, table_name)\\n        ]\\n\\n    def get_sequences(self, cursor, table_name, table_fields=()):\\n        pk_col = self.get_primary_key_column(cursor, table_name)\\n        return [{\\'table\\': table_name, \\'column\\': pk_col}]\\n\\n    def get_relations(self, cursor, table_name):\\n        \"\"\"\\n        Return a dictionary of {field_name: (field_name_other_table, other_table)}\\n        representing all relationships to the given table.\\n        \"\"\"\\n        # Dictionary of relations to return\\n        relations = {}\\n\\n        # Schema for this table\\n        cursor.execute(\\n            \"SELECT sql, type FROM sqlite_master \"\\n            \"WHERE tbl_name = %s AND type IN (\\'table\\', \\'view\\')\",\\n            [table_name]\\n        )\\n        create_sql, table_type = cursor.fetchone()\\n        if table_type == \\'view\\':\\n            # It might be a view, then no results will be returned\\n            return relations\\n        results = create_sql[create_sql.index(\\'(\\') + 1:create_sql.rindex(\\')\\')]\\n\\n        # Walk through and look for references to other tables. SQLite doesn\\'t\\n        # really have enforced references, but since it echoes out the SQL used\\n        # to create the table we can look for REFERENCES statements used there.\\n        for field_desc in results.split(\\',\\'):\\n            field_desc = field_desc.strip()\\n            if field_desc.startswith(\"UNIQUE\"):\\n                continue\\n\\n            m = re.search(r\\'references (\\\\S*) ?\\\\([\"|]?(.*)[\"|]?\\\\)\\', field_desc, re.I)\\n            if not m:\\n                continue\\n            table, column = [s.strip(\\'\"\\') for s in m.groups()]\\n\\n            if field_desc.startswith(\"FOREIGN KEY\"):\\n                # Find name of the target FK field\\n                m = re.match(r\\'FOREIGN KEY\\\\s*\\\\(([^\\\\)]*)\\\\).*\\', field_desc, re.I)\\n                field_name = m.groups()[0].strip(\\'\"\\')\\n            else:\\n                field_name = field_desc.split()[0].strip(\\'\"\\')\\n\\n            cursor.execute(\"SELECT sql FROM sqlite_master WHERE tbl_name = %s\", [table])\\n            result = cursor.fetchall()[0]\\n            other_table_results = result[0].strip()\\n            li, ri = other_table_results.index(\\'(\\'), other_table_results.rindex(\\')\\')\\n            other_table_results = other_table_results[li + 1:ri]\\n\\n            for other_desc in other_table_results.split(\\',\\'):\\n                other_desc = other_desc.strip()\\n                if other_desc.startswith(\\'UNIQUE\\'):\\n                    continue\\n\\n                other_name = other_desc.split(\\' \\', 1)[0].strip(\\'\"\\')\\n                if other_name == column:\\n                    relations[field_name] = (other_name, table)\\n                    break\\n\\n        return relations\\n\\n    def get_key_columns(self, cursor, table_name):\\n        \"\"\"\\n        Return a list of (column_name, referenced_table_name, referenced_column_name)\\n        for all key columns in given table.\\n        \"\"\"\\n        key_columns = []\\n\\n        # Schema for this table\\n        cursor.execute(\"SELECT sql FROM sqlite_master WHERE tbl_name = %s AND type = %s\", [table_name, \"table\"])\\n        results = cursor.fetchone()[0].strip()\\n        results = results[results.index(\\'(\\') + 1:results.rindex(\\')\\')]\\n\\n        # Walk through and look for references to other tables. SQLite doesn\\'t\\n        # really have enforced references, but since it echoes out the SQL used\\n        # to create the table we can look for REFERENCES statements used there.\\n        for field_index, field_desc in enumerate(results.split(\\',\\')):\\n            field_desc = field_desc.strip()\\n            if field_desc.startswith(\"UNIQUE\"):\\n                continue\\n\\n            m = re.search(r\\'\"(.*)\".*references (.*) \\\\([\"|](.*)[\"|]\\\\)\\', field_desc, re.I)\\n            if not m:\\n                continue\\n\\n            # This will append (column_name, referenced_table_name, referenced_column_name) to key_columns\\n            key_columns.append(tuple(s.strip(\\'\"\\') for s in m.groups()))\\n\\n        return key_columns\\n\\n    def get_primary_key_column(self, cursor, table_name):\\n        \"\"\"Return the column name of the primary key for the given table.\"\"\"\\n        # Don\\'t use PRAGMA because that causes issues with some transactions\\n        cursor.execute(\\n            \"SELECT sql, type FROM sqlite_master \"\\n            \"WHERE tbl_name = %s AND type IN (\\'table\\', \\'view\\')\",\\n            [table_name]\\n        )\\n        row = cursor.fetchone()\\n        if row is None:\\n            raise ValueError(\"Table %s does not exist\" % table_name)\\n        create_sql, table_type = row\\n        if table_type == \\'view\\':\\n            # Views don\\'t have a primary key.\\n            return None\\n        fields_sql = create_sql[create_sql.index(\\'(\\') + 1:create_sql.rindex(\\')\\')]\\n        for field_desc in fields_sql.split(\\',\\'):\\n            field_desc = field_desc.strip()\\n            m = re.match(r\\'(?:(?:[\"`\\\\[])(.*)(?:[\"`\\\\]])|(\\\\w+)).*PRIMARY KEY.*\\', field_desc)\\n            if m:\\n                return m.group(1) if m.group(1) else m.group(2)\\n        return None\\n\\n    def _table_info(self, cursor, name):\\n        cursor.execute(\\'PRAGMA table_info(%s)\\' % self.connection.ops.quote_name(name))\\n        # cid, name, type, notnull, default_value, pk\\n        return [{\\n            \\'name\\': field[1],\\n            \\'type\\': field[2],\\n            \\'size\\': get_field_size(field[2]),\\n            \\'null_ok\\': not field[3],\\n            \\'default\\': field[4],\\n            \\'pk\\': field[5],  # undocumented\\n        } for field in cursor.fetchall()]\\n\\n    def _get_foreign_key_constraints(self, cursor, table_name):\\n        constraints = {}\\n        cursor.execute(\\'PRAGMA foreign_key_list(%s)\\' % self.connection.ops.quote_name(table_name))\\n        for row in cursor.fetchall():\\n            # Remaining on_update/on_delete/match values are of no interest.\\n            id_, _, table, from_, to = row[:5]\\n            constraints[\\'fk_%d\\' % id_] = {\\n                \\'columns\\': [from_],\\n                \\'primary_key\\': False,\\n                \\'unique\\': False,\\n                \\'foreign_key\\': (table, to),\\n                \\'check\\': False,\\n                \\'index\\': False,\\n            }\\n        return constraints\\n\\n    def get_constraints(self, cursor, table_name):\\n        \"\"\"\\n        Retrieve any constraints or keys (unique, pk, fk, check, index) across\\n        one or more columns.\\n        \"\"\"\\n        constraints = {}\\n        # Find inline check constraints.\\n        try:\\n            table_schema = cursor.execute(\\n                \"SELECT sql FROM sqlite_master WHERE type=\\'table\\' and name=%s\" % (\\n                    self.connection.ops.quote_name(table_name),\\n                )\\n            ).fetchone()[0]\\n        except TypeError:\\n            # table_name is a view.\\n            pass\\n        else:\\n            # Check constraint parsing is based of SQLite syntax diagram.\\n            # https://www.sqlite.org/syntaxdiagrams.html#table-constraint\\n            def next_ttype(ttype):\\n                for token in tokens:\\n                    if token.ttype == ttype:\\n                        return token\\n\\n            statement = sqlparse.parse(table_schema)[0]\\n            tokens = statement.flatten()\\n            for token in tokens:\\n                name = None\\n                if token.match(sqlparse.tokens.Keyword, \\'CONSTRAINT\\'):\\n                    # Table constraint\\n                    name_token = next_ttype(sqlparse.tokens.Literal.String.Symbol)\\n                    name = name_token.value[1:-1]\\n                    token = next_ttype(sqlparse.tokens.Keyword)\\n                if token.match(sqlparse.tokens.Keyword, \\'UNIQUE\\'):\\n                    constraints[name] = {\\n                        \\'unique\\': True,\\n                        \\'columns\\': [],\\n                        \\'primary_key\\': False,\\n                        \\'foreign_key\\': False,\\n                        \\'check\\': False,\\n                        \\'index\\': False,\\n                    }\\n                if token.match(sqlparse.tokens.Keyword, \\'CHECK\\'):\\n                    # Column check constraint\\n                    if name is None:\\n                        column_token = next_ttype(sqlparse.tokens.Literal.String.Symbol)\\n                        column = column_token.value[1:-1]\\n                        name = \\'__check__%s\\' % column\\n                        columns = [column]\\n                    else:\\n                        columns = []\\n                    constraints[name] = {\\n                        \\'check\\': True,\\n                        \\'columns\\': columns,\\n                        \\'primary_key\\': False,\\n                        \\'unique\\': False,\\n                        \\'foreign_key\\': False,\\n                        \\'index\\': False,\\n                    }\\n        # Get the index info\\n        cursor.execute(\"PRAGMA index_list(%s)\" % self.connection.ops.quote_name(table_name))\\n        for row in cursor.fetchall():\\n            # Sqlite3 3.8.9+ has 5 columns, however older versions only give 3\\n            # columns. Discard last 2 columns if there.\\n            number, index, unique = row[:3]\\n            # Get the index info for that index\\n            cursor.execute(\\'PRAGMA index_info(%s)\\' % self.connection.ops.quote_name(index))\\n            for index_rank, column_rank, column in cursor.fetchall():\\n                if index not in constraints:\\n                    constraints[index] = {\\n                        \"columns\": [],\\n                        \"primary_key\": False,\\n                        \"unique\": bool(unique),\\n                        \"foreign_key\": False,\\n                        \"check\": False,\\n                        \"index\": True,\\n                    }\\n                constraints[index][\\'columns\\'].append(column)\\n            # Add type and column orders for indexes\\n            if constraints[index][\\'index\\'] and not constraints[index][\\'unique\\']:\\n                # SQLite doesn\\'t support any index type other than b-tree\\n                constraints[index][\\'type\\'] = Index.suffix\\n                cursor.execute(\\n                    \"SELECT sql FROM sqlite_master \"\\n                    \"WHERE type=\\'index\\' AND name=%s\" % self.connection.ops.quote_name(index)\\n                )\\n                orders = []\\n                # There would be only 1 row to loop over\\n                for sql, in cursor.fetchall():\\n                    order_info = sql.split(\\'(\\')[-1].split(\\')\\')[0].split(\\',\\')\\n                    orders = [\\'DESC\\' if info.endswith(\\'DESC\\') else \\'ASC\\' for info in order_info]\\n                constraints[index][\\'orders\\'] = orders\\n        # Get the PK\\n        pk_column = self.get_primary_key_column(cursor, table_name)\\n        if pk_column:\\n            # SQLite doesn\\'t actually give a name to the PK constraint,\\n            # so we invent one. This is fine, as the SQLite backend never\\n            # deletes PK constraints by name, as you can\\'t delete constraints\\n            # in SQLite; we remake the table with a new PK instead.\\n            constraints[\"__primary__\"] = {\\n                \"columns\": [pk_column],\\n                \"primary_key\": True,\\n                \"unique\": False,  # It\\'s not actually a unique constraint.\\n                \"foreign_key\": False,\\n                \"check\": False,\\n                \"index\": False,\\n            }\\n        constraints.update(self._get_foreign_key_constraints(cursor, table_name))\\n        return constraints\\n',\n",
       " 'class Solution:\\n    def minSwaps(self, nums: List[int]) -> int:\\n        ones, N = sum(nums), len(nums)\\n        min_swap = s = ones - sum(nums[:ones])\\n        for i in range(N):\\n            s += nums[i] - nums[(i + ones) % N]\\n            min_swap = min(s, min_swap)\\n        return min_swap',\n",
       " '\"\"\"Module for the LoadEnvironment class.\"\"\"\\n\\nimport os\\nimport sys\\nfrom pathlib import Path\\n\\n\\nclass LoadEnvironment:\\n    \"\"\"This class is used for loading the environment from .env and .env.* files.\"\"\"\\n\\n    def __init__(self, environment=None, override=True, only=None):\\n        \"\"\"LoadEnvironment constructor.\\n\\n        Keyword Arguments:\\n            env {string} -- An additional environment file that you want to load. (default: {None})\\n            override {bool} -- Whether or not the environment variables found should overwrite existing ones. (default: {False})\\n            only {string} -- If this is set then it will only load that environment. (default: {None})\\n        \"\"\"\\n        from dotenv import load_dotenv\\n\\n        self.env = load_dotenv\\n\\n        if only:\\n            self._load_environment(only, override=override)\\n            return\\n\\n        env_path = str(Path(\".\") / \".env\")\\n        self.env(env_path, override=override)\\n\\n        if os.environ.get(\"APP_ENV\"):\\n            self._load_environment(os.environ.get(\"APP_ENV\"), override=override)\\n        if environment:\\n            self._load_environment(environment, override=override)\\n\\n        if \"pytest\" in sys.modules:\\n            self._load_environment(\"testing\", override=override)\\n\\n    def _load_environment(self, environment, override=False):\\n        \"\"\"Load the environment depending on the env file.\\n\\n        Arguments:\\n            environment {string} -- Name of the environment file to load from\\n\\n        Keyword Arguments:\\n            override {bool} -- Whether the environment file should overwrite existing environment keys. (default: {False})\\n        \"\"\"\\n        env_path = str(Path(\".\") / \".env.{}\".format(environment))\\n        self.env(dotenv_path=env_path, override=override)\\n\\n\\ndef env(value, default=\"\", cast=True):\\n    env_var = os.getenv(value, default)\\n\\n    if not cast:\\n        return env_var\\n\\n    if env_var == \"\":\\n        env_var = default\\n\\n    if isinstance(env_var, bool):\\n        return env_var\\n    elif env_var is None:\\n        return None\\n    elif env_var.isnumeric():\\n        return int(env_var)\\n    elif env_var in (\"false\", \"False\"):\\n        return False\\n    elif env_var in (\"true\", \"True\"):\\n        return True\\n    else:\\n        return env_var\\n',\n",
       " '\\nfrom pl_bolts.transforms.self_supervised import RandomTranslateWithReflect, Patchify\\n\\ntry:\\n    from torchvision import transforms\\nexcept ImportError:\\n    warn(\\'You want to use `torchvision` which is not installed yet,\\'  # pragma: no-cover\\n                      \\' install it with `pip install torchvision`.\\')\\n\\n\\nclass CPCTrainTransformsCIFAR10:\\n\\n    def __init__(self, patch_size=8, overlap=4):\\n        \"\"\"\\n        Transforms used for CPC:\\n\\n        Args:\\n\\n            patch_size: size of patches when cutting up the image into overlapping patches\\n            overlap: how much to overlap patches\\n\\n        Transforms::\\n\\n            random_flip\\n            img_jitter\\n            col_jitter\\n            rnd_gray\\n            transforms.ToTensor()\\n            normalize\\n            Patchify(patch_size=patch_size, overlap_size=patch_size // 2)\\n\\n        Example::\\n\\n            # in a regular dataset\\n            CIFAR10(..., transforms=CPCTrainTransformsCIFAR10())\\n\\n            # in a DataModule\\n            module = CIFAR10DataModule(PATH)\\n            train_loader = module.train_dataloader(batch_size=32, transforms=CPCTrainTransformsCIFAR10())\\n\\n        \"\"\"\\n        self.patch_size = patch_size\\n        self.overlap = overlap\\n        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)\\n\\n        normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\\n                                         std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\\n        col_jitter = transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.2)], p=0.8)\\n        img_jitter = transforms.RandomApply([RandomTranslateWithReflect(4)], p=0.8)\\n        rnd_gray = transforms.RandomGrayscale(p=0.25)\\n\\n        self.transforms = transforms.Compose([\\n            img_jitter,\\n            col_jitter,\\n            rnd_gray,\\n            transforms.ToTensor(),\\n            normalize,\\n            Patchify(patch_size=patch_size, overlap_size=overlap),\\n        ])\\n\\n    def __call__(self, inp):\\n        inp = self.flip_lr(inp)\\n        out1 = self.transforms(inp)\\n        return out1\\n\\n\\nclass CPCEvalTransformsCIFAR10:\\n\\n    def __init__(self, patch_size=8, overlap=4):\\n        \"\"\"\\n        Transforms used for CPC:\\n\\n        Args:\\n\\n            patch_size: size of patches when cutting up the image into overlapping patches\\n            overlap: how much to overlap patches\\n\\n        Transforms::\\n\\n            random_flip\\n            transforms.ToTensor()\\n            normalize\\n            Patchify(patch_size=patch_size, overlap_size=overlap)\\n\\n        Example::\\n\\n            # in a regular dataset\\n            CIFAR10(..., transforms=CPCEvalTransformsCIFAR10())\\n\\n            # in a DataModule\\n            module = CIFAR10DataModule(PATH)\\n            train_loader = module.train_dataloader(batch_size=32, transforms=CPCEvalTransformsCIFAR10())\\n\\n        \"\"\"\\n\\n        # flipping image along vertical axis\\n        self.patch_size = patch_size\\n        self.overlap = overlap\\n        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)\\n\\n        normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\\n                                         std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\\n\\n        self.transforms = transforms.Compose([\\n            transforms.ToTensor(),\\n            normalize,\\n            Patchify(patch_size=patch_size, overlap_size=overlap),\\n        ])\\n\\n    def __call__(self, inp):\\n        out1 = self.transforms(inp)\\n        return out1\\n\\n\\nclass CPCTrainTransformsSTL10:\\n\\n    def __init__(self, patch_size=16, overlap=8):\\n        \"\"\"\\n        Transforms used for CPC:\\n\\n        Args:\\n\\n            patch_size: size of patches when cutting up the image into overlapping patches\\n            overlap: how much to overlap patches\\n\\n        Transforms::\\n\\n            random_flip\\n            img_jitter\\n            col_jitter\\n            rnd_gray\\n            transforms.ToTensor()\\n            normalize\\n            Patchify(patch_size=patch_size, overlap_size=patch_size // 2)\\n\\n        Example::\\n\\n            # in a regular dataset\\n            STL10(..., transforms=CPCTrainTransformsSTL10())\\n\\n            # in a DataModule\\n            module = STL10DataModule(PATH)\\n            train_loader = module.train_dataloader(batch_size=32, transforms=CPCTrainTransformsSTL10())\\n\\n\\n        \"\"\"\\n        # flipping image along vertical axis\\n        self.patch_size = patch_size\\n        self.overlap = overlap\\n        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)\\n        normalize = transforms.Normalize(mean=(0.43, 0.42, 0.39), std=(0.27, 0.26, 0.27))\\n\\n        # image augmentation functions\\n        col_jitter = transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.2)], p=0.8)\\n        rnd_gray = transforms.RandomGrayscale(p=0.25)\\n        rand_crop = transforms.RandomResizedCrop(64, scale=(0.3, 1.0), ratio=(0.7, 1.4), interpolation=3)\\n\\n        self.transforms = transforms.Compose([\\n            rand_crop,\\n            col_jitter,\\n            rnd_gray,\\n            transforms.ToTensor(),\\n            normalize,\\n            Patchify(patch_size=patch_size, overlap_size=overlap)\\n        ])\\n\\n    def __call__(self, inp):\\n        inp = self.flip_lr(inp)\\n        out1 = self.transforms(inp)\\n        return out1\\n\\n\\nclass CPCEvalTransformsSTL10:\\n\\n    def __init__(self, patch_size=16, overlap=8):\\n        \"\"\"\\n        Transforms used for CPC:\\n\\n        Args:\\n\\n            patch_size: size of patches when cutting up the image into overlapping patches\\n            overlap: how much to overlap patches\\n\\n        Transforms::\\n\\n            random_flip\\n            transforms.ToTensor()\\n            normalize\\n            Patchify(patch_size=patch_size, overlap_size=patch_size // 2)\\n\\n        Example::\\n\\n            # in a regular dataset\\n            STL10(..., transforms=CPCEvalTransformsSTL10())\\n\\n            # in a DataModule\\n            module = STL10DataModule(PATH)\\n            train_loader = module.train_dataloader(batch_size=32, transforms=CPCEvalTransformsSTL10())\\n\\n        \"\"\"\\n        # flipping image along vertical axis\\n        self.patch_size = patch_size\\n        self.overlap = overlap\\n        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)\\n        normalize = transforms.Normalize(mean=(0.43, 0.42, 0.39), std=(0.27, 0.26, 0.27))\\n\\n        self.transforms = transforms.Compose([\\n            transforms.Resize(70, interpolation=3),\\n            transforms.CenterCrop(64),\\n            transforms.ToTensor(),\\n            normalize,\\n            Patchify(patch_size=patch_size, overlap_size=overlap)\\n        ])\\n\\n    def __call__(self, inp):\\n        out1 = self.transforms(inp)\\n        return out1\\n\\n\\nclass CPCTrainTransformsImageNet128:\\n    def __init__(self, patch_size=32, overlap=16):\\n        \"\"\"\\n        Transforms used for CPC:\\n\\n        Args:\\n\\n            patch_size: size of patches when cutting up the image into overlapping patches\\n            overlap: how much to overlap patches\\n\\n        Transforms::\\n\\n            random_flip\\n            transforms.ToTensor()\\n            normalize\\n            Patchify(patch_size=patch_size, overlap_size=patch_size // 2)\\n\\n        Example::\\n\\n            # in a regular dataset\\n            Imagenet(..., transforms=CPCTrainTransformsImageNet128())\\n\\n            # in a DataModule\\n            module = ImagenetDataModule(PATH)\\n            train_loader = module.train_dataloader(batch_size=32, transforms=CPCTrainTransformsImageNet128())\\n        \"\"\"\\n        # image augmentation functions\\n        self.patch_size = patch_size\\n        self.overlap = overlap\\n        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)\\n        rand_crop = transforms.RandomResizedCrop(128, scale=(0.3, 1.0), ratio=(0.7, 1.4), interpolation=3)\\n        col_jitter = transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8)\\n        rnd_gray = transforms.RandomGrayscale(p=0.25)\\n\\n        post_transform = transforms.Compose([\\n            transforms.ToTensor(),\\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\\n                                 std=[0.229, 0.224, 0.225]),\\n            Patchify(patch_size=patch_size, overlap_size=overlap),\\n        ])\\n\\n        self.transforms = transforms.Compose([\\n            rand_crop,\\n            col_jitter,\\n            rnd_gray,\\n            post_transform\\n        ])\\n\\n    def __call__(self, inp):\\n        inp = self.flip_lr(inp)\\n        out1 = self.transforms(inp)\\n        return out1\\n\\n\\nclass CPCEvalTransformsImageNet128:\\n    def __init__(self, patch_size=32, overlap=16):\\n        \"\"\"\\n        Transforms used for CPC:\\n\\n        Args:\\n\\n            patch_size: size of patches when cutting up the image into overlapping patches\\n            overlap: how much to overlap patches\\n\\n        Transforms::\\n\\n            random_flip\\n            transforms.ToTensor()\\n            normalize\\n            Patchify(patch_size=patch_size, overlap_size=patch_size // 2)\\n\\n        Example::\\n\\n            # in a regular dataset\\n            Imagenet(..., transforms=CPCEvalTransformsImageNet128())\\n\\n            # in a DataModule\\n            module = ImagenetDataModule(PATH)\\n            train_loader = module.train_dataloader(batch_size=32, transforms=CPCEvalTransformsImageNet128())\\n        \"\"\"\\n        # image augmentation functions\\n        self.patch_size = patch_size\\n        self.overlap = overlap\\n        self.flip_lr = transforms.RandomHorizontalFlip(p=0.5)\\n        post_transform = transforms.Compose([\\n            transforms.ToTensor(),\\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\\n                                 std=[0.229, 0.224, 0.225]),\\n            Patchify(patch_size=patch_size, overlap_size=overlap),\\n        ])\\n        self.transforms = transforms.Compose([\\n            transforms.Resize(146, interpolation=3),\\n            transforms.CenterCrop(128),\\n            post_transform\\n        ])\\n\\n    def __call__(self, inp):\\n        inp = self.flip_lr(inp)\\n        out1 = self.transforms(inp)\\n        return out1\\n',\n",
       " \"import pyspark.sql.types\\n\\nfirmware_cpes_schema = pyspark.sql.types.StructType([\\n    pyspark.sql.types.StructField('cpe', pyspark.sql.types.StringType()),\\n    pyspark.sql.types.StructField('firmware_hash', pyspark.sql.types.StringType()),\\n    pyspark.sql.types.StructField('evidence', pyspark.sql.types.StructType([\\n        pyspark.sql.types.StructField('type', pyspark.sql.types.StringType()),\\n        pyspark.sql.types.StructField('firmware_hash', pyspark.sql.types.StringType()),\\n        pyspark.sql.types.StructField('product', pyspark.sql.types.StringType()),\\n        pyspark.sql.types.StructField('version', pyspark.sql.types.StringType()),\\n    ]))\\n])\\n\\n\",\n",
       " '\"\"\"\\nThis module contains the class for detecting\\nthe presence of keywords in an audio stream\\n\"\"\"\\nimport logging\\nimport os\\n\\nimport numpy as np  # type: ignore\\n\\nfrom spokestack.context import SpeechContext\\nfrom spokestack.models.tensorflow import TFLiteModel\\nfrom spokestack.ring_buffer import RingBuffer\\n\\nfrom pydub import AudioSegment\\nfrom pydub.playback import play\\n\\n_LOG = logging.getLogger(__name__)\\n\\n\\nclass WakewordTrigger:\\n    \"\"\"Detects the presence of a wakeword in the audio input\\n\\n    Args:\\n            pre_emphasis (float): The value of the pre-emmphasis filter\\n            sample_rate (int): The number of audio samples per second of audio (kHz)\\n            fft_window_type (str): The type of fft window. (only support for hann)\\n            fft_hop_length (int): Audio sliding window for STFT calculation (ms)\\n            model_dir (str): Path to the directory containing .tflite models\\n            posterior_threshold (float): Probability threshold for if a wakeword\\n                                         was detected\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        pre_emphasis: float = 0.0,\\n        sample_rate: int = 16000,\\n        fft_window_type: str = \"hann\",\\n        fft_hop_length: int = 10,\\n        model_dir: str = \"\",\\n        model_type: str = \"\",\\n        posterior_threshold: float = 0.5,\\n        **kwargs,\\n    ) -> None:\\n\\n        self.pre_emphasis: float = pre_emphasis\\n        self.hop_length: int = int(fft_hop_length * sample_rate / 1000)\\n\\n        if fft_window_type != \"hann\":\\n            raise ValueError(\"Invalid fft_window_type\")\\n\\n        self.filter_model: TFLiteModel = TFLiteModel(\\n            model_path=os.path.join(model_dir, \"filter.tflite\")\\n        )\\n        self.encode_model: TFLiteModel = TFLiteModel(\\n            model_path=os.path.join(model_dir, \"encode.tflite\")\\n        )\\n        self.detect_model: TFLiteModel = TFLiteModel(\\n            model_path=os.path.join(model_dir, \"detect.tflite\")\\n        )\\n\\n        # Model architecture \\n        self.model_type = model_type.upper()\\n\\n        # window size calculated based on fft\\n        # the filter inputs are (fft_size - 1) / 2\\n        # which makes the window size (post_fft_size - 1) * 2\\n        self._window_size = (self.filter_model.input_details[0][\"shape\"][-1] - 1) * 2\\n        self._fft_window = np.hanning(self._window_size)\\n\\n        # retrieve the mel_length and mel_width based on the encoder model metadata\\n        # these allocate the buffer to the correct size\\n        if self.model_type == \\'CRNN\\':\\n            self.mel_length: int = self.encode_model.input_details[0][\"shape\"][2]\\n            self.mel_width: int = self.encode_model.input_details[0][\"shape\"][1]\\n        elif self.model_type == \\'WAVENET\\':\\n            self.mel_length: int = self.encode_model.input_details[0][\"shape\"][1]\\n            self.mel_width: int = self.encode_model.input_details[0][\"shape\"][2]\\n\\n        # initialize the first state input for autoregressive encoder model\\n        # retrieve the encode_lenth and encode_width from the model detect_model\\n        # metadata. We get the dimensions from the detect_model inputs because the\\n        # encode_model runs autoregressively and outputs a single encoded sample.\\n        # the detect_model input is a collection of these samples.\\n\\n        if self.model_type == \\'CRNN\\':\\n            self.encode_length: int = self.detect_model.input_details[0][\"shape\"][0]\\n            self.encode_width: int = self.detect_model.input_details[0][\"shape\"][-1]\\n        elif self.model_type == \\'WAVENET\\':\\n            self.encode_length: int = self.detect_model.input_details[0][\"shape\"][1]\\n            self.encode_width: int = self.detect_model.input_details[0][\"shape\"][-1]\\n\\n        self.sample_window: RingBuffer = RingBuffer(shape=[self._window_size])\\n        self.frame_window: RingBuffer = RingBuffer(\\n            shape=[self.mel_length, self.mel_width]\\n        )\\n        self.encode_window: RingBuffer = RingBuffer(\\n            shape=[1, self.encode_length, self.encode_width]\\n        )\\n\\n        # initialize the frame and encode windows with zeros\\n        # this minimizes the delay caused by filling the buffer\\n        self.frame_window.fill(0.0)\\n        self.encode_window.fill(-1.0)\\n\\n        self._posterior_threshold: float = posterior_threshold\\n        self._posterior_max: float = 0.0\\n        self._prev_sample: float = 0.0\\n        self._is_speech: bool = False\\n\\n        # audio segments for reponse on wake\\n        self.load_awake_responses(\\'audio_responses\\')\\n\\n    def load_awake_responses(self, audio_path):\\n        # load all mp3\\'s from audio_path for wake responses\\n        segs = []\\n        for f in os.listdir(audio_path):\\n            f_path = os.path.join(audio_path, f)\\n            if os.path.isfile(f_path) and \\'.mp3\\' in f_path:\\n                segs.append(AudioSegment.from_mp3(f_path))\\n\\n        self.audio_responses = np.array(segs, dtype=object)\\n\\n    def __call__(self, context: SpeechContext, frame) -> None:\\n        \"\"\"Entry point of the trigger\\n\\n        Args:\\n            context (SpeechContext): current state of the speech pipeline\\n            frame (np.ndarray): a single frame of an audio signal\\n\\n        Returns: None\\n\\n        \"\"\"\\n\\n        # detect vad edges for wakeword deactivation\\n        vad_fall = self._is_speech and not context.is_speech\\n        self._is_speech = context.is_speech\\n\\n        # sample frame to detect the presence of wakeword\\n        if not context.is_active:\\n            self._sample(context, frame)\\n\\n        # reset on vad fall deactivation\\n        if vad_fall:\\n            if not context.is_active:\\n                _LOG.info(f\"wake: {self._posterior_max}\")\\n            self.reset()\\n\\n    def _sample(self, context: SpeechContext, frame) -> None:\\n        # convert the PCM-16 audio to float32 in (-1.0, 1.0)\\n        frame = frame.astype(np.float32) / (2 ** 15 - 1)\\n        frame = np.clip(frame, -1.0, 1.0)\\n\\n        # pull out a single value from the frame and apply pre-emphasis\\n        # with the previous sample then cache the previous sample\\n        # to be use in the next iteration\\n        prev_sample = frame[-1]\\n        frame -= self.pre_emphasis * np.append(self._prev_sample, frame[:-1])\\n        self._prev_sample = prev_sample\\n\\n        # fill the sample window to analyze speech containing samples\\n        # after each window fill the buffer advances by the hop length\\n        # to produce an overlapping window\\n        for sample in frame:\\n            self.sample_window.write(sample)\\n            if self.sample_window.is_full:\\n                if context.is_speech:\\n                    self._analyze(context)\\n                self.sample_window.rewind().seek(self.hop_length)\\n\\n    def _analyze(self, context: SpeechContext) -> None:\\n        # read the full contents of the sample window to calculate a single frame\\n        # of the STFT by applying the DFT to a real-valued input and\\n        # taking the magnitude of the complex DFT\\n        frame = self.sample_window.read_all()\\n        frame = np.fft.rfft(frame * self._fft_window, n=self._window_size)\\n        frame = np.abs(frame).astype(np.float32)\\n\\n        # compute mel spectrogram\\n        self._filter(context, frame)\\n\\n    def _filter(self, context: SpeechContext, frame) -> None:\\n        # add the batch dimension and compute the mel spectrogram with filter model\\n        frame = np.expand_dims(frame, 0)\\n        frame = self.filter_model(frame)[0]\\n\\n        # advance the window by 1 and write mel frame to the frame buffer\\n        self.frame_window.rewind().seek(1)\\n        self.frame_window.write(frame)\\n\\n        # encode the mel spectrogram\\n        self._encode(context)\\n\\n    def _encode(self, context: SpeechContext) -> None:\\n        # read the full contents of the frame window and add the batch dimension\\n        # run the encoder and save the output state for autoregression\\n        frame = self.frame_window.read_all()\\n\\n        # different architectures have different input requirements \\n        if self.model_type == \\'CRNN\\':\\n            # swap timesteps and features\\n            frame = np.expand_dims(frame.T, 0)\\n            # add channel dimension\\n            frame = np.expand_dims(frame, -1)\\n\\n        elif self.model_type == \\'WAVENET\\':\\n            frame = np.expand_dims(frame, 0)\\n\\n        # inference for encoder\\n        frame = self.encode_model(frame)\\n\\n        # accumulate encoded samples until size of detection window\\n        self.encode_window.rewind().seek(1)\\n        self.encode_window.write(np.squeeze(frame))\\n        #self.encode_window.write(frame)\\n        self._detect(context)\\n\\n    def _detect(self, context: SpeechContext) -> None:\\n        # read the full contents of the encode window and add the batch dimension\\n        # calculate a scalar probability of if the frame contains the wakeword\\n        # with the detect model\\n        frame = self.encode_window.read_all()\\n\\n        # frame is (batch,timesteps,features), same as Wavenet\\n        # CRNN detect input is (batch,features)\\n        if self.model_type == \\'CRNN\\':\\n            frame = frame.squeeze(0)\\n\\n        if self.model_type == \"CRNN\":\\n            posterior = self.detect_model(frame)[0][0][0]\\n        else:\\n            posterior = self.detect_model(frame)[0][0][1]\\n\\n        if posterior > self._posterior_max:\\n            self._posterior_max = posterior\\n        if posterior > self._posterior_threshold:\\n            if not context.is_active:\\n                _LOG.info(f\"AWAKE!: {self._posterior_max}\")\\n                play(np.random.choice(self.audio_responses))\\n                context.is_active = True\\n\\n    def reset(self) -> None:\\n        \"\"\" Resets the currect WakewordDetector state \"\"\"\\n        self.sample_window.reset()\\n        self.frame_window.reset().fill(0.0)\\n        self.encode_window.reset().fill(-1.0)\\n        self._posterior_max = 0.0\\n\\n    def close(self) -> None:\\n        \"\"\" Close interface for use in the pipeline \"\"\"\\n        self.reset()\\n',\n",
       " \"import torch.nn as nn\\nimport torch\\nfrom torch.autograd import Variable\\nimport torch.nn.functional as F\\n\\n\\nclass AngleLoss(nn.Module):  # 设置loss，超参数gamma，最小比例，和最大比例\\n    def __init__(self, gamma=0, lambda_min=5, lambda_max=1500):\\n        super(AngleLoss, self).__init__()\\n        self.gamma = gamma\\n        self.it = 0\\n        self.lambda_min = lambda_min\\n        self.lambda_max = lambda_max\\n\\n    def forward(self, x, y):  # 分别是output和target\\n        self.it += 1\\n        cos_theta, phi_theta = x  # output包括上面的[cos_theta, phi_theta]\\n        y = y.view(-1, 1)\\n\\n        index = cos_theta.data * 0.0\\n        index.scatter_(1, y.data.view(-1, 1), 1)  # 将label存成稀疏矩阵\\n        index = index.byte()\\n        # index = Variable(index)   # warning occurs, change to following line. see link blew:\\n        # https://github.com/pytorch/pytorch/issues/29365\\n        #index = torch.tensor(index, dtype=torch.bool)\\n        index = index.clone().detach().bool()\\n\\n        lamb = max(self.lambda_min, self.lambda_max / (1 + 0.1 * self.it))  # 动态调整lambda，来调整cos(\\\\theta)和\\\\phi(\\\\theta)的比例\\n        output = cos_theta * 1.0\\n        output[index] -= cos_theta[index]*(1.0+0)/(1 + lamb)  # 减去目标\\\\cos(\\\\theta)的部分\\n        output[index] += phi_theta[index]*(1.0+0)/(1 + lamb)  # 加上目标\\\\phi(\\\\theta)的部分\\n\\n        logpt = F.log_softmax(output, dim=1)\\n        logpt = logpt.gather(1, y)\\n        logpt = logpt.view(-1)\\n        pt = Variable(logpt.data.exp())\\n\\n        loss = -1 * (1-pt)**self.gamma * logpt\\n        loss = loss.mean()\\n\\n        return loss\\n\\n\\nclass AngleLossWithCE(nn.Module):\\n    def __init__(self, lambda_min=5, lambda_max=1500, weight=[1, 1]):\\n        super().__init__()\\n        self.embeddingLoss = AngleLoss(lambda_min, lambda_max)\\n        self.clsLoss = nn.CrossEntropyLoss()\\n        self.weight = weight\\n\\n    def forward(self, x1, x2, label):\\n        embeddingLoss = self.embeddingLoss(x1, label)\\n        clsLoss = self.clsLoss(x2, label)\\n        total_loss = embeddingLoss * self.weight[0] + clsLoss * self.weight[1]\\n        return total_loss\\n\\n\\n# http://papers.nips.cc/paper/6653-learning-with-average-top-k-loss.pdf\\n# not sure working or not\\nclass TopKLossWithBCE(nn.Module):\\n    def __init__(self, p=0.7):\\n        super().__init__()\\n        self.p = p\\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\\n\\n    def forward(self, pred, gt):\\n        k = int(pred.shape[0] * self.p)\\n        loss = self.bce(pred, gt)\\n        loss = loss.topk(k, dim=0)[0]\\n        loss = loss.mean()\\n        return loss\\n\\n\\n# https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354\\nclass F1_Loss(nn.Module):\\n    '''Calculate F1 score. Can work with gpu tensors\\n\\n    The original implmentation is written by Michal Haltuf on Kaggle.\\n\\n    Returns\\n    -------\\n    torch.Tensor\\n        `ndim` == 1. epsilon <= val <= 1\\n\\n    Reference\\n    ---------\\n    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\\n    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\\n    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\\n    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\\n    '''\\n\\n    def __init__(self, classes=2, epsilon=1e-7):\\n        super().__init__()\\n        self.epsilon = epsilon\\n        self.classes = classes\\n\\n    def forward(self, y_pred, y_true, ):\\n        assert y_pred.ndim == 2\\n        assert y_true.ndim == 1\\n        y_true = F.one_hot(y_true, self.classes).to(torch.float32)\\n        y_pred = F.softmax(y_pred, dim=1)\\n\\n        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\\n        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\\n        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\\n        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\\n\\n        precision = tp / (tp + fp + self.epsilon)\\n        recall = tp / (tp + fn + self.epsilon)\\n\\n        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\\n        f1 = f1.clamp(min=self.epsilon, max=1 - self.epsilon)\\n        return 1 - f1.mean()\\n\\n\\nclass F1LossWithBCE(nn.Module):\\n    def __init__(self, classes=264, weights=[1, 1]):\\n        super().__init__()\\n        self.classes = classes\\n        self.weights = weights\\n        self.bce = nn.BCEWithLogitsLoss()\\n        self.f1 = F1_Loss(classes=self.classes)\\n\\n    def forward(self, pred, gt):\\n        bce = self.bce(pred, gt)\\n        f1 = self.f1(pred, gt)\\n        loss = self.weights[0] * bce + self.weights[1] * f1\\n        return loss\\n\\n\\n\",\n",
       " '# -*- coding: utf-8 -*-\\n\"\"\"\\nThis file is part of pyCMBS.\\n(c) 2012- Alexander Loew\\nFor COPYING and LICENSE details, please refer to the LICENSE file\\n\"\"\"\\n\\n\\nclass Koeppen(object):\\n    \"\"\"\\n    KOEPPEN CLASS\\n    class to generate koeppen plot\\n    \"\"\"\\n\\n    def __init__(self, temp=None, precip=None, lsm=None):\\n        \"\"\"\\n        Koeppen class\\n        This class implements the functionality to generate koeppen plots.\\n\\n        Parameters\\n        ----------\\n        temp : Data\\n            data objekt of temperature\\n        precip : Data\\n            data objekt of precipitation\\n        lsm : Data\\n            data objekt of land-sea-mask (0.0 to 1.0)\\n\\n        EXAMPLES\\n        ========\\n\\n        \"\"\"\\n\\n        # check consistency\\n        if temp is None:\\n            raise ValueError(\\'No temperature given\\')\\n        if precip is None:\\n            raise ValueError(\\'No precipitation given\\')\\n        if lsm is None:\\n            raise ValueError(\\'No land-sea-mask given\\')\\n\\n        # set values of class\\n        self.temp = temp\\n        self.precip = precip\\n        self.lsm = lsm\\n\\n        if not self._check_resolution():\\n            raise ValueError(\\'ERROR:The three array differe in the resolution\\')\\n        if not self._check_units():\\n            raise ValueError(\\'ERROR:The units of one value is wrong\\')\\n\\n        # Create new koeppen Color map\\n        self.koeppen_cmap()\\n        self.cmap = cm.get_cmap(\\'koeppen\\')\\n        # convert from [kg m-2 s-1] to [kg m-2 day-1] (= [mm day-1])\\n         # ??? Unklar warum nicht \\'precip.mulc(60. * 60. * 24. * 365.)\\'\\n        self.precip = precip.mulc(60. * 60. * 24. * 365. / 12., copy=True)\\n        self.temp = temp.subc(273.15, copy=True)  # ??? Unklar warum nicht \\'temp.subc(273.15)\\'\\n\\n        Psum = self.precip.timsum(return_object=True)            # Berechnet die Summe der Jahresniederschlag\\n\\n        nt, ny, nx = self.temp.shape\\n        nlat = ny\\n        nlon = nx\\n\\n        Pmin = self.precip.data.min(axis=0)\\n        Pmax = self.precip.data.max(axis=0)\\n\\n        precipHS = self.precip.copy()\\n        precipHS.data[(0, 1, 2, 3, 4, 5), 0:(nlat / 2 - 1), :] \\\\\\n            = self.precip.data[(3, 4, 5, 6, 7, 8), 0:(nlat / 2 - 1), :]\\n        precipHS.data[(6, 7, 8, 9, 10, 11), 0:(nlat / 2 - 1), :] \\\\\\n            = self.precip.data[(3, 4, 5, 6, 7, 8), 0:(nlat / 2 - 1), :]\\n        precipHS.data[(0, 1, 2, 3, 4, 5), (nlat / 2):(nlat - 1), :] \\\\\\n            = self.precip.data[(0, 1, 2, 9, 10, 11), (nlat / 2):(nlat - 1), :]\\n        precipHS.data[(6, 7, 8, 9, 10, 11), (nlat / 2):(nlat - 1), :] \\\\\\n            = self.precip.data[(0, 1, 2, 9, 10, 11), (nlat / 2):(nlat - 1), :]\\n\\n        precipHW = self.precip.copy()\\n        precipHW.data[(0, 1, 2, 3, 4, 5), 0:(nlat / 2 - 1), :] = self.precip.data[(0, 1, 2, 9, 10, 11), 0:(nlat / 2 - 1), :]\\n        precipHW.data[(6, 7, 8, 9, 10, 11), 0:(nlat / 2 - 1), :] = self.precip.data[(0, 1, 2, 9, 10, 11), 0:(nlat / 2 - 1), :]\\n        precipHW.data[(0, 1, 2, 3, 4, 5), (nlat / 2):(nlat - 1), :] = self.precip.data[(3, 4, 5, 6, 7, 8), (nlat / 2):(nlat - 1), :]\\n        precipHW.data[(6, 7, 8, 9, 10, 11), (nlat / 2):(nlat - 1), :] = self.precip.data[(3, 4, 5, 6, 7, 8), (nlat / 2):(nlat - 1), :]\\n\\n        PminHS = precipHS.data.min(axis=0)   # Bestimmt den minimalen Monastniederschlag aus PmaxHS\\n        PmaxHS = precipHS.data.max(axis=0)   # Bestimmt den maximalen Monastniederschlag aus PmaxHS\\n        PminHW = precipHW.data.min(axis=0)   # Bestimmt den minimalen Monastniederschlag aus PminHW\\n        PmaxHW = precipHW.data.max(axis=0)   # Bestimmt den maximalen Monastniederschlag aus PminHW\\n\\n        Tavg = self.temp.data.mean(axis=0)   # Bestimmt die mittlere Jahrestemperatur\\n        Tmin = self.temp.data.min(axis=0)     # Bestimmt die minimale Monatstemperatur\\n        Tmax = self.temp.data.max(axis=0)     # Bestimmt die maximale Jahrestemperatur\\n\\n        self.Clim = self.precip.timmean(return_object=True)\\n        self.Clim.units = \"climate type\"\\n\\n        for lat in range(0, nlat):\\n            for lon in range(0, nlon):\\n                psum = Psum.data.data[lat][lon]\\n                pmin = Pmin[lat][lon]\\n                pminhs = PminHS[lat][lon]\\n                pminhw = PminHW[lat][lon]\\n                pmaxhs = PmaxHS[lat][lon]\\n                pmaxhw = PmaxHW[lat][lon]\\n                tavg = Tavg[lat][lon]\\n                tmin = Tmin[lat][lon]\\n                tmax = Tmax[lat][lon]\\n                self.Clim.data.data[lat][lon] = self.set_clim(psum, pmin, pminhs, pminhw, pmaxhs, pmaxhw, tavg, tmin, tmax)\\n\\n        self.Clim.data.mask[less(self.lsm.data, 0.5)] = True\\n\\n    def koeppen_cmap(self):\\n        \"\"\"\\n        Create a colormap with 14 discrete colors and register it\\n        \"\"\"\\n        # define individual colors as hex values\\n        cpool = [\\'#7f0000\\', \\'#ff0000\\', \\'#ff4c4c\\', \\'#ff9999\\', \\'#ffa500\\',\\n                 \\'#ffff4c\\', \\'#009900\\', \\'#00ff00\\', \\'#99ff99\\', \\'#990099\\',\\n                 \\'#e500e5\\', \\'#ff66ff\\', \\'#0000ff\\', \\'#9999ff\\', \\'#000000\\']\\n        cmap3 = col.ListedColormap(cpool[0:14], \\'koeppen\\')\\n#       plt.cm.register_cmap(cmap=cmap3,name=\\'koeppen\\',lut=15)\\n        plt.cm.register_cmap(cmap=cmap3, name=\\'koeppen\\')\\n        return cmap3\\n\\n    def set_clim(self, psum, pmin, pminhs, pminhw, pmaxhs, pmaxhw, tavg, tmin, tmax):\\n        clim = -999\\n\\n        if tmin > 18:\\n            if pmin > 60:                 # A(B)\\n                clim = 1                    # Af\\n            else:\\n                if pmin > (0.04 * (2500 - psum)):      # A(B)-msw\\n                    clim = 2                  # Am\\n                else:\\n                    if (pminhs < 40) and (pminhs < (pmaxhw / 3)):   # A(B)-sw\\n                        if (psum / 10) < (2 * tavg):          # A(B)-s\\n                            if (psum / 10) < (tavg):            # B\\n                                clim = 6                    # BW\\n                            else:\\n                                clim = 5                    # BS\\n                        else:\\n                            clim = 3                      # As\\n                    else:\\n                        if (psum / 10) < (2 * (tavg + 14)):       # A(B)-w\\n                            if (psum / 10) < (tavg + 14):       # B\\n                                clim = 6                    # BW\\n                            else:\\n                                clim = 5                    # BS\\n                        else:\\n                            clim = 4                      # Aw\\n        else:\\n            if (pminhs < 40) and (pminhs < (pmaxhw / 3)):   # CDE(B)\\n                if (psum / 10) < (2 * tavg):          # CDE(B)-s\\n                    if (psum / 10) < (tavg):            # B\\n                        clim = 6                    # BW\\n                    else:\\n                        clim = 5                    # BS\\n                else:\\n                    if tmax < 10:                # CDE-s\\n                        if tmax < 0:                # E\\n                            clim = 14                 # EF\\n                        else:\\n                            clim = 13                 # ET\\n                    else:\\n                        if (tmin > -3):             # CD-s\\n                            clim = 8                  # Cs\\n                        else:\\n                            clim = 11                 # Ds\\n            else:\\n                if pminhw < (pmaxhs / 10):            # CDE(B)-fw\\n                    if (psum / 10) < (2 * (tavg + 14)):     # CDE(B)-w\\n                        if (psum / 10) < (tavg + 14):         # B\\n                            clim = 6                  # BW\\n                        else:\\n                            clim = 5                  # BS\\n                    else:\\n                        if tmax < 10:               # CDE-w\\n\\n                            if (tmax < 0):                # E\\n                                clim = 14               # EF\\n                            else:\\n                                clim = 13              # ET\\n                        else:\\n                            if (tmin > -3):               # CD-w\\n                                clim = 9                # Cw\\n                            else:\\n                                clim = 12               # Dw\\n                else:\\n                    if (psum / 10) < (2 * (tavg + 7)):      # CDE(B)-f\\n                        if (psum / 10) < (tavg + 7):          # B\\n                            clim = 6                  # BW\\n                        else:\\n                            clim = 5                  # BS\\n                    else:\\n                        if (tmax < 10):             # CDE-f\\n                            if (tmax < 0):                # E\\n                                clim = 14               # EF\\n                            else:\\n                                clim = 13              # ET\\n                        else:\\n                            if (tmin > -3):               # CD-f\\n                                clim = 7                # Cf\\n                            else:\\n                                clim = 10              # Df\\n        return clim\\n\\n    def _check_resolution(self):\\n        \"\"\"\\n        This routine just checks if all three array have a equal number of ny and nx values\\n        \"\"\"\\n        nt_t, ny_t, nx_t = self.temp.shape\\n        nt_p, ny_p, nx_p = self.precip.shape\\n        ny_l, nx_l = self.lsm.shape\\n\\n        if (ny_t != ny_p) or (ny_t != ny_l):\\n            sys.exit(\\'ERROR: The resolution ot the three arrays differ in \\\\\\n       Y-dimension: \\\\n\\' + str(ny_t) + \"(temp)  \" + str(ny_p)\\n                     + \"(precip) \" + str(ny_l) + \"(lsm) \")\\n            return False\\n\\n        if (nx_t != nx_p) or (nx_t != nx_l):\\n            sys.exit(\\'ERROR: The resolution ot the three arrays differ in \\\\\\n       X-dimension: \\\\n\\' + str(nx_t) + \"(temp)  \" + str(nx_p)\\n                     + \"(precip) \" + str(nx_l) + \"(lsm) \")\\n            return False\\n\\n        return True\\n\\n    def _check_units(self):\\n        \"\"\"\\n        This routine just checks if all three array have a equal number of ny and nx values\\n        \"\"\"\\n        if self.precip.unit != \"kg/m^2s\":\\n            raise ValueError(\\'ERROR: The unit of the precip is not [kg/m^2s] its set to [\\' + self.precip.unit + \"]\")\\n\\n        if self.temp.unit != \"K\":\\n            raise ValueError(\\'ERROR: The unit of the temperature is not [K] its set to [\\' + self.temp.unit + \"]\")\\n\\n        if self.lsm.unit != \"fractional\":\\n            raise ValueError(\\'ERROR: The unit of the temperature is not [fractional] its set to [\\' + self.temp.unit + \"]\")\\n\\n        return True\\n\\n    def copy(self):\\n        \"\"\"\\n        Returns the Clim Data as an Data variable\\n        \"\"\"\\n        return self.Clim\\n\\n    def climfrac(self):\\n        \"\"\"\\n        This routine calculats the fraction of each type in per centum.\\n        ToDo:\\n        Unclear id the print is OK or if the values should given back as an array.\\n        \"\"\"\\n        climfrac = [0] * 14\\n\\n        ny, nx = self.Clim.data.data.shape\\n        for ny in range(0, ny - 1):\\n            Aweight = cos(self.Clim.lat[ny][0] / 180 * 3.14159265359)\\n            for nx in range(0, nx - 1):\\n                clim = int(self.Clim.data.data[ny][nx])\\n                climfrac[clim - 1] = climfrac[clim - 1] + Aweight\\n\\n        s = sum(climfrac)\\n        climfrac[:] = [x / s for x in climfrac]\\n\\n        print \"Af: \" + str(climfrac[0])\\n        print \"Am: \" + str(climfrac[1])\\n        print \"As: \" + str(climfrac[2])\\n        print \"Aw: \" + str(climfrac[3])\\n        print \"BS: \" + str(climfrac[4])\\n        print \"BW: \" + str(climfrac[5])\\n        print \"Cf: \" + str(climfrac[6])\\n        print \"Cs: \" + str(climfrac[7])\\n        print \"Cw: \" + str(climfrac[8])\\n        print \"Df: \" + str(climfrac[9])\\n        print \"Ds: \" + str(climfrac[10])\\n        print \"Dw: \" + str(climfrac[11])\\n        print \"ET: \" + str(climfrac[12])\\n        print \"EF: \" + str(climfrac[13])\\n\\n    def legend(self):\\n        \"\"\"\\n        This routine prints a legend of the geiger-koeppen types.\\n        The description is taken from:\\n        MARKUS KOTTEK, JUERGEN GRIESER, CHRISTOPH BECK , BRUNO RUDOLF and FRANZ RUBEL\\n        World Map of the Koeppen-Geiger climate classification updated\\n        Meteorologische Zeitschrift, Vol. 15, No. 3, 259-263 (June 2006)\\n\\n        \"\"\"\\n\\n        print \"|================= Class legend =================|\"\\n        print \"| Af: Equatorial rainforest, fully humid         |\"\\n        print \"| Am: Equatorial climates                        |\"\\n        print \"| As: Equatorial monsoon                         |\"\\n        print \"| Aw: Equatorial savannah with dry winter        |\"\\n        print \"|------------------------------------------------|\"\\n        print \"| BS: Steppe climate                             |\"\\n        print \"| BW: Desert climate                             |\"\\n        print \"|------------------------------------------------|\"\\n        print \"| Cf: Warm temperate climate, fully humid        |\"\\n        print \"| Cs: Warm temperate climate with dry summer     |\"\\n        print \"| Cw: Warm temperate climate with dry winter     |\"\\n        print \"|------------------------------------------------|\"\\n        print \"| Df: Snow climate, fully humid                  |\"\\n        print \"| Ds: Snow climate with dry summer               |\"\\n        print \"| Dw: Snow climate with dry winter               |\"\\n        print \"|------------------------------------------------|\"\\n        print \"| ET: Tundra climate                             |\"\\n        print \"| EF: Frost climate                              |\"\\n        print \"|================================================|\"\\n\\n    def plot(self, **kwargs):\\n        \"\"\"\\n        This routine plots the data of his own geiger-koeppen data by\\n        using the plot-routine map_plot.\\n        It use the own created color-map and sets the color-bar to a\\n        horizontal orientation.\\n        It set the range of values between 0.5 and 14.5. Which are the\\n        possible values of geiger-koeppen.\\n        ToDo:\\n        At the moment the label of the geiger-koeppen types are missing\\n        at the color-bar\\n        \"\"\"\\n        map_plot(self.Clim, cmap_data=self.cmap, colorbar_orientation=\\'horizontal\\', vmin=0.5, vmax=14.5,\\n                 cticks=[1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14.],\\n                 cticklabels=[\"Af\", \"Am\", \"As\", \"Aw\", \"BS\", \"BW\", \"Cf\",\\n                              \"Cs\", \"Cw\", \"Df\", \"Ds\", \"Dw\", \"ET\", \"EF\"],\\n                 nclasses=15, **kwargs)\\n',\n",
       " '##########################################################################\\n#\\n#  Copyright (c) 2011-2012, Image Engine Design Inc. All rights reserved.\\n#  Copyright (c) 2011-2012, John Haddon. All rights reserved.\\n#\\n#  Redistribution and use in source and binary forms, with or without\\n#  modification, are permitted provided that the following conditions are\\n#  met:\\n#\\n#      * Redistributions of source code must retain the above\\n#        copyright notice, this list of conditions and the following\\n#        disclaimer.\\n#\\n#      * Redistributions in binary form must reproduce the above\\n#        copyright notice, this list of conditions and the following\\n#        disclaimer in the documentation and/or other materials provided with\\n#        the distribution.\\n#\\n#      * Neither the name of John Haddon nor the names of\\n#        any other contributors to this software may be used to endorse or\\n#        promote products derived from this software without specific prior\\n#        written permission.\\n#\\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\\n#  IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\\n#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\\n#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR\\n#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\\n#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\\n#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\\n#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\\n#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\\n#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\n#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n#\\n##########################################################################\\n\\nfrom __future__ import with_statement\\n\\nimport IECore\\n\\nimport GafferUI\\nimport GafferCortexUI\\n\\n## Supported parameter userData entries :\\n#\\n# [\"UI\"][\"collapsible\"]\\n# [\"UI\"][\"collapsed\"]\\n#\\n# Supported child userData entries :\\n#\\n# [\"UI\"][\"visible\"]\\nclass CompoundParameterValueWidget( GafferCortexUI.ParameterValueWidget ) :\\n\\n\\t## If collapsible is not None then it overrides any [\"UI][\"collapsible\"] userData the parameter might have.\\n\\tdef __init__( self, parameterHandler, collapsible=None, _plugValueWidgetClass=None, **kw ) :\\n\\n\\t\\tif collapsible is None :\\n\\t\\t\\tcollapsible = True\\n\\t\\t\\twith IECore.IgnoredExceptions( KeyError ) :\\n\\t\\t\\t\\tcollapsible = parameterHandler.parameter().userData()[\"UI\"][\"collapsible\"].value\\n\\n\\t\\tcollapsed = None\\n\\t\\tif collapsible :\\n\\t\\t\\tcollapsed = True\\n\\t\\t\\twith IECore.IgnoredExceptions( KeyError ) :\\n\\t\\t\\t\\tcollapsed = parameterHandler.parameter().userData()[\"UI\"][\"collapsed\"].value\\n\\n\\t\\tif _plugValueWidgetClass is None :\\n\\t\\t\\t_plugValueWidgetClass = _PlugValueWidget\\n\\n\\t\\tGafferCortexUI.ParameterValueWidget.__init__(\\n\\t\\t\\tself,\\n\\t\\t\\t_plugValueWidgetClass( parameterHandler, collapsed ),\\n\\t\\t\\tparameterHandler,\\n\\t\\t\\t**kw\\n\\t\\t)\\n\\n# CompoundParameterValueWidget is simply a lightweight wrapper around this CompoundPlugValueWidget\\n# derived class. This allows us to take advantage of all the code in CompoundPlugValueWidget that\\n# deals with dynamically adding and removing children etc.\\nclass _PlugValueWidget( GafferUI.CompoundPlugValueWidget ) :\\n\\n\\tdef __init__( self, parameterHandler, collapsed ) :\\n\\n\\t\\tGafferUI.CompoundPlugValueWidget.__init__( self, parameterHandler.plug(), collapsed )\\n\\n\\t\\tself.__parameterHandler = parameterHandler\\n\\n\\tdef _childPlugs( self ) :\\n\\n\\t\\tplug = self.getPlug()\\n\\t\\torderedChildren = []\\n\\t\\tfor childName in self.__parameterHandler.parameter().keys() :\\n\\t\\t\\tif childName in plug :\\n\\t\\t\\t\\torderedChildren.append( plug[childName] )\\n\\n\\t\\treturn orderedChildren\\n\\n\\tdef _childPlugWidget( self, childPlug ) :\\n\\n\\t\\tchildParameter = self.__parameterHandler.parameter()[childPlug.getName()]\\n\\n\\t\\twith IECore.IgnoredExceptions( KeyError ) :\\n\\t\\t\\tif not childParameter.userData()[\"UI\"][\"visible\"].value :\\n\\t\\t\\t\\treturn None\\n\\n\\t\\tchildParameterHandler = self.__parameterHandler.childParameterHandler( childParameter )\\n\\t\\tvalueWidget = GafferCortexUI.ParameterValueWidget.create( childParameterHandler )\\n\\t\\tif not valueWidget :\\n\\t\\t\\treturn None\\n\\n\\t\\tif isinstance( valueWidget, CompoundParameterValueWidget ) :\\n\\t\\t\\treturn valueWidget\\n\\n\\t\\treturn GafferUI.PlugWidget( valueWidget )\\n\\n\\tdef _parameter( self ) :\\n\\n\\t\\treturn self.__parameterHandler.parameter()\\n\\n\\tdef _parameterHandler( self ) :\\n\\n\\t\\treturn self.__parameterHandler\\n\\n\\tdef _parameterLabelText( self, parameterHandler ) :\\n\\n \\t\\treturn IECore.CamelCase.toSpaced( parameterHandler.plug().getName() )\\n\\n\\tdef _parameterToolTip( self, parameterHandler ) :\\n\\n\\t\\tplug = parameterHandler.plug()\\n\\n\\t\\tresult = \"<h3>\" + plug.relativeName( plug.node() ) + \"</h3>\"\\n\\t\\tif parameterHandler.parameter().description :\\n\\t\\t\\tresult += \"\\\\n\\\\n\" + parameterHandler.parameter().description\\n\\n\\t\\treturn result\\n\\n# install implementation class as a protected member, so it can be used by\\n# derived classes.\\nCompoundParameterValueWidget._PlugValueWidget = _PlugValueWidget\\n\\nGafferCortexUI.ParameterValueWidget.registerType( IECore.CompoundParameter, CompoundParameterValueWidget )\\n',\n",
       " '#\\n# Copyright (c) 2013 Juniper Networks, Inc. All rights reserved.\\n#\\n\\nfrom setuptools import setup, find_packages\\n\\nsetup(\\n    name=\\'nodemgr\\',\\n    version=\\'0.1dev\\',\\n    packages=[\\'nodemgr\\'],\\n    package_data={\\'\\': [\\'*.html\\', \\'*.css\\', \\'*.xml\\']},\\n    zip_safe=False,\\n    long_description=\"Nodemgr Implementation\",\\n    entry_points={\\n        \\'console_scripts\\': [\\n            \\'contrail-nodemgr = nodemgr.main:main\\',\\n        ],\\n    },\\n)\\n',\n",
       " 'from collections import defaultdict\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\nfrom hyperopt_synthetic import run_one_exp as hyperopt_synthetic_opt\\nfrom xbbo_synthetic import run_one_exp as xbbo_synthetic_opt\\n\\nmax_call = 50\\nif __name__ == \"__main__\":\\n    rng = np.random.RandomState(42)\\n    result_opts = defaultdict(list)\\n    for i in range(3):\\n        seed = rng.randint(1e5)\\n        # result_opts[\\'hyperopt-rand\\'].append(hyperopt_synthetic_opt(\\'rand\\', max_call,seed))\\n        result_opts[\\'hyperopt-tpe\\'].append(hyperopt_synthetic_opt(\\'tpe\\', max_call,seed))\\n        # result_opts[\\'hyperopt-atpe\\'].append(hyperopt_synthetic_opt(\\'atpe\\', max_call,seed))\\n        # result_opts[\\'hyperopt-mix\\'].append(hyperopt_synthetic_opt(\\'mix\\', max_call,seed))\\n        result_opts[\\'hyperopt-anneal\\'].append(hyperopt_synthetic_opt(\\'anneal\\', max_call,seed))\\n        result_opts[\\'XBBO-tpe\\'].append(xbbo_synthetic_opt(\\'tpe\\', max_call,seed))\\n        result_opts[\\'XBBO-anneal\\'].append(xbbo_synthetic_opt(\\'anneal\\',max_call,seed))\\n    plt.figure()\\n    for key in result_opts:\\n        plt.plot(range(1,max_call+1), np.mean(np.minimum.accumulate(np.asarray(result_opts[key]), axis=1),axis=0)[:], label=key)\\n    plt.ylim([-0.1,1000])\\n    plt.xlabel(\\'# of Evaluate\\')\\n    plt.ylabel(\\'OBJ\\')\\n    plt.title(\\'Average of cumulate best on 3 seeds\\')\\n    plt.legend()\\n    plt.savefig(\\'./out/comp_with_hyperopt.png\\')\\n    plt.show()\\n\\n',\n",
       " '#!/usr/bin/env python\\nimport json\\nimport os\\nimport sys\\nfrom textwrap import dedent\\n\\n\\ndef get_hashed_filenames(static_path):\\n    json_file = \\'{}/staticfiles.json\\'.format(static_path)\\n    with open(json_file) as jsonf:\\n        staticfiles = json.load(jsonf)\\n\\n    return list(staticfiles[\\'paths\\'].values())\\n\\n\\ndef move_hashed_files(static_path, hashed_path):\\n    filenames = get_hashed_filenames(static_path)\\n    moved_count = 0\\n    for filename in filenames:\\n        # some filenames in the file are in the form\\n        # fontawesome/fonts/fontawesome-webfont.f7c2b4b747b1.eot?v=4.3.0\\n        # we can skip these as they\\'re duplicated\\n        if \\'?\\' in filename:\\n            continue\\n\\n        src_fn = os.path.join(static_path, filename)\\n        dst_fn = os.path.join(hashed_path, filename)\\n        if not os.path.exists(os.path.dirname(dst_fn)):\\n            os.makedirs(os.path.dirname(dst_fn))\\n\\n        os.rename(src_fn, dst_fn)\\n        moved_count += 1\\n\\n    return moved_count\\n\\n\\ndef main(static_path, hashed_path):\\n    moved = move_hashed_files(static_path, hashed_path)\\n    print(\\'Successfully moved {} files\\'.format(moved))\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        main(sys.argv[1], sys.argv[2])\\n    except IndexError:\\n        sys.exit(dedent(\"\"\"\\\\\\n            ERROR: source and destination directory arguments required.\\n\\n            Usage: move_hashed_staticfiles.py <source_dir> <dest_dir>\\n\\n            Moves hashed static files from source_dir to dest_dir based on the\\n            map of staticfiles in `source_dir/staticfiles.json`.\\n        \"\"\"))\\n',\n",
       " '#!/usr/bin/env python\\n# encoding: utf-8\\n# Carlos Rafael Giani, 2007 (dv)\\n# Thomas Nagy, 2007-2008 (ita)\\n\\nimport os, sys, re, optparse\\nimport ccroot # <- leave this\\nimport TaskGen, Utils, Task, Configure, Logs, Build\\nfrom Logs import debug, error\\nfrom TaskGen import taskgen, feature, after, before, extension\\nfrom Configure import conftest\\n\\nEXT_D = [\\'.d\\', \\'.di\\', \\'.D\\']\\nD_METHS = [\\'apply_core\\', \\'apply_vnum\\', \\'apply_objdeps\\'] # additional d methods\\n\\ndef filter_comments(filename):\\n\\ttxt = Utils.readf(filename)\\n\\tbuf = []\\n\\n\\ti = 0\\n\\tmax = len(txt)\\n\\twhile i < max:\\n\\t\\tc = txt[i]\\n\\t\\t# skip a string\\n\\t\\tif c == \\'\"\\':\\n\\t\\t\\ti += 1\\n\\t\\t\\tc = \\'\\'\\n\\t\\t\\twhile i < max:\\n\\t\\t\\t\\tp = c\\n\\t\\t\\t\\tc = txt[i]\\n\\t\\t\\t\\ti += 1\\n\\t\\t\\t\\tif i == max: return buf\\n\\t\\t\\t\\tif c == \\'\"\\':\\n\\t\\t\\t\\t\\tcnt = 0\\n\\t\\t\\t\\t\\twhile i < cnt and i < max:\\n\\t\\t\\t\\t\\t\\t#print \"cntcnt = \", str(cnt), self.txt[self.i-2-cnt]\\n\\t\\t\\t\\t\\t\\tif txt[i-2-cnt] == \\'\\\\\\\\\\': cnt+=1\\n\\t\\t\\t\\t\\t\\telse: break\\n\\t\\t\\t\\t\\t#print \"cnt is \", str(cnt)\\n\\t\\t\\t\\t\\tif (cnt%2)==0: break\\n\\t\\t\\ti += 1\\n\\t\\t# skip a char\\n\\t\\telif c == \"\\'\":\\n\\t\\t\\ti += 1\\n\\t\\t\\tif i == max: return buf\\n\\t\\t\\tc = txt[i]\\n\\t\\t\\tif c == \\'\\\\\\\\\\':\\n\\t\\t\\t\\ti += 1\\n\\t\\t\\t\\tif i == max: return buf\\n\\t\\t\\t\\tc = txt[i]\\n\\t\\t\\t\\tif c == \\'x\\':\\n\\t\\t\\t\\t\\ti += 2 # skip two chars\\n\\t\\t\\t\\telif c == \\'u\\':\\n\\t\\t\\t\\t\\ti += 4 # skip unicode chars\\n\\t\\t\\ti += 1\\n\\t\\t\\tif i == max: return buf\\n\\t\\t\\tc = txt[i]\\n\\t\\t\\tif c != \\'\\\\\\'\\': error(\"uh-oh, invalid character\")\\n\\n\\t\\t# skip a comment\\n\\t\\telif c == \\'/\\':\\n\\t\\t\\tif i == max: break\\n\\t\\t\\tc = txt[i+1]\\n\\t\\t\\t# eat /+ +/ comments\\n\\t\\t\\tif c == \\'+\\':\\n\\t\\t\\t\\ti += 1\\n\\t\\t\\t\\tnesting = 1\\n\\t\\t\\t\\tprev = 0\\n\\t\\t\\t\\twhile i < max:\\n\\t\\t\\t\\t\\tc = txt[i]\\n\\t\\t\\t\\t\\tif c == \\'+\\':\\n\\t\\t\\t\\t\\t\\tprev = 1\\n\\t\\t\\t\\t\\telif c == \\'/\\':\\n\\t\\t\\t\\t\\t\\tif prev:\\n\\t\\t\\t\\t\\t\\t\\tnesting -= 1\\n\\t\\t\\t\\t\\t\\t\\tif nesting == 0: break\\n\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\tif i < max:\\n\\t\\t\\t\\t\\t\\t\\t\\ti += 1\\n\\t\\t\\t\\t\\t\\t\\t\\tc = txt[i]\\n\\t\\t\\t\\t\\t\\t\\t\\tif c == \\'+\\':\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tnesting += 1\\n\\t\\t\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\t\\t\\treturn buf\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tprev = 0\\n\\t\\t\\t\\t\\ti += 1\\n\\t\\t\\t# eat /* */ comments\\n\\t\\t\\telif c == \\'*\\':\\n\\t\\t\\t\\ti += 1\\n\\t\\t\\t\\twhile i < max:\\n\\t\\t\\t\\t\\tc = txt[i]\\n\\t\\t\\t\\t\\tif c == \\'*\\':\\n\\t\\t\\t\\t\\t\\tprev = 1\\n\\t\\t\\t\\t\\telif c == \\'/\\':\\n\\t\\t\\t\\t\\t\\tif prev: break\\n\\t\\t\\t\\t\\telse:\\n\\t\\t\\t\\t\\t\\tprev = 0\\n\\t\\t\\t\\t\\ti += 1\\n\\t\\t\\t# eat // comments\\n\\t\\t\\telif c == \\'/\\':\\n\\t\\t\\t\\ti += 1\\n\\t\\t\\t\\tc = txt[i]\\n\\t\\t\\t\\twhile i < max and c != \\'\\\\n\\':\\n\\t\\t\\t\\t\\ti += 1\\n\\t\\t\\t\\t\\tc = txt[i]\\n\\t\\t# a valid char, add it to the buffer\\n\\t\\telse:\\n\\t\\t\\tbuf.append(c)\\n\\t\\ti += 1\\n\\treturn buf\\n\\nclass d_parser(object):\\n\\tdef __init__(self, env, incpaths):\\n\\t\\t#self.code = \\'\\'\\n\\t\\t#self.module = \\'\\'\\n\\t\\t#self.imports = []\\n\\n\\t\\tself.allnames = []\\n\\n\\t\\tself.re_module = re.compile(\"module\\\\s+([^;]+)\")\\n\\t\\tself.re_import = re.compile(\"import\\\\s+([^;]+)\")\\n\\t\\tself.re_import_bindings = re.compile(\"([^:]+):(.*)\")\\n\\t\\tself.re_import_alias = re.compile(\"[^=]+=(.+)\")\\n\\n\\t\\tself.env = env\\n\\n\\t\\tself.nodes = []\\n\\t\\tself.names = []\\n\\n\\t\\tself.incpaths = incpaths\\n\\n\\tdef tryfind(self, filename):\\n\\t\\tfound = 0\\n\\t\\tfor n in self.incpaths:\\n\\t\\t\\tfound = n.find_resource(filename.replace(\\'.\\', \\'/\\') + \\'.d\\')\\n\\t\\t\\tif found:\\n\\t\\t\\t\\tself.nodes.append(found)\\n\\t\\t\\t\\tself.waiting.append(found)\\n\\t\\t\\t\\tbreak\\n\\t\\tif not found:\\n\\t\\t\\tif not filename in self.names:\\n\\t\\t\\t\\tself.names.append(filename)\\n\\n\\tdef get_strings(self, code):\\n\\t\\t#self.imports = []\\n\\t\\tself.module = \\'\\'\\n\\t\\tlst = []\\n\\n\\t\\t# get the module name (if present)\\n\\n\\t\\tmod_name = self.re_module.search(code)\\n\\t\\tif mod_name:\\n\\t\\t\\tself.module = re.sub(\\'\\\\s+\\', \\'\\', mod_name.group(1)) # strip all whitespaces\\n\\n\\t\\t# go through the code, have a look at all import occurrences\\n\\n\\t\\t# first, lets look at anything beginning with \"import\" and ending with \";\"\\n\\t\\timport_iterator = self.re_import.finditer(code)\\n\\t\\tif import_iterator:\\n\\t\\t\\tfor import_match in import_iterator:\\n\\t\\t\\t\\timport_match_str = re.sub(\\'\\\\s+\\', \\'\\', import_match.group(1)) # strip all whitespaces\\n\\n\\t\\t\\t\\t# does this end with an import bindings declaration?\\n\\t\\t\\t\\t# (import bindings always terminate the list of imports)\\n\\t\\t\\t\\tbindings_match = self.re_import_bindings.match(import_match_str)\\n\\t\\t\\t\\tif bindings_match:\\n\\t\\t\\t\\t\\timport_match_str = bindings_match.group(1)\\n\\t\\t\\t\\t\\t# if so, extract the part before the \":\" (since the module declaration(s) is/are located there)\\n\\n\\t\\t\\t\\t# split the matching string into a bunch of strings, separated by a comma\\n\\t\\t\\t\\tmatches = import_match_str.split(\\',\\')\\n\\n\\t\\t\\t\\tfor match in matches:\\n\\t\\t\\t\\t\\talias_match = self.re_import_alias.match(match)\\n\\t\\t\\t\\t\\tif alias_match:\\n\\t\\t\\t\\t\\t\\t# is this an alias declaration? (alias = module name) if so, extract the module name\\n\\t\\t\\t\\t\\t\\tmatch = alias_match.group(1)\\n\\n\\t\\t\\t\\t\\tlst.append(match)\\n\\t\\treturn lst\\n\\n\\tdef start(self, node):\\n\\t\\tself.waiting = [node]\\n\\t\\t# while the stack is not empty, add the dependencies\\n\\t\\twhile self.waiting:\\n\\t\\t\\tnd = self.waiting.pop(0)\\n\\t\\t\\tself.iter(nd)\\n\\n\\tdef iter(self, node):\\n\\t\\tpath = node.abspath(self.env) # obtain the absolute path\\n\\t\\tcode = \"\".join(filter_comments(path)) # read the file and filter the comments\\n\\t\\tnames = self.get_strings(code) # obtain the import strings\\n\\t\\tfor x in names:\\n\\t\\t\\t# optimization\\n\\t\\t\\tif x in self.allnames: continue\\n\\t\\t\\tself.allnames.append(x)\\n\\n\\t\\t\\t# for each name, see if it is like a node or not\\n\\t\\t\\tself.tryfind(x)\\n\\ndef scan(self):\\n\\t\"look for .d/.di the .d source need\"\\n\\tenv = self.env\\n\\tgruik = d_parser(env, env[\\'INC_PATHS\\'])\\n\\tgruik.start(self.inputs[0])\\n\\n\\tif Logs.verbose:\\n\\t\\tdebug(\\'deps: nodes found for %s: %s %s\\' % (str(self.inputs[0]), str(gruik.nodes), str(gruik.names)))\\n\\t\\t#debug(\"deps found for %s: %s\" % (str(node), str(gruik.deps)), \\'deps\\')\\n\\treturn (gruik.nodes, gruik.names)\\n\\ndef get_target_name(self):\\n\\t\"for d programs and libs\"\\n\\tv = self.env\\n\\ttp = \\'program\\'\\n\\tfor x in self.features:\\n\\t\\tif x in [\\'dshlib\\', \\'dstaticlib\\']:\\n\\t\\t\\ttp = x.lstrip(\\'d\\')\\n\\treturn v[\\'D_%s_PATTERN\\' % tp] % self.target\\n\\nd_params = {\\n\\'dflags\\': \\'\\',\\n\\'importpaths\\':\\'\\',\\n\\'libs\\':\\'\\',\\n\\'libpaths\\':\\'\\',\\n\\'generate_headers\\':False,\\n}\\n\\n@feature(\\'d\\')\\n@before(\\'apply_type_vars\\')\\ndef init_d(self):\\n\\tfor x in d_params:\\n\\t\\tsetattr(self, x, getattr(self, x, d_params[x]))\\n\\nclass d_taskgen(TaskGen.task_gen):\\n\\tdef __init__(self, *k, **kw):\\n\\t\\tTaskGen.task_gen.__init__(self, *k, **kw)\\n\\n\\t\\t# COMPAT\\n\\t\\tif len(k) > 1:\\n\\t\\t\\tself.features.append(\\'d\\' + k[1])\\n\\n# okay, we borrow a few methods from ccroot\\nTaskGen.bind_feature(\\'d\\', D_METHS)\\n\\n@feature(\\'d\\')\\n@before(\\'apply_d_libs\\')\\ndef init_d(self):\\n\\tUtils.def_attrs(self,\\n\\t\\tdflags=\\'\\',\\n\\t\\timportpaths=\\'\\',\\n\\t\\tlibs=\\'\\',\\n\\t\\tlibpaths=\\'\\',\\n\\t\\tuselib=\\'\\',\\n\\t\\tuselib_local=\\'\\',\\n\\t\\tgenerate_headers=False, # set to true if you want .di files as well as .o\\n\\t\\tcompiled_tasks=[],\\n\\t\\tadd_objects=[],\\n\\t\\tlink_task=None)\\n\\n@feature(\\'d\\')\\n@after(\\'apply_d_link\\', \\'init_d\\')\\n@before(\\'apply_vnum\\')\\ndef apply_d_libs(self):\\n\\t\"\"\"after apply_link because of \\'link_task\\'\\n\\tafter default_cc because of the attribute \\'uselib\\'\"\"\"\\n\\tenv = self.env\\n\\n\\t# 1. the case of the libs defined in the project (visit ancestors first)\\n\\t# the ancestors external libraries (uselib) will be prepended\\n\\tself.uselib = self.to_list(self.uselib)\\n\\tnames = self.to_list(self.uselib_local)\\n\\n\\tseen = set([])\\n\\ttmp = Utils.deque(names) # consume a copy of the list of names\\n\\twhile tmp:\\n\\t\\tlib_name = tmp.popleft()\\n\\t\\t# visit dependencies only once\\n\\t\\tif lib_name in seen:\\n\\t\\t\\tcontinue\\n\\n\\t\\ty = self.name_to_obj(lib_name)\\n\\t\\tif not y:\\n\\t\\t\\traise Utils.WafError(\\'object %r was not found in uselib_local (required by %r)\\' % (lib_name, self.name))\\n\\t\\ty.post()\\n\\t\\tseen.add(lib_name)\\n\\n\\t\\t# object has ancestors to process (shared libraries): add them to the end of the list\\n\\t\\tif getattr(y, \\'uselib_local\\', None):\\n\\t\\t\\tlst = y.to_list(y.uselib_local)\\n\\t\\t\\tif \\'dshlib\\' in y.features or \\'cprogram\\' in y.features:\\n\\t\\t\\t\\tlst = [x for x in lst if not \\'cstaticlib\\' in self.name_to_obj(x).features]\\n\\t\\t\\ttmp.extend(lst)\\n\\n\\t\\t# link task and flags\\n\\t\\tif getattr(y, \\'link_task\\', None):\\n\\n\\t\\t\\tlink_name = y.target[y.target.rfind(os.sep) + 1:]\\n\\t\\t\\tif \\'dstaticlib\\' in y.features or \\'dshlib\\' in y.features:\\n\\t\\t\\t\\tenv.append_unique(\\'DLINKFLAGS\\', env.DLIB_ST % link_name)\\n\\t\\t\\t\\tenv.append_unique(\\'DLINKFLAGS\\', env.DLIBPATH_ST % y.link_task.outputs[0].parent.bldpath(env))\\n\\n\\t\\t\\t# the order\\n\\t\\t\\tself.link_task.set_run_after(y.link_task)\\n\\n\\t\\t\\t# for the recompilation\\n\\t\\t\\tdep_nodes = getattr(self.link_task, \\'dep_nodes\\', [])\\n\\t\\t\\tself.link_task.dep_nodes = dep_nodes + y.link_task.outputs\\n\\n\\t\\t# add ancestors uselib too - but only propagate those that have no staticlib\\n\\t\\tfor v in self.to_list(y.uselib):\\n\\t\\t\\tif not v in self.uselib:\\n\\t\\t\\t\\tself.uselib.insert(0, v)\\n\\n\\t\\t# if the library task generator provides \\'export_incdirs\\', add to the include path\\n\\t\\t# the export_incdirs must be a list of paths relative to the other library\\n\\t\\tif getattr(y, \\'export_incdirs\\', None):\\n\\t\\t\\tfor x in self.to_list(y.export_incdirs):\\n\\t\\t\\t\\tnode = y.path.find_dir(x)\\n\\t\\t\\t\\tif not node:\\n\\t\\t\\t\\t\\traise Utils.WafError(\\'object %r: invalid folder %r in export_incdirs\\' % (y.target, x))\\n\\t\\t\\t\\tself.env.append_unique(\\'INC_PATHS\\', node)\\n\\n@feature(\\'dprogram\\', \\'dshlib\\', \\'dstaticlib\\')\\n@after(\\'apply_core\\')\\ndef apply_d_link(self):\\n\\tlink = getattr(self, \\'link\\', None)\\n\\tif not link:\\n\\t\\tif \\'dstaticlib\\' in self.features: link = \\'static_link\\'\\n\\t\\telse: link = \\'d_link\\'\\n\\n\\toutputs = [t.outputs[0] for t in self.compiled_tasks]\\n\\tself.link_task = self.create_task(link, outputs, self.path.find_or_declare(get_target_name(self)))\\n\\n@feature(\\'d\\')\\n@after(\\'apply_core\\')\\ndef apply_d_vars(self):\\n\\tenv = self.env\\n\\tdpath_st   = env[\\'DPATH_ST\\']\\n\\tlib_st     = env[\\'DLIB_ST\\']\\n\\tlibpath_st = env[\\'DLIBPATH_ST\\']\\n\\n\\timportpaths = self.to_list(self.importpaths)\\n\\tlibpaths = []\\n\\tlibs = []\\n\\tuselib = self.to_list(self.uselib)\\n\\n\\tfor i in uselib:\\n\\t\\tif env[\\'DFLAGS_\\' + i]:\\n\\t\\t\\tenv.append_unique(\\'DFLAGS\\', env[\\'DFLAGS_\\' + i])\\n\\n\\tfor x in self.features:\\n\\t\\tif not x in [\\'dprogram\\', \\'dstaticlib\\', \\'dshlib\\']:\\n\\t\\t\\tcontinue\\n\\t\\tx.lstrip(\\'d\\')\\n\\t\\td_shlib_dflags = env[\\'D_\\' + x + \\'_DFLAGS\\']\\n\\t\\tif d_shlib_dflags:\\n\\t\\t\\tenv.append_unique(\\'DFLAGS\\', d_shlib_dflags)\\n\\n\\t# add import paths\\n\\tfor i in uselib:\\n\\t\\tif env[\\'DPATH_\\' + i]:\\n\\t\\t\\tfor entry in self.to_list(env[\\'DPATH_\\' + i]):\\n\\t\\t\\t\\tif not entry in importpaths:\\n\\t\\t\\t\\t\\timportpaths.append(entry)\\n\\n\\t# now process the import paths\\n\\tfor path in importpaths:\\n\\t\\tif os.path.isabs(path):\\n\\t\\t\\tenv.append_unique(\\'_DIMPORTFLAGS\\', dpath_st % path)\\n\\t\\telse:\\n\\t\\t\\tnode = self.path.find_dir(path)\\n\\t\\t\\tself.env.append_unique(\\'INC_PATHS\\', node)\\n\\t\\t\\tenv.append_unique(\\'_DIMPORTFLAGS\\', dpath_st % node.srcpath(env))\\n\\t\\t\\tenv.append_unique(\\'_DIMPORTFLAGS\\', dpath_st % node.bldpath(env))\\n\\n\\t# add library paths\\n\\tfor i in uselib:\\n\\t\\tif env[\\'LIBPATH_\\' + i]:\\n\\t\\t\\tfor entry in self.to_list(env[\\'LIBPATH_\\' + i]):\\n\\t\\t\\t\\tif not entry in libpaths:\\n\\t\\t\\t\\t\\tlibpaths.append(entry)\\n\\tlibpaths = self.to_list(self.libpaths) + libpaths\\n\\n\\t# now process the library paths\\n\\t# apply same path manipulation as used with import paths\\n\\tfor path in libpaths:\\n\\t\\tenv.append_unique(\\'DLINKFLAGS\\', libpath_st % path)\\n\\n\\t# add libraries\\n\\tfor i in uselib:\\n\\t\\tif env[\\'LIB_\\' + i]:\\n\\t\\t\\tfor entry in self.to_list(env[\\'LIB_\\' + i]):\\n\\t\\t\\t\\tif not entry in libs:\\n\\t\\t\\t\\t\\tlibs.append(entry)\\n\\tlibs.extend(self.to_list(self.libs))\\n\\n\\t# process user flags\\n\\tfor flag in self.to_list(self.dflags):\\n\\t\\tenv.append_unique(\\'DFLAGS\\', flag)\\n\\n\\t# now process the libraries\\n\\tfor lib in libs:\\n\\t\\tenv.append_unique(\\'DLINKFLAGS\\', lib_st % lib)\\n\\n\\t# add linker flags\\n\\tfor i in uselib:\\n\\t\\tdlinkflags = env[\\'DLINKFLAGS_\\' + i]\\n\\t\\tif dlinkflags:\\n\\t\\t\\tfor linkflag in dlinkflags:\\n\\t\\t\\t\\tenv.append_unique(\\'DLINKFLAGS\\', linkflag)\\n\\n@feature(\\'dshlib\\')\\n@after(\\'apply_d_vars\\')\\ndef add_shlib_d_flags(self):\\n\\tfor linkflag in self.env[\\'D_shlib_LINKFLAGS\\']:\\n\\t\\tself.env.append_unique(\\'DLINKFLAGS\\', linkflag)\\n\\n@extension(EXT_D)\\ndef d_hook(self, node):\\n\\t# create the compilation task: cpp or cc\\n\\ttask = self.create_task(self.generate_headers and \\'d_with_header\\' or \\'d\\')\\n\\ttry: obj_ext = self.obj_ext\\n\\texcept AttributeError: obj_ext = \\'_%d.o\\' % self.idx\\n\\n\\ttask.inputs = [node]\\n\\ttask.outputs = [node.change_ext(obj_ext)]\\n\\tself.compiled_tasks.append(task)\\n\\n\\tif self.generate_headers:\\n\\t\\theader_node = node.change_ext(self.env[\\'DHEADER_ext\\'])\\n\\t\\ttask.outputs += [header_node]\\n\\nd_str = \\'${D_COMPILER} ${DFLAGS} ${_DIMPORTFLAGS} ${D_SRC_F}${SRC} ${D_TGT_F}${TGT}\\'\\nd_with_header_str = \\'${D_COMPILER} ${DFLAGS} ${_DIMPORTFLAGS} \\\\\\n${D_HDR_F}${TGT[1].bldpath(env)} \\\\\\n${D_SRC_F}${SRC} \\\\\\n${D_TGT_F}${TGT[0].bldpath(env)}\\'\\nlink_str = \\'${D_LINKER} ${DLNK_SRC_F}${SRC} ${DLNK_TGT_F}${TGT} ${DLINKFLAGS}\\'\\n\\ndef override_exec(cls):\\n\\t\"\"\"stupid dmd wants -of stuck to the file name\"\"\"\\n\\told_exec = cls.exec_command\\n\\tdef exec_command(self, *k, **kw):\\n\\t\\tif isinstance(k[0], list):\\n\\t\\t\\tlst = k[0]\\n\\t\\t\\tfor i in xrange(len(lst)):\\n\\t\\t\\t\\tif lst[i] == \\'-of\\':\\n\\t\\t\\t\\t\\tdel lst[i]\\n\\t\\t\\t\\t\\tlst[i] = \\'-of\\' + lst[i]\\n\\t\\t\\t\\t\\tbreak\\n\\t\\treturn old_exec(self, *k, **kw)\\n\\tcls.exec_command = exec_command\\n\\ncls = Task.simple_task_type(\\'d\\', d_str, \\'GREEN\\', before=\\'static_link d_link\\', shell=False)\\ncls.scan = scan\\noverride_exec(cls)\\n\\ncls = Task.simple_task_type(\\'d_with_header\\', d_with_header_str, \\'GREEN\\', before=\\'static_link d_link\\', shell=False)\\noverride_exec(cls)\\n\\ncls = Task.simple_task_type(\\'d_link\\', link_str, color=\\'YELLOW\\', shell=False)\\noverride_exec(cls)\\n\\n# for feature request #104\\n@taskgen\\ndef generate_header(self, filename, install_path):\\n\\tif not hasattr(self, \\'header_lst\\'): self.header_lst = []\\n\\tself.meths.append(\\'process_header\\')\\n\\tself.header_lst.append([filename, install_path])\\n\\n@before(\\'apply_core\\')\\ndef process_header(self):\\n\\tenv = self.env\\n\\tfor i in getattr(self, \\'header_lst\\', []):\\n\\t\\tnode = self.path.find_resource(i[0])\\n\\n\\t\\tif not node:\\n\\t\\t\\traise Utils.WafError(\\'file not found on d obj \\'+i[0])\\n\\n\\t\\ttask = self.create_task(\\'d_header\\')\\n\\t\\ttask.set_inputs(node)\\n\\t\\ttask.set_outputs(node.change_ext(\\'.di\\'))\\n\\nd_header_str = \\'${D_COMPILER} ${D_HEADER} ${SRC}\\'\\nTask.simple_task_type(\\'d_header\\', d_header_str, color=\\'BLUE\\', shell=False)\\n\\n@conftest\\ndef d_platform_flags(conf):\\n\\tv = conf.env\\n\\tbinfmt = v.DEST_BINFMT or Utils.unversioned_sys_platform_to_binary_format(\\n\\t\\tv.DEST_OS or Utils.unversioned_sys_platform())\\n\\tif binfmt == \\'pe\\':\\n\\t\\tv[\\'D_program_PATTERN\\']   = \\'%s.exe\\'\\n\\t\\tv[\\'D_shlib_PATTERN\\']     = \\'lib%s.dll\\'\\n\\t\\tv[\\'D_staticlib_PATTERN\\'] = \\'lib%s.a\\'\\n\\telse:\\n\\t\\tv[\\'D_program_PATTERN\\']   = \\'%s\\'\\n\\t\\tv[\\'D_shlib_PATTERN\\']     = \\'lib%s.so\\'\\n\\t\\tv[\\'D_staticlib_PATTERN\\'] = \\'lib%s.a\\'\\n\\n# quick test #\\nif __name__ == \"__main__\":\\n\\t#Logs.verbose = 2\\n\\n\\ttry: arg = sys.argv[1]\\n\\texcept IndexError: arg = \"file.d\"\\n\\n\\tprint(\"\".join(filter_comments(arg)))\\n\\t# TODO\\n\\tpaths = [\\'.\\']\\n\\n\\t#gruik = filter()\\n\\t#gruik.start(arg)\\n\\n\\t#code = \"\".join(gruik.buf)\\n\\n\\t#print \"we have found the following code\"\\n\\t#print code\\n\\n\\t#print \"now parsing\"\\n\\t#print \"-------------------------------------------\"\\n\\t\"\"\"\\n\\tparser_ = d_parser()\\n\\tparser_.start(arg)\\n\\n\\tprint \"module: %s\" % parser_.module\\n\\tprint \"imports: \",\\n\\tfor imp in parser_.imports:\\n\\t\\tprint imp + \" \",\\n\\tprint\\n\"\"\"\\n\\n',\n",
       " '# -*- coding: utf-8 -*-\\n#\\n# Configuration file for the Sphinx documentation builder.\\n#\\n# This file does only contain a selection of the most common options. For a\\n# full list see the documentation:\\n# http://www.sphinx-doc.org/en/stable/config\\n\\n# -- Path setup --------------------------------------------------------------\\n\\n# If extensions (or modules to document with autodoc) are in another directory,\\n# add these directories to sys.path here. If the directory is relative to the\\n# documentation root, use os.path.abspath to make it absolute, like shown here.\\n#\\n# import os\\n# import sys\\n# sys.path.insert(0, os.path.abspath(\\'.\\'))\\n\\n\\n# -- Project information -----------------------------------------------------\\n\\nproject = \\'analyze pairing and clustering of molecular systems\\'\\ncopyright = \"2018, Matthew W. Thompson\"\\nauthor = \\'Matthew W. Thompson\\'\\n\\n# The short X.Y version\\nversion = \\'\\'\\n# The full version, including alpha/beta/rc tags\\nrelease = \\'\\'\\n\\n\\n# -- General configuration ---------------------------------------------------\\n\\n# If your documentation needs a minimal Sphinx version, state it here.\\n#\\n# needs_sphinx = \\'1.0\\'\\n\\n# Add any Sphinx extension module names here, as strings. They can be\\n# extensions coming with Sphinx (named \\'sphinx.ext.*\\') or your custom\\n# ones.\\nextensions = [\\n    \\'sphinx.ext.autodoc\\',\\n    \\'sphinx.ext.mathjax\\',\\n]\\n\\n# Add any paths that contain templates here, relative to this directory.\\ntemplates_path = [\\'_templates\\']\\n\\n# The suffix(es) of source filenames.\\n# You can specify multiple suffix as a list of string:\\n#\\n# source_suffix = [\\'.rst\\', \\'.md\\']\\nsource_suffix = \\'.rst\\'\\n\\n# The master toctree document.\\nmaster_doc = \\'index\\'\\n\\n# The language for content autogenerated by Sphinx. Refer to documentation\\n# for a list of supported languages.\\n#\\n# This is also used if you do content translation via gettext catalogs.\\n# Usually you set \"language\" from the command line for these cases.\\nlanguage = None\\n\\n# List of patterns, relative to source directory, that match files and\\n# directories to ignore when looking for source files.\\n# This pattern also affects html_static_path and html_extra_path .\\nexclude_patterns = [\\'_build\\', \\'Thumbs.db\\', \\'.DS_Store\\']\\n\\n# The name of the Pygments (syntax highlighting) style to use.\\npygments_style = \\'sphinx\\'\\n\\n\\n# -- Options for HTML output -------------------------------------------------\\n\\n# The theme to use for HTML and HTML Help pages.  See the documentation for\\n# a list of builtin themes.\\n#\\nhtml_theme = \\'sphinx_rtd_theme\\'\\n\\n# Theme options are theme-specific and customize the look and feel of a theme\\n# further.  For a list of options available for each theme, see the\\n# documentation.\\n#\\n# html_theme_options = {}\\n\\n# Add any paths that contain custom static files (such as style sheets) here,\\n# relative to this directory. They are copied after the builtin static files,\\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\\nhtml_static_path = [\\'_static\\']\\n\\n# Custom sidebar templates, must be a dictionary that maps document names\\n# to template names.\\n#\\n# The default sidebars (for documents that don\\'t match any pattern) are\\n# defined by theme itself.  Builtin themes are using these templates by\\n# default: ``[\\'localtoc.html\\', \\'relations.html\\', \\'sourcelink.html\\',\\n# \\'searchbox.html\\']``.\\n#\\n# html_sidebars = {}\\n\\n\\n# -- Options for HTMLHelp output ---------------------------------------------\\n\\n# Output file base name for HTML help builder.\\nhtmlhelp_basename = \\'pairingdoc\\'\\n\\n\\n# -- Options for LaTeX output ------------------------------------------------\\n\\nlatex_elements = {\\n    # The paper size (\\'letterpaper\\' or \\'a4paper\\').\\n    #\\n    # \\'papersize\\': \\'letterpaper\\',\\n\\n    # The font size (\\'10pt\\', \\'11pt\\' or \\'12pt\\').\\n    #\\n    # \\'pointsize\\': \\'10pt\\',\\n\\n    # Additional stuff for the LaTeX preamble.\\n    #\\n    # \\'preamble\\': \\'\\',\\n\\n    # Latex figure (float) alignment\\n    #\\n    # \\'figure_align\\': \\'htbp\\',\\n}\\n\\n# Grouping the document tree into LaTeX files. List of tuples\\n# (source start file, target name, title,\\n#  author, documentclass [howto, manual, or own class]).\\nlatex_documents = [\\n    (master_doc, \\'pairing.tex\\', \\'pairing Documentation\\',\\n     \\'pairing\\', \\'manual\\'),\\n]\\n\\n\\n# -- Options for manual page output ------------------------------------------\\n\\n# One entry per manual page. List of tuples\\n# (source start file, name, description, authors, manual section).\\nman_pages = [\\n    (master_doc, \\'pairing\\', \\'pairing Documentation\\',\\n     [author], 1)\\n]\\n\\n\\n# -- Options for Texinfo output ----------------------------------------------\\n\\n# Grouping the document tree into Texinfo files. List of tuples\\n# (source start file, target name, title, author,\\n#  dir menu entry, description, category)\\ntexinfo_documents = [\\n    (master_doc, \\'pairing\\', \\'pairing Documentation\\',\\n     author, \\'pairing\\', \\'analyze pairing and clustering of molecular systems\\',\\n     \\'Miscellaneous\\'),\\n]\\n\\n\\n# -- Extension configuration -------------------------------------------------\\n',\n",
       " 'v = \\'2.1\\'\\n\\ntry:\\n    import UEManifestReader\\n    import coloredlogs\\n    import aioconsole\\n    import webbrowser\\n    import subprocess\\n    import crayons\\n    import logging\\n    import asyncio\\n    import psutil\\n    import json\\n    import time\\n    import sys\\n    import os\\nexcept:\\n    print(\\'It seems that some modules are missing. Run \"INSTALL.bat\" and try again.\\')\\n    input(\\'Press ENTER to exit\\')\\n\\nfrom os import kill\\nfrom modules import http\\n\\nlog = logging.getLogger(\\'FortniteLauncher\\')\\n\\nconfiguration = json.load(open(\\'config.json\\', \\'r\\', encoding = \\'utf-8\\'))\\nauths = json.load(open(\\'auths.json\\', \\'r\\', encoding = \\'utf-8\\'))\\n\\ndef get_colored_box(color, text):\\n\\n    return f\\'{color(\"[\")}{text}{color(\"]\")}\\'\\n\\nasync def get_other_clients():\\n\\n    log.debug(\\'Looking for other running clients...\\')\\n\\n    clients = []\\n\\n    for p in psutil.process_iter([\\'name\\', \\'pid\\']):\\n        if p.info[\\'name\\'] == \\'FortniteClient-Win64-Shipping.exe\\':\\n            clients.append(p.info[\\'pid\\'])\\n\\n    log.debug(f\\'Found {len(clients)} clients.\\')\\n\\n    return clients\\n\\nasync def wait_for_game_spawn(process: psutil.Process, ignore: list):\\n\\n    log.debug(f\\'Waiting for game to spawn...\\')\\n\\n    while True:\\n        if process.is_running() == False:\\n            return False\\n        for p in psutil.process_iter([\\'name\\', \\'pid\\']):\\n            if p.info[\\'name\\'] == \\'FortniteClient-Win64-Shipping.exe\\':\\n                if p.info[\\'pid\\'] in ignore:\\n                    continue\\n                return True\\n\\n\\nasync def add_account():\\n\\n    log.debug(\\'add_account flow started.\\')\\n\\n    print()\\n    print(crayons.green(\\'Add Account\\', bold=True))\\n\\n    auth_type = configuration[\\'auth_type\\']\\n\\n    LAUNCHER_AUTHORIZATION_URL = \\'https://www.epicgames.com/id/api/redirect?clientId=34a02cf8f4414e29b15921876da36f9a&responseType=code\\'\\n    LAUNCHER_AUTHORIZATION_URL_LOGIN = \\'https://www.epicgames.com/id/logout?redirectUrl=https%3A%2F%2Fwww.epicgames.com%2Fid%2Flogin%3FredirectUrl%3Dhttps%253A%252F%252Fwww.epicgames.com%252Fid%252Fapi%252Fredirect%253FclientId%253D34a02cf8f4414e29b15921876da36f9a%2526responseType%253Dcode\\'\\n\\n    IOS_AUTHORIZATION_URL = \\'https://www.epicgames.com/id/api/redirect?clientId=3446cd72694c4a4485d81b77adbb2141&responseType=code\\'\\n    IOS_AUTHORIZATION_URL_LOGIN = \\'https://www.epicgames.com/id/logout?redirectUrl=https%3A%2F%2Fwww.epicgames.com%2Fid%2Flogin%3FredirectUrl%3Dhttps%253A%252F%252Fwww.epicgames.com%252Fid%252Fapi%252Fredirect%253FclientId%253D3446cd72694c4a4485d81b77adbb2141%2526responseType%253Dcode\\'\\n\\n    while True:\\n        user_selection = await aioconsole.ainput(f\\'Are you logged in to the required account in your web browser?\\\\nType {crayons.white(\"1\", bold=True)} if yes.\\\\nType {crayons.white(\"2\", bold=True)} if no.\\\\n\\')\\n\\n        user_logged = user_selection.strip(\\' \\')\\n\\n        if user_logged == \\'1\\':\\n\\n            if auth_type == \\'refresh\\':\\n\\n                choosen_url = LAUNCHER_AUTHORIZATION_URL\\n            else:\\n                choosen_url = IOS_AUTHORIZATION_URL\\n\\n        elif user_logged == \\'2\\':\\n\\n            if auth_type == \\'refresh\\':\\n                choosen_url = LAUNCHER_AUTHORIZATION_URL_LOGIN\\n            else:\\n                choosen_url = IOS_AUTHORIZATION_URL_LOGIN\\n\\n        else:\\n\\n            print(\\'Select a valid option! Try again\\\\n\\')\\n\\n            continue\\n        break\\n\\n    webbrowser.open_new_tab(choosen_url)\\n\\n    print(choosen_url)\\n    if user_logged == \\'1\\':\\n        print(\\'An epic games page should be opened in your web brower. Paste the authorizationCode here:\\')\\n    else:\\n        print(\\'An epic games page should be opened in your web brower. Login on the required account and then paste the authorizationCode here:\\')\\n    \\n    user_code = await aioconsole.ainput(\\'> \\')\\n\\n    code = user_code.strip(\\' \\')\\n\\n    if code in [\\'cancel\\', \\'c\\']:\\n        log.debug(\\'add_account flow stopped. User cancelled\\')\\n        print(\\'Account add cancelled\\')\\n        return False\\n\\n    if len(code) != 32:\\n        log.debug(\\'add_account flow stopped. The code from the user was invalid.\\')\\n        print(f\\'Failed account add. The code\\\\\\'s lenght is invalid. A valid authorization code is 32 characters long.\\')\\n        return False\\n\\n    Auth = http.EpicAPI()\\n\\n    if auth_type == \\'refresh\\':\\n\\n        auth_request = await Auth.authorization_code_auth(code)\\n\\n        if \\'errorCode\\' not in auth_request.text:\\n\\n            oauth_json = auth_request.json()\\n\\n            credentials = {}\\n\\n            credentials[\\'auth_type\\'] = \\'refresh\\'\\n            credentials[\\'refresh_token\\'] = str(oauth_json[\\'refresh_token\\'])\\n            credentials[\\'refresh_expires\\'] = int(time.time()) + oauth_json[\\'refresh_expires\\']\\n\\n            auths[oauth_json[\\'displayName\\']] = credentials\\n\\n            with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n            log.debug(\\'add_account flow completed without errors.\\')\\n\\n            return f\\'Account \"{oauth_json[\"displayName\"]}\" added successfully! (Note: this login will expire after 23 days of inactivity)\\'\\n\\n        else:\\n            print(f\\'Authentication failed. {auth_request.json()[\"errorMessage\"]}\\')\\n            log.debug(\\'add_account flow stopped. The authentication failed.\\')\\n            return False\\n    \\n    elif auth_type == \\'device\\':\\n\\n        auth_request = await Auth.authorization_code_auth(code, client = http.Clients.fortniteIOSGameClient)\\n\\n        if \\'errorCode\\' not in auth_request.text:\\n\\n            oauth_json = auth_request.json()\\n\\n            device_create = await Auth.create_device_auth(oauth_json[\\'access_token\\'], oauth_json[\\'account_id\\'])\\n\\n            if \\'errorCode\\' not in device_create.text:\\n\\n                device_json = device_create.json()\\n\\n                credentials = {}\\n                \\n                credentials[\\'auth_type\\'] = \\'device\\'\\n                credentials[\\'account_id\\'] = device_json[\\'accountId\\']\\n                credentials[\\'device_id\\'] = device_json[\\'deviceId\\']\\n                credentials[\\'secret\\'] = device_json[\\'secret\\']\\n\\n\\n                auths[oauth_json[\\'displayName\\']] = credentials\\n\\n                with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                    json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n                await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n\\n                return f\\'Account \"{oauth_json[\"displayName\"]}\" added successfully!\\'\\n\\n            else:\\n                print(f\\'Device auth creation failed. {auth_request.json()[\"errorMessage\"]}\\')\\n                log.debug(\\'add_account flow stopped. The authentication failed.\\')\\n                return False\\n\\n        else:\\n            print(f\\'Authentication failed. {auth_request.json()[\"errorMessage\"]}\\')\\n            log.debug(\\'add_account flow stopped. The authentication failed.\\')\\n            return False\\n\\n\\nasync def remove_account():\\n\\n    log.debug(\\'remove_account flow started.\\')\\n\\n    print()\\n    print(crayons.red(\\'Remove Account\\', bold=True))\\n\\n    while True:\\n\\n        account_list = list(auths.keys())\\n        countlist = []\\n        count = 0\\n\\n        for account in account_list:\\n            count += 1\\n            countlist.append(count)\\n            print(f\\'{get_colored_box(crayons.red, str(count))} {account}\\')\\n\\n        print(f\\'{get_colored_box(crayons.green, \"C\")} Cancel\\\\n\\')\\n\\n\\n        user_selection = await aioconsole.ainput(f\\'Select an account: \\')\\n\\n        try:\\n            user_selection.strip(\\' \\')\\n\\n            if user_selection.lower() in [\\'c\\', \\'cancel\\']:\\n                print(crayons.red(\\'Account remove cancelled.\\'))\\n                log.debug(\\'remove_account flow cancelled by user.\\')\\n                return False\\n\\n            if int(user_selection) not in countlist:\\n                print(crayons.red(\\'Invalid selection\\\\n\\'))\\n                continue\\n\\n            else:\\n                break\\n        except:\\n            print(crayons.red(\\'Select a valid option\\\\n\\'))\\n            continue\\n\\n    credentials = auths[account_list[int(user_selection) - 1]]\\n\\n    if credentials[\\'auth_type\\'] == \\'refresh\\':\\n\\n        if int(time.time()) > credentials[\\'refresh_expires\\']:\\n\\n            del auths[account_list[int(user_selection) - 1]]\\n\\n            with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n            log.debug(\\'remove_account flow completed. The saved refresh wasn\\\\\\'t valid and removed from auths.json file\\')\\n            print(\\'Account removed successfully.\\')\\n            return True\\n        \\n        else:\\n\\n            Auth = http.EpicAPI()\\n            auth_request = await Auth.refresh_token_auth(refresh_token = credentials[\\'refresh_token\\'])\\n\\n            if \\'errorCode\\' not in auth_request.text:\\n\\n                oauth_json = auth_request.json()\\n                \\n                kill_request = await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n\\n                if kill_request not in [401, 403]:\\n\\n                    del auths[account_list[int(user_selection) - 1]]\\n\\n                    with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                        json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n                    log.debug(\\'remove_account flow completed without errors\\')\\n                    print(\\'Account removed successfully.\\')\\n                    return True\\n\\n            else:\\n\\n                print(f\\'Authentication failed. {auth_request.json()[\"errorMessage\"]}\\')\\n                print(\\'Removing account from auths.json file anyway.\\')\\n\\n                del auths[account_list[int(user_selection) - 1]]\\n\\n                with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                    json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n                log.debug(\\'remove_account flow failed successfully. Authentication failed but removed from auths.json anyways\\')\\n\\n                print(\\'Account removed.\\') # task failed successfully\\n                return True\\n\\n    elif credentials[\\'auth_type\\'] == \\'device\\':\\n\\n        Auth = http.EpicAPI()\\n\\n        auth_request = await Auth.device_auths_auth(credentials)\\n\\n        if \\'errorCode\\' not in auth_request.text:\\n\\n            oauth_json = auth_request.json()\\n\\n            kill_device = await Auth.delete_device_auth(oauth_json[\\'access_token\\'], account_id=credentials[\\'account_id\\'], device_id=credentials[\\'device_id\\'])\\n\\n            if kill_device.status_code not in [401, 403]:\\n\\n                del auths[account_list[int(user_selection) - 1]]\\n\\n                with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                    json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n                await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n\\n                log.debug(\\'remove_account flow completed without errors\\')\\n                print(\\'Account removed successfully.\\')\\n                return True\\n\\n            else:\\n\\n                print(f\\'Device auth delete failed. {kill_device.json()[\"errorMessage\"]}\\')\\n                print(\\'Removing account from auths.json anyway. Change the account password to make sure you kill the device auth.\\')\\n\\n                del auths[account_list[int(user_selection) - 1]]\\n\\n                with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                    json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n                log.debug(\\'remove_account flow failed successfully. Device delete failed but removed from auths.json anyways\\')\\n\\n                await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n\\n                print(\\'Account removed.\\') # task failed successfully\\n                return True\\n\\n        else:\\n\\n                print(f\\'Authentication failed. {auth_request.json()[\"errorMessage\"]}\\')\\n                print(\\'Removing account from auths.json anyway.\\')\\n\\n                del auths[account_list[int(user_selection) - 1]]\\n\\n                with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                    json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n                log.debug(\\'remove_account flow failed successfully. Authentication failed but removed from auths.json anyways\\')\\n\\n                print(\\'Account removed.\\') # task failed successfully\\n                return True\\n\\n    \\nasync def launch_game(exchange_code: str, launch_command: str):\\n\\n    log.debug(\\'Launching game...\\')\\n\\n    fortnite_path = configuration[\\'fortnite_path\\']\\n    executable_args = launch_command\\n    additional_args = configuration[\"commandline_arguments\"]\\n\\n    log.debug(\\'Preparing command line arguments.\\')\\n\\n    args = [\\n        executable_args,\\n        \\'-AUTH_LOGIN=unused\\',\\n        f\\'-AUTH_PASSWORD={exchange_code}\\',\\n        \\'-AUTH_TYPE=exchangecode\\',\\n        \\'-epicapp=Fortnite\\',\\n        \\'-epicenv=Prod\\',\\n        \\'-EpicPortal\\',\\n    ]\\n\\n    for i in additional_args:\\n        if i.startswith(\\'-\\'):\\n            args.append(i)\\n\\n    ignore_list = await get_other_clients()\\n\\n    log.debug(f\\'Starting FortniteLauncher.exe with args {args}...\\')\\n\\n    FortniteLauncher = subprocess.Popen([f\\'{fortnite_path}/FortniteGame/Binaries/Win64/FortniteLauncher.exe\\'] + args, cwd=f\\'{fortnite_path}/FortniteGame/Binaries/Win64/\\', stdout=subprocess.DEVNULL)\\n    process = psutil.Process(pid = FortniteLauncher.pid)\\n\\n    wait_spawn = await wait_for_game_spawn(process, ignore_list)\\n\\n    if wait_spawn == True:\\n\\n        log.debug(\\'Game launched correctly.\\')\\n        return True\\n\\n    else:\\n\\n        log.debug(\\'Game did\\\\\\'nt launch.\\')\\n        return False\\n\\n\\nasync def start():\\n\\n    if \\'--debug\\' in sys.argv:\\n        coloredlogs.install(\\n            level=\\'DEBUG\\'\\n        )\\n\\n    while True:\\n\\n        print()\\n\\n        print(f\\'\\\\n{crayons.cyan(\"Fortnite Launcher\", bold=True)} | {crayons.white(f\"Beta v{v}\", bold=True)}\\\\n\\')\\n\\n        try:\\n            configuration = json.load(open(\\'config.json\\', \\'r\\', encoding = \\'utf-8\\'))\\n\\n            if configuration[\\'auth_type\\'] not in [\\'refresh\\', \\'device\\']:\\n                print(\\'Error, the choosen auth type in configuration file isn\\\\\\'t valid. Auth type must be \"refresh\" or \"device\".\\')\\n                await aioconsole.ainput(\\'Press ENTER to exit\\')\\n                exit()\\n\\n        except Exception as e:\\n            print(f\\'An error ocurred loading config.json file. {e}\\')\\n            await aioconsole.ainput(\\'Press ENTER to exit\\')\\n            exit()\\n\\n        try:\\n            auths = json.load(open(\\'auths.json\\', \\'r\\', encoding = \\'utf-8\\'))\\n        except Exception as e:\\n            print(f\\'An error ocurred loading auths.json file. {e}\\')\\n            await aioconsole.ainput(\\'Press ENTER to exit\\')\\n            exit()\\n\\n        account_list = list(auths.keys())\\n        countlist = []\\n        count = 0\\n\\n        for account in account_list:\\n            count += 1\\n            countlist.append(count)\\n            print(f\\'{get_colored_box(crayons.green, str(count))} {account}\\')\\n\\n        print(f\\'\\\\n{get_colored_box(crayons.blue, \"A\")} Add an account\\')\\n        print(f\\'{get_colored_box(crayons.blue, \"R\")} Remove an account\\\\n\\')\\n        print(f\\'{get_colored_box(crayons.red, \"X\")} Exit\\\\n\\')\\n\\n        user_selection = await aioconsole.ainput(f\\'Select an option: \\')\\n\\n        try:\\n            user_selection.strip(\\' \\')\\n\\n            if user_selection.lower() == \\'x\\':\\n                exit()\\n\\n            if user_selection.lower() == \\'a\\':\\n                add = await add_account()\\n                if isinstance(add, str):\\n                    print(add)\\n                continue\\n\\n            if user_selection.lower() == \\'r\\':\\n                if len(account_list) == 0:\\n                    print(\\'There is no accounts to remove!\\\\n\\')\\n                    continue\\n                \\n                else:\\n                    await remove_account()\\n                    continue\\n\\n            if int(user_selection) not in countlist:\\n                print(crayons.red(\\'Invalid selection\\\\n\\'))\\n                continue\\n\\n        except:\\n            print(crayons.red(\\'Select a valid option\\\\n\\'))\\n            continue\\n\\n        selected_account = int(user_selection) - 1\\n\\n        game_folder = configuration[\\'fortnite_path\\']\\n\\n        if os.path.isdir(game_folder) == False:\\n            print(\\'Seems like the fortnite path in configuration is not valid. Check it and try again\\')\\n            await aioconsole.ainput(\\'Press ENTER to exit\\')\\n\\n        else:\\n\\n            credentials = auths[account_list[selected_account]]\\n\\n            auth_type = credentials[\\'auth_type\\']\\n\\n            if auth_type == \\'refresh\\':\\n\\n                if int(time.time()) > credentials[\\'refresh_expires\\']:\\n                    print(\\'The credentials of this account have expired. Re-add the account and try again\\')\\n\\n                Auth = http.EpicAPI()\\n                auth_request = await Auth.refresh_token_auth(refresh_token = credentials[\\'refresh_token\\'])\\n\\n                if \\'errorCode\\' not in auth_request.text:\\n\\n                    oauth_json = auth_request.json()\\n\\n                    credentials[\\'refresh_token\\'] = str(oauth_json[\\'refresh_token\\'])\\n                    credentials[\\'refresh_expires\\'] = int(time.time()) + oauth_json[\\'refresh_expires\\']\\n\\n                    auths[account_list[selected_account]] = credentials\\n\\n                    with open(\\'auths.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n                        json.dump(auths, f, indent=4, ensure_ascii=False)\\n\\n                    exchange_request = await Auth.get_exchange_code(oauth_json[\\'access_token\\'])\\n\\n                    if \\'errorCode\\' not in exchange_request.text:\\n\\n                        exchange_json = exchange_request.json()\\n                        launch_command = \\'\\'\\n\\n                        launch_info = await Auth.get_launch_info()\\n                        if launch_info.status_code == 200:\\n                            log.debug(\\'Using baydev api launch args.\\')\\n                            launch_command = launch_info.json()[\\'data\\'][\\'launch_args\\']\\n                            log.debug(f\\'Launch args for build {launch_info.json()[\"data\"][\"build\"]}\\')\\n\\n                        else:\\n                            log.debug(\\'Using epicgames manifest launch args.\\')\\n                            Reader = UEManifestReader.UEManifestReader()\\n                            manifest = await Reader.download_manifest()\\n                            launch_command = manifest[\\'LaunchCommand\\']\\n\\n                        print(\\'Launching...\\')\\n\\n                        launch_try = await launch_game(exchange_json[\\'code\\'], launch_command)\\n\\n                        if launch_try == False:\\n                            print(\\'Failed game launch.\\')\\n                            await asyncio.sleep(2)\\n                            continue\\n\\n                        else:\\n\\n                            print(\\'Launched.\\')\\n                            await asyncio.sleep(3)\\n                            exit()\\n\\n                    else:\\n                        print(f\\'Exchange code request failed. {exchange_request.json()[\"errorMessage\"]}\\')\\n                        continue\\n\\n                else:\\n                    print(f\\'Authentication failed. {auth_request.json()[\"errorMessage\"]}\\')\\n                    continue\\n\\n            else:\\n\\n                Auth = http.EpicAPI()\\n                auth_request = await Auth.device_auths_auth(credentials)\\n\\n                if \\'errorCode\\' not in auth_request.text:\\n\\n                    oauth_json = auth_request.json()\\n\\n                    exchange_request = await Auth.get_exchange_code(oauth_json[\\'access_token\\'])\\n\\n                    if \\'errorCode\\' not in exchange_request.text:\\n\\n                        exchange_auth = exchange_request.json()\\n\\n                        launcher_auth_request = await Auth.exchange_code_auth(exchange_auth[\\'code\\'])\\n\\n                        if \\'errorCode\\' not in launcher_auth_request.text:\\n\\n                            launcher_auth = launcher_auth_request.json()\\n\\n                            launcher_exchange_request = await Auth.get_exchange_code(launcher_auth[\\'access_token\\'])\\n\\n                            if \\'errorCode\\' not in launcher_exchange_request.text:\\n\\n                                final_exchange_json = launcher_exchange_request.json()\\n\\n                                launch_command = \\'\\'\\n\\n                                launch_info = await Auth.get_launch_info()\\n                                if launch_info.status_code == 200:\\n                                    log.debug(\\'Using baydev api launch args.\\')\\n                                    launch_command = launch_info.json()[\\'data\\'][\\'launch_args\\']\\n\\n                                else:\\n                                    print(f\\'Baydev api returned status {launch_info.status_code}. Downloading and parsing manifest manually. (This may take a while)\\')\\n                                    log.debug(\\'Using epicgames manifest launch args. (This may take a while)\\')\\n                                    Reader = UEManifestReader.UEManifestReader()\\n                                    manifest = await Reader.download_manifest()\\n                                    launch_command = manifest[\\'LaunchCommand\\']\\n\\n                                print(\\'Launching...\\')\\n\\n                                launch_try = await launch_game(final_exchange_json[\\'code\\'], launch_command)\\n\\n                                if launch_try == False:\\n                                    print(\\'Failed game launch.\\')\\n                                    await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n                                    await Auth.kill_oauth_session(launcher_auth[\\'access_token\\'])\\n                                    await asyncio.sleep(2)\\n                                    continue\\n\\n                                else:\\n\\n                                    print(\\'Launched.\\')\\n                                    await asyncio.sleep(3)\\n                                    await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n                                    await Auth.kill_oauth_session(launcher_auth[\\'access_token\\'])\\n                                    exit()\\n                            \\n                            else:\\n                                print(f\\'Launcher exchange code generate failed. {launcher_exchange_request.json()[\"errorMessage\"]}\\')\\n                                await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n                                await Auth.kill_oauth_session(launcher_auth[\\'access_token\\'])\\n                                continue\\n\\n                        else:\\n                            print(f\\'Launcher exchange auth failed. {launcher_auth_request.json()[\"errorMessage\"]}\\')\\n                            await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n                            continue\\n\\n                    else:\\n                        print(f\\'Exchange code request failed. {exchange_request.json()[\"errorMessage\"]}\\')\\n                        await Auth.kill_oauth_session(oauth_json[\\'access_token\\'])\\n                        continue\\n                \\n                else:\\n                    print(f\\'Authentication failed. {auth_request.json()[\"errorMessage\"]}\\')\\n                    continue\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n\\n    loop = asyncio.get_event_loop()\\n    loop.run_until_complete(start())',\n",
       " 'import io\\nimport numpy as np\\nimport json\\nimport requests\\nimport h5py\\n\\n\\nseq_len = 100\\nfile_name = \"output_video.mp4\"\\ndata_file = \"flame_params.hdf5\"\\n\\n\\ndef byteify(x):\\n    memfile = io.BytesIO()\\n    np.save(memfile, x)\\n    memfile.seek(0)\\n    return memfile.read().decode(\"latin-1\")\\n\\n\\ndef get_face(x, seq_len):\\n    return {\\n        \"expression\": byteify(x[\"tf_exp\"][:seq_len]),\\n        \"pose\": byteify(x[\"tf_pose\"][:seq_len]),\\n        \"shape\": byteify(x[\"tf_shape\"][:seq_len]),\\n        \"rotation\": byteify(x[\"tf_rot\"][:seq_len]),\\n    }\\n\\n\\nwith h5py.File(data_file, \"r\") as f:\\n    p1 = f[\"sessions/1/participants/P1\"]\\n    p2 = f[\"sessions/1/participants/P2\"]\\n\\n    serialized = json.dumps(\\n        {\\n            \"seqs\": [get_face(p1, seq_len), get_face(p2, seq_len)],\\n            \"file_name\": file_name,\\n            \"fps\": 25,\\n        }\\n    )\\ntry:\\n    resp = requests.post(\"http://localhost:8000/render\", data=serialized, timeout=600)\\n    resp.raise_for_status()\\n    print(resp.json())\\nexcept requests.exceptions.HTTPError:\\n    print(\"render request: failed on the server..\")\\nexcept requests.exceptions.Timeout:\\n    print(\"render request: timed out\")\\nexcept requests.exceptions.ConnectionError:\\n    print(\"render request: connection error\")\\n',\n",
       " \"# Generated by Django 3.2.2 on 2021-05-18 15:01\\n\\nfrom django.db import migrations, models\\nimport django.db.models.deletion\\n\\n\\nclass Migration(migrations.Migration):\\n\\n    dependencies = [\\n        ('usd_rest_api', '0006_alter_lesson_group'),\\n    ]\\n\\n    operations = [\\n        migrations.AddField(\\n            model_name='event',\\n            name='account',\\n            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, to='usd_rest_api.account'),\\n        ),\\n    ]\\n\",\n",
       " '#!/usr/bin/env python\\n\\n\"\"\"\\nA basic tool for working with the Cards corpus transcripts.\\n\"\"\"\\n\\n__author__ = \"Christopher Potts\"\\n__date__ = \"2012-03-03\"\\n__credits__ = \"Thanks to Karl Schultz for designing the data collecting program, and \\\\\\n               to David Clausen, Alex Djalali, Sven Lauer, Victoria Schwanda, Chriz Czyzewicz, \\\\\\n               and the rest of the SUBTLE team for their help with data collection. \\\\\\n               This research was supported in part by ONR grant No. N00014-10-1-0109 and \\\\\\n               ARO grant No. W911NF-07-1-0216.\"\\n__license__ = \"Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License: \\\\\\n               http://creativecommons.org/licenses/by-nc-sa/3.0/\"\\n__version__ = \"2.0\"\\n__maintainer__ = \"Christopher Potts\"\\n__email__ = \"See the author\\'s website\"\\n\\n######################################################################\\n\\nimport re\\nimport os\\nimport sys\\nimport csv\\nimport codecs\\nimport datetime\\n#import pytz\\nfrom glob import iglob\\n\\n######################################################################\\n###### ACTIONS CAPTURED BY PRAGBOT TRANSCRIPTS\\n#\\n# These variables are useful to have named, and this also serves as a\\n# reference for the complete list of meta-data values.\\n\\n# Official names for the two players, as written in the transcripts.\\nPLAYER1 = \"Player 1\"\\nPLAYER2 = \"Player 2\"\\n\\n# META-DATA ABOUT THE GAME\\nORIGINAL_FILENAME  = r\"ORIGINAL_FILENAME\"   # Filename as created by the program; included for book-keeping purposes.\\nCOLLECTION_SITE    = r\"COLLECTION_SITE\"     # \\'Amazon Mechanical Turk\\' or \\'Penn\\'\\nTASK_COMPLETED     = r\"TASK_COMPLETED\"      # Completion time - also encode in ORIGINAL_FILENAME\\nPLAYER             = r\"PLAYER_[12]\"         # Player Id in the format A[0-9]{5}, else \\'UNKNOWN\\'\\nP1_MAX_LINEOFSIGHT = r\"P1_MAX_LINEOFSIGHT\"  # Distance the players could \\'see\\'\\nP2_MAX_LINEOFSIGHT = r\"P2_MAX_LINEOFSIGHT\"\\nENVIRONMENT        = r\"CREATE_ENVIRONMENT\"  # ASCII representation of the world\\nP1_MAX_TURNS       = r\"P1_MAX_TURNS\"        # Player\\'s maximum allowed turns\\nP2_MAX_TURNS       = r\"P2_MAX_TURNS\"\\nP1_MAX_CARDS       = r\"P1_MAX_CARDS\"        # Number of cards the player could hold at any given time\\nP2_MAX_CARDS       = r\"P2_MAX_CARDS\"\\nPLAYER_2_TASK_ID   = r\"PLAYER_2_TASK_ID\"\\nPLAYER_1_TASK_ID   = r\"PLAYER_1_TASK_ID\"\\nGOAL_DESCRIPTION   = r\"GOAL_DESCRIPTION\"    # High-level goal (always the same in the v1 release)\\n# ACTIONS\\n## utterances\\nUTTERANCE          = r\"CHAT_MESSAGE_PREFIX\"\\n## locations\\nINITIAL_LOCATION   = r\"PLAYER_INITIAL_LOCATION\"\\nMOVE               = r\"PLAYER_MOVE\"\\n## card movements\\nPICKUP             = r\"PLAYER_PICKUP_CARD\"\\nDROP               = r\"PLAYER_DROP_CARD\"\\n# finish\\nTASK_COMPLETE      = r\"TASK_COMPLETE_CLICKED\"\\nCLOSE_SOCKETS      = r\"CLOSE_SOCKETS\"\\n\\n######################################################################\\n\\nclass Corpus:\\n    \"\"\"\\n    Corpus instances are built from the directory name of the\\n    corpus. Thus, if your program is in the same directory as the root\\n    of the corpus, you can use\\n\\n    corpus = Corpus(\\'transcripts\\')\\n\\n    to build a corpus object.  Relative and full absolute paths also\\n    work.\\n\\n    Corpus objects exist mainly as iterators. The methods\\n    iter_transcripts() and iter_events() allow you to move through the\\n    entire corpus efficiently.\\n    \"\"\"\\n        \\n    def __init__(self, dirname):\\n        \"\"\"\\n        Argument:\\n        dirname -- the root of the corpus transcripts\\n        \"\"\"\\n        self.dirname = dirname\\n\\n    def iter_transcripts(self, display_progress=True):\\n        \"\"\"\\n        Iterate through the transcripts, by yielding Transcript objects one-by-one.\\n\\n        Keyword argument:\\n        display_progress -- should create an overwriting progress bar to stderr if set to True (default: True)        \\n        \"\"\"\\n        trans_no = 1\\n        for filename in iglob(os.path.join(self.dirname, \\'*/*.csv\\')):\\n            if display_progress:\\n                sys.stderr.write(\\'\\\\r\\') ; sys.stderr.write(\\'transcript %s\\' % trans_no) ; sys.stderr.flush() ; trans_no += 1\\n            yield Transcript(filename)\\n        if display_progress:\\n            sys.stderr.write(\\'\\\\n\\')\\n\\n    def iter_events(self, display_progress=True):\\n        \"\"\"\\n        Iterate through the events, by yielding Event objects\\n        one-by-one.  This is useful if you don\\'t need to rely on the\\n        transcripts as a unit --- say, because you\\'re just counting\\n        words for the whole corpus.\\n\\n        Keyword argument:\\n        display_progress -- should create an overwriting progress bar to stderr if set to True (default: True)        \\n        \"\"\"\\n        for trans in self.iter_transcripts(display_progress=display_progress):\\n            for event in trans.iter_events():\\n                yield event\\n\\n######################################################################\\n\\nclass Transcript:\\n    \"\"\"\\n    Transcript objects correspond to individual files.\\n    You can build a Transcript object directly with\\n\\n    trans = Transcript(filename)\\n\\n    where filename is the absolute or relative path to the file you\\n    want to study.\\n    \"\"\"\\n    \\n    def __init__(self, filename):\\n        \"\"\"\\n        Argument:\\n        filename -- the source filename\\n\\n        At intialization, the code turns the filename contents into a\\n        CSV reader and then turns each row into an Event instance. The\\n        attribute self.events is an ordered list of those Event\\n        instances.\\n        \"\"\"\\n        self.filename = filename\\n        csvreader = csv.reader(codecs.open(self.filename, \\'r\\', \\'utf8\\'))\\n        self.events = []\\n        for row in csvreader:                \\n            self.events.append(Event(row))\\n\\n    def iter_events(self):\\n        \"\"\"\\n        Iterate through self.events.\\n        \"\"\"\\n        for event in iter(self.events):\\n            yield event\\n\\n######################################################################\\n\\nclass Event:\\n    \"\"\"\\n    Events are the basic unit of the corpus. Event objects are not\\n    really designed to be instantiated directly, but rather only as\\n    part of a Trancript or Corpus instance. However, you can build\\n    then directly from a 4-membered list of strings. Here, I\\'ve\\n    copied and pasted a row from one of the CSV files and turned\\n    it into a list:\\n\\n    event = Event([\\'Player 1\\',\\'4555\\',\\'PLAYER_INITIAL_LOCATION\\',\\'16,25\\']\\n    \"\"\"\\n    def __init__(self, row):\\n        \"\"\"\\n        Argument:\\n        row -- a row of a csvreader (or a list)\\n\\n        The attributes created:\\n\\n        self.agent (str): Server, Player 1, or Player 2\\n\\n        self.time (int): an integer representing the time of the\\n                         event relative to the start of the game\\n\\n        self.action (str): a string capturing the type of action; see the\\n                           top of this file for details\\n\\n        self.contents (str): the actual contents of the event; the structure\\n                             depends on self.action; see self.parse_contents()\\n                             for details\\n        \"\"\"\\n        self.agent, time, self.action, self.contents = row\\n        self.time = int(time)\\n\\n    def parse_contents(self):\\n        \"\"\"\\n        This method seeks to do something useful with the contents of\\n        the event. Summary:\\n\\n        -- utterances: return the list of strings as tokenized by Tokenizer().tokenize() [see below]\\n        -- pickup, drop: return a triple (x-coord, y-coord, card), where the coordinates are the location of the action\\n        -- move, initial location: return (x-coord, y-coord) of the resulting position\\n        -- task completed: return parsed date\\n        -- max cards, max turns, max line-of-sight: return int() of the value\\n        -- all else: return self.contents\\n        \"\"\"\\n        # Utterances are tokenized using a basic card-aware tokenizer:\\n        if self.action == UTTERANCE:\\n            return Tokenizer().tokenize(self.contents)\\n        # Card manipulations return a trip (x-coord, y-coord, card)\\n        elif self.action in (PICKUP, DROP):\\n            loc, card = self.contents.split(\":\")\\n            lr, tb = loc.split(\",\")\\n            return (int(lr), int(tb), card)\\n        # Locations: pairs (x-coord, y-coord)\\n        elif self.action in (MOVE, INITIAL_LOCATION):\\n            lr, tb = self.contents.split(\",\")\\n            return (int(lr), int(tb))\\n        # Completion times are parsed as dates:\\n        elif self.action == TASK_COMPLETED:\\n            time_format = \"%Y-%m-%d %H:%M:%S\"\\n            dt = datetime.datetime.strptime(self.contents.replace(\\' EDT\\', \\'\\'), time_format)\\n            ##### Uncomment if localization is important:\\n            # eastern = pytz.timezone(\\'US/Eastern\\')\\n            # dt = eastern.localize(dt)\\n            return dt\\n        # These values are parsed as integers:\\n        elif self.action in (P1_MAX_CARDS, P2_MAX_CARDS, P1_MAX_TURNS, P2_MAX_TURNS, P1_MAX_LINEOFSIGHT, P2_MAX_LINEOFSIGHT):\\n            return int(self.contents)\\n        # All other values are returned as strings:\\n        else:\\n            return self.contents\\n  \\n    def __repr__(self):\\n        \"\"\"Computable representation of the object.\"\"\"\\n        return \\'[%s]\\' % \\',\\'.join(map((lambda x : \\'\"%s\"\\' % x), (self.agent, self.time, self.action, self.contents)))        \\n\\n######################################################################    \\n\\nclass Tokenizer:\\n    \"\"\"\\n    This is a very basic tokenizer that seeks to keep intact emoticons\\n    and card references in a basic way. The class-level variables are\\n    put into a regular expression word_re (order matters) and then the\\n    input string is parsed with token_re.findall(). The output list\\n    treats things like \\'4 H\\', \\'four of hearts\\', and >:( as single\\n    tokens. All characters are retained except whitespace not deemed\\n    to be word-internal.\\n\\n    The tokenizer can be used on any string:\\n\\n    words = Tokenizer().tokenize(s)\\n\\n    where s is a str or unicode instance. The return value is a list\\n    of strings or unicode instances.\\n    \"\"\"\\n\\n    # Emoticon identification:\\n    emoticons = r\"\"\"\\n        (?:\\n            [<>]?\\n            [:;=8] # eyes\\n            [\\\\-o\\\\*\\\\\\']? # optional nose\\n            [\\\\)\\\\]\\\\(\\\\[dDpP/\\\\:\\\\}\\\\{@\\\\|\\\\\\\\] # mouth      \\n            |\\n            [\\\\)\\\\]\\\\(\\\\[dDpP/\\\\:\\\\}\\\\{@\\\\|\\\\\\\\] # mouth\\n            [\\\\-o\\\\*\\\\\\']? # optional nose\\n            [:;=8] # eyes\\n           [<>]?\\n        )\"\"\"\\n\\n    # A few final kinds of plain words, and a last resort:\\n    words = r\"\"\"\\n        (?:[a-zA-Z][a-zA-Z\\'\\\\-_]+[a-zA-Z]) # Words with apostrophes or dashes.\\n        |\\n        (?:[\\\\w_]+)                        # Words without apostrophes or dashes.\\n        \"\"\"\\n\\n    # Ranks:\\n    ranks = r\"\"\"[Tt]wo|[Tt]hree|[Ff]our|[Ff]ive|[Ss]ix|[Ss]even|[Ee]ight|[Nn]ine|[Tt]en|[Jj]ack|[Qq]ueen|[Kk]ing|[Aa]ce\\n                |\\n                2|3|4|5|6|7|8|9|10|[Jj]|[Qq]|[Kk]|A\"\"\"\\n\\n    # Suits:\\n    suits = r\"[Hh]earts?|[Dd]iamonds?|[Ss]pades?|[Cc]lubs?|[Hh]|[Ss]|[Dd]|[Cc]\"\\n\\n    # Last-ditch effort to create tokens; finall splits on whitespace:\\n    misc_punc = r\"\"\"\\n        (?:[+\\\\-]?\\\\d+[,/.:-]\\\\d+[+\\\\-]?)     # Numbers, including fractions, decimals.\\n        |\\n        (?:\\\\.(?:\\\\s*\\\\.){1,})               # Ellipsis dots. \\n        |\\n        (?:\\\\S)                            # Everything else that isn\\'t whitespace.\\n        \"\"\"\\n\\n    # The actual tokenizing regular expression:\\n    token_re = re.compile(r\"(%s)\" % \"|\".join((emoticons, words, ranks, suits, misc_punc)), re.VERBOSE)\\n   \\n    def tokenize(self, s):\\n        \"\"\"\\n        Tokenize the string s using token_re.findall(). Return value\\n        is a list of strings or unicode instances.\\n        \"\"\"\\n        return Tokenizer.token_re.findall(s)\\n',\n",
       " 'from fpssh.clients.base_parallel import BaseParallelSSHClient\\nfrom gevent.lock import RLock\\nfrom fpssh.clients.native.single import SSHClient\\n\\n\\nclass ParallelSSHClient(BaseParallelSSHClient):\\n\\n    def __init__(self, hosts, user, password, port):\\n        BaseParallelSSHClient.__init__(self, hosts, user, password, port)\\n        self._clients_lock = RLock()\\n\\n    def _run_command(self, host, command):\\n        self._make_ssh_client(host)\\n        return self.host_clients[host].run_command(command)\\n\\n    def _make_ssh_client(self, host):\\n        # with 相当于 acquire release\\n        with self._clients_lock:\\n            self.host_clients[host] = SSHClient(host, self.user, self.password, self.port)\\n\\n\\nif \"__main__\" == __name__:\\n    fake_cmd = \"echo foo\"\\n    fake_res = \"foo\\\\n\"\\n    hosts = [\"127.0.0.1\", \"127.0.0.1\"]\\n    port = 2222\\n    user = \"foo\"\\n    password = \"foo\"\\n    client = ParallelSSHClient(hosts, user, password, port)\\n\\n    def test_run_command():\\n        outputs = client.run_command(fake_cmd)\\n        for host, output in outputs.items():\\n            print(host, list(output[0]))\\n\\n\\n    test_run_command()',\n",
       " '# -*- coding: utf-8 -*-\\n\\n# This code is part of Qiskit.\\n#\\n# (C) Copyright IBM 2017, 2019.\\n#\\n# This code is licensed under the Apache License, Version 2.0. You may\\n# obtain a copy of this license in the LICENSE.txt file in the root directory\\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\\n#\\n# Any modifications or derivative works of this code must retain this\\n# copyright notice, and modified files need to carry a notice indicating\\n# that they have been altered from the originals.\\n\\n\"\"\"Assemble function for converting a list of circuits into a qobj\"\"\"\\nimport uuid\\nimport copy\\n\\nfrom qiskit.circuit import QuantumCircuit\\nfrom qiskit.exceptions import QiskitError\\nfrom qiskit.pulse import ScheduleComponent, LoConfig\\nfrom qiskit.assembler.run_config import RunConfig\\nfrom qiskit.assembler import assemble_circuits, assemble_schedules\\nfrom qiskit.qobj import QobjHeader\\nfrom qiskit.validation.exceptions import ModelValidationError\\nfrom qiskit.qobj.utils import MeasLevel, MeasReturnType\\n\\n\\n# TODO: parallelize over the experiments (serialize each separately, then add global header/config)\\ndef assemble(experiments,\\n             backend=None,\\n             qobj_id=None, qobj_header=None,\\n             shots=1024, memory=False, max_credits=None, seed_simulator=None,\\n             qubit_lo_freq=None, meas_lo_freq=None,\\n             qubit_lo_range=None, meas_lo_range=None,\\n             schedule_los=None, meas_level=MeasLevel.CLASSIFIED,\\n             meas_return=MeasReturnType.AVERAGE, meas_map=None,\\n             memory_slot_size=100, rep_time=None, parameter_binds=None,\\n             **run_config):\\n    \"\"\"Assemble a list of circuits or pulse schedules into a Qobj.\\n\\n    This function serializes the payloads, which could be either circuits or schedules,\\n    to create Qobj \"experiments\". It further annotates the experiment payload with\\n    header and configurations.\\n\\n    Args:\\n        experiments (QuantumCircuit or list[QuantumCircuit] or Schedule or list[Schedule]):\\n            Circuit(s) or pulse schedule(s) to execute\\n\\n        backend (BaseBackend):\\n            If set, some runtime options are automatically grabbed from\\n            backend.configuration() and backend.defaults().\\n            If any other option is explicitly set (e.g., rep_rate), it\\n            will override the backend\\'s.\\n            If any other options is set in the run_config, it will\\n            also override the backend\\'s.\\n\\n        qobj_id (str):\\n            String identifier to annotate the Qobj\\n\\n        qobj_header (QobjHeader or dict):\\n            User input that will be inserted in Qobj header, and will also be\\n            copied to the corresponding Result header. Headers do not affect the run.\\n\\n        shots (int):\\n            Number of repetitions of each circuit, for sampling. Default: 1024\\n\\n        memory (bool):\\n            If True, per-shot measurement bitstrings are returned as well\\n            (provided the backend supports it). For OpenPulse jobs, only\\n            measurement level 2 supports this option. Default: False\\n\\n        max_credits (int):\\n            Maximum credits to spend on job. Default: 10\\n\\n        seed_simulator (int):\\n            Random seed to control sampling, for when backend is a simulator\\n\\n        qubit_lo_freq (list):\\n            List of default qubit LO frequencies in Hz. Will be overridden by\\n            `schedule_los` if set.\\n\\n        meas_lo_freq (list):\\n            List of default measurement LO frequencies in Hz. Will be overridden\\n            by `schedule_los` if set.\\n\\n        qubit_lo_range (list):\\n            List of drive LO ranges each of form `[range_min, range_max]` in Hz.\\n            Used to validate the supplied qubit frequencies.\\n\\n        meas_lo_range (list):\\n            List of measurement LO ranges each of form `[range_min, range_max]` in Hz.\\n            Used to validate the supplied qubit frequencies.\\n\\n        schedule_los (None or list[Union[Dict[PulseChannel, float], LoConfig]] or \\\\\\n                      Union[Dict[PulseChannel, float], LoConfig]):\\n            Experiment LO configurations, frequencies are given in Hz.\\n\\n        meas_level (int or MeasLevel):\\n            Set the appropriate level of the measurement output for pulse experiments.\\n\\n        meas_return (str or MeasReturn):\\n            Level of measurement data for the backend to return.\\n\\n            For `meas_level` 0 and 1:\\n                * \"single\" returns information from every shot.\\n                * \"avg\" returns average measurement output (averaged over number of shots).\\n\\n        meas_map (list):\\n            List of lists, containing qubits that must be measured together.\\n\\n        memory_slot_size (int):\\n            Size of each memory slot if the output is Level 0.\\n\\n        rep_time (int): repetition time of the experiment in μs.\\n            The delay between experiments will be rep_time.\\n            Must be from the list provided by the device.\\n\\n        parameter_binds (list[dict{Parameter: Value}]):\\n            List of Parameter bindings over which the set of experiments will be\\n            executed. Each list element (bind) should be of the form\\n            {Parameter1: value1, Parameter2: value2, ...}. All binds will be\\n            executed across all experiments; e.g., if parameter_binds is a\\n            length-n list, and there are m experiments, a total of m x n\\n            experiments will be run (one for each experiment/bind pair).\\n\\n        **run_config (dict):\\n            extra arguments used to configure the run (e.g., for Aer configurable\\n            backends). Refer to the backend documentation for details on these\\n            arguments.\\n\\n    Returns:\\n            Qobj: a qobj that can be run on a backend. Depending on the type of input,\\n            this will be either a QasmQobj or a PulseQobj.\\n\\n    Raises:\\n        QiskitError: if the input cannot be interpreted as either circuits or schedules\\n    \"\"\"\\n    experiments = experiments if isinstance(experiments, list) else [experiments]\\n    qobj_id, qobj_header, run_config_common_dict = _parse_common_args(backend, qobj_id, qobj_header,\\n                                                                      shots, memory, max_credits,\\n                                                                      seed_simulator, **run_config)\\n\\n    # assemble either circuits or schedules\\n    if all(isinstance(exp, QuantumCircuit) for exp in experiments):\\n        run_config = _parse_circuit_args(parameter_binds, **run_config_common_dict)\\n\\n        # If circuits are parameterized, bind parameters and remove from run_config\\n        bound_experiments, run_config = _expand_parameters(circuits=experiments,\\n                                                           run_config=run_config)\\n        return assemble_circuits(circuits=bound_experiments, qobj_id=qobj_id,\\n                                 qobj_header=qobj_header, run_config=run_config)\\n\\n    elif all(isinstance(exp, ScheduleComponent) for exp in experiments):\\n        run_config = _parse_pulse_args(backend, qubit_lo_freq, meas_lo_freq,\\n                                       qubit_lo_range, meas_lo_range,\\n                                       schedule_los, meas_level, meas_return,\\n                                       meas_map, memory_slot_size, rep_time,\\n                                       **run_config_common_dict)\\n\\n        return assemble_schedules(schedules=experiments, qobj_id=qobj_id,\\n                                  qobj_header=qobj_header, run_config=run_config)\\n\\n    else:\\n        raise QiskitError(\"bad input to assemble() function; \"\\n                          \"must be either circuits or schedules\")\\n\\n\\n# TODO: rework to return a list of RunConfigs (one for each experiments), and a global one\\ndef _parse_common_args(backend, qobj_id, qobj_header, shots,\\n                       memory, max_credits, seed_simulator,\\n                       **run_config):\\n    \"\"\"Resolve the various types of args allowed to the assemble() function through\\n    duck typing, overriding args, etc. Refer to the assemble() docstring for details on\\n    what types of inputs are allowed.\\n\\n    Here the args are resolved by converting them to standard instances, and prioritizing\\n    them in case a run option is passed through multiple args (explicitly setting an arg\\n    has more priority than the arg set by backend)\\n\\n    Returns:\\n        RunConfig: a run config, which is a standardized object that configures the qobj\\n            and determines the runtime environment.\\n\\n    Raises:\\n        QiskitError: if the memory arg is True and the backend does not support\\n        memory.\\n    \"\"\"\\n    # grab relevant info from backend if it exists\\n    backend_config = None\\n    if backend:\\n        backend_config = backend.configuration()\\n        # check for memory flag applied to backend that does not support memory\\n        if memory and not backend_config.memory:\\n            raise QiskitError(\"memory not supported by backend {}\"\\n                              .format(backend_config.backend_name))\\n\\n    # an identifier for the Qobj\\n    qobj_id = qobj_id or str(uuid.uuid4())\\n\\n    # The header that goes at the top of the Qobj (and later Result)\\n    # we process it as dict, then write entries that are not None to a QobjHeader object\\n    qobj_header = qobj_header or {}\\n    if isinstance(qobj_header, QobjHeader):\\n        qobj_header = qobj_header.to_dict()\\n    backend_name = getattr(backend_config, \\'backend_name\\', None)\\n    backend_version = getattr(backend_config, \\'backend_version\\', None)\\n    qobj_header = {**dict(backend_name=backend_name, backend_version=backend_version),\\n                   **qobj_header}\\n    qobj_header = QobjHeader(**{k: v for k, v in qobj_header.items() if v is not None})\\n\\n    # create run configuration and populate\\n    run_config_dict = dict(shots=shots,\\n                           memory=memory,\\n                           max_credits=max_credits,\\n                           seed_simulator=seed_simulator,\\n                           **run_config)\\n\\n    return qobj_id, qobj_header, run_config_dict\\n\\n\\ndef _parse_pulse_args(backend, qubit_lo_freq, meas_lo_freq, qubit_lo_range,\\n                      meas_lo_range, schedule_los, meas_level,\\n                      meas_return, meas_map,\\n                      memory_slot_size, rep_time,\\n                      **run_config):\\n    \"\"\"Build a pulse RunConfig replacing unset arguments with defaults derived from the `backend`.\\n    See `assemble` for more information on the required arguments.\\n\\n    Returns:\\n        RunConfig: a run config, which is a standardized object that configures the qobj\\n            and determines the runtime environment.\\n    \"\"\"\\n    # grab relevant info from backend if it exists\\n    backend_config = None\\n    backend_default = None\\n    if backend:\\n        backend_config = backend.configuration()\\n        # TODO : Remove usage of config.defaults when backend.defaults() is updated.\\n        try:\\n            backend_default = backend.defaults()\\n        except (ModelValidationError, AttributeError):\\n            from collections import namedtuple\\n            backend_config_defaults = getattr(backend_config, \\'defaults\\', {})\\n            BackendDefault = namedtuple(\\'BackendDefault\\', (\\'qubit_freq_est\\', \\'meas_freq_est\\'))\\n            backend_default = BackendDefault(\\n                qubit_freq_est=backend_config_defaults.get(\\'qubit_freq_est\\'),\\n                meas_freq_est=backend_config_defaults.get(\\'meas_freq_est\\')\\n            )\\n\\n    meas_map = meas_map or getattr(backend_config, \\'meas_map\\', None)\\n\\n    schedule_los = schedule_los or []\\n    if isinstance(schedule_los, (LoConfig, dict)):\\n        schedule_los = [schedule_los]\\n\\n    # Convert to LoConfig if LO configuration supplied as dictionary\\n    schedule_los = [lo_config if isinstance(lo_config, LoConfig) else LoConfig(lo_config)\\n                    for lo_config in schedule_los]\\n\\n    if not qubit_lo_freq and hasattr(backend_default, \\'qubit_freq_est\\'):\\n        qubit_lo_freq = backend_default.qubit_freq_est\\n    if not meas_lo_freq and hasattr(backend_default, \\'meas_freq_est\\'):\\n        meas_lo_freq = backend_default.meas_freq_est\\n\\n    qubit_lo_range = qubit_lo_range or getattr(backend_config, \\'qubit_lo_range\\', None)\\n    meas_lo_range = meas_lo_range or getattr(backend_config, \\'meas_lo_range\\', None)\\n\\n    rep_time = rep_time or getattr(backend_config, \\'rep_times\\', None)\\n    if isinstance(rep_time, list):\\n        rep_time = rep_time[0]\\n\\n    # create run configuration and populate\\n    run_config_dict = dict(qubit_lo_freq=qubit_lo_freq,\\n                           meas_lo_freq=meas_lo_freq,\\n                           qubit_lo_range=qubit_lo_range,\\n                           meas_lo_range=meas_lo_range,\\n                           schedule_los=schedule_los,\\n                           meas_level=meas_level,\\n                           meas_return=meas_return,\\n                           meas_map=meas_map,\\n                           memory_slot_size=memory_slot_size,\\n                           rep_time=rep_time,\\n                           **run_config)\\n    run_config = RunConfig(**{k: v for k, v in run_config_dict.items() if v is not None})\\n\\n    return run_config\\n\\n\\ndef _parse_circuit_args(parameter_binds, **run_config):\\n    \"\"\"Build a circuit RunConfig replacing unset arguments with defaults derived from the `backend`.\\n    See `assemble` for more information on the required arguments.\\n\\n    Returns:\\n        RunConfig: a run config, which is a standardized object that configures the qobj\\n            and determines the runtime environment.\\n    \"\"\"\\n    parameter_binds = parameter_binds or []\\n\\n    # create run configuration and populate\\n    run_config_dict = dict(parameter_binds=parameter_binds, **run_config)\\n    run_config = RunConfig(**{k: v for k, v in run_config_dict.items() if v is not None})\\n\\n    return run_config\\n\\n\\ndef _expand_parameters(circuits, run_config):\\n    \"\"\"Verifies that there is a single common set of parameters shared between\\n    all circuits and all parameter binds in the run_config. Returns an expanded\\n    list of circuits (if parameterized) with all parameters bound, and a copy of\\n    the run_config with parameter_binds cleared.\\n\\n    If neither the circuits nor the run_config specify parameters, the two are\\n    returned unmodified.\\n\\n    Raises:\\n        QiskitError: if run_config parameters are not compatible with circuit parameters\\n\\n    Returns:\\n        Tuple(List[QuantumCircuit], RunConfig):\\n          - List of input circuits expanded and with parameters bound\\n          - RunConfig with parameter_binds removed\\n    \"\"\"\\n\\n    parameter_binds = run_config.parameter_binds\\n    if parameter_binds or \\\\\\n       any(circuit.parameters for circuit in circuits):\\n\\n        all_bind_parameters = [bind.keys()\\n                               for bind in parameter_binds]\\n        all_circuit_parameters = [circuit.parameters for circuit in circuits]\\n\\n        # Collect set of all unique parameters across all circuits and binds\\n        unique_parameters = {param\\n                             for param_list in all_bind_parameters + all_circuit_parameters\\n                             for param in param_list}\\n\\n        # Check that all parameters are common to all circuits and binds\\n        if not all_bind_parameters \\\\\\n           or not all_circuit_parameters \\\\\\n           or any(unique_parameters != bind_params for bind_params in all_bind_parameters) \\\\\\n           or any(unique_parameters != parameters for parameters in all_circuit_parameters):\\n            raise QiskitError(\\n                (\\'Mismatch between run_config.parameter_binds and all circuit parameters. \\' +\\n                 \\'Parameter binds: {} \\' +\\n                 \\'Circuit parameters: {}\\').format(all_bind_parameters, all_circuit_parameters))\\n\\n        circuits = [circuit.bind_parameters(binds)\\n                    for circuit in circuits\\n                    for binds in parameter_binds]\\n\\n        # All parameters have been expanded and bound, so remove from run_config\\n        run_config = copy.deepcopy(run_config)\\n        run_config.parameter_binds = []\\n\\n    return circuits, run_config\\n',\n",
       " '# coding: utf-8\\n\\n\"\"\"\\n    Buy Marketing API\\n\\n    The Marketing API retrieves eBay products based on a metric, such as Best Selling, as well as products that were also bought and also viewed.  # noqa: E501\\n\\n    OpenAPI spec version: v1_beta.1.0\\n    \\n    Generated by: https://github.com/swagger-api/swagger-codegen.git\\n\"\"\"\\n\\nimport pprint\\nimport re  # noqa: F401\\n\\nimport six\\n\\nclass Image(object):\\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\\n\\n    Do not edit the class manually.\\n    \"\"\"\\n    \"\"\"\\n    Attributes:\\n      swagger_types (dict): The key is attribute name\\n                            and the value is attribute type.\\n      attribute_map (dict): The key is attribute name\\n                            and the value is json key in definition.\\n    \"\"\"\\n    swagger_types = {\\n        \\'height\\': \\'int\\',\\n        \\'image_url\\': \\'str\\',\\n        \\'width\\': \\'int\\'\\n    }\\n\\n    attribute_map = {\\n        \\'height\\': \\'height\\',\\n        \\'image_url\\': \\'imageUrl\\',\\n        \\'width\\': \\'width\\'\\n    }\\n\\n    def __init__(self, height=None, image_url=None, width=None):  # noqa: E501\\n        \"\"\"Image - a model defined in Swagger\"\"\"  # noqa: E501\\n        self._height = None\\n        self._image_url = None\\n        self._width = None\\n        self.discriminator = None\\n        if height is not None:\\n            self.height = height\\n        if image_url is not None:\\n            self.image_url = image_url\\n        if width is not None:\\n            self.width = width\\n\\n    @property\\n    def height(self):\\n        \"\"\"Gets the height of this Image.  # noqa: E501\\n\\n        Reserved for future use.  # noqa: E501\\n\\n        :return: The height of this Image.  # noqa: E501\\n        :rtype: int\\n        \"\"\"\\n        return self._height\\n\\n    @height.setter\\n    def height(self, height):\\n        \"\"\"Sets the height of this Image.\\n\\n        Reserved for future use.  # noqa: E501\\n\\n        :param height: The height of this Image.  # noqa: E501\\n        :type: int\\n        \"\"\"\\n\\n        self._height = height\\n\\n    @property\\n    def image_url(self):\\n        \"\"\"Gets the image_url of this Image.  # noqa: E501\\n\\n        The URL of the image.  # noqa: E501\\n\\n        :return: The image_url of this Image.  # noqa: E501\\n        :rtype: str\\n        \"\"\"\\n        return self._image_url\\n\\n    @image_url.setter\\n    def image_url(self, image_url):\\n        \"\"\"Sets the image_url of this Image.\\n\\n        The URL of the image.  # noqa: E501\\n\\n        :param image_url: The image_url of this Image.  # noqa: E501\\n        :type: str\\n        \"\"\"\\n\\n        self._image_url = image_url\\n\\n    @property\\n    def width(self):\\n        \"\"\"Gets the width of this Image.  # noqa: E501\\n\\n        Reserved for future use.  # noqa: E501\\n\\n        :return: The width of this Image.  # noqa: E501\\n        :rtype: int\\n        \"\"\"\\n        return self._width\\n\\n    @width.setter\\n    def width(self, width):\\n        \"\"\"Sets the width of this Image.\\n\\n        Reserved for future use.  # noqa: E501\\n\\n        :param width: The width of this Image.  # noqa: E501\\n        :type: int\\n        \"\"\"\\n\\n        self._width = width\\n\\n    def to_dict(self):\\n        \"\"\"Returns the model properties as a dict\"\"\"\\n        result = {}\\n\\n        for attr, _ in six.iteritems(self.swagger_types):\\n            value = getattr(self, attr)\\n            if isinstance(value, list):\\n                result[attr] = list(map(\\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\\n                    value\\n                ))\\n            elif hasattr(value, \"to_dict\"):\\n                result[attr] = value.to_dict()\\n            elif isinstance(value, dict):\\n                result[attr] = dict(map(\\n                    lambda item: (item[0], item[1].to_dict())\\n                    if hasattr(item[1], \"to_dict\") else item,\\n                    value.items()\\n                ))\\n            else:\\n                result[attr] = value\\n        if issubclass(Image, dict):\\n            for key, value in self.items():\\n                result[key] = value\\n\\n        return result\\n\\n    def to_str(self):\\n        \"\"\"Returns the string representation of the model\"\"\"\\n        return pprint.pformat(self.to_dict())\\n\\n    def __repr__(self):\\n        \"\"\"For `print` and `pprint`\"\"\"\\n        return self.to_str()\\n\\n    def __eq__(self, other):\\n        \"\"\"Returns true if both objects are equal\"\"\"\\n        if not isinstance(other, Image):\\n            return False\\n\\n        return self.__dict__ == other.__dict__\\n\\n    def __ne__(self, other):\\n        \"\"\"Returns true if both objects are not equal\"\"\"\\n        return not self == other\\n',\n",
       " 'import logging\\nimport time\\nimport os\\nfrom collections import OrderedDict\\nfrom copy import deepcopy\\nfrom typing import Tuple\\n\\nimport numpy as np\\nfrom tqdm.auto import tqdm\\n\\nfrom .reporter import FakeReporter\\nfrom ..searcher import searcher_factory\\nfrom ..searcher.local_searcher import LocalSearcher\\nfrom ..utils import EasyDict\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass LocalReporter:\\n    \"\"\"\\n    Reporter implementation for LocalSequentialScheduler\\n    \"\"\"\\n\\n    def __init__(self, trial, searcher_config, training_history: dict, config_history: dict):\\n        self.trial = trial\\n        self.training_history = training_history\\n        self.training_history[trial] = []\\n        self.searcher_config = deepcopy(searcher_config)\\n        self.config_history = config_history\\n        self.trial_started = time.time()\\n        self.last_reported_time = self.trial_started\\n        self.last_result = None\\n\\n    def __call__(self, *args, **kwargs):\\n        result = deepcopy(kwargs)\\n        if \\'done\\' not in result:\\n            result[\\'trial\\'] = self.trial\\n\\n            now = time.time()\\n            result[\\'time_this_iter\\'] = now - self.last_reported_time\\n            result[\\'time_since_start\\'] = now - self.trial_started\\n            self.last_reported_time = now\\n\\n            self.training_history[self.trial].append(result)\\n\\n            if self.trial not in self.config_history:\\n                self.config_history[self.trial] = self.searcher_config\\n                if \\'util_args\\' in self.searcher_config:\\n                    self.searcher_config.pop(\\'util_args\\')\\n\\n            self.last_result = result\\n\\n    def terminate(self):\\n        pass  # compatibility\\n\\n\\nclass LocalSequentialScheduler(object):\\n    \"\"\" Simple scheduler which schedules all HPO jobs in sequence without any parallelism.\\n    The next trial scheduling will be decided based on the available time left withing `time_out` setting\\n    and average time required for a trial to complete multiplied by the fill_factor (0.95) by default to\\n    accommodate variance in runtimes per HPO run.\\n\\n    Parameters\\n    ----------\\n    train_fn : callable\\n        A task launch function for training.\\n    resource : dict\\n        Computation resources. For example, `{\\'num_cpus\\':2, \\'num_gpus\\':1}`\\n    searcher : str\\n        Searcher (get_config decisions). If str, this is passed to\\n        searcher_factory along with search_options.\\n    search_options : dict\\n        If searcher is str, these arguments are passed to searcher_factory.\\n    num_trials : int\\n        Maximum number of jobs run in experiment. One of `num_trials`,\\n        `time_out` must be given.\\n    time_out : float\\n        If given, jobs are started only until this time_out (wall clock time).\\n        One of `num_trials`, `time_out` must be given.\\n    reward_attr : str\\n        Name of reward (i.e., metric to maximize) attribute in data obtained\\n        from reporter\\n    time_attr : str\\n        Name of resource (or time) attribute in data obtained from reporter.\\n        This attribute is optional for FIFO scheduling, but becomes mandatory\\n        in multi-fidelity scheduling (e.g., Hyperband).\\n        Note: The type of resource must be int.\\n    \"\"\"\\n\\n    def __init__(self, train_fn, search_space, util_args=None, searcher=\\'auto\\', reward_attr=\\'reward\\', resource=None, **kwargs):\\n        self.train_fn = train_fn\\n        self.training_history = None\\n        self.config_history = None\\n        self._reward_attr = reward_attr\\n        self.time_attr = kwargs.get(\\'time_attr\\', None)\\n        self.resource = resource\\n        self.max_reward = kwargs.get(\\'max_reward\\', None)\\n        self.searcher: LocalSearcher = self.get_searcher_(searcher, train_fn, search_space=search_space, **kwargs)\\n        self.init_limits_(kwargs)\\n        self.util_args = util_args\\n        self.metadata = {\\n            \\'search_space\\': search_space,\\n            \\'search_strategy\\': self.searcher,\\n            \\'stop_criterion\\': {\\n                \\'time_limits\\': self.time_out,\\n                \\'max_reward\\': self.max_reward},\\n            \\'resources_per_trial\\': self.resource\\n        }\\n\\n    def init_limits_(self, kwargs):\\n        if kwargs.get(\\'num_trials\\', None) is None:\\n            assert kwargs.get(\\'time_out\\', None) is not None, \"Need stopping criterion: Either num_trials or time_out\"\\n        self.num_trials = kwargs.get(\\'num_trials\\', 9999)\\n        self.time_out = kwargs.get(\\'time_out\\', None)\\n        if self.num_trials is None:\\n            assert self.time_out is not None, \"Need stopping criterion: Either num_trials or time_out\"\\n\\n    def get_searcher_(self, searcher, train_fn, search_space, **kwargs) -> LocalSearcher:\\n        scheduler_opts = {}\\n        if searcher == \\'auto\\':\\n            searcher = \\'local_random\\'\\n            scheduler_opts = {\\'scheduler\\': \\'local\\'}\\n        elif searcher == \\'random\\':\\n            # FIXME: Hack to be compatible with gluoncv\\n            searcher = \\'local_random\\'\\n\\n        search_options = kwargs.get(\\'search_options\\', None)\\n        if isinstance(searcher, str):\\n            if search_options is None:\\n                search_options = dict()\\n            _search_options = search_options.copy()\\n            if searcher.startswith(\\'local_\\'):\\n                _search_options[\\'search_space\\'] = search_space\\n            else:\\n                _search_options[\\'configspace\\'] = train_fn.cs\\n                _search_options[\\'resource_attribute\\'] = kwargs.get(\\'time_attr\\', None)\\n            _search_options[\\'reward_attribute\\'] = self._reward_attr\\n            # Adjoin scheduler info to search_options, if not already done by\\n            # subclass\\n            if \\'scheduler\\' not in _search_options:\\n                _search_options[\\'scheduler\\'] = \\'local\\'\\n            searcher = searcher_factory(searcher, **{**scheduler_opts, **_search_options})\\n        else:\\n            assert isinstance(searcher, LocalSearcher)\\n        return searcher\\n\\n    def run(self, **kwargs):\\n        \"\"\"Run multiple trials given specific time and trial numbers limits.\\n        \"\"\"\\n        self.searcher.configure_scheduler(self)\\n\\n        self.training_history = OrderedDict()\\n        self.config_history = OrderedDict()\\n\\n        trial_run_times = []\\n        time_start = time.time()\\n\\n        r = range(self.num_trials)\\n        for i in (tqdm(r) if self.num_trials < 1000 else r):\\n            trial_start_time = time.time()\\n            try:\\n                is_failed, result = self.run_trial(task_id=i)\\n            except Exception:\\n                # TODO: Add special exception type when there are no more new configurations to try (exhausted search space)\\n                logger.log(30, f\\'\\\\tWARNING: Encountered unexpected exception during trial {i}, stopping HPO early.\\')\\n                logger.exception(\\'Detailed Traceback:\\')  # TODO: Avoid logging if verbosity=0\\n                break\\n            trial_end_time = time.time()\\n            trial_run_times.append(np.NaN if is_failed else (trial_end_time - trial_start_time))\\n\\n            if self.max_reward and self.get_best_reward() >= self.max_reward:\\n                logger.log(20, f\\'\\\\tMax reward is reached\\')\\n                break\\n\\n            if self.time_out is not None:\\n                avg_trial_run_time = np.nanmean(trial_run_times)\\n                avg_trial_run_time = 0 if np.isnan(avg_trial_run_time) else avg_trial_run_time\\n                if not self.has_enough_time_for_trial_(self.time_out, time_start, trial_start_time, trial_end_time, avg_trial_run_time):\\n                    logger.log(20, f\\'\\\\tTime limit exceeded\\')\\n                    break\\n\\n    @classmethod\\n    def has_enough_time_for_trial_(cls, time_out, time_start, trial_start_time, trial_end_time, avg_trial_run_time, fill_factor=0.95):\\n        \"\"\"\\n        Checks if the remaining time is enough to run another trial.\\n\\n        Parameters\\n        ----------\\n        time_out total\\n            timeout in m\\n        time_start\\n            trials start time\\n        trial_start_time\\n            last trial start time\\n        trial_end_time\\n            last trial end time\\n        avg_trial_run_time\\n            running average of all trial runs\\n        fill_factor: float\\n            discount of `avg_trial_run_time` allowed for a next trial. Default is 0.95 of `avg_trial_run_time`\\n\\n        Returns\\n        -------\\n            True if there is enough time to run another trial give runs statistics and remaining time\\n\\n        \"\"\"\\n        time_spent = trial_end_time - time_start\\n        is_timeout_exceeded = time_spent >= time_out\\n        time_left = time_start + time_out - trial_end_time\\n        is_enough_time_for_another_trial = True\\n        if avg_trial_run_time:\\n            is_enough_time_for_another_trial = time_left > avg_trial_run_time * fill_factor\\n        return is_enough_time_for_another_trial and not is_timeout_exceeded\\n\\n    @classmethod\\n    def get_average_trial_time_(cls, i, avg_trial_run_time, trial_start_time, time_end):\\n        trial_time = time_end - trial_start_time\\n        if avg_trial_run_time is None:\\n            avg_trial_run_time = trial_time\\n        else:\\n            avg_trial_run_time = ((avg_trial_run_time * i) + trial_time) / (i + 1)\\n        return avg_trial_run_time\\n\\n    def run_trial(self, task_id=0) -> Tuple[bool, dict]:\\n        \"\"\"\\n        Start a trial with a given task_id\\n\\n        Parameters\\n        ----------\\n        task_id\\n            task\\n\\n        Returns\\n        -------\\n        is_failed: bool\\n            True if task completed successfully\\n        trial_start_time\\n            Trial start time\\n        trial_end_time\\n            Trial end time\\n\\n        \"\"\"\\n        new_searcher_config = self.searcher.get_config()\\n        searcher_config = deepcopy(self.metadata[\\'search_space\\'])\\n        searcher_config.update(new_searcher_config)\\n        reporter = LocalReporter(task_id, searcher_config, self.training_history, self.config_history)\\n        return self.run_job_(task_id, searcher_config, reporter)\\n\\n    def run_job_(self, task_id, searcher_config, reporter):\\n        args = dict()\\n        if self.util_args is not None:\\n            args[\\'util_args\\'] = deepcopy(self.util_args)\\n        args.update(searcher_config)\\n\\n        args[\\'task_id\\'] = task_id\\n        args = EasyDict(args)  # TODO: Remove, currently used for compatibility with gluoncv\\n        self.searcher.register_pending(searcher_config)\\n        is_failed = False\\n        try:\\n            result = self.train_fn(args, reporter=reporter)\\n            if type(reporter) is not FakeReporter and reporter.last_result:\\n                self.searcher.update(config=searcher_config, **reporter.last_result)\\n        except Exception as e:\\n            logger.error(f\\'Exception during a trial: {e}\\')\\n            self.searcher.evaluation_failed(config=searcher_config)\\n            reporter(traceback=e)\\n            is_failed = True\\n            result = {\\'traceback\\': str(e)}\\n        return is_failed, result\\n\\n    def run_with_config(self, config):\\n        \"\"\"Run with config for final fit.\\n        It launches a single training trial under any fixed values of the hyperparameters.\\n        For example, after HPO has identified the best hyperparameter values based on a hold-out dataset,\\n        one can use this function to retrain a model with the same hyperparameters on all the available labeled data\\n        (including the hold out set). It can also returns other objects or states.\\n        \"\"\"\\n        is_failed, result = self.run_job_(\\'run_with_config\\', config, FakeReporter())\\n        return result\\n\\n    def join_jobs(self, timeout=None):\\n        pass  # Compatibility\\n\\n    def get_best_config(self):\\n        \"\"\"Get the best configuration from the finished jobs.\\n        \"\"\"\\n        return self.searcher.get_best_config()\\n\\n    def get_best_reward(self):\\n        \"\"\"Get the best reward from the finished jobs.\\n        \"\"\"\\n        return self.searcher.get_best_reward()\\n\\n    def get_training_curves(self, filename=None, plot=False, use_legend=True):\\n        \"\"\"Get Training Curves\\n        \"\"\"\\n        if filename is None and not plot:\\n            logger.warning(\\'Please either provide filename or allow plot in get_training_curves\\')\\n        import matplotlib.pyplot as plt\\n\\n        eval_metric = self.__get_training_history_metric(\\'eval_metric\\', default=\\'validation_performance\\')\\n        sign_mult = int(self.__get_training_history_metric(\\'greater_is_better\\', default=True)) * 2 - 1\\n\\n        plt.ylabel(eval_metric)\\n        plt.xlabel(self.time_attr)\\n        plt.title(\"Performance vs Training-Time in each HPO Trial\")\\n        for task_id, task_res in self.training_history.items():\\n            rewards = [x[self._reward_attr] * sign_mult for x in task_res]\\n            x = [x[self.time_attr] for x in task_res]\\n            plt.plot(x, rewards, label=f\\'task {task_id}\\')\\n        if use_legend:\\n            plt.legend(loc=\\'best\\')\\n        if filename:\\n            logger.info(f\\'Saving Training Curve in {filename}\\')\\n            file_dir = os.path.split(os.path.abspath(filename))[0]\\n            if not os.path.exists(file_dir):\\n                os.makedirs(file_dir)\\n            plt.savefig(filename)\\n        if plot:\\n            plt.show()\\n\\n    def __get_training_history_metric(self, metric, default=None):\\n        for _, task_res in self.training_history.items():\\n            if task_res and metric in task_res[0]:\\n                return task_res[0][metric]\\n        return default\\n\\n    def get_best_task_id(self):\\n        \"\"\"Get the task id that results in the best configuration/best reward.\\n\\n        If there are duplicated configurations, we return the id of the first one.\\n        \"\"\"\\n        best_config = self.get_best_config()\\n        for task_id, config in self.config_history.items():\\n            if best_config == config:\\n                return task_id\\n        raise RuntimeError(\\'The best config {} is not found in config history = {}. \\'\\n                           \\'This should never happen!\\'.format(best_config, self.config_history))\\n',\n",
       " 'from copy import deepcopy\\nfrom ditk import logging\\nfrom ding.model import DQN\\nfrom ding.policy import DQNPolicy\\nfrom ding.envs import DingEnvWrapper, SubprocessEnvManagerV2\\nfrom ding.data import DequeBuffer\\nfrom ding.config import compile_config\\nfrom ding.framework import task\\nfrom ding.framework.context import OnlineRLContext\\nfrom ding.framework.middleware import OffPolicyLearner, StepCollector, interaction_evaluator, data_pusher, \\\\\\n    eps_greedy_handler, CkptSaver, nstep_reward_enhancer, termination_checker\\nfrom ding.utils import set_pkg_seed\\nfrom dizoo.atari.envs.atari_env import AtariEnv\\nfrom dizoo.atari.config.serial.pong.pong_dqn_config import main_config, create_config\\n\\n\\ndef main():\\n    logging.getLogger().setLevel(logging.INFO)\\n    cfg = compile_config(main_config, create_cfg=create_config, auto=True)\\n    with task.start(async_mode=False, ctx=OnlineRLContext()):\\n        collector_cfg = deepcopy(cfg.env)\\n        collector_cfg.is_train = True\\n        evaluator_cfg = deepcopy(cfg.env)\\n        evaluator_cfg.is_train = False\\n        collector_env = SubprocessEnvManagerV2(\\n            env_fn=[lambda: AtariEnv(collector_cfg) for _ in range(cfg.env.collector_env_num)], cfg=cfg.env.manager\\n        )\\n        evaluator_env = SubprocessEnvManagerV2(\\n            env_fn=[lambda: AtariEnv(evaluator_cfg) for _ in range(cfg.env.evaluator_env_num)], cfg=cfg.env.manager\\n        )\\n\\n        set_pkg_seed(cfg.seed, use_cuda=cfg.policy.cuda)\\n\\n        model = DQN(**cfg.policy.model)\\n        buffer_ = DequeBuffer(size=cfg.policy.other.replay_buffer.replay_buffer_size)\\n        policy = DQNPolicy(cfg.policy, model=model)\\n\\n        task.use(interaction_evaluator(cfg, policy.eval_mode, evaluator_env))\\n        task.use(eps_greedy_handler(cfg))\\n        task.use(StepCollector(cfg, policy.collect_mode, collector_env))\\n        task.use(nstep_reward_enhancer(cfg))\\n        task.use(data_pusher(cfg, buffer_))\\n        task.use(OffPolicyLearner(cfg, policy.learn_mode, buffer_))\\n        task.use(CkptSaver(cfg, policy, train_freq=1000))\\n        task.use(termination_checker(max_train_iter=int(1e7)))\\n        task.run()\\n\\n\\nif __name__ == \"__main__\":\\n    main()\\n',\n",
       " '\\'\\'\\'\\nModels (mostly base classes) for the various kinds of renderer\\ntypes that Bokeh supports.\\n\\n\\'\\'\\'\\n#-----------------------------------------------------------------------------\\n# Boilerplate\\n#-----------------------------------------------------------------------------\\nimport logging # isort:skip\\nlog = logging.getLogger(__name__)\\n\\n#-----------------------------------------------------------------------------\\n# Imports\\n#-----------------------------------------------------------------------------\\n\\n# Standard library imports\\nfrom difflib import get_close_matches\\n\\n# Bokeh imports\\nfrom ..core.enums import RenderLevel\\nfrom ..core.has_props import abstract\\nfrom ..core.properties import (\\n    Auto,\\n    Bool,\\n    Either,\\n    Enum,\\n    Float,\\n    Instance,\\n    Override,\\n    String,\\n)\\nfrom ..core.validation import error\\nfrom ..core.validation.errors import (\\n    BAD_COLUMN_NAME,\\n    CDSVIEW_FILTERS_WITH_CONNECTED,\\n    CDSVIEW_SOURCE_DOESNT_MATCH,\\n    MALFORMED_GRAPH_SOURCE,\\n    MISSING_GLYPH,\\n    NO_SOURCE_FOR_GLYPH,\\n)\\nfrom ..model import Model\\nfrom .glyphs import Circle, ConnectedXYGlyph, Glyph, MultiLine\\nfrom .graphs import GraphHitTestPolicy, LayoutProvider, NodesOnly\\nfrom .sources import CDSView, ColumnDataSource, DataSource, WebSource\\nfrom .tiles import TileSource, WMTSTileSource\\n\\n#-----------------------------------------------------------------------------\\n# Globals and constants\\n#-----------------------------------------------------------------------------\\n\\n__all__ = (\\n    \\'DataRenderer\\',\\n    \\'GlyphRenderer\\',\\n    \\'GraphRenderer\\',\\n    \\'GuideRenderer\\',\\n    \\'Renderer\\',\\n    \\'TileRenderer\\',\\n)\\n\\n#-----------------------------------------------------------------------------\\n# General API\\n#-----------------------------------------------------------------------------\\n\\n#-----------------------------------------------------------------------------\\n# Dev API\\n#-----------------------------------------------------------------------------\\n\\n@abstract\\nclass Renderer(Model):\\n    \\'\\'\\'\\n    An abstract base class for renderer types.\\n\\n    \\'\\'\\'\\n\\n    level = Enum(RenderLevel, help=\"\"\"\\n    Specifies the level in which to paint this renderer.\\n    \"\"\")\\n\\n    visible = Bool(default=True, help=\"\"\"\\n    Is the renderer visible.\\n    \"\"\")\\n\\n    x_range_name = String(\\'default\\', help=\"\"\"\\n    A particular (named) x-range to use for computing screen locations when\\n    rendering glyphs on the plot. If unset, use the default x-range.\\n    \"\"\")\\n\\n    y_range_name = String(\\'default\\', help=\"\"\"\\n    A particular (named) y-range to use for computing screen locations when\\n    rendering glyphs on the plot. If unset, use the default y-range.\\n    \"\"\")\\n\\n\\n@abstract\\nclass DataRenderer(Renderer):\\n    \\'\\'\\'\\n    An abstract base class for data renderer types (e.g. ``GlyphRenderer``, ``TileRenderer``, ``GraphRenderer``).\\n\\n    \\'\\'\\'\\n\\n    level = Override(default=\"glyph\")\\n\\nclass TileRenderer(DataRenderer):\\n    \\'\\'\\'\\n\\n    \\'\\'\\'\\n\\n    tile_source = Instance(TileSource, default=lambda: WMTSTileSource(), help=\"\"\"\\n    Local data source to use when rendering glyphs on the plot.\\n    \"\"\")\\n\\n    alpha = Float(1.0, help=\"\"\"\\n    tile opacity 0.0 - 1.0\\n    \"\"\")\\n\\n    smoothing = Bool(default=True, help=\"\"\"\\n    Enable image smoothing for the rendered tiles.\\n    \"\"\")\\n\\n    render_parents = Bool(default=True, help=\"\"\"\\n    Flag enable/disable drawing of parent tiles while waiting for new tiles to arrive. Default value is True.\\n    \"\"\")\\n\\nclass GlyphRenderer(DataRenderer):\\n    \\'\\'\\'\\n\\n    \\'\\'\\'\\n\\n    def __str__(self):\\n        return f\"GlyphRenderer(id={self.id}, glyph={str(self.glyph)}, ...)\"\\n\\n    @error(CDSVIEW_FILTERS_WITH_CONNECTED)\\n    def _check_cdsview_filters_with_connected(self):\\n        if isinstance(self.glyph, ConnectedXYGlyph) and len(self.view.filters) > 0:\\n            return str(self)\\n\\n    @error(MISSING_GLYPH)\\n    def _check_missing_glyph(self):\\n        if not self.glyph: return str(self)\\n\\n    @error(NO_SOURCE_FOR_GLYPH)\\n    def _check_no_source_for_glyph(self):\\n        if not self.data_source: return str(self)\\n\\n    @error(CDSVIEW_SOURCE_DOESNT_MATCH)\\n    def _check_cdsview_source(self):\\n        if self.data_source is not self.view.source: return str(self)\\n\\n    @error(BAD_COLUMN_NAME)\\n    def _check_bad_column_name(self):\\n        if not self.glyph: return\\n        if not self.data_source: return\\n        if isinstance(self.data_source, WebSource): return\\n        missing_values = set()\\n        specs = self.glyph.dataspecs()\\n        for name, item in self.glyph.properties_with_values(include_defaults=False).items():\\n            if name not in specs: continue\\n            if not isinstance(item, dict): continue\\n            if not isinstance(self.data_source, ColumnDataSource): continue\\n            if \\'field\\' in item and item[\\'field\\'] not in self.data_source.column_names:\\n                missing_values.add((item[\\'field\\'], name))\\n        if missing_values:\\n            suggestions = [\\'\" (closest match: \"%s\")\\' % s[0] if s else \\'\"\\' for s in [\\n                get_close_matches(term[0], self.data_source.column_names, n=1) for term in missing_values]]\\n            missing_values = [(\"\".join([m[0], s]), m[1]) for m, s in zip(missing_values, suggestions)]\\n            missing = [\\'key \"%s\" value \"%s\\' % (k, v) for v, k in missing_values]\\n            return \"%s [renderer: %s]\" % (\", \".join(sorted(missing)), self)\\n\\n    def __init__(self, **kw):\\n        super().__init__(**kw)\\n        if \"view\" not in kw:\\n            self.view = CDSView(source=self.data_source)\\n\\n    data_source = Instance(DataSource, help=\"\"\"\\n    Local data source to use when rendering glyphs on the plot.\\n    \"\"\")\\n\\n    view = Instance(CDSView, help=\"\"\"\\n    A view into the data source to use when rendering glyphs. A default view\\n    of the entire data source is created when a view is not passed in during\\n    initialization.\\n\\n    .. note:\\n        Only the default (filterless) CDSView is compatible with glyphs that\\n        have connected topology, such as Line and Patch. Setting filters on\\n        views for these glyphs will result in a warning and undefined behavior.\\n    \"\"\")\\n\\n    glyph = Instance(Glyph, help=\"\"\"\\n    The glyph to render, in conjunction with the supplied data source\\n    and ranges.\\n    \"\"\")\\n\\n    selection_glyph = Either(Auto, Instance(Glyph), default=\"auto\", help=\"\"\"\\n    An optional glyph used for selected points.\\n\\n    If set to \"auto\" then the standard glyph will be used for selected\\n    points.\\n    \"\"\")\\n\\n    nonselection_glyph = Either(Auto, Instance(Glyph), default=\"auto\", help=\"\"\"\\n    An optional glyph used for explicitly non-selected points\\n    (i.e., non-selected when there are other points that are selected,\\n    but not when no points at all are selected.)\\n\\n    If set to \"auto\" then a glyph with a low alpha value (0.1) will\\n    be used for non-selected points.\\n    \"\"\")\\n\\n    hover_glyph = Instance(Glyph, help=\"\"\"\\n    An optional glyph used for inspected points, e.g., those that are\\n    being hovered over by a ``HoverTool``.\\n    \"\"\")\\n\\n    muted_glyph = Instance(Glyph, help=\"\"\"\\n    \"\"\")\\n\\n    muted = Bool(False, help=\"\"\"\\n    \"\"\")\\n\\n_DEFAULT_NODE_RENDERER = lambda: GlyphRenderer(\\n    glyph=Circle(), data_source=ColumnDataSource(data=dict(index=[]))\\n)\\n\\n_DEFAULT_EDGE_RENDERER = lambda: GlyphRenderer(\\n    glyph=MultiLine(), data_source=ColumnDataSource(data=dict(start=[], end=[]))\\n)\\n\\nclass GraphRenderer(DataRenderer):\\n    \\'\\'\\'\\n\\n    \\'\\'\\'\\n\\n    @error(MALFORMED_GRAPH_SOURCE)\\n    def _check_malformed_graph_source(self):\\n        missing = []\\n        if \"index\" not in self.node_renderer.data_source.column_names:\\n            missing.append(\"Column \\'index\\' is missing in GraphSource.node_renderer.data_source\")\\n        if \"start\" not in self.edge_renderer.data_source.column_names:\\n            missing.append(\"Column \\'start\\' is missing in GraphSource.edge_renderer.data_source\")\\n        if \"end\" not in self.edge_renderer.data_source.column_names:\\n            missing.append(\"Column \\'end\\' is missing in GraphSource.edge_renderer.data_source\")\\n        if missing:\\n            return \" ,\".join(missing) + \" [%s]\" % self\\n\\n    layout_provider = Instance(LayoutProvider, help=\"\"\"\\n    An instance of a ``LayoutProvider`` that supplies the layout of the network\\n    graph in cartesian space.\\n    \"\"\")\\n\\n    node_renderer = Instance(GlyphRenderer, default=_DEFAULT_NODE_RENDERER, help=\"\"\"\\n    Instance of a ``GlyphRenderer`` containing an ``XYGlyph`` that will be rendered\\n    as the graph nodes.\\n    \"\"\")\\n\\n    edge_renderer = Instance(GlyphRenderer, default=_DEFAULT_EDGE_RENDERER, help=\"\"\"\\n    Instance of a ``GlyphRenderer`` containing an ``MultiLine`` Glyph that will be\\n    rendered as the graph edges.\\n    \"\"\")\\n\\n    selection_policy = Instance(GraphHitTestPolicy, default=lambda: NodesOnly(), help=\"\"\"\\n    An instance of a ``GraphHitTestPolicy`` that provides the logic for selection\\n    of graph components.\\n    \"\"\")\\n\\n    inspection_policy = Instance(GraphHitTestPolicy, default=lambda: NodesOnly(), help=\"\"\"\\n    An instance of a ``GraphHitTestPolicy`` that provides the logic for inspection\\n    of graph components.\\n    \"\"\")\\n\\n@abstract\\nclass GuideRenderer(Renderer):\\n    \\'\\'\\' A base class for all guide renderer types. ``GuideRenderer`` is\\n    not generally useful to instantiate on its own.\\n\\n    \\'\\'\\'\\n\\n    level = Override(default=\"guide\")\\n\\n#-----------------------------------------------------------------------------\\n# Private API\\n#-----------------------------------------------------------------------------\\n\\n#-----------------------------------------------------------------------------\\n# Code\\n#-----------------------------------------------------------------------------\\n',\n",
       " \"# model settings\\ntemperature = 0.2\\nwith_norm = True\\nquery_dim = 128\\nmodel = dict(\\n    type='SimSiamBaseTracker',\\n    backbone=dict(\\n        type='ResNet',\\n        pretrained=None,\\n        depth=18,\\n        out_indices=(3, ),\\n        # strides=(1, 2, 1, 1),\\n        norm_cfg=dict(type='SyncBN', requires_grad=True),\\n        norm_eval=False,\\n        zero_init_residual=True),\\n    # cls_head=None,\\n    # patch_head=None,\\n    img_head=dict(\\n        type='SimSiamHead',\\n        in_channels=512,\\n        norm_cfg=dict(type='SyncBN'),\\n        num_projection_fcs=3,\\n        projection_mid_channels=512,\\n        projection_out_channels=512,\\n        num_predictor_fcs=2,\\n        predictor_mid_channels=128,\\n        predictor_out_channels=512,\\n        with_norm=True,\\n        loss_feat=dict(type='CosineSimLoss', negative=False),\\n        spatial_type='avg'))\\n# model training and testing settings\\ntrain_cfg = dict(intra_video=False, transpose_temporal=True)\\ntest_cfg = dict(\\n    precede_frames=20,\\n    topk=10,\\n    temperature=0.2,\\n    strides=(1, 2, 1, 1),\\n    out_indices=(2, 3),\\n    neighbor_range=24,\\n    with_first=True,\\n    with_first_neighbor=True,\\n    output_dir='eval_results')\\n# dataset settings\\ndataset_type = 'VideoDataset'\\ndataset_type_val = 'DavisDataset'\\ndata_prefix = 'data/kinetics400/videos_train'\\nann_file_train = 'data/kinetics400/kinetics400_train_list_videos.txt'\\ndata_prefix_val = 'data/davis/DAVIS/JPEGImages/480p'\\nanno_prefix_val = 'data/davis/DAVIS/Annotations/480p'\\ndata_root_val = 'data/davis/DAVIS'\\nann_file_val = 'data/davis/DAVIS/ImageSets/davis2017_val_list_rawframes.txt'\\nimg_norm_cfg = dict(\\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\\ntrain_pipeline = [\\n    dict(type='DecordInit'),\\n    dict(type='SampleFrames', clip_len=2, frame_interval=8, num_clips=1),\\n    # dict(type='DuplicateFrames', times=2),\\n    dict(type='DecordDecode'),\\n    dict(\\n        type='RandomResizedCrop',\\n        area_range=(0.2, 1.),\\n        same_across_clip=False,\\n        same_on_clip=False),\\n    dict(type='Resize', scale=(224, 224), keep_ratio=False),\\n    dict(\\n        type='Flip',\\n        flip_ratio=0.5,\\n        same_across_clip=False,\\n        same_on_clip=False),\\n    # dict(\\n    #     type='ColorJitter',\\n    #     brightness=0.4,\\n    #     contrast=0.4,\\n    #     saturation=0.4,\\n    #     hue=0.1,\\n    #     p=0.8,\\n    #     same_across_clip=False,\\n    #     same_on_clip=False),\\n    # dict(\\n    #     type='RandomGrayScale',\\n    #     p=0.2,\\n    #     same_across_clip=False,\\n    #     same_on_clip=False),\\n    # dict(\\n    #     type='RandomGaussianBlur',\\n    #     p=0.5,\\n    #     same_across_clip=False,\\n    #     same_on_clip=False),\\n    dict(type='Normalize', **img_norm_cfg),\\n    dict(type='FormatShape', input_format='NCTHW'),\\n    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\\n    dict(type='ToTensor', keys=['imgs', 'label'])\\n]\\nval_pipeline = [\\n    dict(type='SequentialSampleFrames', frame_interval=1),\\n    dict(type='RawFrameDecode'),\\n    dict(type='Resize', scale=(-1, 480), keep_ratio=True),\\n    dict(type='Flip', flip_ratio=0),\\n    dict(type='Normalize', **img_norm_cfg),\\n    dict(type='FormatShape', input_format='NCTHW'),\\n    dict(\\n        type='Collect',\\n        keys=['imgs', 'ref_seg_map'],\\n        meta_keys=('frame_dir', 'frame_inds', 'original_shape', 'seg_map')),\\n    dict(type='ToTensor', keys=['imgs', 'ref_seg_map'])\\n]\\ndata = dict(\\n    videos_per_gpu=128,\\n    workers_per_gpu=16,\\n    val_workers_per_gpu=1,\\n    train=dict(\\n        type='RepeatDataset',\\n        times=5,\\n        dataset=dict(\\n            type=dataset_type,\\n            ann_file=ann_file_train,\\n            data_prefix=data_prefix,\\n            pipeline=train_pipeline)),\\n    val=dict(\\n        type=dataset_type_val,\\n        ann_file=ann_file_val,\\n        data_prefix=data_prefix_val,\\n        data_root=data_root_val,\\n        anno_prefix=anno_prefix_val,\\n        pipeline=val_pipeline,\\n        test_mode=True),\\n    test=dict(\\n        type=dataset_type_val,\\n        ann_file=ann_file_val,\\n        data_prefix=data_prefix_val,\\n        data_root=data_root_val,\\n        anno_prefix=anno_prefix_val,\\n        pipeline=val_pipeline,\\n        test_mode=True))\\n# optimizer\\n# optimizer = dict(type='Adam', lr=1e-4)\\noptimizer = dict(type='SGD', lr=0.05, momentum=0.9, weight_decay=0.0001)\\noptimizer_config = dict(grad_clip=None)\\n# learning policy\\nlr_config = dict(policy='CosineAnnealing', min_lr=0, by_epoch=False)\\n# lr_config = dict(policy='Fixed')\\n# lr_config = dict(\\n#     policy='step',\\n#     warmup='linear',\\n#     warmup_iters=100,\\n#     warmup_ratio=0.001,\\n#     step=[1, 2])\\ntotal_epochs = 100\\ncheckpoint_config = dict(interval=1)\\nevaluation = dict(\\n    interval=1,\\n    metrics='davis',\\n    key_indicator='feat_1.J&F-Mean',\\n    rule='greater')\\nlog_config = dict(\\n    interval=50,\\n    hooks=[\\n        dict(type='TextLoggerHook'),\\n        # dict(type='TensorboardLoggerHook'),\\n        dict(\\n            type='WandbLoggerHook',\\n            init_kwargs=dict(\\n                project='mmaction2',\\n                name='{{fileBasenameNoExtension}}',\\n                resume=True,\\n                tags=['ssb'],\\n                dir='wandb/{{fileBasenameNoExtension}}',\\n                config=dict(\\n                    model=model,\\n                    train_cfg=train_cfg,\\n                    test_cfg=test_cfg,\\n                    data=data))),\\n    ])\\n# runtime settings\\ndist_params = dict(backend='nccl')\\nlog_level = 'INFO'\\nload_from = None\\nresume_from = None\\nworkflow = [('train', 1)]\\nfind_unused_parameters = False\\n\",\n",
       " \"import radiate\\nimport numpy as np\\nimport os\\n\\n# path to the sequence\\nroot_path = 'data/radiate/'\\nsequence_name = 'tiny_foggy'\\n\\n# time (s) to retrieve next frame\\ndt = 0.25\\n\\n# load sequence\\nseq = radiate.Sequence(os.path.join(root_path, sequence_name))\\n\\n# play sequence\\nfor t in np.arange(seq.init_timestamp, seq.end_timestamp, dt):\\n    output = seq.get_from_timestamp(t)\\n    seq.vis_all(output, 0)\",\n",
       " '\"\"\"The :mod:`sklearn.kernel_regressor` module implements the Kernel Regressor.\\n\"\"\"\\n# Author: Jan Hendrik Metzen <janmetzen@mailbox.de>\\n#\\n# License: BSD 3 clause\\n\\nimport numpy as np\\n\\nfrom sklearn.metrics.pairwise import pairwise_kernels\\nfrom sklearn.base import BaseEstimator, RegressorMixin\\nimport gc\\n\\nclass KernelRegression(BaseEstimator, RegressorMixin):\\n    \"\"\"Nadaraya-Watson kernel regression with automatic bandwidth selection.\\n\\n    This implements Nadaraya-Watson kernel regression with (optional) automatic\\n    bandwith selection of the kernel via leave-one-out cross-validation. Kernel\\n    regression is a simple non-parametric kernelized technique for learning\\n    a non-linear relationship between input variable(s) and a target variable.\\n\\n    Parameters\\n    ----------\\n    kernel : string or callable, default=\"rbf\"\\n        Kernel map to be approximated. A callable should accept two arguments\\n        and the keyword arguments passed to this object as kernel_params, and\\n        should return a floating point number.\\n\\n    gamma : float, default=None\\n        Gamma parameter for the RBF (\"bandwidth\"), polynomial,\\n        exponential chi2 and sigmoid kernels. Interpretation of the default\\n        value is left to the kernel; see the documentation for\\n        sklearn.metrics.pairwise. Ignored by other kernels. If a sequence of\\n        values is given, one of these values is selected which minimizes\\n        the mean-squared-error of leave-one-out cross-validation.\\n\\n    See also\\n    --------\\n\\n    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\\n    \"\"\"\\n\\n    def __init__(self, kernel=\"rbf\", gamma=None):\\n        self.kernel = kernel\\n        self.gamma = gamma\\n\\n    def fit(self, X, y):\\n        \"\"\"Fit the model\\n\\n        Parameters\\n        ----------\\n        X : array-like of shape = [n_samples, n_features]\\n            The training input samples.\\n\\n        y : array-like, shape = [n_samples]\\n            The target values\\n\\n        Returns\\n        -------\\n        self : object\\n            Returns self.\\n        \"\"\"\\n        self.X = X\\n        self.y = y\\n\\n        if hasattr(self.gamma, \"__iter__\"):\\n            self.gamma = self._optimize_gamma(self.gamma)\\n\\n        return self\\n\\n    def predict(self, X):\\n        \"\"\"Predict target values for X.\\n\\n        Parameters\\n        ----------\\n        X : array-like of shape = [n_samples, n_features]\\n            The input samples.\\n\\n        Returns\\n        -------\\n        y : array of shape = [n_samples]\\n            The predicted target value.\\n        \"\"\"\\n        K = pairwise_kernels(self.X, X, metric=self.kernel, gamma=self.gamma)\\n        try:\\n            ret = (K * self.y[:, None]).sum(axis=0) / K.sum(axis=0)\\n        except MemoryError:\\n            gc.collect()  # gc and retry\\n            ret = (K * self.y[:, None]).sum(axis=0) / K.sum(axis=0)\\n\\n        return ret\\n\\n    def _optimize_gamma(self, gamma_values):\\n        # Select specific value of gamma from the range of given gamma_values\\n        # by minimizing mean-squared error in leave-one-out cross validation\\n        mse = np.empty_like(gamma_values, dtype=np.float)\\n        for i, gamma in enumerate(gamma_values):\\n            K = pairwise_kernels(self.X, self.X, metric=self.kernel,\\n                                 gamma=gamma)\\n            np.fill_diagonal(K, 0)  # leave-one-out\\n            Ky = K * self.y[:, np.newaxis]\\n            y_pred = Ky.sum(axis=0) / K.sum(axis=0)\\n            mse[i] = ((y_pred - self.y) ** 2).mean()\\n\\n        return gamma_values[np.nanargmin(mse)]\\n',\n",
       " '#!/usr/bin/env python\\n\\n# summary: physical touchscreen buttons listener\\n\\n# deps: sudo apt-get install python-dev python-rpi.gpio\\n\\nimport RPi.GPIO as GPIO\\nfrom time import sleep\\nimport signal, os, subprocess, sys\\n\\nbuttons = [24, 23, 18]\\n\\n\\ndef button_pressed(channel):\\n    if channel == 23:\\n        print(\"[buttons.python] MIDDLE: restart all processes...\")\\n        subprocess.Popen([\\'/bin/sh\\', \\'/home/pi/pdac/AUTOSTART.sh\\'])\\n    elif channel == 24:\\n        print(\"[buttons.python] BOTTOM: toggle browser...\")\\n        subprocess.Popen([\\'/bin/sh\\', \\'/home/pi/pdac/system/utilityToggleBrowser.sh\\'])\\ndef unregister_events():\\n    for pin in buttons:\\n        GPIO.remove_event_detect(pin)\\n\\nif __name__ == \\'__main__\\':\\n    signal.signal(signal.SIGINT, unregister_events)\\n    try:\\n        GPIO.setmode(GPIO.BCM)\\n        for pin in buttons:\\n            GPIO.setup(pin, GPIO.IN, pull_up_down=GPIO.PUD_UP)\\n            GPIO.add_event_detect(pin, GPIO.RISING, callback=button_pressed, bouncetime=200)\\n        while True:\\n            sleep(10)\\n    except Exception as e:\\n        print(\"Caught exception:\", e)\\n        unregister_events()',\n",
       " \"# Generated by Django 3.1.13 on 2021-09-19 08:36\\n\\nfrom django.db import migrations, models\\nimport django.db.models.deletion\\nimport taggit.managers\\n\\n\\nclass Migration(migrations.Migration):\\n\\n    initial = True\\n\\n    dependencies = [\\n        ('wagtailimages', '0023_add_choose_permissions'),\\n        ('taggit', '0003_taggeditem_add_unique_index'),\\n    ]\\n\\n    operations = [\\n        migrations.CreateModel(\\n            name='Volunteer',\\n            fields=[\\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\\n                ('name', models.CharField(max_length=255, verbose_name='name')),\\n                ('email', models.EmailField(blank=True, max_length=254, null=True, verbose_name='email')),\\n                ('bio', models.TextField(verbose_name='biography')),\\n                ('affiliation', models.CharField(max_length=128, verbose_name='Affiliation')),\\n                ('website', models.URLField(blank=True, verbose_name='Website')),\\n                ('twitter_profile', models.URLField(blank=True, null=True, verbose_name='Twitter Profile')),\\n                ('linkedin_profile', models.URLField(blank=True, null=True, verbose_name='LinkedIn Profile')),\\n                ('orcid_profile', models.URLField(blank=True, null=True, verbose_name='OrcID Link')),\\n                ('creation_date', models.DateTimeField(auto_now_add=True)),\\n                ('last_updated', models.DateTimeField(auto_now_add=True)),\\n                ('active_since', models.DateTimeField(default=None, null=True)),\\n                ('areas_expertise', taggit.managers.TaggableManager(help_text='A comma-separated list of tags.', through='taggit.TaggedItem', to='taggit.Tag', verbose_name='areas of expertise')),\\n                ('picture', models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='+', to='wagtailimages.image')),\\n            ],\\n            options={\\n                'ordering': ['name'],\\n            },\\n        ),\\n    ]\\n\",\n",
       " '#! /usr/bin/env python\\nfrom __future__ import print_function\\n\\nimport pytest\\nimport sys\\nimport os\\nimport subprocess\\n\\n\\nPYTEST_ARGS = {\\n    \\'default\\': [\\'tests\\'],\\n    \\'fast\\': [\\'tests\\', \\'-q\\'],\\n}\\n\\nFLAKE8_ARGS = [\\'rest_framework_tracking\\', \\'tests\\', \\'--ignore=E501\\']\\n\\n\\nsys.path.append(os.path.dirname(__file__))\\n\\n\\ndef exit_on_failure(ret, message=None):\\n    if ret:\\n        sys.exit(ret)\\n\\n\\ndef flake8_main(args):\\n    print(\\'Running flake8 code linting\\')\\n    ret = subprocess.call([\\'flake8\\'] + args)\\n    print(\\'flake8 failed\\' if ret else \\'flake8 passed\\')\\n    return ret\\n\\n\\ndef split_class_and_function(string):\\n    class_string, function_string = string.split(\\'.\\', 1)\\n    return \"%s and %s\" % (class_string, function_string)\\n\\n\\ndef is_function(string):\\n    # `True` if it looks like a test function is included in the string.\\n    return string.startswith(\\'test_\\') or \\'.test_\\' in string\\n\\n\\ndef is_class(string):\\n    # `True` if first character is uppercase - assume it\\'s a class name.\\n    return string[0] == string[0].upper()\\n\\n\\nif __name__ == \"__main__\":\\n    try:\\n        sys.argv.remove(\\'--nolint\\')\\n    except ValueError:\\n        run_flake8 = True\\n    else:\\n        run_flake8 = False\\n\\n    try:\\n        sys.argv.remove(\\'--lintonly\\')\\n    except ValueError:\\n        run_tests = True\\n    else:\\n        run_tests = False\\n\\n    try:\\n        sys.argv.remove(\\'--fast\\')\\n    except ValueError:\\n        style = \\'default\\'\\n    else:\\n        style = \\'fast\\'\\n        run_flake8 = False\\n\\n    if len(sys.argv) > 1:\\n        pytest_args = sys.argv[1:]\\n        first_arg = pytest_args[0]\\n        if first_arg.startswith(\\'-\\'):\\n            # `runtests.py [flags]`\\n            pytest_args = [\\'tests\\'] + pytest_args\\n        elif is_class(first_arg) and is_function(first_arg):\\n            # `runtests.py TestCase.test_function [flags]`\\n            expression = split_class_and_function(first_arg)\\n            pytest_args = [\\'tests\\', \\'-k\\', expression] + pytest_args[1:]\\n        elif is_class(first_arg) or is_function(first_arg):\\n            # `runtests.py TestCase [flags]`\\n            # `runtests.py test_function [flags]`\\n            pytest_args = [\\'tests\\', \\'-k\\', pytest_args[0]] + pytest_args[1:]\\n    else:\\n        pytest_args = PYTEST_ARGS[style]\\n\\n    if run_tests:\\n        exit_on_failure(pytest.main(pytest_args))\\n        # ipdb support: comment the previous line and uncomment the nextone.\\n        # pytest.main(pytest_args + [\\'-s\\'])\\n    if run_flake8:\\n        exit_on_failure(flake8_main(FLAKE8_ARGS))\\n',\n",
       " '#!/usr/bin/env python3\\n# Copyright (c) 2014-2017 The Bitcoin Core developers\\n# Distributed under the MIT software license, see the accompanying\\n# file COPYING or http://www.opensource.org/licenses/mit-license.php.\\n\"\"\"Test running bitcoind with -reindex and -reindex-chainstate options.\\n\\n- Start a single node and generate 3 blocks.\\n- Stop the node and restart it with -reindex. Verify that the node has reindexed up to block 3.\\n- Stop the node and restart it with -reindex-chainstate. Verify that the node has reindexed up to block 3.\\n\"\"\"\\n\\nfrom test_framework.test_framework import UnoTestFramework\\nfrom test_framework.util import assert_equal\\nimport time\\n\\nclass ReindexTest(UnoTestFramework):\\n\\n    def set_test_params(self):\\n        self.setup_clean_chain = True\\n        self.num_nodes = 1\\n\\n    def reindex(self):\\n        self.nodes[0].generate(3)\\n        blockcount = self.nodes[0].getblockcount()\\n        self.stop_nodes()\\n        extra_args = [[\"-reindex\", \"-checkblockindex=1\"]]\\n        self.start_nodes(extra_args)\\n        assert_equal(self.nodes[0].getblockcount(), blockcount)  # start_node is blocking on reindex\\n        self.log.info(\"Success\")\\n\\n    def run_test(self):\\n        self.reindex()\\n\\nif __name__ == \\'__main__\\':\\n    ReindexTest().main()\\n',\n",
       " '# bottom-up dp: traverses iteratively starting from the smallest subset (bottom) going up\\n# ex: fib(1), fib(2), fib(3), fib(4), ... , fib(n)\\ndef knapsack_bottom_up_dp(weights, values, W):\\n    # generating array for storing optimal values\\n    n = len(weights)\\n    opt_vals = [[0 for _ in range(W + 1)] for _ in range(n + 1)]\\n    \\n    # computing possible optimal values\\n    for i in range(1, n + 1):\\n        for w in range(1, W + 1):\\n            wi = weights[i - 1]\\n            # if weight of the current item is greater than the current weight\\n            # take the previous optimal value from previous top slot (i - 1)\\n            if wi > w:\\n                opt_vals[i][w] = opt_vals[i - 1][w]\\n            # otherwise, take the maximum between:\\n            # putting the current item into the knapsack or not\\n            else:\\n                opt_vals[i][w] = max(values[i - 1] + opt_vals[i - 1][w - wi], \\n                                     opt_vals[i - 1][w])\\n\\n    # backtracking\\n    opt_subset = backtrack(n, W, weights, values, opt_vals)\\n\\n    # for i in opt_vals: print(i)\\n    return (opt_vals[-1][-1], opt_subset)\\n\\n# top-down: recursively computes values starting from the biggest (top) going down\\n# ex: fib(n), fib(n-1), fib(n-2), ... , fib(1)\\ndef knapsack_top_down_dp(weights, values, W):\\n    # generating array for storing optimal values with 0 in edges and -1 elsewhere\\n    n = len(weights)\\n    opt_vals = [[0 for _ in range(W + 1)]]\\n    [opt_vals.append([0 if j == 0 else -1 for j in range(W + 1)]) for _ in range(n)]\\n\\n    # run recursion\\n    max_val = helper(weights, values, opt_vals, n, W)\\n\\n    # backtracking\\n    opt_subset = backtrack(n, W, weights, values, opt_vals)\\n\\n    # for i in opt_vals: print(i)\\n    return (max_val, opt_subset)\\n  \\ndef helper(weights, values, opt_vals, i, w):\\n    # base case\\n    if opt_vals[i][w] >= 0:\\n        return opt_vals[i][w]\\n\\n    # skip the item if the wieght of item is bigger than the remaining weight in the knapsack\\n    if weights[i - 1] > w :\\n        max_val = helper(weights, values, opt_vals, i - 1, w)\\n    # otherwise, recursively compute maximum between picking the item or not picking the item\\n    else:\\n        max_val = max(values[i - 1] + helper(weights, values, opt_vals, i - 1, w - weights[i - 1]),\\n                      helper(weights, values, opt_vals, i - 1, w))\\n    \\n    # memorize the computed maximum value\\n    opt_vals[i][w] = max_val\\n    return max_val\\n\\ndef backtrack(n, W, weights, values, opt_vals):\\n    opt_subset = [0 for i in range(n)]\\n    i, w = n, W\\n    while i > 0 and w > 0:\\n        wi = weights[i - 1]\\n        if w - wi >= 0 and opt_vals[i][w] == values[i - 1] + opt_vals[i - 1][w - wi]:\\n            opt_subset[i - 1] = 1\\n            w -= wi\\n        i -= 1 \\n    return opt_subset\\n\\n# brute force: generate all possible 2^n variants and determine the maximum optimal value\\n# brute force: without bit manipulation\\nimport itertools\\ndef knapsack_brute_force(weights, values, W):\\n    # initializing length, maximum total value and optimal subset of selected items\\n    n, max_val, opt_subset = len(weights), 0, []\\n\\n    # creating a generator, that traveses through all possible combinations of selecting n items\\n    combinations = map(list, itertools.product([0, 1], repeat=n))\\n\\n    # iterating over all combinations\\n    for cmb in combinations:\\n        # calcualting total weight and total value for current combination\\n        tot_weights = sum([a*b for a,b in zip(weights, cmb)])\\n        tot_values = sum([a*b for a,b in zip(values, cmb)])\\n\\n        # updating maximum total value and optimal subset to current \\n        if tot_weights <= W and tot_values > max_val:\\n            max_val = tot_values\\n            opt_subset = cmb\\n    \\n    return (max_val, opt_subset)\\n\\n# brute force: with bit manipulation\\ndef knapsack_brute_force_bm(weights, values, W):\\n    # initializing length, maximum total value and optimal subset of selected items\\n    n, max_val = len(weights), 0\\n    opt_subset = [0]*n\\n    i, m = 1, 2**n\\n\\n    # iterating over all combinations\\n    while i < m:\\n        j, tot_weights, tot_values, cur = i, 0, 0, 0\\n        cur_subset = [0]*n\\n        while j:\\n            if j & 1:\\n                tot_weights += weights[cur]\\n                tot_values += values[cur]\\n                cur_subset[cur] = 1\\n            j >>= 1\\n            cur += 1\\n        i+=1\\n        \\n        # updating maximum total value and optimal subset to current \\n        if tot_weights <= W and tot_values > max_val:\\n            max_val = tot_values\\n            opt_subset = cur_subset\\n    \\n    return (max_val, opt_subset)\\n\\n# correctness testing\\ndef corr_test():\\n    functions = [\\n        (\"BOTTOM UP:\", knapsack_bottom_up_dp), \\n        (\"TOP DOWN:\", knapsack_top_down_dp),\\n        (\"BRUTE FORCE:\", knapsack_brute_force),\\n        (\"BRUTE FORCE (bit manip.):\", knapsack_brute_force_bm)\\n    ]\\n\\n    # source of the test cases: http://people.sc.fsu.edu/~jburkardt/datasets/knapsack_01/knapsack_01.html\\n    test_cases = [\\n        [([2,2,3], [2,3,4], 6), [0, 1, 1]],\\n        [([2,2,3], [7,2,1], 6), [1, 1, 0]],\\n        [([23,31,29,44,53,38,63,85,89,82], [92,57,49,68,60,43,67,84,87,72], 165), [1,1,1,1,0,1,0,0,0,0]],\\n        [([12,7,11,8,9], [24,13,23,15,16], 26), [0,1,1,1,0]],\\n        [([56,59,80,64,75,17], [50,50,64,46,50,5], 190), [1,1,0,0,1,0]],\\n        [([31,10,20,19,4,3,6], [70,20,39,37,7,5,10], 50), [1,0,0,1,0,0,0]],\\n        [([25,35,45,5,25,3,2,2], [350,400,450,20,70,8,5,5], 104), [1,0,1,1,1,0,1,1]],\\n        [([41,50,49,59,55,57,60], [442,525,511,593,546,564,617], 170), [0,1,0,1,0,0,1]]\\n    ]\\n    for fn in functions:\\n        for tc in test_cases:\\n            max_val, opt_subset = fn[1](*tc[0])\\n            is_correct = opt_subset == tc[1]\\n            print(fn[0], max_val)\\n            print(\"Correct:\", is_correct)\\n            print(\"Output:\", opt_subset)\\n            print(\"Answer:\", tc[1])\\n\\nimport random\\nimport time\\nimport numpy as np\\nimport pandas as pd\\n\\ndef main():\\n    # Brute force vs. DP bottom-up vs. DP top-down\\n    test(*get_inputs_BF_vs_DPbu_vs_DPtd())\\n\\n    # DP bottom-up vs. DP top-down\\n    test(*get_inputs_DPbu_vs_DPtd())\\n    \\n# generate inputs for testing DP bottom-up vs. DP top-down\\ndef get_inputs_BF_vs_DPbu_vs_DPtd():\\n    # N = np.arange(1, 26, 1)         #[1, 2, ..., 25]\\n    # K = np.arange(0.2, 1.1, 0.2)    #[0.1, 0.2, ..., 1]\\n    # wi_vi_pow = np.arange(3, 10, 2)  #[3, 5, 7, 9]\\n    N = np.arange(1, 3, 1)         #[1, 2, ..., 25]\\n    K = np.arange(0.2, 0.3, 0.2)    #[0.1, 0.2, ..., 1]\\n    wi_vi_pow = np.arange(3, 4, 2)  #[3, 5, 7, 9]\\n    algorithms = [\\n        (\"Brute force\", knapsack_brute_force), \\n        (\"DP bottom-up\",knapsack_bottom_up_dp),\\n        (\"DP top-down\", knapsack_top_down_dp)\\n    ]\\n    filename = \"DP bottom-up vs. DP top-down\"\\n    return (N, K, wi_vi_pow, algorithms, filename)\\n\\n# generate inputs for testing DP bottom-up vs. DP top-down\\ndef get_inputs_DPbu_vs_DPtd():\\n    # N = [2**i for i in range(0,32)]\\n    # K = np.arange(0.2, 1.1, 0.2)\\n    # wi_vi_pow = np.arange(3, 10, 2)\\n    N = [2**i for i in range(0,2)]\\n    K = np.arange(0.2, 0.3, 0.2)\\n    wi_vi_pow = np.arange(3, 4, 2)\\n    algorithms = [\\n        (\"DP bottom-up\", knapsack_bottom_up_dp),\\n        (\"DP top-down\", knapsack_top_down_dp)\\n    ]\\n    filename = \"DP bottom-up vs. DP top-down\"\\n    return (N, K, wi_vi_pow, algorithms, filename)\\n    \\n# full testing\\ndef test(N, K, wi_vi_pow, algorithms, filename):\\n    # arrays to store columns of the output table\\n    runtimes = [[] for _ in algorithms]\\n    n_arr = []\\n    W_arr = []\\n    wi_vi_range_arr = []\\n\\n    # run over all combinations of the inputs\\n    # different number of input items \\n    for ni in N:\\n        # different range of weights and values (ni = n) => 1,2,3,...,n\\n        for wi_vi in wi_vi_pow:\\n            # generate weights and values and compute sum of weights \\n            # (range = (1, 10^wi_vi)) => (1, 10^3),...,(1, 10^m)\\n            weights = np.random.randint(10**wi_vi, size=ni)\\n            values = np.random.randint(10**wi_vi, size=ni)\\n            sum_weights = sum(weights)\\n            # different capacity of the knapsack \\n            # (W = sum(weights) * ki) => W*1,W*0.8,...,W*0.2\\n            for ki in K:\\n                # add inputs values into the table columns           \\n                n_arr.append(ni)\\n                W_arr.append(int(sum_weights * ki))\\n                wi_vi_range_arr.append(10**wi_vi)\\n                # run algorithms and time performance\\n                print(\"Inputs: n={}, W={}\".format(ni, W_arr[-1]))\\n                for i in range(len(algorithms)):\\n                    print(\"Running: {} with wi_vi_range: 1-{}\".format(algorithms[i][0], wi_vi_range_arr[-1]))\\n                    start = time.clock()\\n                    algorithms[i][1](weights, values, int(sum_weights * ki))\\n                    end = time.clock()\\n                    runtimes[i].append(end - start)\\n        # save table as csv            \\n        save(runtimes, algorithms, n_arr, W_arr, wi_vi_range_arr, filename)\\n    # save table as csv    \\n    save(runtimes, algorithms, n_arr, W_arr, wi_vi_range_arr, filename)                \\n\\n# save table as csv\\ndef save(runtimes, algorithms, n_arr, W_arr, wi_vi_range_arr, filename):\\n    total_runtime = sum([sum(rn) for rn in runtimes])\\n    print(\"Saving to csv\\\\nTotal runtime: {}\".format(total_runtime))\\n    df_algorithms = pd.concat([pd.Series(rn) for rn in runtimes], keys=[alg[0] for alg in algorithms], axis=1)\\n    df_inputs = pd.DataFrame({\"n\": n_arr, \"W\": W_arr, \"wi, vi range\": wi_vi_range_arr})\\n    df_concat = pd.concat([df_algorithms, df_inputs], axis = 1)\\n    df_concat.to_csv(filename+\".csv\")\\n\\nif __name__ == \"__main__\":\\n    main()',\n",
       " '# Author    : Andrzej Wojciechowski (AAWO)\\n# Copyright : Andrzej Wojciechowski (AAWO)\\n# --------------------------------------------\\nfrom sys import argv, stdout\\nfrom random import randrange\\n\\nif len(argv) == 3:\\n   stdout.write(str(randrange(int(argv[1]), int(argv[2])+1)))\\nelif len(argv) == 4:\\n   stdout.write(str(randrange(int(argv[1]), int(argv[2])+1, int(argv[3]))))\\nelse:\\n   argv_num = (len(argv)-1)\\n   raise TypeError(\"Wrong number of arguments. Expected 2 or 3 - received %d\" % argv_num)\\n',\n",
       " '#!/usr/bin/env python2\\n# -*- coding: utf-8 -*-\\n##################################################\\n# GNU Radio Python Flow Graph\\n# Title: IP3\\n# Author: Alexandros-Apostolos A. Boulogeorgos\\n# Generated: Mon Aug 12 09:13:37 2019\\n##################################################\\n\\nif __name__ == \\'__main__\\':\\n    import ctypes\\n    import sys\\n    if sys.platform.startswith(\\'linux\\'):\\n        try:\\n            x11 = ctypes.cdll.LoadLibrary(\\'libX11.so\\')\\n            x11.XInitThreads()\\n        except:\\n            print \"Warning: failed to XInitThreads()\"\\n\\nfrom PyQt4 import Qt\\nfrom gnuradio import analog\\nfrom gnuradio import blocks\\nfrom gnuradio import channels\\nfrom gnuradio import eng_notation\\nfrom gnuradio import gr\\nfrom gnuradio import qtgui\\nfrom gnuradio.eng_option import eng_option\\nfrom gnuradio.filter import firdes\\nfrom gnuradio.qtgui import Range, RangeWidget\\nfrom optparse import OptionParser\\nimport math\\nimport sip\\nimport sys\\n\\n\\nclass amplifiers_nonlinearities(gr.top_block, Qt.QWidget):\\n\\n    def __init__(self):\\n        gr.top_block.__init__(self, \"IP3\")\\n        Qt.QWidget.__init__(self)\\n        self.setWindowTitle(\"IP3\")\\n        try:\\n            self.setWindowIcon(Qt.QIcon.fromTheme(\\'gnuradio-grc\\'))\\n        except:\\n            pass\\n        self.top_scroll_layout = Qt.QVBoxLayout()\\n        self.setLayout(self.top_scroll_layout)\\n        self.top_scroll = Qt.QScrollArea()\\n        self.top_scroll.setFrameStyle(Qt.QFrame.NoFrame)\\n        self.top_scroll_layout.addWidget(self.top_scroll)\\n        self.top_scroll.setWidgetResizable(True)\\n        self.top_widget = Qt.QWidget()\\n        self.top_scroll.setWidget(self.top_widget)\\n        self.top_layout = Qt.QVBoxLayout(self.top_widget)\\n        self.top_grid_layout = Qt.QGridLayout()\\n        self.top_layout.addLayout(self.top_grid_layout)\\n\\n        self.settings = Qt.QSettings(\"GNU Radio\", \"amplifiers_nonlinearities\")\\n        self.restoreGeometry(self.settings.value(\"geometry\").toByteArray())\\n\\n        ##################################################\\n        # Variables\\n        ##################################################\\n        self.samp_rate = samp_rate = 100000\\n        self.signal_amp = signal_amp = 0\\n        self.sigfreq = sigfreq = samp_rate*1.0247385/21.0\\n        self.ip3 = ip3 = 0\\n\\n        ##################################################\\n        # Blocks\\n        ##################################################\\n        self._signal_amp_range = Range(-150, 10, 5, 0, 200)\\n        self._signal_amp_win = RangeWidget(self._signal_amp_range, self.set_signal_amp, \\'Singal Power\\', \"counter_slider\", float)\\n        self.top_grid_layout.addWidget(self._signal_amp_win, 2,0,1,1)\\n        self._sigfreq_range = Range(0, samp_rate/2, 1000, samp_rate*1.0247385/21.0, 200)\\n        self._sigfreq_win = RangeWidget(self._sigfreq_range, self.set_sigfreq, \\'Signal Freq\\', \"counter_slider\", float)\\n        self.top_grid_layout.addWidget(self._sigfreq_win, 3,0,1,1)\\n        self._ip3_range = Range(0, 2, 0.01, 0, 200)\\n        self._ip3_win = RangeWidget(self._ip3_range, self.set_ip3, \\'IP3\\', \"counter_slider\", float)\\n        self.top_grid_layout.addWidget(self._ip3_win, 3,1,1,1)\\n        self.qtgui_freq_sink_x_0 = qtgui.freq_sink_c(\\n        \\t2048, #size\\n        \\tfirdes.WIN_FLATTOP, #wintype\\n        \\t0, #fc\\n        \\tsamp_rate, #bw\\n        \\t\\'\\', #name\\n        \\t2 #number of inputs\\n        )\\n        self.qtgui_freq_sink_x_0.set_update_time(0.10)\\n        self.qtgui_freq_sink_x_0.set_y_axis(-200, 0)\\n        self.qtgui_freq_sink_x_0.set_y_label(\\'Relative Gain\\', \\'dB\\')\\n        self.qtgui_freq_sink_x_0.set_trigger_mode(qtgui.TRIG_MODE_FREE, 0.0, 0, \"\")\\n        self.qtgui_freq_sink_x_0.enable_autoscale(False)\\n        self.qtgui_freq_sink_x_0.enable_grid(False)\\n        self.qtgui_freq_sink_x_0.set_fft_average(1.0)\\n        self.qtgui_freq_sink_x_0.enable_axis_labels(True)\\n        self.qtgui_freq_sink_x_0.enable_control_panel(False)\\n        \\n        if not True:\\n          self.qtgui_freq_sink_x_0.disable_legend()\\n        \\n        if \"complex\" == \"float\" or \"complex\" == \"msg_float\":\\n          self.qtgui_freq_sink_x_0.set_plot_pos_half(not True)\\n        \\n        labels = [\\'Input\\', \\'With IP3\\', \\'\\', \\'\\', \\'\\',\\n                  \\'\\', \\'\\', \\'\\', \\'\\', \\'\\']\\n        widths = [2, 2, 1, 1, 1,\\n                  1, 1, 1, 1, 1]\\n        colors = [\"blue\", \"red\", \"green\", \"black\", \"cyan\",\\n                  \"magenta\", \"yellow\", \"dark red\", \"dark green\", \"dark blue\"]\\n        alphas = [0.5, 0.5, 1.0, 1.0, 1.0,\\n                  1.0, 1.0, 1.0, 1.0, 1.0]\\n        for i in xrange(2):\\n            if len(labels[i]) == 0:\\n                self.qtgui_freq_sink_x_0.set_line_label(i, \"Data {0}\".format(i))\\n            else:\\n                self.qtgui_freq_sink_x_0.set_line_label(i, labels[i])\\n            self.qtgui_freq_sink_x_0.set_line_width(i, widths[i])\\n            self.qtgui_freq_sink_x_0.set_line_color(i, colors[i])\\n            self.qtgui_freq_sink_x_0.set_line_alpha(i, alphas[i])\\n        \\n        self._qtgui_freq_sink_x_0_win = sip.wrapinstance(self.qtgui_freq_sink_x_0.pyqwidget(), Qt.QWidget)\\n        self.top_grid_layout.addWidget(self._qtgui_freq_sink_x_0_win, 0,0,1,2)\\n        self.channels_distortion_3_gen_0 = channels.distortion_3_gen(ip3)\\n        self.blocks_throttle_0 = blocks.throttle(gr.sizeof_gr_complex*1, samp_rate,True)\\n        self.blocks_add_xx_0 = blocks.add_vcc(1)\\n        self.analog_sig_source_x_0_0 = analog.sig_source_c(samp_rate, analog.GR_COS_WAVE, 2.45*sigfreq, pow(10.0,signal_amp/20.0), 0)\\n        self.analog_sig_source_x_0 = analog.sig_source_c(samp_rate, analog.GR_COS_WAVE, sigfreq, 1, 0)\\n\\n        ##################################################\\n        # Connections\\n        ##################################################\\n        self.connect((self.analog_sig_source_x_0, 0), (self.blocks_add_xx_0, 1))    \\n        self.connect((self.analog_sig_source_x_0_0, 0), (self.blocks_add_xx_0, 0))    \\n        self.connect((self.blocks_add_xx_0, 0), (self.blocks_throttle_0, 0))    \\n        self.connect((self.blocks_throttle_0, 0), (self.channels_distortion_3_gen_0, 0))    \\n        self.connect((self.blocks_throttle_0, 0), (self.qtgui_freq_sink_x_0, 0))    \\n        self.connect((self.channels_distortion_3_gen_0, 0), (self.qtgui_freq_sink_x_0, 1))    \\n\\n    def closeEvent(self, event):\\n        self.settings = Qt.QSettings(\"GNU Radio\", \"amplifiers_nonlinearities\")\\n        self.settings.setValue(\"geometry\", self.saveGeometry())\\n        event.accept()\\n\\n    def get_samp_rate(self):\\n        return self.samp_rate\\n\\n    def set_samp_rate(self, samp_rate):\\n        self.samp_rate = samp_rate\\n        self.set_sigfreq(self.samp_rate*1.0247385/21.0)\\n        self.qtgui_freq_sink_x_0.set_frequency_range(0, self.samp_rate)\\n        self.blocks_throttle_0.set_sample_rate(self.samp_rate)\\n        self.analog_sig_source_x_0_0.set_sampling_freq(self.samp_rate)\\n        self.analog_sig_source_x_0.set_sampling_freq(self.samp_rate)\\n\\n    def get_signal_amp(self):\\n        return self.signal_amp\\n\\n    def set_signal_amp(self, signal_amp):\\n        self.signal_amp = signal_amp\\n        self.analog_sig_source_x_0_0.set_amplitude(pow(10.0,self.signal_amp/20.0))\\n\\n    def get_sigfreq(self):\\n        return self.sigfreq\\n\\n    def set_sigfreq(self, sigfreq):\\n        self.sigfreq = sigfreq\\n        self.analog_sig_source_x_0_0.set_frequency(2.45*self.sigfreq)\\n        self.analog_sig_source_x_0.set_frequency(self.sigfreq)\\n\\n    def get_ip3(self):\\n        return self.ip3\\n\\n    def set_ip3(self, ip3):\\n        self.ip3 = ip3\\n        self.channels_distortion_3_gen_0.set_beta(self.ip3)\\n\\n\\ndef main(top_block_cls=amplifiers_nonlinearities, options=None):\\n\\n    from distutils.version import StrictVersion\\n    if StrictVersion(Qt.qVersion()) >= StrictVersion(\"4.5.0\"):\\n        style = gr.prefs().get_string(\\'qtgui\\', \\'style\\', \\'raster\\')\\n        Qt.QApplication.setGraphicsSystem(style)\\n    qapp = Qt.QApplication(sys.argv)\\n\\n    tb = top_block_cls()\\n    tb.start()\\n    tb.show()\\n\\n    def quitting():\\n        tb.stop()\\n        tb.wait()\\n    qapp.connect(qapp, Qt.SIGNAL(\"aboutToQuit()\"), quitting)\\n    qapp.exec_()\\n\\n\\nif __name__ == \\'__main__\\':\\n    main()\\n',\n",
       " '#    ____  ____\\n#   /   /\\\\/   /\\n#  /___/  \\\\  /   Copyright (c) 2021, Xilinx®.\\n#  \\\\   \\\\   \\\\/    Author: Víctor Mayoral Vilches <victorma@xilinx.com>\\n#   \\\\   \\\\\\n#   /   /\\n#  /___/   /\\\\\\n#  \\\\   \\\\  /  \\\\\\n#   \\\\___\\\\/\\\\___\\\\\\n#\\n# Licensed under the Apache License, Version 2.0\\n#\\n__version__ = \"0.2.0\"\\n',\n",
       " '# -*- coding: utf-8 -*-\\n\\n\"\"\"Top-level package for Easy graphviz.\"\"\"\\n\\n__author__ = \"\"\"Gus Dunn\"\"\"\\n__email__ = \\'w.gus.dunn@gmail.com\\'\\n__version__ = \\'0.1.1\\'\\n',\n",
       " 'from django.apps import AppConfig\\r\\n\\r\\n\\r\\nclass LogicConfig(AppConfig):\\r\\n    default_auto_field = \"django.db.models.BigAutoField\"\\r\\n    name = \"Logic\"\\r\\n',\n",
       " '# Auto generated configuration file\\n# using: \\n# Revision: 1.19 \\n# Source: /local/reps/CMSSW/CMSSW/Configuration/Applications/python/ConfigBuilder.py,v \\n# with command line options: l1Ntuple -s RAW2DIGI --python_filename=mc_L1TReEmulMCFromRAW_L1NtupleEMU.py -n 2 --no_output --era=Run3 --mc --conditions=112X_mcRun3_2021_realistic_v13 --customise=L1Trigger/Configuration/customiseReEmul.L1TReEmulMCFromRAW --customise=L1Trigger/L1TNtuples/customiseL1Ntuple.L1NtupleEMU --customise=L1Trigger/Configuration/customiseSettings.L1TSettingsToCaloParams_2018_v1_4 --filein=/store/mc/Run3Winter20DRPremixMiniAOD/TT_TuneCP5_14TeV-powheg-pythia8/GEN-SIM-RAW/110X_mcRun3_2021_realistic_v6-v2/20000/CFCAE998-5A17-FB48-A36F-A31EA28D2A72.root\\nimport FWCore.ParameterSet.Config as cms\\n\\nfrom Configuration.Eras.Era_Run3_cff import Run3\\n\\nprocess = cms.Process(\\'RAW2DIGI\\',Run3)\\n\\n# import of standard configurations\\nprocess.load(\\'Configuration.StandardSequences.Services_cff\\')\\nprocess.load(\\'SimGeneral.HepPDTESSource.pythiapdt_cfi\\')\\nprocess.load(\\'FWCore.MessageService.MessageLogger_cfi\\')\\nprocess.load(\\'Configuration.EventContent.EventContent_cff\\')\\nprocess.load(\\'SimGeneral.MixingModule.mixNoPU_cfi\\')\\nprocess.load(\\'Configuration.StandardSequences.GeometryRecoDB_cff\\')\\nprocess.load(\\'Configuration.StandardSequences.MagneticField_cff\\')\\nprocess.load(\\'Configuration.StandardSequences.RawToDigi_cff\\')\\nprocess.load(\\'Configuration.StandardSequences.EndOfProcess_cff\\')\\nprocess.load(\\'Configuration.StandardSequences.FrontierConditions_GlobalTag_cff\\')\\n\\nprocess.maxEvents = cms.untracked.PSet(\\n    input = cms.untracked.int32(10),\\n    output = cms.optional.untracked.allowed(cms.int32,cms.PSet)\\n)\\n\\n# Input source\\nprocess.source = cms.Source(\"PoolSource\",\\n    fileNames = cms.untracked.vstring(\\'/store/mc/Run3Winter20DRPremixMiniAOD/TT_TuneCP5_14TeV-powheg-pythia8/GEN-SIM-RAW/110X_mcRun3_2021_realistic_v6-v2/20000/CFCAE998-5A17-FB48-A36F-A31EA28D2A72.root\\'),\\n    secondaryFileNames = cms.untracked.vstring()\\n)\\n\\nprocess.options = cms.untracked.PSet(\\n    FailPath = cms.untracked.vstring(),\\n    IgnoreCompletely = cms.untracked.vstring(),\\n    Rethrow = cms.untracked.vstring(),\\n    SkipEvent = cms.untracked.vstring(),\\n    allowUnscheduled = cms.obsolete.untracked.bool,\\n    canDeleteEarly = cms.untracked.vstring(),\\n    emptyRunLumiMode = cms.obsolete.untracked.string,\\n    eventSetup = cms.untracked.PSet(\\n        forceNumberOfConcurrentIOVs = cms.untracked.PSet(\\n            allowAnyLabel_=cms.required.untracked.uint32\\n        ),\\n        numberOfConcurrentIOVs = cms.untracked.uint32(1)\\n    ),\\n    fileMode = cms.untracked.string(\\'FULLMERGE\\'),\\n    forceEventSetupCacheClearOnNewRun = cms.untracked.bool(False),\\n    makeTriggerResults = cms.obsolete.untracked.bool,\\n    numberOfConcurrentLuminosityBlocks = cms.untracked.uint32(1),\\n    numberOfConcurrentRuns = cms.untracked.uint32(1),\\n    numberOfStreams = cms.untracked.uint32(0),\\n    numberOfThreads = cms.untracked.uint32(1),\\n    printDependencies = cms.untracked.bool(False),\\n    sizeOfStackForThreadsInKB = cms.optional.untracked.uint32,\\n    throwIfIllegalParameter = cms.untracked.bool(True),\\n    wantSummary = cms.untracked.bool(False)\\n)\\n\\n# Production Info\\nprocess.configurationMetadata = cms.untracked.PSet(\\n    annotation = cms.untracked.string(\\'l1Ntuple nevts:2\\'),\\n    name = cms.untracked.string(\\'Applications\\'),\\n    version = cms.untracked.string(\\'$Revision: 1.19 $\\')\\n)\\n\\n# Output definition\\n\\n# Additional output definition\\n\\n# Other statements\\nfrom Configuration.AlCa.GlobalTag import GlobalTag\\nprocess.GlobalTag = GlobalTag(process.GlobalTag, \\'112X_mcRun3_2021_realistic_v13\\', \\'\\')\\n\\nprocess.GlobalTag.toGet = cms.VPSet(\\n        cms.PSet(record = cms.string(\"GEMeMapRcd\"),\\n                       tag = cms.string(\"GEMeMapDummy\"),\\n                       connect = cms.string(\"sqlite_file:L1Trigger/Configuration/test/GEMeMapDummy.db\")\\n                )\\n)\\nprocess.muonGEMDigis.useDBEMap = True\\n\\n# Path and EndPath definitions\\nprocess.raw2digi_step = cms.Path(process.RawToDigi)\\nprocess.endjob_step = cms.EndPath(process.endOfProcess)\\n\\n# Schedule definition\\nprocess.schedule = cms.Schedule(process.raw2digi_step,process.endjob_step)\\nfrom PhysicsTools.PatAlgos.tools.helpers import associatePatAlgosToolsTask\\nassociatePatAlgosToolsTask(process)\\n\\n# customisation of the process.\\n\\n# Automatic addition of the customisation function from L1Trigger.Configuration.customiseReEmul\\nfrom L1Trigger.Configuration.customiseReEmul import L1TReEmulMCFromRAW \\n\\n#call to customisation function L1TReEmulMCFromRAW imported from L1Trigger.Configuration.customiseReEmul\\nprocess = L1TReEmulMCFromRAW(process)\\n\\n# Automatic addition of the customisation function from L1Trigger.L1TNtuples.customiseL1Ntuple\\nfrom L1Trigger.L1TNtuples.customiseL1Ntuple import L1NtupleEMU \\n\\n#call to customisation function L1NtupleEMU imported from L1Trigger.L1TNtuples.customiseL1Ntuple\\nprocess = L1NtupleEMU(process)\\n\\n# Automatic addition of the customisation function from L1Trigger.Configuration.customiseSettings\\nfrom L1Trigger.Configuration.customiseSettings import L1TSettingsToCaloParams_2018_v1_4 \\n\\n#call to customisation function L1TSettingsToCaloParams_2018_v1_4 imported from L1Trigger.Configuration.customiseSettings\\nprocess = L1TSettingsToCaloParams_2018_v1_4(process)\\n\\n# End of customisation functions\\n\\n\\n# Customisation from command line\\n\\n# Add early deletion of temporary data products to reduce peak memory need\\nfrom Configuration.StandardSequences.earlyDeleteSettings_cff import customiseEarlyDelete\\nprocess = customiseEarlyDelete(process)\\n# End adding early deletion\\n',\n",
       " '# pylint: disable=g-bad-file-header\\n# Copyright 2016 The Bazel Authors. All rights reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#    http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"Wrapper script for executing the Microsoft Compiler.\"\"\"\\nimport os\\nimport sys\\nimport msvc_link\\nimport msvc_tools\\n\\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\\nsys.path.append(SCRIPT_DIR)\\n\\nGCCPATTERNS = [\\n    (\\'-I(.+)\\', [\\'/I$0\\']),\\n    (\\'-m(32|64)\\', [\\'$TARGET_ARCH\\']),\\n    (\\'-Xcompilation-mode=(dbg|fastbuild|opt)\\', [\\'$COMPILATION_MODE\\']),\\n    (\\'-msse\\', [\\'/arch:SSE\\']),\\n    (\\'-msse2\\', [\\'/arch:SSE2\\']),\\n    (\\'-D(.+)\\', [\\'/D$0\\']),\\n    (\\'-U(.+)\\', [\\'/U$0\\']),\\n    (\\'-E\\', [\\'/E\\']),\\n    (\\'-O0\\', [\\'/Od\\']),\\n    (\\'-Os\\', [\\'/O1\\']),\\n    (\\'-O2\\', [\\'/O2\\']),\\n    (\\'-g0\\', []),\\n    (\\'-g\\', [\\'$DEBUG_RT\\']),\\n    (\\'-fexceptions\\', [\\'/U_HAS_EXCEPTIONS\\', \\'/D_HAS_EXCEPTIONS=1\\', \\'/EHsc\\']),\\n    (\\'-fomit-frame-pointer\\', [\\'/Oy\\']),\\n    (\\'-fno-rtti\\', [\\'/GR-\\']),\\n    (\\'-frtti\\', [\\'/GR\\']),\\n    (\\'-fPIC\\', []),\\n\\n    # This is unneeded for Windows.\\n    ((\\'-include\\', \\'(.+)\\'), [\\'/FI$PATH0\\']),\\n    (\\'-w\\', [\\'/w\\']),\\n    (\\'-Wall\\', [\\'/Wall\\']),\\n    (\\'-Wsign-compare\\', [\\'/we4018\\']),\\n    (\\'-Wno-sign-compare\\', [\\'/wd4018\\']),\\n    (\\'-Wconversion\\', [\\'/we4244\\', \\'/we4267\\']),\\n    (\\'-Wno-conversion\\', [\\'/wd4244\\', \\'/wd4267\\']),\\n    (\\'-Wno-sign-conversion\\', []),\\n    (\\'-Wno-implicit-fallthrough\\', []),\\n    (\\'-Wno-implicit-function-declaration\\', [\\'/wd4013\\']),\\n    (\\'-Wimplicit-function-declaration\\', [\\'/we4013\\']),\\n    (\\'-Wcovered-switch-default\\', [\\'/we4062\\']),\\n    (\\'-Wno-covered-switch-default\\', [\\'/wd4062\\']),\\n    (\\'-Wno-error\\', []),\\n    (\\'-Wno-invalid-offsetof\\', []),\\n    (\\'-Wno-overloaded-virtual\\', []),\\n    (\\'-Wno-reorder\\', []),\\n    (\\'-Wno-string-plus-int\\', []),\\n    (\\'-Wl,S\\', []),  # Stripping is unnecessary since msvc uses pdb files.\\n    (\\'-Wl,-rpath(.+)\\', []),\\n    (\\'-B(.+)\\', []),\\n    (\\'-static\\', []),\\n    (\\'-shared\\', [\\'/DLL\\']),\\n    (\\'-std=(.+)\\', []),\\n]\\n\\n\\ndef _IsLink(args):\\n  \"\"\"Determines whether we need to link rather than compile.\\n\\n  A set of arguments is for linking if they contain -static, -shared, are adding\\n  adding library search paths through -L, or libraries via -l.\\n\\n  Args:\\n    args: List of arguments\\n\\n  Returns:\\n    Boolean whether this is a link operation or not.\\n  \"\"\"\\n  for arg in args:\\n    # Certain flags indicate we are linking.\\n    if (arg in [\\'-shared\\', \\'-static\\'] or arg[:2] in [\\'-l\\', \\'-L\\'] or\\n        arg[:3] == \\'-Wl\\'):\\n      return True\\n  return False\\n\\n\\nclass MsvcCompiler(msvc_tools.WindowsRunner):\\n  \"\"\"Driver for the Microsoft compiler.\"\"\"\\n\\n  def Run(self, argv):\\n    \"\"\"Runs the compiler using the passed clang/gcc style argument list.\\n\\n    Args:\\n      argv: List of arguments\\n\\n    Returns:\\n      The return code of the compilation.\\n\\n    Raises:\\n      ValueError: if target architecture isn\\'t specified\\n    \"\"\"\\n    parser = msvc_tools.ArgParser(self, argv, GCCPATTERNS)\\n\\n    # Select runtime option\\n    # Find the last runtime option passed\\n    rt = None\\n    rt_idx = -1\\n    for i, opt in enumerate(reversed(parser.options)):\\n      if opt in [\\'/MT\\', \\'/MTd\\', \\'/MD\\', \\'/MDd\\']:\\n        if opt[-1] == \\'d\\':\\n          parser.enforce_debug_rt = True\\n        rt = opt[:3]\\n        rt_idx = len(parser.options) - i - 1\\n        break\\n    rt = rt or \\'/MT\\'  # Default to static runtime\\n    # Add debug if necessary\\n    if parser.enforce_debug_rt:\\n      rt += \\'d\\'\\n    # Include runtime option\\n    if rt_idx >= 0:\\n      parser.options[rt_idx] = rt\\n    else:\\n      parser.options.append(rt)\\n\\n    compiler = \\'cl\\'\\n    if parser.is_cuda_compilation:\\n      compiler = \\'nvcc\\'\\n    return self.RunBinary(compiler, parser.options, parser)\\n\\n\\ndef main(argv):\\n  # If we are supposed to link create a static library.\\n  if _IsLink(argv[1:]):\\n    return msvc_link.main(argv)\\n  else:\\n    return MsvcCompiler().Run(argv[1:])\\n\\n\\nif __name__ == \\'__main__\\':\\n  sys.exit(main(sys.argv[1:]))  # need to skip the first argument\\n',\n",
       " '\"\"\"\\r\\nThe following program leverages the regression coefficients generated after training the model in task 2 as an input file\\r\\n\"\"\"\\r\\nfrom __future__ import print_function\\r\\nimport sys\\r\\nimport re\\r\\nfrom operator import add\\r\\nimport numpy as np \\r\\nfrom pyspark import SparkContext\\r\\n\\r\\nif __name__ == \"__main__\":\\r\\n\\r\\n    sc = SparkContext(appName=\"LogisticRegression_task3\")\\r\\n    \\r\\n    # Read the dataset \\r\\n    d_corpus = sc.textFile(sys.argv[1])\\r\\n    \\r\\n    # Each entry in validLines will be a line from the text file\\r\\n    validDocLines = d_corpus.filter(lambda x : \\'id\\' in x and \\'url=\\' in x)\\r\\n\\r\\n    # Now, we transform it into a set of (docID, text) pairs\\r\\n    keyAndText = validDocLines.map(lambda x : (x[x.index(\\'id=\"\\') + 4 : x.index(\\'\" url=\\')], x[x.index(\\'\">\\') + 2:][:-6])) \\r\\n\\r\\n    # leveraged the code from assignment 2\\r\\n    # remove all non letter characters\\r\\n    regex = re.compile(\\'[^a-zA-Z]\\')\\r\\n    keyAndWordsList = keyAndText.map(lambda x : (str(x[0]), regex.sub(\\' \\', x[1]).lower().split()))\\r\\n    \\r\\n    # Now get the top 20,000 words... first change (docID, [\"word1\", \"word2\", \"word3\", ...])\\r\\n    # to (\"word1\", 1) (\"word2\", 1)...\\r\\n    conslidatedWords = keyAndWordsList.flatMap(lambda x: x[1]).map(lambda x: (x,1))\\r\\n\\r\\n    # Now, count all of the words, giving us (\"word1\", 1433), (\"word2\", 3423423), etc.\\r\\n    allCounts = conslidatedWords.reduceByKey(add)\\r\\n\\r\\n    # Get the top 20,000 words in a local array in a sorted format based on frequency\\r\\n    topWordsinDict = allCounts.top(20000, key = lambda x : x[1])\\r\\n\\r\\n    # We\\'ll create a RDD that has a set of (word, dictNum) pairs\\r\\n    # start by creating an RDD that has the number 0 through 20000\\r\\n    # 20000 is the number of words that will be in our dictionary\\r\\n    top20000Words = sc.parallelize(range(20000))\\r\\n\\r\\n    # Now, we transform (0), (1), (2), ... to (\"MostCommonWord\", 1)\\r\\n    # (\"NextMostCommon\", 2), ...\\r\\n    # the number will be the spot in the dictionary used to tell us\\r\\n    # where the word is located\\r\\n    dictionary = top20000Words.map (lambda x : (topWordsinDict[x][0], x))\\r\\n    \\r\\n    \\r\\n    # The following function gets a list of dictionaryPos values,\\r\\n    # and then creates a TF vector\\r\\n    # corresponding to those values... for example,\\r\\n    # if we get [3, 4, 1, 1, 2] we would in the\\r\\n    # end have [0, 2/5, 1/5, 1/5, 1/5] because 0 appears zero times,\\r\\n    # 1 appears twice, 2 appears once, etc.\\r\\n\\r\\n    def buildArray(listOfIndices):\\r\\n        \\r\\n        returnVal = np.zeros(20000)\\r\\n        \\r\\n        for index in listOfIndices:\\r\\n            returnVal[index] = returnVal[index] + 1\\r\\n        \\r\\n        mysum = np.sum(returnVal)\\r\\n        \\r\\n        returnVal = np.divide(returnVal, mysum)\\r\\n        \\r\\n        return returnVal\\r\\n        \\r\\n    def getLabel(x):\\r\\n      if x[:2] == \\'AU\\':\\r\\n        return 1\\r\\n\\r\\n      else:\\r\\n        return 0\\r\\n    \\r\\n    \\r\\n    # Leverage the regression coefficients genereated by task2 (model training) to make the prediction\\r\\n    #filePathOutputTask2 = sys.argv[2]\\r\\n    \\r\\n    # Open the file containing regression coefficients and read it\\r\\n    \\r\\n    #task2Lines = filePathOutputTask2.map(lambda x: x.split(\"Final Regression Coeffients:\\\\n[\"))\\r\\n    filePathOutputTask2 =sc.textFile(sys.argv[2])\\r\\n    \\r\\n    #filePathOutputTask2.map(lambda x: )\\r\\n    #with open(filePath) as file:\\r\\n     #   allLines = file.read()\\r\\n     \\r\\n    # Extract out all of the lines present in the output of task 2\\r\\n    task2Lines = filePathOutputTask2.map(lambda x: x.split(\",\"))\\r\\n    \\r\\n    # Extract the line containing the regression coefficients and remove \\'[\\' and \\']\\' from the extremes\\r\\n    listOfLines = task2Lines.collect()[10]\\r\\n    listOfLines[0] = listOfLines[0][1:]\\r\\n    listOfLines[len(listOfLines)-1] = listOfLines[len(listOfLines)-1][:len(listOfLines[len(listOfLines)-1])-2]\\r\\n\\r\\n    # Convert the list of regression coefficients to numpy array to be used as an input for prediction in task 3\\r\\n    regressionCoefficients = np.array(listOfLines, dtype = np.float64 )\\r\\n        \\r\\n    # Split the file and extract the \\'Regression Coefficients\\'    \\r\\n    #listOfLines = allLines.split(\"Final Regression Coeffients:\\\\n[\")\\r\\n    #listOfLines[len(listOfLines)-1] = listOfLines[len(listOfLines)-1][:len(listOfLines[len(listOfLines)-1])-2]\\r\\n    #regressionCoefficients = np.array(listOfLines[1].split(\\',\\'), dtype = np.float64 )\\r\\n    \\r\\n    # Threshold for logistic regression\\r\\n    threshold = 0.3\\r\\n\\r\\n    # Prediction Function using logistic regression\\r\\n    def predictionLogisticRegresison(x):\\r\\n      value = 1/(1+np.exp(-(  np.dot( x, regressionCoefficients )  )  )) \\r\\n      # return value\\r\\n      if value >= threshold:\\r\\n        return 1\\r\\n      else:\\r\\n        return 0\\r\\n        \\r\\n    ###################################################### PREDICTION/ EVALUATION - TAKS 3 ########\\r\\n    # Read the dataset \\r\\n    testData = sc.textFile(sys.argv[3])\\r\\n\\r\\n    # Each entry in validLines will be a line from the text file\\r\\n    validDocLinesTest = testData.filter(lambda x : \\'id\\' in x and \\'url=\\' in x)\\r\\n\\r\\n    # Now, we transform it into a set of (docID, text) pairs\\r\\n    keyAndTextTest = validDocLinesTest.map(lambda x : (x[x.index(\\'id=\"\\') + 4 : x.index(\\'\" url=\\')], x[x.index(\\'\">\\') + 2:][:-6])) \\r\\n\\r\\n    # remove all non letter characters\\r\\n    keyAndWordsListTest = keyAndTextTest.map(lambda x : (str(x[0]), regex.sub(\\' \\', x[1]).lower().split()))\\r\\n\\r\\n    # Get a RDD that has, for each (docID, [\"word1\", \"word2\", \"word3\", ...]),\\r\\n    # (\"word1\", docID), (\"word2\", docId), ...\\r\\n    allWordsWithDocIDTest = keyAndWordsListTest.flatMap(lambda x: ((j, x[0]) for j in x[1]))\\r\\n\\r\\n    # Join and link them, to get a set of (\"word1\", (dictionaryPos, docID)) pairs\\r\\n    allDictionaryWordsTest = dictionary.join(allWordsWithDocIDTest)\\r\\n\\r\\n    # Drop the actual word itself to get a set of (docID, dictionaryPos) pairs\\r\\n    justDocAndPosTest = allDictionaryWordsTest.map(lambda x: (x[1][1],x[1][0]))\\r\\n\\r\\n    # Get a set of (docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\\r\\n    allDictionaryWordsInEachDocTest = justDocAndPosTest.groupByKey()\\r\\n\\r\\n    # The following line this gets us a set of\\r\\n    # (docID,  [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\\r\\n    # and converts the dictionary positions to a bag-of-words numpy array...\\r\\n    allDocsAsNumpyArraysTest = allDictionaryWordsInEachDocTest.map(lambda x: (x[0], buildArray(x[1])))\\r\\n\\r\\n    # Now, create a version of allDocsAsNumpyArrays where, in the array,\\r\\n    # every entry is either zero or one.\\r\\n    # A zero means that the word does not occur,\\r\\n    # and a one means that it does.\\r\\n    zeroOrOneTest = allDocsAsNumpyArraysTest.map(lambda x: (x[0],np.where(x[1] > 0, 1, 0)))\\r\\n    \\r\\n    # Create a RDD of testing data and derive features and labels ... x[0]-> label, x[1]-> features\\r\\n    yLabelAndXFeatures = zeroOrOneTest.map(lambda x: (getLabel(x[0]),x[1]))\\r\\n    \\r\\n    # Make the prediction using the function \\'predictionLogisticRegresison\\'\\r\\n    yLabelAndXFeaturesPrediction = yLabelAndXFeatures.map(lambda x: (x[0],x[1],predictionLogisticRegresison(x[1])))\\r\\n\\r\\n    # Function to calculate True Positives\\r\\n    def calculateTruePositives(x):\\r\\n      if (x[0] == 1 and x[2] == 1): # the article was Australian court case (x[0]) and the prediction was also Australian court case x[2]\\r\\n        return 1\\r\\n      else:\\r\\n        return 0\\r\\n\\r\\n    # Function to calculate False Positives\\r\\n    def calculateFalsePositives(x):\\r\\n      if (x[0] == 0 and x[2] == 1): # the article was not Australian court case (x[0]) but the prediction was Australian court case x[2]\\r\\n        return 1\\r\\n      else:\\r\\n        return 0\\r\\n\\r\\n    # Function to calculate False Negatives\\r\\n    def calculateFalseNegatives(x):\\r\\n      if (x[0] == 1 and x[2] == 0): # the article was Australian court case (x[0]) but the prediction was not Australian court case x[2]\\r\\n        return 1\\r\\n      else:\\r\\n        return 0\\r\\n    \\r\\n    # Function to calculate True Negatives\\r\\n    def calculateTrueNegatives(x):\\r\\n      if (x[0] == 0 and x[2] == 0): # the article was not Australian court case (x[0]) and the prediction was not Australian court case x[2]\\r\\n        return 1\\r\\n      else:\\r\\n        return 0\\r\\n\\r\\n    # Out of total positive labels predicted, how many correctly classified as positive, that is PPV\\r\\n    def precision(x):\\r\\n      # Number of true positives/ (Number of true positives + Number of false positives) \\r\\n      # return truePositive/(truePositive + falsePositive)\\r\\n      return x[1][0]/(float(x[1][0] + x[1][1]))\\r\\n\\r\\n    # Out of actual positive labels, how many correctly classified as positive, that is, TPR\\r\\n    def recall(x):\\r\\n      # Number of true positives/ (Number of true positives + Number of false Negatives) \\r\\n      # return truePositive/(truePositive + falseNegative)\\r\\n      return x[1][0]/(float(x[1][0] +  x[1][2]))\\r\\n      \\r\\n      \\r\\n    # Calculate \\'True Positives\\', \\'False Positives\\' and \\'False Negatives\\'\\r\\n    calcTP_FP_FN = yLabelAndXFeaturesPrediction.map(lambda x: (1, np.array([calculateTruePositives(x), calculateFalsePositives(x), calculateFalseNegatives(x),calculateTrueNegatives(x)]))).reduceByKey(np.add)\\r\\n    \\r\\n    print(\\'\\')\\r\\n    print (\\'#\\'*20)\\r\\n    print(\\'Number of True Positives:\\', calcTP_FP_FN.collect()[0][1][0])\\r\\n    print(\\'Number of False Positives:\\', calcTP_FP_FN.collect()[0][1][1])\\r\\n    print(\\'Number of False Negatives:\\', calcTP_FP_FN.collect()[0][1][2])\\r\\n    print(\\'Number of True Negatives:\\', calcTP_FP_FN.collect()[0][1][3])\\r\\n    print(\\'\\')\\r\\n    \\r\\n    \\r\\n    # if \\'Number of True Positives: 0 and \\'Number of False Positives: 0, then F1 score is N/A\\r\\n    if calcTP_FP_FN.collect()[0][1][0] == 0  and calcTP_FP_FN.collect()[0][1][1] == 0:\\r\\n        calculateF1score = \\'N/A\\'\\r\\n        print(\\'F1 score for classifier =\\',\\'N/A\\')\\r\\n        print (\\'#\\'*20)\\r\\n        print(\\'\\')\\r\\n    else:    \\r\\n        calculateF1score = calcTP_FP_FN.map(lambda x: (precision(x), recall(x))).map(lambda x: 2*x[0]*x[1] / (x[0] + x[1])).collect()[0]\\r\\n        print(\\'F1 score for classifier =\\',round(calculateF1score*100,2),\\'%\\')\\r\\n        print (\\'#\\'*20)\\r\\n        print(\\'\\')\\r\\n    \\r\\n    # List to store the results of task 3\\r\\n    ansForTask3 = []\\r\\n    \\r\\n    if calculateF1score != \\'N/A\\':\\r\\n        ansForTask3.append((\\'F1 score for classifier =\\',round(calculateF1score*100,2),\\'%\\'))\\r\\n    else:\\r\\n        ansForTask3.append((\\'F1 score for classifier =\\',\\'N/A\\'))\\r\\n    ansForTask3.append(\\'\\')\\r\\n    ansForTask3.append((\\'Number of True Positives\\', calcTP_FP_FN.collect()[0][1][0]))\\r\\n    ansForTask3.append((\\'Number of False Positives\\', calcTP_FP_FN.collect()[0][1][1]))\\r\\n    ansForTask3.append((\\'Number of False Negatives\\', calcTP_FP_FN.collect()[0][1][2]))\\r\\n    ansForTask3.append((\\'Number of True Negatives\\', calcTP_FP_FN.collect()[0][1][3]))\\r\\n    \\r\\n    # Save the results of task3 in a text file\\r\\n    sc.parallelize(ansForTask3).coalesce(1, shuffle = False).saveAsTextFile(sys.argv[4]) \\r\\n    \\r\\n    sc.stop()',\n",
       " '#!/usr/bin/env python\\n# This code is part of the Biopython distribution and governed by its\\n# license.  Please see the LICENSE file that should have been included\\n# as part of this package.\\n#\\n# Bio.Wise contains modules for running and processing the output of\\n# some of the models in the Wise2 package by Ewan Birney available from:\\n# ftp://ftp.ebi.ac.uk/pub/software/unix/wise2/\\n# http://www.ebi.ac.uk/Wise2/\\n#\\n# Bio.Wise.psw is for protein Smith-Waterman alignments\\n# Bio.Wise.dnal is for Smith-Waterman DNA alignments\\n\\nfrom __future__ import print_function\\n\\nimport os\\nimport sys\\nimport tempfile\\n\\nfrom Bio import SeqIO\\n\\n\\ndef _build_align_cmdline(cmdline, pair, output_filename, kbyte=None, force_type=None, quiet=False):\\n    \"\"\"Helper function to build a command line string (PRIVATE).\\n\\n    >>> os.environ[\"WISE_KBYTE\"]=\"300000\"\\n    >>> if os.isatty(sys.stderr.fileno()):\\n    ...    c = _build_align_cmdline([\"dnal\"], (\"seq1.fna\", \"seq2.fna\"),\\n    ...                             \"/tmp/output\", kbyte=100000)\\n    ...    assert c == \\'dnal -kbyte 100000 seq1.fna seq2.fna > /tmp/output\\', c\\n    ...    c = _build_align_cmdline([\"psw\"], (\"seq1.faa\", \"seq2.faa\"),\\n    ...                             \"/tmp/output_aa\")\\n    ...    assert c == \\'psw -kbyte 300000 seq1.faa seq2.faa > /tmp/output_aa\\', c\\n    ... else:\\n    ...    c = _build_align_cmdline([\"dnal\"], (\"seq1.fna\", \"seq2.fna\"),\\n    ...                             \"/tmp/output\", kbyte=100000)\\n    ...    assert c == \\'dnal -kbyte 100000 -quiet seq1.fna seq2.fna > /tmp/output\\', c\\n    ...    c = _build_align_cmdline([\"psw\"], (\"seq1.faa\", \"seq2.faa\"),\\n    ...                             \"/tmp/output_aa\")\\n    ...    assert c == \\'psw -kbyte 300000 -quiet seq1.faa seq2.faa > /tmp/output_aa\\', c\\n\\n    \"\"\"\\n    cmdline = cmdline[:]\\n\\n    ### XXX: force_type ignored\\n\\n    if kbyte is None:\\n        try:\\n            cmdline.extend((\"-kbyte\", os.environ[\"WISE_KBYTE\"]))\\n        except KeyError:\\n            pass\\n    else:\\n        cmdline.extend((\"-kbyte\", str(kbyte)))\\n\\n    if not os.isatty(sys.stderr.fileno()):\\n        cmdline.append(\"-quiet\")\\n\\n    cmdline.extend(pair)\\n    cmdline.extend((\">\", output_filename))\\n    if quiet:\\n        cmdline.extend((\"2>\", \"/dev/null\"))\\n    cmdline_str = \\' \\'.join(cmdline)\\n\\n    return cmdline_str\\n\\n\\ndef align(cmdline, pair, kbyte=None, force_type=None, dry_run=False, quiet=False, debug=False):\\n    \"\"\"\\n    Returns a filehandle\\n    \"\"\"\\n    if not pair or len(pair) != 2:\\n        raise ValueError(\"Expected pair of filename, not %s\" % repr(pair))\\n\\n    output_file = tempfile.NamedTemporaryFile(mode=\\'r\\')\\n    input_files = tempfile.NamedTemporaryFile(mode=\"w\"), tempfile.NamedTemporaryFile(mode=\"w\")\\n\\n    if dry_run:\\n        print(_build_align_cmdline(cmdline,\\n                                   pair,\\n                                   output_file.name,\\n                                   kbyte,\\n                                   force_type,\\n                                   quiet))\\n        return\\n\\n    for filename, input_file in zip(pair, input_files):\\n        # Pipe the file through Biopython\\'s Fasta parser/writer\\n        # to make sure it conforms to the Fasta standard (in particular,\\n        # Wise2 may choke on long lines in the Fasta file)\\n        records = SeqIO.parse(open(filename), \\'fasta\\')\\n        SeqIO.write(records, input_file, \\'fasta\\')\\n        input_file.flush()\\n\\n    input_file_names = [input_file.name for input_file in input_files]\\n\\n    cmdline_str = _build_align_cmdline(cmdline,\\n                                       input_file_names,\\n                                       output_file.name,\\n                                       kbyte,\\n                                       force_type,\\n                                       quiet)\\n\\n    if debug:\\n        sys.stderr.write(\"%s\\\\n\" % cmdline_str)\\n\\n    status = os.system(cmdline_str) >> 8\\n\\n    if status > 1:\\n        if kbyte != 0: # possible memory problem; could be None\\n            sys.stderr.write(\"INFO trying again with the linear model\\\\n\")\\n            return align(cmdline, pair, 0, force_type, dry_run, quiet, debug)\\n        else:\\n            raise OSError(\"%s returned %s\" % (\" \".join(cmdline), status))\\n\\n    return output_file\\n\\n\\ndef all_pairs(singles):\\n    \"\"\"\\n    Generate pairs list for all-against-all alignments\\n\\n    >>> all_pairs(range(4))\\n    [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\\n    \"\"\"\\n    pairs = []\\n\\n    singles = list(singles)\\n    while singles:\\n        suitor = singles.pop(0) # if sorted, stay sorted\\n        pairs.extend((suitor, single) for single in singles)\\n\\n    return pairs\\n\\n\\ndef main():\\n    pass\\n\\n\\ndef _test(*args, **keywds):\\n    import doctest\\n    doctest.testmod(sys.modules[__name__], *args, **keywds)\\n\\nif __name__ == \"__main__\":\\n    if __debug__:\\n        _test()\\n    main()\\n',\n",
       " 'import os\\nfrom pathlib import Path\\n\\nfrom yaml import load, SafeLoader\\n\\n\\ndef _service_path(folder: str, filename: str) -> str:\\n    current_directory = os.path.dirname(os.path.realpath(__file__))\\n    return os.path.normpath(os.path.join(current_directory, \"..\", folder, filename))\\n\\n\\ndef load_config() -> dict:\\n    config_file_path = Path(__file__).parent.parent / \"config/config.yml\"\\n    with open(config_file_path, \"r\") as f:\\n        data = f.read()\\n        config_dict = load(data, SafeLoader)\\n        return config_dict\\n\\n\\nclass Config:\\n    conf = load_config()\\n    if conf is None:\\n        conf = dict()\\n\\n    # Server config\\n    BASE_URL = conf.get(\"BASE_URL\", \"http://cv.local\")\\n    HTTP_PORT = conf.get(\"HTTP_PORT\", 8099)\\n    STATIC_PATH = conf.get(\"STATIC_PATH\", Path(__file__).parent.parent / \"templates/static\")\\n    TEMPLATES_PATH = conf.get(\"STATIC_PATH\", Path(__file__).parent.parent / \"templates\")\\n    LOG_LEVEL = conf.get(\"LOG_LEVEL\", \"info\")\\n    LOG_FORMAT = conf.get(\"LOG_FORMAT\", \"color\")\\n    WEB_SECURE_COOKIES = conf.get(\"WEB_SECURE_COOKIES\", False)\\n\\n    # Database config\\n    DEFAULT_PG_URL = conf.get(\"PDB_URL\", \"postgresql://api:hackme@127.0.0.1:5488/app_sharer\")\\n    PG_POOL_MIN_SIZE = conf.get(\"PG_POOL_MIN_SIZE\", 10)\\n    PG_POOL_MAX_SIZE = conf.get(\"PG_POOL_MAX_SIZE\", 10)\\n\\n\\nconfig = Config()\\n',\n",
       " 'from datetime import datetime\\n\\nfrom flaskblog import db\\n\\n\\nclass User(db.Model):\\n    id = db.Column(db.Integer, primary_key=True)\\n    username = db.Column(db.String(20), unique=True, nullable=False)\\n    email = db.Column(db.String(120), unique=True, nullable=False)\\n    image_file = db.Column(db.String(20), nullable=False, default=\\'default.jpg\\')\\n    password = db.Column(db.String(60), nullable=False)\\n    posts = db.relationship(\\'Post\\', backref=\\'author\\', lazy=True)\\n\\n    def __repr__(self):\\n        return f\"User(\\'{self.username}\\', \\'{self.email}\\', \\'{self.image_file}\\')\"\\n\\n\\nclass Post(db.Model):\\n    id = db.Column(db.Integer, primary_key=True)\\n    title = db.Column(db.String(100), nullable=False)\\n    date_posted = db.Column(db.DateTime, nullable=False, default=datetime.utcnow)\\n    content = db.Column(db.Text, nullable=False)\\n    user_id = db.Column(db.Integer, db.ForeignKey(\\'user.id\\'), nullable=False)\\n\\n    def __repr__(self):\\n        return f\"User(\\'{self.title}\\', \\'{self.date_posted}\\')\"\\n',\n",
       " 'from nose.plugins.attrib import attr\\nfrom test.integration.base import DBTIntegrationTest\\n\\n\\nclass TestMacros(DBTIntegrationTest):\\n\\n    def setUp(self):\\n        DBTIntegrationTest.setUp(self)\\n        self.run_sql_file(\"test/integration/016_macro_tests/seed.sql\")\\n\\n    @property\\n    def schema(self):\\n        return \"test_macros_016\"\\n\\n    @property\\n    def models(self):\\n        return \"test/integration/016_macro_tests/models\"\\n\\n    @property\\n    def project_config(self):\\n        return {\\n            \"models\": {\\n                \"vars\": {\\n                    \"test\": \"DUMMY\"\\n                }\\n            },\\n            \"macro-paths\": [\"test/integration/016_macro_tests/macros\"],\\n            \"repositories\": [\\n                \\'https://github.com/fishtown-analytics/dbt-integration-project\\'\\n            ]\\n        }\\n\\n    @attr(type=\\'postgres\\')\\n    def test_working_macros(self):\\n        self.run_dbt([\"deps\"])\\n        self.run_dbt([\"run\"])\\n\\n        self.assertTablesEqual(\"expected_dep_macro\", \"dep_macro\")\\n        self.assertTablesEqual(\"expected_local_macro\", \"local_macro\")\\n\\n\\nclass TestInvalidMacros(DBTIntegrationTest):\\n\\n    def setUp(self):\\n        DBTIntegrationTest.setUp(self)\\n\\n    @property\\n    def schema(self):\\n        return \"test_macros_016\"\\n\\n    @property\\n    def models(self):\\n        return \"test/integration/016_macro_tests/models\"\\n\\n    @property\\n    def project_config(self):\\n        return {\\n            \"macro-paths\": [\"test/integration/016_macro_tests/bad-macros\"]\\n        }\\n\\n    @attr(type=\\'postgres\\')\\n    def test_invalid_macro(self):\\n\\n        try:\\n            self.run_dbt([\"run\"], expect_pass=False)\\n            self.assertTrue(False,\\n                            \\'compiling bad macro should raise a runtime error\\')\\n\\n        except RuntimeError:\\n            pass\\n\\n\\nclass TestMisusedMacros(DBTIntegrationTest):\\n\\n    def setUp(self):\\n        DBTIntegrationTest.setUp(self)\\n\\n    @property\\n    def schema(self):\\n        return \"test_macros_016\"\\n\\n    @property\\n    def models(self):\\n        return \"test/integration/016_macro_tests/bad-models\"\\n\\n    @property\\n    def project_config(self):\\n        return {\\n            \"macro-paths\": [\"test/integration/016_macro_tests/macros\"],\\n            \"repositories\": [\\n                \\'https://github.com/fishtown-analytics/dbt-integration-project\\'\\n            ]\\n        }\\n\\n    # TODO: compilation no longer exists, so while the model calling this macro\\n    # fails, it does not raise a runtime exception. change this test to verify\\n    # that the model finished with ERROR state.\\n    #\\n    # @attr(type=\\'postgres\\')\\n    # def test_working_macros(self):\\n    #     self.run_dbt([\"deps\"])\\n\\n    #     try:\\n    #         self.run_dbt([\"run\"])\\n    #         self.assertTrue(False, \\'invoked a package macro from global scope\\')\\n    #     except RuntimeError:\\n    #         pass\\n',\n",
       " \"import abc\\nfrom typing import Any, Generic, TypeVar\\nfrom types import SimpleNamespace\\n\\nfrom amino import List, Lists, Nil, Maybe\\nfrom amino.util.string import ToStr\\n\\nA = TypeVar('A')\\n\\n\\ndef is_algebra(bases: List[type]) -> bool:\\n    return bases.exists(lambda a: hasattr(a, '__algebra_base__'))\\n\\n\\ndef find_algebra(name: str, bases: List[type]) -> Maybe[type]:\\n    return bases.find(lambda a: hasattr(a, '__algebra_variants__'))\\n\\n\\ndef setup_algebra(name: str, inst: type, bases: List[type]) -> None:\\n    if is_algebra(bases):\\n        inst.__algebra_variants__ = List()\\n    else:\\n        raise Exception(f'algebra subclass has no algebra superclass: {name}')\\n\\n\\ndef setup_variant(name: str, inst: type, bases: List[type], algebra: type) -> None:\\n    inst.__algebra_index__ = len(algebra.__algebra_variants__)\\n    algebra.__algebra_variants__.append(inst)\\n\\n\\ndef setup_algebraic_type(name: str, inst: type, bases: List[type]) -> None:\\n    return (\\n        find_algebra(name, bases)\\n        .cata_f(\\n            lambda a: setup_variant(name, inst, bases, a),\\n            lambda: setup_algebra(name, inst, bases)\\n        )\\n    )\\n\\n\\nclass AlgebraMeta(abc.ABCMeta):\\n\\n    def __new__(\\n            cls,\\n            name: str,\\n            bases: list,\\n            namespace: SimpleNamespace,\\n            algebra_base: bool=False,\\n            **kw: Any,\\n    ) -> None:\\n        inst = super().__new__(cls, name, bases, namespace, **kw)\\n        if not hasattr(inst, '__args__') or inst.__args__ is None:\\n            if algebra_base:\\n                inst.__algebra_base__ = None\\n            else:\\n                setup_algebraic_type(name, inst, Lists.wrap(bases))\\n        return inst\\n\\n\\nclass Algebra(Generic[A], ToStr, metaclass=AlgebraMeta, algebra_base=True):\\n    pass\\n\\n\\n__all__ = ('AlgebraMeta', 'Algebra')\\n\",\n",
       " 'i = 0  # This is an example of the while loop\\nwhile i < 5:\\n    print(i)\\n    i += 1\\ni = None  # 0\\n            # 1\\n            # 2\\n            # 3\\n            # 4\\n\\n\\n\\nfor i in range(5):  # This example uses the iterable object \\'range\\'\\n    print(i)  # 0\\n                # 1\\n                # 2\\n                # 3\\n                # 4\\n\\n\\n\\nfor i in [1, 2, 3, 4]:\\n    print(i)  # 1\\n                # 2\\n                # 3\\n                # 4\\n\\n\\n\\nfor c in \\'hello\\':\\n    print(c)  # h\\n                # e\\n                # l\\n                # l\\n                # o\\n\\n\\n\\nfor x in (\\'a\\',\"b\", \\'c\\', 4):\\n    print(x)  # a\\n                # b\\n                # c\\n                # 4\\n\\nfor x in [(1, 2), (3, 4), (5, 6)]:\\n    print(x)  # (1, 2)\\n            # (3, 4)\\n            # (5, 6)\\n\\n\\n\\nfor i in range(5):\\n    if i==3:\\n        continue\\n    print(i)  # 0\\n                # 1\\n                # 2\\n                # 4\\n\\n\\n\\n\\n\\n\\n\\nfor i in range(5):\\n    if i==3:\\n        break\\n    print(i)  # 0\\n                # 1\\n                # 2\\n\\n\\n\\nfor i in range(1, 5):\\n    print(i)\\n    if i % 7 == 0:\\n        print(\\'multiple of 7 found\\')\\n        break\\nelse:\\n    print(\\'no multiple of 7 found\\')  # 1\\n                                    # 2\\n                                    # 3\\n                                    # 4\\n                                    # no multiple of 7 found\\n\\n\\n\\nfor i in range(1, 8):\\n    print(i)\\n    if i % 7 == 0:\\n        print(\\'multiple of 7 found\\')\\n        break\\nelse:\\n    print(\\'no multiple of 7 found\\')  # 1\\n                                    # 2\\n                                    # 3\\n                                    # 4\\n                                    # 5\\n                                    # 6\\n                                    # 7\\n                                    # multiple of 7 found\\n\\n\\n\\nfor i in range(6):\\n    print(\\'------------------\\')\\n    \\n',\n",
       " '# -*- coding: utf-8 -*-\\n\"\"\"\\nTencent is pleased to support the open source community by making 蓝鲸智云PaaS平台社区版 (BlueKing PaaS Community\\nEdition) available.\\nCopyright (C) 2017-2021 THL A29 Limited, a Tencent company. All rights reserved.\\nLicensed under the MIT License (the \"License\"); you may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://opensource.org/licenses/MIT\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\"\"\"\\nimport os\\nimport uuid\\nfrom unittest import mock\\n\\nimport pytest\\nfrom kubernetes import client\\n\\nfrom backend.tests.conftest import TESTING_API_SERVER_URL\\nfrom backend.tests.testing_utils.mocks.collection import StubComponentCollection\\n\\n\\nclass FakeBcsKubeConfigurationService:\\n    \"\"\"Fake configuration service which return local apiserver as config\"\"\"\\n\\n    def __init__(self, *args, **kwargs):\\n        pass\\n\\n    def make_configuration(self):\\n        configuration = client.Configuration()\\n        configuration.api_key = {\"authorization\": f\\'Bearer {os.environ.get(\"TESTING_SERVER_API_KEY\")}\\'}\\n        configuration.verify_ssl = False\\n        configuration.host = TESTING_API_SERVER_URL\\n        return configuration\\n\\n\\n@pytest.fixture(autouse=True)\\ndef setup_fake_cluster_dependencies():\\n    # 替换所有 Comp 系统为测试专用的 Stub 系统；替换集群地址为测试用 API Server\\n    with mock.patch(\\n        \\'backend.container_service.core.ctx_models.ComponentCollection\\', new=StubComponentCollection\\n    ), mock.patch(\\n        \\'backend.resources.utils.kube_client.BcsKubeConfigurationService\\', new=FakeBcsKubeConfigurationService\\n    ):\\n        yield\\n',\n",
       " '#\\r\\n# This file is part of pyasn1-modules software.\\r\\n#\\r\\n# Created by Russ Housley.\\r\\n#\\r\\n# Copyright (c) 2019, Vigil Security, LLC\\r\\n# License: http://snmplabs.com/pyasn1/license.html\\r\\n#\\r\\n# Diffie-Hellman Proof-of-Possession Algorithms\\r\\n#\\r\\n# ASN.1 source from:\\r\\n# https://www.rfc-editor.org/rfc/rfc6955.txt\\r\\n#\\r\\n\\r\\nfrom pyasn1.type import namedtype\\r\\nfrom pyasn1.type import univ\\r\\n\\r\\nfrom pyasn1_modules import rfc3279\\r\\nfrom pyasn1_modules import rfc5280\\r\\nfrom pyasn1_modules import rfc5652\\r\\n\\r\\n\\r\\n# Imports from RFC 5652\\r\\n\\r\\nMessageDigest = rfc5652.MessageDigest\\r\\n\\r\\nIssuerAndSerialNumber = rfc5652.IssuerAndSerialNumber\\r\\n\\r\\n\\r\\n# Imports from RFC 5280\\r\\n\\r\\nid_pkix = rfc5280.id_pkix\\r\\n\\r\\n\\r\\n# Imports from RFC 3279\\r\\n\\r\\nDss_Sig_Value = rfc3279.Dss_Sig_Value\\r\\n\\r\\nDomainParameters = rfc3279.DomainParameters\\r\\n\\r\\n\\r\\n# Static DH Proof-of-Possession\\r\\n\\r\\nclass DhSigStatic(univ.Sequence):\\r\\n    componentType = namedtype.NamedTypes(\\r\\n        namedtype.OptionalNamedType(\\'issuerAndSerial\\', IssuerAndSerialNumber()),\\r\\n        namedtype.NamedType(\\'hashValue\\', MessageDigest())\\r\\n    )\\r\\n\\r\\n\\r\\n# Object Identifiers\\r\\n\\r\\nid_dh_sig_hmac_sha1 = id_pkix + (6, 3, )\\r\\n\\r\\nid_dhPop_static_sha1_hmac_sha1 = univ.ObjectIdentifier(id_dh_sig_hmac_sha1)\\r\\n\\r\\n\\r\\nid_alg_dh_pop = id_pkix + (6, 4, )\\r\\n\\r\\nid_alg_dhPop_sha1 = univ.ObjectIdentifier(id_alg_dh_pop)\\r\\n\\r\\nid_alg_dhPop_sha224 = id_pkix + (6, 5, )\\r\\n\\r\\nid_alg_dhPop_sha256 = id_pkix + (6, 6, )\\r\\n\\r\\nid_alg_dhPop_sha384 = id_pkix + (6, 7, )\\r\\n\\r\\nid_alg_dhPop_sha512 = id_pkix + (6, 8, )\\r\\n\\r\\n\\r\\nid_alg_dhPop_static_sha224_hmac_sha224 = id_pkix + (6, 15, )\\r\\n\\r\\nid_alg_dhPop_static_sha256_hmac_sha256 = id_pkix + (6, 16, )\\r\\n\\r\\nid_alg_dhPop_static_sha384_hmac_sha384 = id_pkix + (6, 17, )\\r\\n\\r\\nid_alg_dhPop_static_sha512_hmac_sha512 = id_pkix + (6, 18, )\\r\\n\\r\\n\\r\\nid_alg_ecdhPop_static_sha224_hmac_sha224 = id_pkix + (6, 25, )\\r\\n\\r\\nid_alg_ecdhPop_static_sha256_hmac_sha256 = id_pkix + (6, 26, )\\r\\n\\r\\nid_alg_ecdhPop_static_sha384_hmac_sha384 = id_pkix + (6, 27, )\\r\\n\\r\\nid_alg_ecdhPop_static_sha512_hmac_sha512 = id_pkix + (6, 28, )\\r\\n\\r\\n\\r\\n# Update the Algorithm Identifier map in rfc5280.py\\r\\n\\r\\n_algorithmIdentifierMapUpdate = {\\r\\n    id_alg_dh_pop: DomainParameters(),\\r\\n    id_alg_dhPop_sha224: DomainParameters(),\\r\\n    id_alg_dhPop_sha256: DomainParameters(),\\r\\n    id_alg_dhPop_sha384: DomainParameters(),\\r\\n    id_alg_dhPop_sha512: DomainParameters(),\\r\\n    id_dh_sig_hmac_sha1: univ.Null(\"\"),\\r\\n    id_alg_dhPop_static_sha224_hmac_sha224: univ.Null(\"\"),\\r\\n    id_alg_dhPop_static_sha256_hmac_sha256: univ.Null(\"\"),\\r\\n    id_alg_dhPop_static_sha384_hmac_sha384: univ.Null(\"\"),\\r\\n    id_alg_dhPop_static_sha512_hmac_sha512: univ.Null(\"\"),\\r\\n    id_alg_ecdhPop_static_sha224_hmac_sha224: univ.Null(\"\"),\\r\\n    id_alg_ecdhPop_static_sha256_hmac_sha256: univ.Null(\"\"),\\r\\n    id_alg_ecdhPop_static_sha384_hmac_sha384: univ.Null(\"\"),\\r\\n    id_alg_ecdhPop_static_sha512_hmac_sha512: univ.Null(\"\"),\\r\\n}\\r\\n\\r\\nrfc5280.algorithmIdentifierMap.update(_algorithmIdentifierMapUpdate)\\r\\n']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_python['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content', 'lang', 'size', 'ext', 'max_stars_count', 'avg_line_length', 'max_line_length', 'alphanum_fraction', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 20/20 [06:20<00:00, 19.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 380.7458, 'train_samples_per_second': 0.21, 'train_steps_per_second': 0.053, 'train_loss': 1.6358243942260742, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=1.6358243942260742, metrics={'train_runtime': 380.7458, 'train_samples_per_second': 0.21, 'train_steps_per_second': 0.053, 'total_flos': 41806725120000.0, 'train_loss': 1.6358243942260742, 'epoch': 1.0})"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:13<00:00,  4.45s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.172829270362854,\n",
       " 'eval_runtime': 21.6043,\n",
       " 'eval_samples_per_second': 0.926,\n",
       " 'eval_steps_per_second': 0.139,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text using the trained model\n",
    "def generate_code(prompt, model, tokenizer, max_length=10240):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, top_p=0.95, temperature=0.7)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import\n",
      "\n",
      "\n",
      "from django.db import models\n",
      "\n",
      "\n",
      "\n",
      "from flask import Flask\n",
      "\n",
      " from djang.py import import logging\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of generating code\n",
    "prompt = \"def add_numbers(a, b, c):\"\n",
    "prompt2 = '''\n",
    "import\n",
    "\n",
    "\n",
    "'''\n",
    "generated_code = generate_code(prompt2, model, tokenizer)\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate code on evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sqlite3\n",
      "\n",
      "\n",
      "def init():\n",
      "    global database_file\n",
      "    global db\n",
      "    global cursor\n",
      "\n",
      "    database_file = \"db/database.db\"\n",
      "    db = sqlite3.connect(database_file)\n",
      "    cursor = db.cursor()\n",
      "\n",
      "    # Nomes das colunas do database\n",
      "    # sql = \"select * from database where 1=0;\"\n",
      "    # cursor.execute(sql)\n",
      "    # p = [d[0] for d in cursor.description]\n",
      "    # print(p)\n",
      "\n",
      "    # def query(command, arguments=[]):\n",
      "    #     _db = sqlite3.connect(database_file)\n",
      "    #     _c = _db.cursor()\n",
      "    #     _c.execute(command, arguments)\n",
      "    #     results = _c.fetchall()\n",
      "    #     return results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset_python[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sqlite3\n",
      "\n",
      "\n",
      "def init():\n",
      "    global database_file\n",
      "    global db\n",
      "    global cursor\n",
      "\n",
      "    database_file = \"db/database.db\"\n",
      "    db = sqlite3.connect(database_file)\n",
      "    cursor = db.cursor()\n",
      "\n",
      "    # Nomes das colunas do database\n",
      "    # sql = \"select * from database where 1=0;\"\n",
      "    # cursor.execute(sql)\n",
      "    # p = [d[0] for d in cursor.description]\n",
      "    # print(p)\n",
      "\n",
      "    # def query(command, arguments=[]):\n",
      "    #     _db = sqlite3.connect(database_file)\n",
      "    #     _c = _db.cursor()\n",
      "    #     _c.execute(command, arguments)\n",
      "    #     results = _c.fetchall()\n",
      "    #     return results\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sqlite3\n",
      "\n",
      "\n",
      "def init():\n",
      "    global database_file\n",
      "    global db\n",
      "    global cursor\n",
      "\n",
      "    database_file = \"db/database.db\"\n",
      "    db = sqlite3.connect(database_file)\n",
      "    cursor = db.cursor()\n",
      "\n",
      "    # Nomes das colunas do database\n",
      "    # sql = \"select * from database where 1=0;\"\n",
      "    # cursor.execute(sql)\n",
      "    # p = [d[0] for d in cursor.description]\n",
      "    # print(p)\n",
      "\n",
      "    # def query(command, arguments=[]):\n",
      "    #     _db = sqlite3.connect(database_file)\n",
      "    #     _c = _db.cursor()\n",
      "    #     _c.execute(command, arguments)\n",
      "    #     results = _c.fetchall()\n",
      "    #     return results\n",
      "\n",
      "-------------------------------\n",
      "# This file is a part of Arjuna\n",
      "# Copyright 2015-2021 Rahul Verma\n",
      "\n",
      "# Website: www.RahulVerma.net\n",
      "\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "\n",
      "#   http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import os\n",
      "import uuid\n",
      "import time\n",
      "import logging\n",
      "import shutil\n",
      "from arjuna import ArjunaOption\n",
      "\n",
      "\n",
      "from arjuna.tpi.config import Configuration\n",
      "\n",
      "from arjuna.configure.configurator import TestConfigurator\n",
      "from arjuna.drive.invoker.databroker import TestSessionDataBrokerHandler\n",
      "from arjuna.interact.gui.gom.guimgr import GuiManager\n",
      "\n",
      "class TestSessionController:\n",
      "    \n",
      "    def __init__(self):\n",
      "        self.__id = uuid.uuid4()\n",
      "        self.__DEF_CONF_NAME = \"ref\"\n",
      "        self.__default_ref_config = None\n",
      "        self.__config_map = {}\n",
      "        self.__cli_central_config = None\n",
      "        self.__cli_test_config = None\n",
      "        self.__configurator = None\n",
      "        self.__project_config_loaded = False\n",
      "        self.__guimgr = None\n",
      "\n",
      "    @property\n",
      "    def id(self):\n",
      "        return self.__id\n",
      "\n",
      "    @property\n",
      "    def configurator(self):\n",
      "        return self.__configurator\n",
      "\n",
      "    @property\n",
      "    def data_broker(self):\n",
      "        return self.__data_broker   \n",
      "\n",
      "    @property\n",
      "    def gui_manager(self):\n",
      "        return self.__guimgr\n",
      "\n",
      "    def init(self, root_dir, cli_config=None, run_id=None, linked_projects=[]):\n",
      "        self.__configurator = TestConfigurator(root_dir, cli_config, run_id, linked_projects)\n",
      "        ref_config = self.__configurator.ref_config\n",
      "        data_env_confs = self.__configurator.file_confs\n",
      "        self.__guimgr = GuiManager(ref_config)\n",
      "        ref_conf = self._create_config(ref_config)\n",
      "        self.__add_to_map(ref_conf)\n",
      "        for run_env_conf in [self._create_config(econf, name=name) for name, econf in data_env_confs.items()]:\n",
      "            self.__add_to_map(run_env_conf)\n",
      "\n",
      "        return ref_conf\n",
      "\n",
      "    def __msession_config(self, ref_conf_name):\n",
      "        from arjuna import Arjuna\n",
      "        if ref_conf_name is None:\n",
      "            ref_conf_name = \"ref\"\n",
      "        return Arjuna.get_config(ref_conf_name)\n",
      "\n",
      "    def _create_config(self, config, name=None):\n",
      "        config = Configuration(\n",
      "            self,\n",
      "            name and name or self.__DEF_CONF_NAME,\n",
      "            config\n",
      "        )\n",
      "        return config\n",
      "\n",
      "    def finish(self):\n",
      "        pass\n",
      "\n",
      "    def __add_to_map(self, config):\n",
      "        from arjuna import Arjuna\n",
      "        Arjuna.register_config(config)\n",
      "\n",
      "    def load_options_from_file(self, fpath, *, conf_stage):\n",
      "        return self.configurator.load_options_from_file(fpath, conf_stage=conf_stage)\n",
      "\n",
      "    def register_config(self, name, arjuna_options, user_options, *, conf_stage, parent_config=None):\n",
      "        config = self.configurator.register_new_config(arjuna_options, user_options, parent_config=parent_config, conf_stage=conf_stage)\n",
      "        conf = self._create_config(config, name=name)\n",
      "        self.__add_to_map(conf)\n",
      "        return conf\n",
      "\n",
      "    def create_file_data_source(self, record_type, file_name, *arg_pairs):\n",
      "        raise NotImplementedError()\n",
      "        # response = self._send_request(\n",
      "        #     ArjunaComponent.DATA_SOURCE,\n",
      "        #     DataSourceActionType.CREATE_FILE_DATA_SOURCE,\n",
      "        #     *arg_pairs\n",
      "        # )\n",
      "        # return response.get_data_source_id()\n",
      "\n",
      "    def define_gui(self, automator, label=None, name=None, qual_name=None, def_file_path=None):\n",
      "        return self.gui_manager.define_gui(automator, label=label, name=name, qual_name=qual_name, def_file_path=def_file_path)\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m input_text \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_text)\n\u001b[1;32m----> 5\u001b[0m generated_code \u001b[38;5;241m=\u001b[39m generate_code(input_text, model, tokenizer)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_code)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[134], line 4\u001b[0m, in \u001b[0;36mgenerate_code\u001b[1;34m(prompt, model, tokenizer, max_length)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_code\u001b[39m(prompt, model, tokenizer, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10240\u001b[39m):\n\u001b[0;32m      3\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(inputs, max_length\u001b[38;5;241m=\u001b[39mmax_length, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, no_repeat_ngram_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\transformers\\generation\\utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2212\u001b[0m     )\n\u001b[0;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2216\u001b[0m         input_ids,\n\u001b[0;32m   2217\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2218\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2219\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2220\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2221\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2222\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2223\u001b[0m     )\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2235\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\transformers\\generation\\utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3210\u001b[0m     outputs,\n\u001b[0;32m   3211\u001b[0m     model_kwargs,\n\u001b[0;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3213\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1271\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1271\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m   1272\u001b[0m     input_ids,\n\u001b[0;32m   1273\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1274\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1275\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1276\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1277\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1278\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1279\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1280\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   1281\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1282\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1283\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1284\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1285\u001b[0m )\n\u001b[0;32m   1286\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1031\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1030\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwte(input_ids)\n\u001b[1;32m-> 1031\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe(position_ids)\n\u001b[0;32m   1032\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# Attention mask.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx,\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq,\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[0;32m    198\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for example in eval_dataset_python:\n",
    "    input_text = example['content']\n",
    "    print(input_text)\n",
    "    generated_code = generate_code(input_text, model, tokenizer)\n",
    "    print(generated_code)\n",
    "    print(\"-------------------------------\")\n",
    "    predictions.append(generated_code)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate predictions on the evaluation dataset\n",
    "# def generate_predictions(dataset, model, tokenizer):\n",
    "#     predictions = []\n",
    "#     for example in dataset:\n",
    "#         input_text = example['content']\n",
    "#         generated_code = generate_code(input_text, model, tokenizer)\n",
    "#         print(generated_code)\n",
    "#         predictions.append(generated_code)\n",
    "#     return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:  # -*- coding: utf-8 -*-\n",
      "# Generated by Django 1.9.1 on 2016-11-14 19:51\n",
      "from __future__ import unicode_literals\n",
      "\n",
      "from django.db import migrations, models\n",
      "\n",
      "\n",
      "class Migration(migrations.Migration):\n",
      "\n",
      "    dependencies = [\n",
      "        ('annotation', '0031_auto_20161111_1943'),\n",
      "    ]\n",
      "\n",
      "    operations = [\n",
      "        migrations.AddField(\n",
      "            model_name='masterobservation',\n",
      "            name='observation_time',\n",
      "            field=models.PositiveIntegerField(blank=True, null=True),\n",
      "        ),\n",
      "        migrations.AddField(\n",
      "            model_name='observation',\n",
      "            name='observation_time',\n",
      "            field=models.PositiveIntegerField(blank=True, null=True),\n",
      "        ),\n",
      "    ]\n",
      "\n",
      "Generating code...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -*- coding: utf-8 -*-\n",
      "# Generated by Django 1.9.1 on 2016-11-14 19:51\n",
      "from __future__ import unicode_literals\n",
      "\n",
      "from django.db import migrations, models\n",
      "\n",
      "\n",
      "class Migration(migrations.Migration):\n",
      "\n",
      "    dependencies = [\n",
      "        ('annotation', '0031_auto_20161111_1943'),\n",
      "    ]\n",
      "\n",
      "    operations = [\n",
      "        migrations.AddField(\n",
      "            model_name='masterobservation',\n",
      "            name='observation_time',\n",
      "            field=models.PositiveIntegerField(blank=True, null=True),\n",
      "        ),\n",
      "        migrations.AddField(\n",
      "            model_name='observation',\n",
      "            name='observation_time',\n",
      "            field=models.PositiveIntegerField(blank=True, null=True),\n",
      "        ),\n",
      "    ]\n",
      "\n",
      "Input text:  from collections import defaultdict\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from hyperopt_synthetic import run_one_exp as hyperopt_synthetic_opt\n",
      "from xbbo_synthetic import run_one_exp as xbbo_synthetic_opt\n",
      "\n",
      "max_call = 50\n",
      "if __name__ == \"__main__\":\n",
      "    rng = np.random.RandomState(42)\n",
      "    result_opts = defaultdict(list)\n",
      "    for i in range(3):\n",
      "        seed = rng.randint(1e5)\n",
      "        # result_opts['hyperopt-rand'].append(hyperopt_synthetic_opt('rand', max_call,seed))\n",
      "        result_opts['hyperopt-tpe'].append(hyperopt_synthetic_opt('tpe', max_call,seed))\n",
      "        # result_opts['hyperopt-atpe'].append(hyperopt_synthetic_opt('atpe', max_call,seed))\n",
      "        # result_opts['hyperopt-mix'].append(hyperopt_synthetic_opt('mix', max_call,seed))\n",
      "        result_opts['hyperopt-anneal'].append(hyperopt_synthetic_opt('anneal', max_call,seed))\n",
      "        result_opts['XBBO-tpe'].append(xbbo_synthetic_opt('tpe', max_call,seed))\n",
      "        result_opts['XBBO-anneal'].append(xbbo_synthetic_opt('anneal',max_call,seed))\n",
      "    plt.figure()\n",
      "    for key in result_opts:\n",
      "        plt.plot(range(1,max_call+1), np.mean(np.minimum.accumulate(np.asarray(result_opts[key]), axis=1),axis=0)[:], label=key)\n",
      "    plt.ylim([-0.1,1000])\n",
      "    plt.xlabel('# of Evaluate')\n",
      "    plt.ylabel('OBJ')\n",
      "    plt.title('Average of cumulate best on 3 seeds')\n",
      "    plt.legend()\n",
      "    plt.savefig('./out/comp_with_hyperopt.png')\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "Generating code...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from collections import defaultdict\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from hyperopt_synthetic import run_one_exp as hyperopt_synthetic_opt\n",
      "from xbbo_synthetic import run_one_exp as xbbo_synthetic_opt\n",
      "\n",
      "max_call = 50\n",
      "if __name__ == \"__main__\":\n",
      "    rng = np.random.RandomState(42)\n",
      "    result_opts = defaultdict(list)\n",
      "    for i in range(3):\n",
      "        seed = rng.randint(1e5)\n",
      "        # result_opts['hyperopt-rand'].append(hyperopt_synthetic_opt('rand', max_call,seed))\n",
      "        result_opts['hyperopt-tpe'].append(hyperopt_synthetic_opt('tpe', max_call,seed))\n",
      "        # result_opts['hyperopt-atpe'].append(hyperopt_synthetic_opt('atpe', max_call,seed))\n",
      "        # result_opts['hyperopt-mix'].append(hyperopt_synthetic_opt('mix', max_call,seed))\n",
      "        result_opts['hyperopt-anneal'].append(hyperopt_synthetic_opt('anneal', max_call,seed))\n",
      "        result_opts['XBBO-tpe'].append(xbbo_synthetic_opt('tpe', max_call,seed))\n",
      "        result_opts['XBBO-anneal'].append(xbbo_synthetic_opt('anneal',max_call,seed))\n",
      "    plt.figure()\n",
      "    for key in result_opts:\n",
      "        plt.plot(range(1,max_call+1), np.mean(np.minimum.accumulate(np.asarray(result_opts[key]), axis=1),axis=0)[:], label=key)\n",
      "    plt.ylim([-0.1,1000])\n",
      "    plt.xlabel('# of Evaluate')\n",
      "    plt.ylabel('OBJ')\n",
      "    plt.title('Average of cumulate best on 3 seeds')\n",
      "    plt.legend()\n",
      "    plt.savefig('./out/comp_with_hyperopt.png')\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "\n",
      "Input text:  from django.conf import settings\n",
      "from django.contrib.auth.models import Group\n",
      "from django.contrib.sites.models import Site\n",
      "from django.db import models\n",
      "from django.template import Context, Template\n",
      "from django.utils.translation import ugettext_lazy as _\n",
      "\n",
      "from rdmo.conditions.models import Condition\n",
      "from rdmo.core.models import TranslationMixin\n",
      "from rdmo.core.utils import copy_model, get_pandoc_version, join_url\n",
      "from rdmo.questions.models import Catalog\n",
      "\n",
      "from .managers import ViewManager\n",
      "from .utils import ProjectWrapper\n",
      "\n",
      "\n",
      "class View(models.Model, TranslationMixin):\n",
      "\n",
      "    objects = ViewManager()\n",
      "\n",
      "    uri = models.URLField(\n",
      "        max_length=640, blank=True,\n",
      "        verbose_name=_('URI'),\n",
      "        help_text=_('The Uniform Resource Identifier of this view (auto-generated).')\n",
      "    )\n",
      "    uri_prefix = models.URLField(\n",
      "        max_length=256,\n",
      "        verbose_name=_('URI Prefix'),\n",
      "        help_text=_('The prefix for the URI of this view.')\n",
      "    )\n",
      "    key = models.SlugField(\n",
      "        max_length=128, blank=True,\n",
      "        verbose_name=_('Key'),\n",
      "        help_text=_('The internal identifier of this view.')\n",
      "    )\n",
      "    comment = models.TextField(\n",
      "        blank=True,\n",
      "        verbose_name=_('Comment'),\n",
      "        help_text=_('Additional internal information about this view.')\n",
      "    )\n",
      "    locked = models.BooleanField(\n",
      "        default=False,\n",
      "        verbose_name=_('Locked'),\n",
      "        help_text=_('Designates whether this view can be changed.')\n",
      "    )\n",
      "    catalogs = models.ManyToManyField(\n",
      "        Catalog, blank=True,\n",
      "        verbose_name=_('Catalogs'),\n",
      "        help_text=_('The catalogs this view can be used with. An empty list implies that this view can be used with every catalog.')\n",
      "    )\n",
      "    sites = models.ManyToManyField(\n",
      "        Site, blank=True,\n",
      "        verbose_name=_('Sites'),\n",
      "        help_text=_('The sites this view belongs to (in a multi site setup).')\n",
      "    )\n",
      "    groups = models.ManyToManyField(\n",
      "        Group, blank=True,\n",
      "        verbose_name=_('Group'),\n",
      "        help_text=_('The groups for which this view is active.')\n",
      "    )\n",
      "    template = models.TextField(\n",
      "        blank=True,\n",
      "        verbose_name=_('Template'),\n",
      "        help_text=_('The template for this view, written in Django template language.')\n",
      "    )\n",
      "    title_lang1 = models.CharField(\n",
      "        max_length=256, blank=True,\n",
      "        verbose_name=_('Title (primary)'),\n",
      "        help_text=_('The title for this view in the primary language.')\n",
      "    )\n",
      "    title_lang2 = models.CharField(\n",
      "        max_length=256, blank=True,\n",
      "        verbose_name=_('Title (secondary)'),\n",
      "        help_text=_('The title for this view in the secondary language.')\n",
      "    )\n",
      "    title_lang3 = models.CharField(\n",
      "        max_length=256, blank=True,\n",
      "        verbose_name=_('Title (tertiary)'),\n",
      "        help_text=_('The title for this view in the tertiary language.')\n",
      "    )\n",
      "    title_lang4 = models.CharField(\n",
      "        max_length=256, blank=True,\n",
      "        verbose_name=_('Title (quaternary)'),\n",
      "        help_text=_('The title for this view in the quaternary language.')\n",
      "    )\n",
      "    title_lang5 = models.CharField(\n",
      "        max_length=256, blank=True,\n",
      "        verbose_name=_('Title (quinary)'),\n",
      "        help_text=_('The title for this view in the quinary language.')\n",
      "    )\n",
      "    help_lang1 = models.TextField(\n",
      "        blank=True,\n",
      "        verbose_name=_('Help (primary)'),\n",
      "        help_text=_('The help text for this view in the primary language.')\n",
      "    )\n",
      "    help_lang2 = models.TextField(\n",
      "        blank=True,\n",
      "        verbose_name=_('Help (secondary)'),\n",
      "        help_text=_('The help text for this view in the secondary language.')\n",
      "    )\n",
      "    help_lang3 = models.TextField(\n",
      "        blank=True,\n",
      "        verbose_name=_('Help (tertiary)'),\n",
      "        help_text=_('The help text for this view in the tertiary language.')\n",
      "    )\n",
      "    help_lang4 = models.TextField(\n",
      "        blank=True,\n",
      "        verbose_name=_('Help (quaternary)'),\n",
      "        help_text=_('The help text for this view in the quaternary language.')\n",
      "    )\n",
      "    help_lang5 = models.TextField(\n",
      "        blank=True,\n",
      "        verbose_name=_('Help (quinary)'),\n",
      "        help_text=_('The help text for this view in the quinary language.')\n",
      "    )\n",
      "    available = models.BooleanField(\n",
      "        default=True,\n",
      "        verbose_name=_('Available'),\n",
      "        help_text=_('Designates whether this view is generally available for projects.')\n",
      "    )\n",
      "\n",
      "    class Meta:\n",
      "        ordering = ('key', )\n",
      "        verbose_name = _('View')\n",
      "        verbose_name_plural = _('Views')\n",
      "\n",
      "    def __str__(self):\n",
      "        return self.key\n",
      "\n",
      "    def save(self, *args, **kwargs):\n",
      "        self.uri = self.build_uri(self.uri_prefix, self.key)\n",
      "        super().save(*args, **kwargs)\n",
      "\n",
      "    def copy(self, uri_prefix, key):\n",
      "        view = copy_model(self, uri_prefix=uri_prefix, key=key)\n",
      "\n",
      "        # copy m2m fields\n",
      "        view.catalogs.set(self.catalogs.all())\n",
      "        view.sites.set(self.sites.all())\n",
      "        view.groups.set(self.groups.all())\n",
      "\n",
      "        return view\n",
      "\n",
      "    @property\n",
      "    def title(self):\n",
      "        return self.trans('title')\n",
      "\n",
      "    @property\n",
      "    def help(self):\n",
      "        return self.trans('help')\n",
      "\n",
      "    @property\n",
      "    def is_locked(self):\n",
      "        return self.locked\n",
      "\n",
      "    def render(self, project, snapshot=None, export_format=None):\n",
      "        # render the template to a html string\n",
      "        # it is important not to use models here\n",
      "\n",
      "        return Template(self.template).render(Context({\n",
      "            'project': ProjectWrapper(project, snapshot),\n",
      "            'conditions': {\n",
      "                condition.key: condition.resolve(project, snapshot)\n",
      "                for condition in Condition.objects.select_related('source')\n",
      "            },\n",
      "            'format': export_format,\n",
      "            'pandoc_version': get_pandoc_version()\n",
      "        }))\n",
      "\n",
      "    @classmethod\n",
      "    def build_uri(cls, uri_prefix, key):\n",
      "        assert key\n",
      "        return join_url(uri_prefix or settings.DEFAULT_URI_PREFIX, '/views/', key)\n",
      "\n",
      "Generating code...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generated_codes \u001b[38;5;241m=\u001b[39m generate_predictions(eval_dataset_python, model, tokenizer)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_codes[:\u001b[38;5;241m5\u001b[39m])\n",
      "Cell \u001b[1;32mIn[84], line 9\u001b[0m, in \u001b[0;36mgenerate_predictions\u001b[1;34m(dataset, model, tokenizer)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput text: \u001b[39m\u001b[38;5;124m\"\u001b[39m, input_text)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating code...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m generated_code \u001b[38;5;241m=\u001b[39m generate_code(input_text, model, tokenizer)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_code)\n\u001b[0;32m     11\u001b[0m predictions\u001b[38;5;241m.\u001b[39mappend(generated_code)\n",
      "Cell \u001b[1;32mIn[79], line 4\u001b[0m, in \u001b[0;36mgenerate_code\u001b[1;34m(prompt, model, tokenizer, max_length)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_code\u001b[39m(prompt, model, tokenizer, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10240\u001b[39m):\n\u001b[0;32m      3\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(inputs, max_length\u001b[38;5;241m=\u001b[39mmax_length, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, no_repeat_ngram_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\transformers\\generation\\utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2212\u001b[0m     )\n\u001b[0;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2216\u001b[0m         input_ids,\n\u001b[0;32m   2217\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2218\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2219\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2220\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2221\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2222\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2223\u001b[0m     )\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2235\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\transformers\\generation\\utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3210\u001b[0m     outputs,\n\u001b[0;32m   3211\u001b[0m     model_kwargs,\n\u001b[0;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3213\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1271\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1271\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m   1272\u001b[0m     input_ids,\n\u001b[0;32m   1273\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1274\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1275\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1276\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1277\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1278\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1279\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1280\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   1281\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1282\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1283\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1284\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1285\u001b[0m )\n\u001b[0;32m   1286\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1031\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1030\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwte(input_ids)\n\u001b[1;32m-> 1031\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe(position_ids)\n\u001b[0;32m   1032\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# Attention mask.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx,\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq,\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[0;32m    198\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# generated_codes = generate_predictions(eval_dataset_python, model, tokenizer)\n",
    "# print(generated_codes[:5])  # Print the first 5 generated predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
