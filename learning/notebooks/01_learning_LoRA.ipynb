{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,TrainingArguments,pipeline\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"knkarthick/samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13818513</td>\n",
       "      <td>Amanda: I baked  cookies. Do you want some?\\nJ...</td>\n",
       "      <td>Amanda baked cookies and will bring Jerry some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13728867</td>\n",
       "      <td>Olivia: Who are you voting for in this electio...</td>\n",
       "      <td>Olivia and Olivier are voting for liberals in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13681000</td>\n",
       "      <td>Tim: Hi, what's up?\\nKim: Bad mood tbh, I was ...</td>\n",
       "      <td>Kim may try the pomodoro technique recommended...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13730747</td>\n",
       "      <td>Edward: Rachel, I think I'm in ove with Bella....</td>\n",
       "      <td>Edward thinks he is in love with Bella. Rachel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13728094</td>\n",
       "      <td>Sam: hey  overheard rick say something\\nSam: i...</td>\n",
       "      <td>Sam is confused, because he overheard Rick com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           dialogue  \\\n",
       "0  13818513  Amanda: I baked  cookies. Do you want some?\\nJ...   \n",
       "1  13728867  Olivia: Who are you voting for in this electio...   \n",
       "2  13681000  Tim: Hi, what's up?\\nKim: Bad mood tbh, I was ...   \n",
       "3  13730747  Edward: Rachel, I think I'm in ove with Bella....   \n",
       "4  13728094  Sam: hey  overheard rick say something\\nSam: i...   \n",
       "\n",
       "                                             summary  \n",
       "0  Amanda baked cookies and will bring Jerry some...  \n",
       "1  Olivia and Olivier are voting for liberals in ...  \n",
       "2  Kim may try the pomodoro technique recommended...  \n",
       "3  Edward thinks he is in love with Bella. Rachel...  \n",
       "4  Sam is confused, because he overheard Rick com...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the train split to a pandas DataFrame and display the first few rows\n",
    "df_train = ds['train'].to_pandas()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13862856</td>\n",
       "      <td>Hannah: Hey, do you have Betty's number?\\nAman...</td>\n",
       "      <td>Hannah needs Betty's number but Amanda doesn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13729565</td>\n",
       "      <td>Eric: MACHINE!\\nRob: That's so gr8!\\nEric: I k...</td>\n",
       "      <td>Eric and Rob are going to watch a stand-up on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13680171</td>\n",
       "      <td>Lenny: Babe, can you help me with something?\\n...</td>\n",
       "      <td>Lenny can't decide which trousers to buy. Bob ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13729438</td>\n",
       "      <td>Will: hey babe, what do you want for dinner to...</td>\n",
       "      <td>Emma will be home soon and she will let Will k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13828600</td>\n",
       "      <td>Ollie: Hi , are you in Warsaw\\nJane: yes, just...</td>\n",
       "      <td>Jane is in Warsaw. Ollie and Jane has a party....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           dialogue  \\\n",
       "0  13862856  Hannah: Hey, do you have Betty's number?\\nAman...   \n",
       "1  13729565  Eric: MACHINE!\\nRob: That's so gr8!\\nEric: I k...   \n",
       "2  13680171  Lenny: Babe, can you help me with something?\\n...   \n",
       "3  13729438  Will: hey babe, what do you want for dinner to...   \n",
       "4  13828600  Ollie: Hi , are you in Warsaw\\nJane: yes, just...   \n",
       "\n",
       "                                             summary  \n",
       "0  Hannah needs Betty's number but Amanda doesn't...  \n",
       "1  Eric and Rob are going to watch a stand-up on ...  \n",
       "2  Lenny can't decide which trousers to buy. Bob ...  \n",
       "3  Emma will be home soon and she will let Will k...  \n",
       "4  Jane is in Warsaw. Ollie and Jane has a party....  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = ds['test'].to_pandas()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "#Makes training faster but a little less accurate \n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "#setting padding instructions for tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_instruction_format(sample):\n",
    "  return f\"\"\"### Instruction:\n",
    "    Use the Task below and the Input given to write the Response:\n",
    "\n",
    "    ### Task:\n",
    "    Summarize the Input\n",
    "\n",
    "    ### Input:\n",
    "    {sample['dialogue']}\n",
    "\n",
    "    ### Response:\n",
    "    {sample['summary']}\n",
    "    \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': packing. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:212: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 512\n",
      "  warnings.warn(\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error occurred while packing the dataset. Make sure that your dataset has enough samples to at least yield one packed sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\datasets\\builder.py:1607\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[0;32m   1606\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 1607\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, record \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[0;32m   1608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m writer\u001b[38;5;241m.\u001b[39m_num_bytes \u001b[38;5;241m>\u001b[39m max_shard_size:\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\datasets\\packaged_modules\\generator\\generator.py:33\u001b[0m, in \u001b[0;36mGenerator._generate_examples\u001b[1;34m(self, **gen_kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_examples\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs):\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, ex \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgenerator(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs)):\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m idx, ex\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:586\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_packed_dataloader.<locals>.data_generator\u001b[1;34m(constant_length_iterator)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata_generator\u001b[39m(constant_length_iterator):\n\u001b[1;32m--> 586\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m constant_length_iterator\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\trl\\trainer\\utils.py:643\u001b[0m, in \u001b[0;36mConstantLengthDataset.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 643\u001b[0m     buffer\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatting_func(\u001b[38;5;28mnext\u001b[39m(iterator)))\n\u001b[0;32m    644\u001b[0m     buffer_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(buffer[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[21], line 9\u001b[0m, in \u001b[0;36mprompt_instruction_format\u001b[1;34m(sample)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprompt_instruction_format\u001b[39m(sample):\n\u001b[0;32m      2\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m### Instruction:\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124m    Use the Task below and the Input given to write the Response:\u001b[39m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m    ### Task:\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m    Summarize the Input\u001b[39m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;124m    ### Input:\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdialogue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m    ### Response:\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:589\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_packed_dataloader\u001b[1;34m(self, processing_class, dataset, dataset_text_field, max_seq_length, num_of_sequences, chars_per_token, formatting_func, append_concat_token, add_special_tokens)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 589\u001b[0m     packed_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_generator(\n\u001b[0;32m    590\u001b[0m         data_generator, gen_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant_length_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m: constant_length_iterator}\n\u001b[0;32m    591\u001b[0m     )\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (DatasetGenerationError, SchemaInferenceError) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\datasets\\arrow_dataset.py:1117\u001b[0m, in \u001b[0;36mDataset.from_generator\u001b[1;34m(generator, features, cache_dir, keep_in_memory, gen_kwargs, num_proc, split, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeneratorDatasetInputStream\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GeneratorDatasetInputStream(\n\u001b[0;32m   1109\u001b[0m     generator\u001b[38;5;241m=\u001b[39mgenerator,\n\u001b[0;32m   1110\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   1111\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1112\u001b[0m     keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[0;32m   1113\u001b[0m     gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs,\n\u001b[0;32m   1114\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[0;32m   1115\u001b[0m     split\u001b[38;5;241m=\u001b[39msplit,\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m-> 1117\u001b[0m )\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\datasets\\io\\generator.py:49\u001b[0m, in \u001b[0;36mGeneratorDatasetInputStream.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mdownload_and_prepare(\n\u001b[0;32m     50\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m     51\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m     52\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[0;32m     53\u001b[0m     base_path\u001b[38;5;241m=\u001b[39mbase_path,\n\u001b[0;32m     54\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_proc,\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mas_dataset(\n\u001b[0;32m     57\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msplit, verification_mode\u001b[38;5;241m=\u001b[39mverification_mode, in_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_in_memory\n\u001b[0;32m     58\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\datasets\\builder.py:924\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[1;32m--> 924\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    925\u001b[0m     dl_manager\u001b[38;5;241m=\u001b[39mdl_manager,\n\u001b[0;32m    926\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m    929\u001b[0m )\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\datasets\\builder.py:1648\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[0;32m   1647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[1;32m-> 1648\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m   1649\u001b[0m         dl_manager,\n\u001b[0;32m   1650\u001b[0m         verification_mode,\n\u001b[0;32m   1651\u001b[0m         check_duplicate_keys\u001b[38;5;241m=\u001b[39mverification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS\n\u001b[0;32m   1652\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS,\n\u001b[0;32m   1653\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs,\n\u001b[0;32m   1654\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\datasets\\builder.py:1000\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[1;32m-> 1000\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split(split_generator, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs)\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\datasets\\builder.py:1486\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split\u001b[1;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[1;32m-> 1486\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[0;32m   1487\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[0;32m   1488\u001b[0m     ):\n\u001b[0;32m   1489\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\datasets\\builder.py:1643\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[0;32m   1642\u001b[0m         e \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39m__context__\n\u001b[1;32m-> 1643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1645\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 18\u001b[0m\n\u001b[0;32m      2\u001b[0m trainingArgs \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      3\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-4\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[0;32m     11\u001b[0m       lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m     12\u001b[0m       lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m       task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     20\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdf_train,\n\u001b[0;32m     21\u001b[0m     eval_dataset \u001b[38;5;241m=\u001b[39m df_test,\n\u001b[0;32m     22\u001b[0m     peft_config\u001b[38;5;241m=\u001b[39mpeft_config,\n\u001b[0;32m     23\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m     24\u001b[0m     packing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     25\u001b[0m     formatting_func\u001b[38;5;241m=\u001b[39mprompt_instruction_format,\n\u001b[0;32m     26\u001b[0m     args\u001b[38;5;241m=\u001b[39mtrainingArgs,\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[0;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\transformers\\utils\\deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:368\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PartialState()\u001b[38;5;241m.\u001b[39mlocal_main_process_first():\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 368\u001b[0m         train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_dataset(\n\u001b[0;32m    369\u001b[0m             train_dataset,\n\u001b[0;32m    370\u001b[0m             processing_class,\n\u001b[0;32m    371\u001b[0m             args\u001b[38;5;241m.\u001b[39mpacking,\n\u001b[0;32m    372\u001b[0m             args\u001b[38;5;241m.\u001b[39mdataset_text_field,\n\u001b[0;32m    373\u001b[0m             args\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[0;32m    374\u001b[0m             formatting_func,\n\u001b[0;32m    375\u001b[0m             args\u001b[38;5;241m.\u001b[39mnum_of_sequences,\n\u001b[0;32m    376\u001b[0m             args\u001b[38;5;241m.\u001b[39mchars_per_token,\n\u001b[0;32m    377\u001b[0m             remove_unused_columns\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mremove_unused_columns \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    378\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs\u001b[38;5;241m.\u001b[39mdataset_kwargs,\n\u001b[0;32m    379\u001b[0m         )\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m         _multiple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, \u001b[38;5;28mdict\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:488\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_dataset\u001b[1;34m(self, dataset, processing_class, packing, dataset_text_field, max_seq_length, formatting_func, num_of_sequences, chars_per_token, remove_unused_columns, append_concat_token, add_special_tokens, skip_prepare_dataset)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_non_packed_dataloader(\n\u001b[0;32m    478\u001b[0m         processing_class,\n\u001b[0;32m    479\u001b[0m         dataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    484\u001b[0m         remove_unused_columns,\n\u001b[0;32m    485\u001b[0m     )\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_packed_dataloader(\n\u001b[0;32m    489\u001b[0m         processing_class,\n\u001b[0;32m    490\u001b[0m         dataset,\n\u001b[0;32m    491\u001b[0m         dataset_text_field,\n\u001b[0;32m    492\u001b[0m         max_seq_length,\n\u001b[0;32m    493\u001b[0m         num_of_sequences,\n\u001b[0;32m    494\u001b[0m         chars_per_token,\n\u001b[0;32m    495\u001b[0m         formatting_func,\n\u001b[0;32m    496\u001b[0m         append_concat_token,\n\u001b[0;32m    497\u001b[0m         add_special_tokens,\n\u001b[0;32m    498\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MarijaJolovic\\miniconda3\\envs\\myenv1\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:593\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_packed_dataloader\u001b[1;34m(self, processing_class, dataset, dataset_text_field, max_seq_length, num_of_sequences, chars_per_token, formatting_func, append_concat_token, add_special_tokens)\u001b[0m\n\u001b[0;32m    589\u001b[0m     packed_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_generator(\n\u001b[0;32m    590\u001b[0m         data_generator, gen_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant_length_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m: constant_length_iterator}\n\u001b[0;32m    591\u001b[0m     )\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (DatasetGenerationError, SchemaInferenceError) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    594\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred while packing the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    595\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that your dataset has enough samples to at least yield one packed sequence.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    596\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m packed_dataset\n",
      "\u001b[1;31mValueError\u001b[0m: Error occurred while packing the dataset. Make sure that your dataset has enough samples to at least yield one packed sequence."
     ]
    }
   ],
   "source": [
    "# Create the trainer\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "      lora_alpha=16,\n",
    "      lora_dropout=0.1,\n",
    "      r=64,\n",
    "      bias=\"none\",\n",
    "      task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=df_train,\n",
    "    eval_dataset = df_test,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=prompt_instruction_format,\n",
    "    args=trainingArgs,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 78. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: \n",
      "Alex: Hon, could you buy some milk?\n",
      "Martin: You kiddin me, I just bought two yesterday\n",
      "Alex: Well, I have nothing for my defense, I'm just craving milk lately:D\n",
      "Martin: You're lucky that I love you, two milks coming up in couple of hours, I gotta finish the project\n",
      "Alex: You are my hero!\n",
      "---------------\n",
      "flan-t5-small summary:\n",
      "Martin bought two milks yesterday. Alex has nothing for his defense. Alex is a hero. Alex wants to finish the project. Alex and Alex will buy milk in two hours.\n",
      "true summary:\n",
      "Martin will get Alex two bottles of milk after he finishes the project.\n"
     ]
    }
   ],
   "source": [
    "# define the summarizer pipeline\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# select a random test sample\n",
    "sample = df_test.iloc[randrange(len(df_test))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "\n",
    "# summarize dialogue\n",
    "res = summarizer(sample[\"dialogue\"])\n",
    "\n",
    "print(f\"flan-t5-small summary:\\n{res[0]['summary_text']}\")\n",
    "print(f\"true summary:\\n{sample['summary']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
