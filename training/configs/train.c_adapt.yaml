# === Training config: C/C++ ADAPTATION (short polish) ===
seed: 42
output_dir: outputs/lora-c-adapt-${now:%Y%m%d-%H%M}

# --- Data ---
sequence_length: 1024
pack_sequences: true

# --- Optimization ---
train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 5.0e-5
weight_decay: 0.01
num_train_steps: 400
warmup_steps: 40
lr_scheduler_type: cosine
gradient_checkpointing: true
bf16: true

# --- QLoRA ---
use_qlora: true
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true
bnb_compute_dtype: bfloat16

# --- LoRA params ---
lora_r: 16
lora_alpha: 64
lora_dropout: 0.05
lora_target_modules: ["q_attn", "c_attn", "c_proj", "c_fc"]

# --- FIM ---
use_fim: false          # true for StarCoder; false for Mistral/Llama
fim_rate: 0.5
fim_spm_rate: 0.5
