# === Training config: MULTI-LANG (QLoRA, FIM-ready) ===
seed: 42
output_dir: outputs/lora-${now:%Y%m%d-%H%M}

# --- Data ---
sequence_length: 1024
pack_sequences: true

# --- Optimization ---
train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 2.0e-4
weight_decay: 0.01
num_train_steps: 2000
warmup_steps: 100
lr_scheduler_type: cosine
gradient_checkpointing: true
bf16: true

# --- QLoRA / bitsandbytes ---
use_qlora: true
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true
bnb_compute_dtype: bfloat16

# --- LoRA params ---
lora_r: 16
lora_alpha: 64
lora_dropout: 0.05
lora_target_modules: ["q_attn", "c_attn", "c_proj", "c_fc"]

# --- FIM (fill-in-the-middle) ---
use_fim: false          # true for StarCoder infill; false for causal-only models
fim_rate: 0.5
fim_spm_rate: 0.5
