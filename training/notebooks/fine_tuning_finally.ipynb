{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7020be71-d219-4015-adcb-75e571c48b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb93ef3b-427f-46d5-97e6-7def03cb95e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5289e373-28d4-470e-aa76-01ae48cce53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"bigcode/starcoderbase-7b\" # Model checkpoint on the Hugging Face Hub\n",
    "DATASET=\"smangrul/hf-stack-v1\"   # Dataset on the Hugging Face Hub\n",
    "DATA_COLUMN=\"content\"            # Column name containing the code content\n",
    "\n",
    "SEQ_LENGTH=1024                  # Sequence length\n",
    "\n",
    "# Training arguments\n",
    "MAX_STEPS=50                # max_steps\n",
    "BATCH_SIZE=2                    # batch_size\n",
    "GR_ACC_STEPS=2                   # gradient_accumulation_steps\n",
    "LR=5e-4                          # learning_rate\n",
    "LR_SCHEDULER_TYPE=\"cosine\"       # lr_scheduler_type\n",
    "WEIGHT_DECAY=0.01                # weight_decay\n",
    "NUM_WARMUP_STEPS=30              # num_warmup_steps\n",
    "EVAL_FREQ=100                    # eval_freq\n",
    "SAVE_FREQ=100                    # save_freq\n",
    "LOG_FREQ=25                      # log_freq\n",
    "OUTPUT_DIR=\"models\"\n",
    "OUTPUT_DIR_1=\"starcoderbase-7b-1024-seq-length\" # output_dir\n",
    "BF16=True                        # bf16\n",
    "FP16=False                       # no_fp16\n",
    "\n",
    "# FIM trasformations arguments\n",
    "FIM_RATE=0.5                     # fim_rate\n",
    "FIM_SPM_RATE=0.5                 # fim_spm_rate\n",
    "\n",
    "# LORA\n",
    "LORA_R=4                         # lora_r\n",
    "LORA_ALPHA=16                    # lora_alpha\n",
    "LORA_DROPOUT=0.0                 # lora_dropout\n",
    "LORA_TARGET_MODULES=\"c_proj,c_attn,q_attn,c_fc\"    # lora_target_modules\n",
    "\n",
    "# bitsandbytes config\n",
    "USE_NESTED_QUANT=True            # use_nested_quant\n",
    "BNB_4BIT_COMPUTE_DTYPE=\"bfloat16\"# bnb_4bit_compute_dtype\n",
    "\n",
    "SEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8556ab80-904a-468b-8659-5ae749ae6fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 14:27:09.166069: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-24 14:27:09.216162: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740407229.248943  227466 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740407229.259499  227466 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-24 14:27:09.299561: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    "    set_seed,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41d10f4c-a0d5-42f4-8353-75e92271da59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca35d6a-c292-42b6-a428-ac3f4405ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = load_dataset(\n",
    "    DATASET,\n",
    "    data_dir=\"data\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bce2892-0c22-4eb8-8b6b-55111a4461ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = dataset.take(4000)\n",
    "train_data = dataset.skip(4000)\n",
    "train_data = train_data.shuffle(buffer_size=5000, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5fc8130-bb30-49da-ab31-6a731ad021a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:07<00:00, 55.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character to token ratio of the dataset is: 2.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "\n",
    "def chars_token_ratio(dataset, tokenizer, data_column, nb_examples=400):\n",
    "    \"\"\"\n",
    "    Estimate the average number of characters per token in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    total_characters, total_tokens = 0, 0\n",
    "    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n",
    "        total_characters += len(example[data_column])\n",
    "        total_tokens += len(tokenizer(example[data_column]).tokens())\n",
    "\n",
    "    return total_characters / total_tokens\n",
    "\n",
    "\n",
    "chars_per_token = chars_token_ratio(train_data, tokenizer, DATA_COLUMN)\n",
    "print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eea1d92-1b93-4d75-aca5-967aa0037c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Helper function to get token ids of the special tokens for prefix, suffix and middle for FIM transformations.\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def get_fim_token_ids(tokenizer):\n",
    "    try:\n",
    "        FIM_PREFIX, FIM_MIDDLE, FIM_SUFFIX, FIM_PAD = tokenizer.special_tokens_map[\"additional_special_tokens\"][1:5]\n",
    "        suffix_tok_id, prefix_tok_id, middle_tok_id, pad_tok_id = (\n",
    "            tokenizer.vocab[tok] for tok in [FIM_SUFFIX, FIM_PREFIX, FIM_MIDDLE, FIM_PAD]\n",
    "        )\n",
    "    except KeyError:\n",
    "        suffix_tok_id, prefix_tok_id, middle_tok_id, pad_tok_id = None, None, None, None\n",
    "    return suffix_tok_id, prefix_tok_id, middle_tok_id, pad_tok_id\n",
    "\n",
    "\n",
    "## Adapted from https://github.com/bigcode-project/Megatron-LM/blob/6c4bf908df8fd86b4977f54bf5b8bd4b521003d1/megatron/data/gpt_dataset.py\n",
    "def permute(\n",
    "    sample,\n",
    "    np_rng,\n",
    "    suffix_tok_id,\n",
    "    prefix_tok_id,\n",
    "    middle_tok_id,\n",
    "    pad_tok_id,\n",
    "    fim_rate=0.5,\n",
    "    fim_spm_rate=0.5,\n",
    "    truncate_or_pad=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Take in a sample (list of tokens) and perform a FIM transformation on it with a probability of fim_rate, using two FIM modes:\n",
    "    PSM and SPM (with a probability of fim_spm_rate).\n",
    "    \"\"\"\n",
    "\n",
    "    # The if condition will trigger with the probability of fim_rate\n",
    "    # This means FIM transformations will apply to samples with a probability of fim_rate\n",
    "    if np_rng.binomial(1, fim_rate):\n",
    "\n",
    "        # Split the sample into prefix, middle, and suffix, based on randomly generated indices stored in the boundaries list.\n",
    "        boundaries = list(np_rng.randint(low=0, high=len(sample) + 1, size=2))\n",
    "        boundaries.sort()\n",
    "\n",
    "        prefix = np.array(sample[: boundaries[0]], dtype=np.int64)\n",
    "        middle = np.array(sample[boundaries[0] : boundaries[1]], dtype=np.int64)\n",
    "        suffix = np.array(sample[boundaries[1] :], dtype=np.int64)\n",
    "\n",
    "        if truncate_or_pad:\n",
    "            # calculate the new total length of the sample, taking into account tokens indicating prefix, middle, and suffix\n",
    "            new_length = suffix.shape[0] + prefix.shape[0] + middle.shape[0] + 3\n",
    "            diff = new_length - len(sample)\n",
    "\n",
    "            # trancate or pad if there's a difference in length between the new length and the original\n",
    "            if diff > 0:\n",
    "                if suffix.shape[0] <= diff:\n",
    "                    return sample, np_rng\n",
    "                suffix = suffix[: suffix.shape[0] - diff]\n",
    "            elif diff < 0:\n",
    "                suffix = np.concatenate([suffix, np.full((-1 * diff), pad_tok_id)])\n",
    "\n",
    "        # With the probability of fim_spm_rateapply SPM variant of FIM transformations\n",
    "        # SPM: suffix, prefix, middle\n",
    "        if np_rng.binomial(1, fim_spm_rate):\n",
    "            new_sample = np.concatenate(\n",
    "                [\n",
    "                    [prefix_tok_id, suffix_tok_id],\n",
    "                    suffix,\n",
    "                    [middle_tok_id],\n",
    "                    prefix,\n",
    "                    middle,\n",
    "                ]\n",
    "            )\n",
    "        # Otherwise, apply the PSM variant of FIM transformations\n",
    "        # PSM: prefix, suffix, middle\n",
    "        else:\n",
    "\n",
    "            new_sample = np.concatenate(\n",
    "                [\n",
    "                    [prefix_tok_id],\n",
    "                    prefix,\n",
    "                    [suffix_tok_id],\n",
    "                    suffix,\n",
    "                    [middle_tok_id],\n",
    "                    middle,\n",
    "                ]\n",
    "            )\n",
    "    else:\n",
    "        # don't apply FIM transformations\n",
    "        new_sample = sample\n",
    "\n",
    "    return list(new_sample), np_rng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d1f42dc-cdbb-4aee-987d-3fc2c23951a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import random\n",
    "\n",
    "# Create an Iterable dataset that returns constant-length chunks of tokens from a stream of text files.\n",
    "\n",
    "class ConstantLengthDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Iterable dataset that returns constant length chunks of tokens from stream of text files.\n",
    "        Args:\n",
    "            tokenizer (Tokenizer): The processor used for proccessing the data.\n",
    "            dataset (dataset.Dataset): Dataset with text files.\n",
    "            infinite (bool): If True the iterator is reset after dataset reaches end else stops.\n",
    "            seq_length (int): Length of token sequences to return.\n",
    "            num_of_sequences (int): Number of token sequences to keep in buffer.\n",
    "            chars_per_token (int): Number of characters per token used to estimate number of tokens in text buffer.\n",
    "            fim_rate (float): Rate (0.0 to 1.0) that sample will be permuted with FIM.\n",
    "            fim_spm_rate (float): Rate (0.0 to 1.0) of FIM permuations that will use SPM.\n",
    "            seed (int): Seed for random number generator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        dataset,\n",
    "        infinite=True,\n",
    "        seq_length=1024,\n",
    "        num_of_sequences=1024,\n",
    "        chars_per_token=3.6,\n",
    "        content_field=\"content\",\n",
    "        fim_rate=0.5,\n",
    "        fim_spm_rate=0.5,\n",
    "        seed=0,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = tokenizer.eos_token_id\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.infinite = infinite\n",
    "        self.current_size = 0\n",
    "        self.max_buffer_size = seq_length * chars_per_token * num_of_sequences\n",
    "        self.content_field = content_field\n",
    "        self.fim_rate = fim_rate\n",
    "        self.fim_spm_rate = fim_spm_rate\n",
    "        self.seed = seed\n",
    "\n",
    "        (\n",
    "            self.suffix_tok_id,\n",
    "            self.prefix_tok_id,\n",
    "            self.middle_tok_id,\n",
    "            self.pad_tok_id,\n",
    "        ) = get_fim_token_ids(self.tokenizer)\n",
    "        if not self.suffix_tok_id and self.fim_rate > 0:\n",
    "            print(\"FIM is not supported by tokenizer, disabling FIM\")\n",
    "            self.fim_rate = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        np_rng = np.random.RandomState(seed=self.seed)\n",
    "        while more_examples:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.max_buffer_size:\n",
    "                    break\n",
    "                try:\n",
    "                    buffer.append(next(iterator)[self.content_field])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    if self.infinite:\n",
    "                        iterator = iter(self.dataset)\n",
    "                    else:\n",
    "                        more_examples = False\n",
    "                        break\n",
    "            tokenized_inputs = self.tokenizer(buffer, truncation=False)[\"input_ids\"]\n",
    "            all_token_ids = []\n",
    "\n",
    "            for tokenized_input in tokenized_inputs:\n",
    "                # optionally do FIM permutations\n",
    "                if self.fim_rate > 0:\n",
    "                    tokenized_input, np_rng = permute(\n",
    "                        tokenized_input,\n",
    "                        np_rng,\n",
    "                        self.suffix_tok_id,\n",
    "                        self.prefix_tok_id,\n",
    "                        self.middle_tok_id,\n",
    "                        self.pad_tok_id,\n",
    "                        fim_rate=self.fim_rate,\n",
    "                        fim_spm_rate=self.fim_spm_rate,\n",
    "                        truncate_or_pad=False,\n",
    "                    )\n",
    "\n",
    "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
    "            examples = []\n",
    "            for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                input_ids = all_token_ids[i : i + self.seq_length]\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    examples.append(input_ids)\n",
    "            random.shuffle(examples)\n",
    "            for example in examples:\n",
    "                self.current_size += 1\n",
    "                yield {\n",
    "                    \"input_ids\": torch.LongTensor(example), \n",
    "                    \"labels\": torch.LongTensor(example),\n",
    "                }\n",
    "\n",
    "\n",
    "train_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        train_data,\n",
    "        infinite=True,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        chars_per_token=chars_per_token,\n",
    "        content_field=DATA_COLUMN,\n",
    "        fim_rate=FIM_RATE,\n",
    "        fim_spm_rate=FIM_SPM_RATE,\n",
    "        seed=SEED,\n",
    ")\n",
    "eval_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        valid_data,\n",
    "        infinite=False,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        chars_per_token=chars_per_token,\n",
    "        content_field=DATA_COLUMN,\n",
    "        fim_rate=FIM_RATE,\n",
    "        fim_spm_rate=FIM_SPM_RATE,\n",
    "        seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f512b39-4fd7-4ac1-8e92-993626f716fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e1d345d-f9c2-4054-bacd-b6ca1d58d05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9eb82c3d3940da9ab01c91f9297e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from peft.tuners.lora import LoraLayer\n",
    "\n",
    "load_in_8bit = False\n",
    "\n",
    "# 4-bit quantization\n",
    "compute_dtype = getattr(torch, BNB_4BIT_COMPUTE_DTYPE)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=USE_NESTED_QUANT,\n",
    ")\n",
    "\n",
    "device_map = \"auto\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device_map,\n",
    "        use_cache=False,  # We will be using gradient checkpointing\n",
    "        trust_remote_code=True,\n",
    "        use_flash_attention_2=False\n",
    ")\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7804f4b8-d527-4089-b7aa-ac72774ea49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6890c1-4cec-466a-8763-af0b11817e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5f7528a-6393-4ccd-8024-8be1d5dd6716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,676,800 || all params: 7,336,940,032 || trainable%: 0.1319\n"
     ]
    }
   ],
   "source": [
    "# Set up lora\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    r=LORA_R,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=LORA_TARGET_MODULES.split(\",\"),\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "745d1820-7f9f-4b84-8e4e-a58afca34999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjolovic/myenv/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_data.start_iteration = 0\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"marijajolovic/{OUTPUT_DIR}/{OUTPUT_DIR_1}\",\n",
    "    dataloader_drop_last=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    max_steps=MAX_STEPS,\n",
    "    eval_steps=EVAL_FREQ,\n",
    "    save_steps=SAVE_FREQ,\n",
    "    logging_steps=LOG_FREQ,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    warmup_steps=NUM_WARMUP_STEPS,\n",
    "    gradient_accumulation_steps=GR_ACC_STEPS,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=FP16,\n",
    "    bf16=BF16,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    push_to_hub=True,\n",
    "    include_tokens_per_second=True,\n",
    "    no_cuda=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a005a13-4093-40ab-9e62-37d6a0b34deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming train_dataset is your dataset object\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=BATCH_SIZE,\n",
    "                            drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4615c542-5df8-4a12-a915-d680f7db9e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: 0\n",
      "Available devices: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "print(\"Available devices:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c2b9c6-6f4b-4908-bc11-5a90cbb3b097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9/50 10:10 < 59:34, 0.01 it/s, Epoch 0.16/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "print(\"Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df188f9-9bc0-4aba-a91f-4fc6715d4633",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
