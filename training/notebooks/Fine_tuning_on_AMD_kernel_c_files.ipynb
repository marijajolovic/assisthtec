{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6abed07c-6a18-4941-8b50-3c318adaf81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a07769a-f41f-47c7-bc94-31b5ec71a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7c93e8-6649-40f6-b8d3-6a2fd58bbd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"bigcode/starcoderbase-3b\" # Model checkpoint on the Hugging Face Hub\n",
    "DATASET=\"smangrul/hf-stack-v1\"   # Dataset on the Hugging Face Hub\n",
    "DATA_COLUMN=\"content\"            # Column name containing the code content\n",
    "\n",
    "SEQ_LENGTH=2048                  # Sequence length\n",
    "\n",
    "# Training arguments\n",
    "MAX_STEPS=200                # max_steps\n",
    "BATCH_SIZE=4                    # batch_size\n",
    "GR_ACC_STEPS=1                   # gradient_accumulation_steps\n",
    "LR=5e-4                          # learning_rate\n",
    "LR_SCHEDULER_TYPE=\"cosine\"       # lr_scheduler_type\n",
    "WEIGHT_DECAY=0.01                # weight_decay\n",
    "NUM_WARMUP_STEPS=30              # num_warmup_steps\n",
    "EVAL_FREQ=100                    # eval_freq\n",
    "SAVE_FREQ=100                    # save_freq\n",
    "LOG_FREQ=25                      # log_freq\n",
    "OUTPUT_DIR=\"models\"\n",
    "OUTPUT_DIR_1=\"starcoderbase-3b-amd-200-steps\" # output_dir\n",
    "BF16=True                        # bf16\n",
    "FP16=False                       # no_fp16\n",
    "\n",
    "# FIM trasformations arguments\n",
    "FIM_RATE=0.5                     # fim_rate\n",
    "FIM_SPM_RATE=0.5                 # fim_spm_rate\n",
    "\n",
    "# LORA\n",
    "LORA_R=4                         # lora_r\n",
    "LORA_ALPHA=16                    # lora_alpha\n",
    "LORA_DROPOUT=0.0                 # lora_dropout\n",
    "LORA_TARGET_MODULES = \"c_proj,c_attn,q_attn,c_fc,c_proj\"    # lora_target_modules\n",
    "\n",
    "# bitsandbytes config\n",
    "USE_NESTED_QUANT=True            # use_nested_quant\n",
    "BNB_4BIT_COMPUTE_DTYPE=\"bfloat16\"# bnb_4bit_compute_dtype\n",
    "\n",
    "SEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d462a67d-e9bf-4550-93af-0f17cf50134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 14:39:30.146079: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-20 14:39:30.168459: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740062370.194385  180193 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740062370.202382  180193 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-20 14:39:30.229287: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    "    set_seed,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da9685b5-d47a-404e-b0af-3c0275bf27ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import Dataset\n",
    "import glob\n",
    "\n",
    "# Define the path to the cloned repository\n",
    "repo_path = \"ROCK-Kernel-Driver/kernel\"\n",
    "\n",
    "# Get all .c files\n",
    "c_files = glob.glob(os.path.join(repo_path, \"**/*.c\"), recursive=True)\n",
    "\n",
    "# Read the content of each .c file\n",
    "data = []\n",
    "for file_path in c_files:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        content = f.read()\n",
    "        data.append({\"content\": content})\n",
    "\n",
    "# Convert to Hugging Face dataset format\n",
    "dataset = Dataset.from_list(data)\n",
    "split1 = dataset.train_test_split(test_size=0.3, seed=SEED)\n",
    "split2 = split1[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "\n",
    "train_data = split1[\"train\"]\n",
    "valid_data = split2[\"train\"]\n",
    "test_data = split2[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbff0a7-8cc2-4651-abd1-4c6e7b3ab96c",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7898c6e9-6302-43ae-b770-20e9e6af6231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–Š| 300/400 [00:10<00:03, 29.78it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character to token ratio of the dataset is: 2.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "\n",
    "def chars_token_ratio(dataset, tokenizer, data_column, nb_examples=400):\n",
    "    \"\"\"\n",
    "    Estimate the average number of characters per token in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    total_characters, total_tokens = 0, 0\n",
    "    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n",
    "        total_characters += len(example[data_column])\n",
    "        total_tokens += len(tokenizer(example[data_column]).tokens())\n",
    "\n",
    "    return total_characters / total_tokens\n",
    "\n",
    "\n",
    "chars_per_token = chars_token_ratio(train_data, tokenizer, DATA_COLUMN)\n",
    "print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f18cca17-efc7-42c1-9a12-296e80356121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Helper function to get token ids of the special tokens for prefix, suffix and middle for FIM transformations.\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def get_fim_token_ids(tokenizer):\n",
    "    try:\n",
    "        FIM_PREFIX, FIM_MIDDLE, FIM_SUFFIX, FIM_PAD = tokenizer.special_tokens_map[\"additional_special_tokens\"][1:5]\n",
    "        suffix_tok_id, prefix_tok_id, middle_tok_id, pad_tok_id = (\n",
    "            tokenizer.vocab[tok] for tok in [FIM_SUFFIX, FIM_PREFIX, FIM_MIDDLE, FIM_PAD]\n",
    "        )\n",
    "    except KeyError:\n",
    "        suffix_tok_id, prefix_tok_id, middle_tok_id, pad_tok_id = None, None, None, None\n",
    "    return suffix_tok_id, prefix_tok_id, middle_tok_id, pad_tok_id\n",
    "\n",
    "\n",
    "## Adapted from https://github.com/bigcode-project/Megatron-LM/blob/6c4bf908df8fd86b4977f54bf5b8bd4b521003d1/megatron/data/gpt_dataset.py\n",
    "def permute(\n",
    "    sample,\n",
    "    np_rng,\n",
    "    suffix_tok_id,\n",
    "    prefix_tok_id,\n",
    "    middle_tok_id,\n",
    "    pad_tok_id,\n",
    "    fim_rate=0.5,\n",
    "    fim_spm_rate=0.5,\n",
    "    truncate_or_pad=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Take in a sample (list of tokens) and perform a FIM transformation on it with a probability of fim_rate, using two FIM modes:\n",
    "    PSM and SPM (with a probability of fim_spm_rate).\n",
    "    \"\"\"\n",
    "\n",
    "    # The if condition will trigger with the probability of fim_rate\n",
    "    # This means FIM transformations will apply to samples with a probability of fim_rate\n",
    "    if np_rng.binomial(1, fim_rate):\n",
    "\n",
    "        # Split the sample into prefix, middle, and suffix, based on randomly generated indices stored in the boundaries list.\n",
    "        boundaries = list(np_rng.randint(low=0, high=len(sample) + 1, size=2))\n",
    "        boundaries.sort()\n",
    "\n",
    "        prefix = np.array(sample[: boundaries[0]], dtype=np.int64)\n",
    "        middle = np.array(sample[boundaries[0] : boundaries[1]], dtype=np.int64)\n",
    "        suffix = np.array(sample[boundaries[1] :], dtype=np.int64)\n",
    "\n",
    "        if truncate_or_pad:\n",
    "            # calculate the new total length of the sample, taking into account tokens indicating prefix, middle, and suffix\n",
    "            new_length = suffix.shape[0] + prefix.shape[0] + middle.shape[0] + 3\n",
    "            diff = new_length - len(sample)\n",
    "\n",
    "            # trancate or pad if there's a difference in length between the new length and the original\n",
    "            if diff > 0:\n",
    "                if suffix.shape[0] <= diff:\n",
    "                    return sample, np_rng\n",
    "                suffix = suffix[: suffix.shape[0] - diff]\n",
    "            elif diff < 0:\n",
    "                suffix = np.concatenate([suffix, np.full((-1 * diff), pad_tok_id)])\n",
    "\n",
    "        # With the probability of fim_spm_rateapply SPM variant of FIM transformations\n",
    "        # SPM: suffix, prefix, middle\n",
    "        if np_rng.binomial(1, fim_spm_rate):\n",
    "            new_sample = np.concatenate(\n",
    "                [\n",
    "                    [prefix_tok_id, suffix_tok_id],\n",
    "                    suffix,\n",
    "                    [middle_tok_id],\n",
    "                    prefix,\n",
    "                    middle,\n",
    "                ]\n",
    "            )\n",
    "        # Otherwise, apply the PSM variant of FIM transformations\n",
    "        # PSM: prefix, suffix, middle\n",
    "        else:\n",
    "\n",
    "            new_sample = np.concatenate(\n",
    "                [\n",
    "                    [prefix_tok_id],\n",
    "                    prefix,\n",
    "                    [suffix_tok_id],\n",
    "                    suffix,\n",
    "                    [middle_tok_id],\n",
    "                    middle,\n",
    "                ]\n",
    "            )\n",
    "    else:\n",
    "        # don't apply FIM transformations\n",
    "        new_sample = sample\n",
    "\n",
    "    return list(new_sample), np_rng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0dd9e74-e875-4142-a2a6-a6b51ed71ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import random\n",
    "\n",
    "# Create an Iterable dataset that returns constant-length chunks of tokens from a stream of text files.\n",
    "\n",
    "class ConstantLengthDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Iterable dataset that returns constant length chunks of tokens from stream of text files.\n",
    "        Args:\n",
    "            tokenizer (Tokenizer): The processor used for proccessing the data.\n",
    "            dataset (dataset.Dataset): Dataset with text files.\n",
    "            infinite (bool): If True the iterator is reset after dataset reaches end else stops.\n",
    "            seq_length (int): Length of token sequences to return.\n",
    "            num_of_sequences (int): Number of token sequences to keep in buffer.\n",
    "            chars_per_token (int): Number of characters per token used to estimate number of tokens in text buffer.\n",
    "            fim_rate (float): Rate (0.0 to 1.0) that sample will be permuted with FIM.\n",
    "            fim_spm_rate (float): Rate (0.0 to 1.0) of FIM permuations that will use SPM.\n",
    "            seed (int): Seed for random number generator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        dataset,\n",
    "        infinite=True,\n",
    "        seq_length=1024,\n",
    "        num_of_sequences=1024,\n",
    "        chars_per_token=3.6,\n",
    "        content_field=\"content\",\n",
    "        fim_rate=0.5,\n",
    "        fim_spm_rate=0.5,\n",
    "        seed=0,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = tokenizer.eos_token_id\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.infinite = infinite\n",
    "        self.current_size = 0\n",
    "        self.max_buffer_size = seq_length * chars_per_token * num_of_sequences\n",
    "        self.content_field = content_field\n",
    "        self.fim_rate = fim_rate\n",
    "        self.fim_spm_rate = fim_spm_rate\n",
    "        self.seed = seed\n",
    "\n",
    "        (\n",
    "            self.suffix_tok_id,\n",
    "            self.prefix_tok_id,\n",
    "            self.middle_tok_id,\n",
    "            self.pad_tok_id,\n",
    "        ) = get_fim_token_ids(self.tokenizer)\n",
    "        if not self.suffix_tok_id and self.fim_rate > 0:\n",
    "            print(\"FIM is not supported by tokenizer, disabling FIM\")\n",
    "            self.fim_rate = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        np_rng = np.random.RandomState(seed=self.seed)\n",
    "        while more_examples:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.max_buffer_size:\n",
    "                    break\n",
    "                try:\n",
    "                    buffer.append(next(iterator)[self.content_field])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    if self.infinite:\n",
    "                        iterator = iter(self.dataset)\n",
    "                    else:\n",
    "                        more_examples = False\n",
    "                        break\n",
    "            tokenized_inputs = self.tokenizer(buffer, truncation=False)[\"input_ids\"]\n",
    "            all_token_ids = []\n",
    "\n",
    "            for tokenized_input in tokenized_inputs:\n",
    "                # optionally do FIM permutations\n",
    "                if self.fim_rate > 0:\n",
    "                    tokenized_input, np_rng = permute(\n",
    "                        tokenized_input,\n",
    "                        np_rng,\n",
    "                        self.suffix_tok_id,\n",
    "                        self.prefix_tok_id,\n",
    "                        self.middle_tok_id,\n",
    "                        self.pad_tok_id,\n",
    "                        fim_rate=self.fim_rate,\n",
    "                        fim_spm_rate=self.fim_spm_rate,\n",
    "                        truncate_or_pad=False,\n",
    "                    )\n",
    "\n",
    "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
    "            examples = []\n",
    "            for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                input_ids = all_token_ids[i : i + self.seq_length]\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    examples.append(input_ids)\n",
    "            random.shuffle(examples)\n",
    "            for example in examples:\n",
    "                self.current_size += 1\n",
    "                yield {\n",
    "                    \"input_ids\": torch.LongTensor(example), \n",
    "                    \"labels\": torch.LongTensor(example),\n",
    "                }\n",
    "\n",
    "\n",
    "train_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        train_data,\n",
    "        infinite=True,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        chars_per_token=chars_per_token,\n",
    "        content_field=DATA_COLUMN,\n",
    "        fim_rate=FIM_RATE,\n",
    "        fim_spm_rate=FIM_SPM_RATE,\n",
    "        seed=SEED,\n",
    ")\n",
    "eval_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        valid_data,\n",
    "        infinite=False,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        chars_per_token=chars_per_token,\n",
    "        content_field=DATA_COLUMN,\n",
    "        fim_rate=FIM_RATE,\n",
    "        fim_spm_rate=FIM_SPM_RATE,\n",
    "        seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eed8539b-ffcb-4f29-91c8-95ab71f39a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfd4c69fcb74ebbbadcee528ed26789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from peft.tuners.lora import LoraLayer\n",
    "\n",
    "load_in_8bit = False\n",
    "\n",
    "# 4-bit quantization\n",
    "compute_dtype = getattr(torch, BNB_4BIT_COMPUTE_DTYPE)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=USE_NESTED_QUANT,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device_map,\n",
    "        use_cache=False,  # We will be using gradient checkpointing\n",
    "        trust_remote_code=True,\n",
    "        use_flash_attention_2=False\n",
    ")\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41b995fd-1399-467e-a410-51b5bc6a34ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fcaa163-bb8e-4db4-bdc4-12d30940affb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,713,920 || all params: 3,049,025,024 || trainable%: 0.1874\n"
     ]
    }
   ],
   "source": [
    "# Set up lora\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    r=LORA_R,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=LORA_TARGET_MODULES.split(\",\"),\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16409ef9-d425-4e8e-8d6c-95b6e8c40f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming train_dataset is your dataset object\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=BATCH_SIZE,\n",
    "                            drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df300259-71cf-40e4-a5f5-354d052bf194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjolovic/myenv/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_data.start_iteration = 0\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"marijajolovic/{OUTPUT_DIR}/{OUTPUT_DIR_1}\",\n",
    "    dataloader_drop_last=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    max_steps=MAX_STEPS,\n",
    "    eval_steps=EVAL_FREQ,\n",
    "    save_steps=SAVE_FREQ,\n",
    "    logging_steps=LOG_FREQ,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    warmup_steps=NUM_WARMUP_STEPS,\n",
    "    gradient_accumulation_steps=GR_ACC_STEPS,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=FP16,\n",
    "    bf16=BF16,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    push_to_hub=True,\n",
    "    include_tokens_per_second=True,\n",
    "    no_cuda=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c093d-8a9a-4748-afc6-0ae62ebdf6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "print(\"Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413620f-6be8-4044-aa15-255a81c32375",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
